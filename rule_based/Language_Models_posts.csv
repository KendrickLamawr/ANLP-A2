id,title,tags,answer
79419884,underfitting pretrained glove lstm model accurcacy unchanged,keras deeplearning nlp lstm sentimentanalysis,based on extra information in the comments im going to say the reason the lstm model hits a wall at an unspecified lower accuracy than the you are trying to reach is because it is not the best type of model for the problem in which case tweaking parameters is likely to be wasted effort im fairly sure encoder transformers eg bert surpassed them in sentiment analysis benchmarks a number of years back but sorry a quick search couldnt find a killer reference to insert here and transformers have only got bigger and better since then extra thought building on top of glove embeddings presents you with the problem that they dont handle multiple meanings of the word so queen might be a female king as in embeddings party trick king male female queen or it might be a pop group or it might be a gay man or it might be a chess piece this is going to put a limit on the accuracy of models built on them whereas transformers dont have that limitation because they look at the whole string to see the words in context it is possible to argue with that of course because bringing in the context is where the lstm comes in but transformers are still scaling strongly with layers whereas lstms tend to choke after two layers
79298368,inspect all probabilities of bertopic model,python nlp topicmodeling,for individual topic probability across each document you need to add one more argument note this calculateprobabilities true will only work if you are using hdbscan clustering embedding model and bertopic by default uses allminilmlv official documentation they have mentioned the same in document as well
79192130,using an aws service to execute a python script that will extract keywords from text using keybert,python amazonwebservices awslambda nlp largelanguagemodel,in such cases i would normally think of two resources aligned with the best practices of aws and software engineering sagemaker or lambda if the model im using is resourceintensive and requires gpu acceleration id go with sagemaker otherwise lambda is a good solution so for your case heres what id do package your keybert script in a lambda and easily deploy it with a container invoke it whenever you need to process text blocks aws lambda charges you only for the execution time so its costefficient for occasional tasks
79178041,normalization of token embeddings in bert encoder blocks,nlp normalization bertlanguagemodel attentionmodel,layer normalization is applied to each tokens embedding individually this means each token gets its own normalization based on its specific features this helps to ensure that the model can process each token effectively regardless of the other tokens in the sequence bert differs from the original transformer architecture in the placement of layer normalization in bert its applied before the selfattention mechanism while in the original transformer its applied after this subtle difference can have a significant impact on the models performance see here update please refer to the following paper on layer normalization in the transformer architecture the authors explored both approaches of applying layer normalization before and after attention layernamely preln and postln in bert their results indicate that using layer normalization before the attention layer yields better results for a summarized review of the same paper you can see here overall you might find different bert diagrams in which each used a different approach of using layer normalization
79173053,how to convert character indices to bert token indices,python nlp dataset largelanguagemodel bertlanguagemodel,you should encode both the question and context locate the token span for the answer within the tokenized context and update the dataset with the tokenlevel indices the following function does the above for you heres how you can use and test this function output
79155290,dutch sentiment analysis robbertje outputs just positivenegative labels netural label is missing,python nlp bertlanguagemodel robertalanguagemodel,this model was trained only on negative and positive labels therefore it will try to categorize every input as positive or negative even if it is nonsensical or neutral what you can do is to find other models that was trained to include neutral label finetune this model on a dataset that includes neutral label empirically define a threshold based on the confidence outputs and interpret it as neutral the first choices are extensive in effort i would suggest you go with the third option for a quick workaround try feeding the model with a few neutral input and observe the range of confidence score in the output then use that threshold to classify as neutral heres a sample
78985137,alternative to devicemap auto in huggingface pretrained,machinelearning deeplearning nlp huggingfacetransformers,i found out that there are actually several methods in accelerate for this the first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model the second one is used to match your model with the devices so basically in your case you can use the following code ps this code still needs to be tested on finetuning
78943401,finetuning a pretrained model with quantization and amp scaler error attempting to unscale fp gradients,python pytorch nlp huggingfacetransformers finetuning,you cant finetune a fpuint model with amp amp uses fp parameters the params are autocast to fp for the forward pass but amp expects the master set of parameters to be fp you also shouldnt finetune a quantized model in the first place the quantization causes all sorts of numerical issues and instability during training what you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model you can find more details here
78905614,why doesnt permuting positional encodings in bert affect the output as expected,python pytorch nlp huggingfacetransformers,the model inputs have token ids and position ids there are four scenarios to consider baseline correct order for tokens and positions permute position ids only permute token ids only permute position ids and token ids you are correct that scenario and should produce the same results however you are incorrect in assuming that permuting tokens or positions separately should give the same result consider given tokens positions permutation ex permute tokens but not positions permuted tokens standard positions ex permute positions but not tokens standard tokens permuted positions in ex the model is told that token occurs at position in ex the model is told that token occurs at position even though we used the same permutation the mapping of tokens to positions is different this results in different model outputs the reason you sometimes see these results line up is because you can through random chance sample a permutation that results in tokenposition embeddings lining up the same way or mostly the same way when permuting just one of them this is luck the average case produces different results it is simple to test this huggingface models take a positionids input parameter we can use this to test permutations of the input ids without messing with the weight matrices to test this well create input data permute as needed compute logits and compare logits when comparing logits we will permute or depermute as needed to compare on a token to token basis for example if token i in scenario is permuted to token j in scenario we want to compare logits i from scenario to logits j in scenario import torch from transformers import bertformaskedlm autotokenizer def getlogitsinputs with torchnograd outputs modelinputs logits outputslogits return logits def permuteinputsinputs permutation permuteidstrue permutepositionstrue outputs for kv in inputsitems if kpositionids and permutepositions outputsk vpermutation elif kpositionids and permuteids outputsk vpermutation else outputsk v return outputs load tokenizermodel tokenizer autotokenizerfrompretrainedbertbaseuncased model bertformaskedlmfrompretrainedbertbaseuncased modeleval remember to set model to eval create input ids and position ids inputs tokenizerinput text test sequence returntensorspt inputspositionids torchtensorlistrangeinputsinputidsshape create permutation tensor permutation torchrandperminputsinputidsshape compute scenario data data s scenario baseline inputs inputs permutedids false s scenario permute positions only inputs permuteinputsinputs permutation permuteidsfalse permutepositionstrue permutedids false s scenario permute token ids only inputs permuteinputsinputs permutation permuteidstrue permutepositionsfalse permutedids true s scenario permute tokens and positions inputs permuteinputsinputs permutation permutedids true compute logits for kv in dataitems vlogits getlogitsvinputs comparisons s s s s s s s s s s s s compare scenarios for sa sb in comparisons dataa datasa datab datasb logitsa dataalogits logitsb datablogits if dataapermutedids databpermutedids either both logits are permuted or both logits are unpermuted so we can compare directly val logitsa logitsbabsmean elif dataapermutedids and not databpermutedids if is permuted but is not we permute to make tokens line up val logitsa logitsbpermutationabsmean else otherwise we permute to make tokens line up val logitsapermutation logitsbabsmean printfcomparison sa sb valitemf the code should produce an output like run the code a bunch of times you will find that the s s comparison always has a small deviation this is because permuting tokens and positions together always produces the same result ignoring small deviations caused by numeric issues you will find the s s comparison generally has a large deviation but sometimes has a small deviation as discussed this is due to getting a lucky permutation where positions and ids mostly line up
78853409,nllb finetuning error missing dataprefix configuration englishgerman translation,python nlp machinetranslation finetuning fairseq,while i cant help you with the concrete error message you are getting my guess would be issues with structure of the provided json files my personal recommendation would be to finetune nllb in the transformers library specifically using the seqseqtrainer i did this before for multiple models including nllb check out this repository this way the finetuning and inference process for the nllb model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this
78685093,alternative to receptive field in transformers and what factors impact it,nlp huggingfacetransformers receptivefield,how do you understand whether one architecture is more optimal to use versus another having a set of text documents with unique properties for modern transformerbased language models lms there are some empirical scaling laws such as the chinchilla scaling laws wikipedia that essentially say that larger deeper models with more layers ie with more parameters tend to perform better so far most lms seem to roughly follow chinchilla scaling there is another kind of scaling which is closer to a receptive field that i talk about below do you guys have something similar in nlp kind of transformerbased lms can be thought to have a receptive field similar to cnn layers as the attention mechanism in the transformer operates on a predefined context window or context length which is the maximum number of tokens the layer can look at attend to at any given time similar to a cnn kernel however with the introduction of new positional encoding pe approaches such as rotary positional encoding rope and modified attention architectures like sliding window attention swa this is not strictly accurate scaling in terms of context length is of much interest but usually it is very difficult to scale transformers this way because of attention being a mathcalon on operation so usually researchers go towards deeper architectures with more parameters overparameterization that can allow the model to memorize as much of the large training corpus as it can overfitting so that it can perform reasonably well when finetuned for most downstream tasks that have at least some representative examples in the training corpus
78639577,understanding the results of transformers learn in context with gradient descent,machinelearning nlp largelanguagemodel transformermodel metalearning,what data are you using in your replication as far as i can tell this paper does not mention explicitly the parameters of the data used for the particular result you are trying to replicate indeed it tests a variety of alpha values for the distributions used in figure it is feasible for the loss to be low even after one step of gd if the alpha value is low if you find the same trends in relative behavior of gd and transformer layers i dont think its important to match the exact loss values
78612251,how do we addmodify the normalizer in a pretrained huggingface tokenizer,python nlp largelanguagemodel huggingfacetokenizers,this looks like a bug the v tokenizer has a normalizer by default which can be seen by looking at the mistralvtokenizerjson file after modifying the backendtokenizernormalizer object the modification are saved to the tokenizerjson file in the v version the mistralvtokenizerjson file has no value for the normalizer modifying the normalizer and saving the model does write the changes to the json file but it is not getting picked up on reload using autotokenizerfrompretrained i am not sure why but it is entirely possible the tokenizermodel file indicates no normalizer is the default and it simply does not load it however you can get the tokenizer to load correctly with the custom normalizer by instantiating the matched tokenizer class explicitly and passing in the tokenizermodel and tokenizerjson paths along with the values from the tokenizerconfigjson file in this case it is the llamatokenizerfast class from transformers import autotokenizer llamatokenizerfast addedtoken from tokenizersnormalizers import sequence replace prepend load modify and save tok autotokenizerfrompretrainedmistralaimistralbv tokbackendtokenizernormalizer sequence prepend replace replacefoo bar replace n toksavepretrainedmistralbvcustom read in config construct the addedtoken objects with openmistralbvcustomtokenizerconfigjson as fp config jsonloadfp configaddedtokensdecoder intk addedtokenv for k v in configpopaddedtokensdecoderitems load from saved files tokcustom llamatokenizerfast mistralbvcustomtokenizermodel mistralbvcustomtokenizerjson config teststr i foo youhello world print jointokcustombatchdecodetokcustomteststrinputids prints i bar you hello world if you dont want to specify the tokenizer class explicitly you can load the model the the autotokenizer and then load it again using from the resulting class it is a hacky workaround
78552651,how to fix error when loading custom finetuned model,pytorch nlp huggingfacetransformers largelanguagemodel peft,your directory contains only the files of the peftadapter and the files required to load the tokenizer but the base model weights are missing i assume you have used the savepretrained method from peft this method only saves the adapter weights and config i use a smaller model for my answer and a different task type from peft import loraconfig tasktype getpeftmodel peftmodel from transformers import automodelfortokenclassification from pathlib import path fergusollamabpclv in your case adapterpath bla metallamametallamab in your case basemodelid distilbertdistilbertbaseuncased peftconfig loraconfigtasktypetasktypetokencls targetmodulesalllinear automodelforcausallm in your case model automodelfortokenclassificationfrompretrainedbasemodelid model getpeftmodelmodel peftconfig modelsavepretrainedadapterpath printlistpathadapterpathiterdir sepn output to load your pretrained model successfully you need to load this basemodel weights as well and use the peft model class to load the adapter model automodelfortokenclassificationfrompretrainedbasemodelid model peftmodelfrompretrainedmodel adapterpath you can also merge the adapter weights back with mergeandunload and save it output this way you will be able to load the model without peft and only transformers as you tried in the example code of your question
78365608,finetuning bert with deterministic masking instead of random masking,nlp huggingfacetransformers bertlanguagemodel,probably you want domain specific adaption of bert so far i also could not find customized masking but i found this paper useful perl pivotbased domain adaptation for pretrained deep contextualized embedding models if any one has a way around to customized masking for bertformaskedlm please guide
78317989,is it possible to finetune a pretrained word embedding model like vecword,python nlp artificialintelligence wordvec wordembedding,as has been pointed out before there is no goto way for finetuning wordvec type models i would suggest training your own model from scratch combining your data with other available data from a similar domain wordvec models are fairly quick to train and this would probably give you the best results if you do not need static wordlevel embeddings i would recommend considering contextualized embeddings for example through the use of sentencetransformers or similar frameworks which has a wide selection of already pretrained models you can choose from you can finetune these types of models on your specific data rather easily and there are tons of resources online on how to do that for your use case you can embed all the documents into dense vector representations using the abovementioned library and then construct a searchable index over this semantic space in order to match queries all you have to do then is to embed the query using the same model and then retrieve the documents with the highest approximate inner product often referred to as a mips search an example library to take a look at would be faiss
78240828,is bertforsequenceclassification using the cls vector,nlp huggingfacetransformers bertlanguagemodel,would a sentence embedding be equivalent or even better than the cls token embedding a sentence embedding is everything that represents the input sequence as a numerical vector the question is whether this embedding is semantical meaningful eg can we use it with similarity metrics this is for example not the case for the pretrained bert weights released by google refer to this answer for more information is the cls token a sentence embedding yes is some kind of pooling a sentence embedding yes are they semantically meaningful with the bert weights release by google no shouldnt it be pooledoutput outputs no because when you check the code you will see that the first element of the tuple is the lasthiddenstate sequenceoutput encoderoutputs pooledoutput selfpoolersequenceoutput if selfpooler is not none else none if not returndict return sequenceoutput pooledoutput encoderoutputs i am confused as to whyhow masked language modeling would lead to the start token learning a sentence level representation because it is included in every training sequence and the cls absorbs the other tokens you can also see this in the attention mechanism compare revealing the dark secrets of bert paper as mentioned above the questions is if they are semantically meaningful without any further finetuning no compare this stackoverflow answer
78219076,how to remove layers in huggingfaces transformers gpt pretrained models,python machinelearning deeplearning nlp transformermodel,try these parameters to bypass the embedding layer for the input embeddings you can use the following load the config of gpt send it to the class and then use the inputsembeds for the new model
78129126,typeerror exception encountered when calling layer embeddings type tfbertembeddings,tensorflow deeplearning nlp bertlanguagemodel transformermodel,it works with transformers
78091661,how to apply a linear layer atop a sentence transformer,python pytorch nlp bertlanguagemodel,here is a simple code snippet that adds one simple linear layer on top of a sentence transformer import torch from sentencetransformers import sentencetransformer class sentencetransformerwithlinearlayertorchnnmodule def initself transformermodelname supersentencetransformerwithlinearlayer selfinit load the sentence transformer model selfsentencetransformer sentencetransformertransformermodelname lastlayerdimension selfsentencetransformergetsentenceembeddingdimension new linear layer with output dimensions selflinear torchnnlinearlastlayerdimension def forwardself x pass the input through the sentence transformer x selfsentencetransformerencodex converttonumpyfalseunsqueeze pass through the linear layer x selflinearx return x this can than be used similarly to a simple sentence transformer in this example i loaded the allmpnetbasev model as the base sentence transformer the input of hello world is passed through the sentence transformer and then the linear layer resulting in a dimensional vector model sentencetransformerwithlinearlayerallmpnetbasev output modelforwardhello world this vector can then be used in a loss function eg a mseloss lossfunction torchnnmseloss expected loss lossfunctionoutput expected lossbackward
77965060,key matrix redundant in transformer language models,nlp transformermodel,the n d vs d d structure is very similar to how a lora works or indeed a wals model so as long as n d this might well be a good way of saving parameters
77933640,how to calculate the weighted sum of last hidden layers using roberta,python machinelearning deeplearning nlp huggingfacetransformers,first lets do some digging from the og bert code if we just do a quick search for sum on the github repo we find this then a quick search on stackoverflow reveals how to get intermediate layers output of pretrained bert model in huggingface transformers library now lets validate if your code logic works by working backwards a little then out why encoder hidden layer output final pooler output out to validate that the last layer output is the last layer in the hiddenstates lets check if the size for each layers output matches checks out out why its batchsize sequencelength hiddensize sentences batch size of longest sequence length no of tokens ie in the case of your longest example sentence and from leninputids outputs fixed for all hidden layers output bddu aeins wait a minute does that mean ive sequencelength outputs for each batch and if my batches are not equal lengths the output size are different yes that is correct and to get some sense of equality for all inputs itll be good to padtruncate all outputs to a fixed length if youre still going to use the featurebased bert approaches soooo is my torchstack approach right yes it seems so but it depends on whether you consider the pooler output to be last or second to last if second to last if you consider the pooler to be the last minor nitpicking you can access the outputhiddenstates through slices because its a tuple object next you wont need to squeeze the stacked output because the the outer most layer tensor is nonempty this is a special case for stack in nlp where the st dimension is batch size and nd is token length so summing the hidden dimensions up ends up the same when youre not explicitly stating which dimension you stack to be a little more explicit but in practice you can do this to comfort yourself interesting what about concat last four hidden in the case of concat youll need to be explicit when in the dimensions out but note you still ends with sequencelength hiddensize which makes batches with unequal lengths a pain since youve covered almost everything on the table what about the embeddings output this is the interesting part its actually not accessible through the modelinputsids directly youll need to do this finally why didnt you just answer yes you are right if i did would that convince you more than you proving the above for yourself
77846486,bertopic make sure that the iterable only contains strings,python pythonx nlp topicmodeling,datawords is a nested list it contains lists and strings fit is expecting an iterable with only strings you can try flattening datawords so that it only contains strings and then use a related issue
77839628,loading encorewebsm results in attributeerror module transformers has no attribute berttokenizerfast,python pip nlp anaconda spacy,i think that there is older version of the transformers in your global environment that cause the problem to avoid version conflict create a new virtual environment using conda activate myenv install scipy check the instalation page download encorewebsm now you can run your code
77835506,truncate texts in the middle for bert,nlp token tokenize bertlanguagemodel,the post references a paper which says that the first tokens and the last tokens not including the cls and sep tokens should be kept for tokenization you can use the bert tokenizer from huggingfaces transformers library to tokenize the full string and then trim out everything besides the first and last tokens because we include the initial cls token and because we include the ending sep token code generate sample text from string import asciilowercase sampletext for c in asciilowercase for c in asciilowercase sampletext fcc get tokenizer from transformers import berttokenizer tokenizer berttokenizerfrompretrainedbertbaseuncased perform tokenization tokenized tokenizersampletext trim tokens if lentokenizedinputids for kv in tokenizeditems tokenizedk v v verify result printtokenizerdecodetokenizedinputids printlentokenizedinputids
77822202,how to add a dense layer on top of sentencetransformer,python nlp huggingfacetransformers sentencetransformers,according to the documentation replace this line with the following
77805776,how to calculate word and sentence embedding using roberta,python machinelearning nlp huggingfacetransformers transformermodel,warning this answer only shows ways to retrieve word and sentence embeddings from a technical perspective as requested by op in the comments the respective embeddings will not be useful from a performance perspective to for example calculate the similarity between two sentences or words compare this so answer for further information word embeddings it is important to note that roberta was trained with a bytelevel bpe tokenizer this is a socalled subword tokenizer which means that one word of your input string can be split into several tokens for example your second caption lorem ipsum from transformers import robertamodel robertatokenizerfast import torch m robertamodelfrompretrainedrobertabase t robertatokenizerfastfrompretrainedrobertabase captions example caption lorem ipsum this bird is yellow has red wings hi example printtcaptionsinputids output as you can see the two words were mapped to tokens and are special tokens that means to retrieve the actual word embeddings and not the token embeddings you need to apply some kind of aggregation a common approach is applying mean pooling compare this so answer using the respective fast tokenizer of the model helps you here because it returns a batchencoding object that can be used to map the tokens back to the respective words no need to pad manually the tokenizer can do that for you tokenizedcaptions tcaptions returntensorspt paddinglongest with torchinferencemode modelinferenceoutput mtokenizedcaptions contextualizedtokenembeddings modelinferenceoutputlasthiddenstate properly padded printcontextualizedtokenembeddingsshape def fetchwordembeddingsidx sentence tokenizedcaptions contextualizedtokenembeddings wordembeddings fetching wordids each id is a word in the original sentence wordids i for i in tokenizedcaptionsidxwordids if i is not none for wordid in wordids tokenstart tokenend tokenizedcaptionsidxwordtotokenswordid wordstart wordend tokenizedcaptionsidxwordtocharswordid wordsentencewordstartwordend wordembeddingsword contextualizedtokenembeddingsidxtokenstarttokenendmeandim return wordembeddings result for idx sentence in enumeratecaptions wordembeddings fetchwordembeddingsidx sentence tokenizedcaptions contextualizedtokenembeddings resultappendsentence sentence wordembeddingswordembeddings contextualized word embedding of the word of the second caption printresultwordembeddingsipsumshape output sentence embeddings sentence embeddings represent the whole sentence in a vector there are different strategies to retrieve them commonly used are mean or clspooling with meanpooling delivering better results as shown in this paper section the only challenge from a technical perspective compare warning preamble is that you want to exclude the padding tokens has for nonepaddingtokens and for paddingtokens attentionmask tokenizedcaptionsattentionmaskunsqueeze mutiply the contextualized embeddings with the attention mask to set the padding token weights to zero sumembeddings torchsumcontextualizedtokenembeddings attentionmask printsumembeddingsshape numnonepaddingtokens attentionmasksum printnumnonepaddingtokens sentenceembeddings sumembeddings numnonepaddingtokens printsentenceembeddingsshape output you also wanted to know in the comments if you could use the pooleroutput of robertabase directly to retrieve the sentence embeddings yes you can do that the pooleroutput is retrieved via a form of clspooling code please note in addition to the warning preamble that the layers used for to generate the pooleroutput are randomly initialized ie untrained for the robertabase weights you load that means they are even less meaningful
77748737,how to calculate word and sentence embedding using gpt,python machinelearning nlp huggingfacetransformers transformermodel,here is your modified code to compute sentence and word embeddings some relevant facts as you said word embeddings are the last hidden output if you print the out put you see vectors number of sentences of length maximum number of tokens in the list of sentences and shape model dimension it means that some sentences have embeddings for non existent tokens so we need to mask the output to consider only existent tokens mask consists on multiplying by zero or whatever special value but zero is the more accepted and useful as it nulls values on non existent token places of the word vector the attention mask is crucial for handling variablelength sequences and ensuring that padding tokens do not contribute to the embeddings usually sentence embeddings are computed as the sum mean or max of the masked word embeddings it depends on your use case mean is more suitable to variable length sum is intended to force importance on relevant parts it exist a lot of techniques and it depends on how embeddings perform for your task i would choose a method that maximices the cosine similarity between vectors i consider similar for my task ex if the sum gets more similarity than mean it may be more suitable additionally i suggest you to normalize values by the number of tokens in the sentence so that with that normalization larger sentences tend to have lower vector values it is to embed information on the number of tokens in the sentence it prevents to get high similarity scores between a sentence of tokens with a whole book that its meaningless
77673353,how to use adapter transformers with a huggingface pipeline,python machinelearning nlp huggingfacetransformers,the old legacy package is pip install u adaptertransformers create the model outside of the pipeline
77651770,how to load the pretrained word embeddings in npy files,python nlp wordembedding,to set it up you can do the following in your shell
77594086,how to run a nlptransformers llm on low memory gpus,python nlp gpu huggingfacetransformers huggingfacetokenizers,i would recommend looking into model quantization as this is one of the approaches which specifically addresses this type of problem of loading a large model for inference thebloke has provided a quantized version of this model which is available here neuralchatbvawq to use this youll need to use autoawq and as per hugging face in this notebook for colab you need to install an earlier version given colabs cuda version you should also make sure your model is using gpu not cpu by adding cuda to the input tensor after it is generated pip install q transformers accelerate pip install q u import torch from awq import autoawqforcausallm from transformers import autotokenizer modelname theblokeneuralchatbvawq use autoawq and from quantized instead of transformers here model autoawqforcausallmfromquantizedmodelname tokenizer autotokenizerfrompretrainedmodelname def generateresponsesysteminput userinput format the input using the provided template prompt f systemnsysteminputn usernuserinputn assistantn add cuda inputs tokenizerencodeprompt returntensorspt addspecialtokensfalsecuda generate a response outputs modelgenerateinputs maxlength numreturnsequences response tokenizerdecodeoutputs skipspecialtokenstrue extract only the assistants response return responsesplit assistantn
77579658,pretrained model with stride doesnt predict long text,python nlp huggingfacetransformers,you cant just move the call parameters like stride to frompretrained from transformers import automodelfortokenclassification autotokenizer pipeline modelid davlandistilbertbasemultilingualcasednerhrl t autotokenizerfrompretrainedmodelid stride returnoverflowingtokenstrue modelmaxlength truncationtrue issplitintowordstrue sample test sliding window will not be applied printlentsampleinputids sliding window will be applied printlentsample maxlength truncationtrue stride returnoverflowingtokenstrueinputids output with the pipeline you can pass the value for stride as init parameter from transformers import automodelfortokenclassification autotokenizer pipeline modelid davlandistilbertbasemultilingualcasednerhrl ner pipelinetokenclassification modelid stride aggregationstrategyfirst sample hi my name is cronoik and i live in germany o nersample printleno printo output
77563897,bert topic clasiffying over a quarter of documents in outlier topic,python nlp bertlanguagemodel topicmodeling,from the documentation the main way to reduce your outliers in bertopic is by using the reduceoutliers function to make it work without too much tweaking you will only need to pass the docs and their corresponding topics you can pass outlier and nonoutlier documents together since it will only try to reduce outlier documents and label them to a nonoutlier topic the following is a minimal example you can find all the strategies for reducing outliers in this page outlier reduction
77341456,why does my transformer model have more parameters than the huggingface implementation,machinelearning pytorch nlp huggingfacetransformers huggingface,that is because the linear layer of lmhead doesnt have separate weights it shares its weight tensor with the token embedding layer you can confirm this with dataptr which returns the address of the first element of the tensor from torch import nn from transformers import autotokenizer gptlmheadmodel autoconfig modelid gpt tokenizer autotokenizerfrompretrainedmodelid standardgpt gptlmheadmodelfrompretrainedmodelid standardgptmodelsize sumtnumel for t in standardgptparameters printfgpt size standardgptmodelsize parameters printftoken embedding layer address standardgpttransformerwteweightuntypedstoragedataptr printflmhead address standardgptlmheadweightuntypedstoragedataptr replacing the default head standardgptlmhead nnlinearinfeatures outfeatures biasfalse standardgptmodelsize sumtnumel for t in standardgptparameters printfgpt size after replacing lmhead standardgptmodelsize parameters printftoken embedding layer address after replacing lmhead standardgpttransformerwteweightuntypedstoragedataptr printflmhead address after replacing lmhead standardgptlmheadweightuntypedstoragedataptr output i assume you want to keep sharing the weights in this case you should call something like this after assigning your new head standardgptlmhead nnsequential nnlinearinfeatures outfeatures biasfalse standardgptlmheadweight standardgpttransformerwteweight standardgptmodelsize sumtnumel for t in standardgptparameters printfgpt size with tied weightscustom head standardgptmodelsize parameters printftoken embedding layer address with tied weightscustom head standardgpttransformerwteweightuntypedstoragedataptr printflmhead address with tied weightscustom head standardgptlmheadweightuntypedstoragedataptr output
77337720,how to change the fully connected network in a gpt model on huggingface,machinelearning pytorch nlp huggingfacetransformers gpt,im assuming by the fully connected network youre referring to the fully connected fc linear layer the above would show you the modules inside the model you can now access and update the fc layer by the above is just a sample you can experiment with different combinations
77248165,what does the vocabulary of a pretrained finetuned t model look like,python pytorch nlp huggingfacetransformers,the t default vocabulary consists of subword tokens utilizing the sentencepiece tokenizer not word tokens thus it can generate a larger vocabulary than the specified hello and hello are treated as different tokens because ts tokenizer is casesensitive
77237818,how to load a huggingface pretrained transformer model directly to gpu,python nlp huggingfacetransformers,im answering my own question hugging face accelerate add via pip install accelerate could be helpful in moving the model to gpu before its fully loaded in cpu its useful when gpu memory model size cpu memory also specify devicemapcuda from transformers import automodelforcausallm model automodelforcausallmfrompretrainedbertbaseuncased devicemapcuda
77210041,troubleshooting pytorch and hugging faces pretrained deberta model on windows with an rtx gpu,pytorch nlp gpu huggingfacetransformers huggingfacetokenizers,i have installed pytorch on multiple combinations oshardware i have installed pytorch successfully using those commands in a virtual environment pip install upgrade transformers pip install upgrade torch torchvision torchaudio indexurl pip install accelerate will take latest at the time of writing pip install evaluate datasets these helped me kickstart any of the projects which require huggingface i hope it helps you
77116207,what is the correct approach to evaluate huggingface models on the masked language modeling task,machinelearning nlp huggingfacetransformers huggingface,as pointed out in the linked post this is a warning to indicate that those weights are not used this is raised since youre loading a model that has its pooler weights initialised bertbasecased but arent used by a maskedlm model the bertpooler weights are typically used for classification tasks such as bertforsequenceclassification for bertbasecased the model was also trained on the next sentence prediction nsp task so the pooler weights are also trained as pointed out in this github comment after passing a sentence through the model the representation corresponding to the first token in the output is used for finetuning on tasks like squad and glue so the pooler layer does precisely that applies a linear transformation over the representation of the first token the linear transformation is trained while using the next sentence prediction nsp strategy in fact whenever initialising the model with the next sentence prediction task the warning isnt raised from transformers import automodelformaskedlm automodelfornextsentenceprediction automodelforpretraining raises a warning automodelformaskedlmfrompretrainedbertbasecased doesnt raise a warning automodelfornextsentencepredictionfrompretrainedbertbasecased doesnt raise a warning initialised with both mlm nsp automodelforpretrainingfrompretrainedbertbasecased since youre interested in masked language modelling mlm you can disregard the warning since this isnt used for this task for the masked language modelling task you should initialise using automodelformaskedlm since this includes the appropriate head to predict the masked token this forum post has further details about the differences in initialisations
77068054,training difficulties on transformer seqseq task using pytorch,machinelearning pytorch nlp transformermodel formallanguages,ok i think i could solve this for people experiencing same problems what did not solve my problem but might help you try different positional encoding strategies for tasks that are highly repetitive try using gradient clipping train your model on simpler tasks to see whether the task is simply too hard try different learning rates try different easier tokenization strategies like wordwise tokenization try using label smoothing use the adamw betas what did in the end solve my problem employing the learning rate scheduler used in the paper it will linearly increase the learning rate untill step and then decrease employing the sqrt function that can be found here tldr use this wrapper around your optimizer it will change the lr according to the attention is all you need paper
76982659,how to get attentions part from the output of a bert model,python nlp huggingfacetransformers bertlanguagemodel wordembedding,you cant really extract keywords from the code you provided your output has no attribute called attentions thats why outputattentions is returning none hence the error youre facing i benchmarked some transformer models for keyword extraction last year for my university project i will provide you with a solution from there this script will return you the extracted keywords with their score as a dictionary solution install the dependencies necessary imports utility functions keyword extraction method output output returned the keywords sorted according to their score
76969521,nlp how to fix that pretrained model paraphrasemultilingualmpnetbasev isnt accurate on some examples,python machinelearning nlp huggingfacetransformers,what youre looking for is called finetuning and hugging face does provide a trainer api the general steps are from the above tutorial ill be using the bertbasecased pretrained model and the yelpreviewfull dataset to finetune both can be found on huggingface as well as pytorch trainer for the actual finetuning the map function requires a function which it uses on every item in this case we want to tokenize only the text key in each item the logging statement above will show keys label and text to keep things simple we dont want the label but we could use it and concat the text onto the end of it as per the tutorial the above line to load the model will display a warning you will see a warning about some of the pretrained weights not being used and some weights being randomly initialized dont worry this is completely normal the pretrained head of the bert model is discarded and replaced with a randomly initialized classification head you will finetune this new model head on your sequence classification task transferring the knowledge of the pretrained model to it here we create the class that contains all hyperparameters outputdir says where to save checkpoints from training and evaluationstrategy sets when to report evaluation metrics in this case we want to do this at the end of every epoch while this covers the basics of finetuning there are many ways to optimize and customize every step in the process for a tutorial with an indepth look into things like the tokenizer and preprocessing adding layers ontop of tasks batchdistributed training hyperparameter customization experimentation and much more you can find it over at
76882664,why do we add v in the denominator in the addone smoothing for ngram language models,nlp smoothing languagemodel,the v variable that we see in the determiner of additive smoothing function is not actually a direct definition of the probabilisitic estimation of the ngram it is derived from first we start with the naive assumption that if we add to the numerator we also add to denominator to avoid math division error but instead of adding to all terms in the vocabulary we could simply add the size of the vocab thus you see the sumcwi v in the denominator instead of sumcwi note the scope of the sum function more details sometimes i find it easier to see the math in code consider this ngram without laplace from try from itertools import pairwise except from itertools import tee def pairwiseiterable pairwiseabcdefg ab bc cd de ef fg a b teeiterable nextb none return zipa b def countnumeratorngrams sentences return sumlistpairwisescountngrams for s in sentences def countdenominatorword sentences return sumsumintng word for ng in pairwises for s in sentences s this is foo bar s foo bar is good s fruit bar is better s this is good s i like this thing sentences s s s s s probbosthis countnumerator this sentences countdenominator sentences probthisis countnumeratorthis is sentences countdenominatorthis sentences probisgood countnumeratoris good sentences countdenominatoris sentences probgoodeos countnumeratorgood sentences countdenominatorgood sentences printthisisgood probbosthis probthisis probisgood probgoodeos printthisisgood outputs now consider the laplace smoothing with the incremental on the numerator and denominator note that the on the numerator is adding simple to each wi wi count the denominator is adding for each ngram that exists in the corpus containing the wi now we see that we are summing for every ngrams that occurs in a similar fashion we can just add the no of ngrams that exists eg def countnumeratorlaplacengrams sentences return sumlistpairwisescountngrams for s in sentences def countdenominatorlaplaceword sentences return sumsumintng word for ng in pairwises lenlistpairwises for s in sentences probbosthis countnumeratorlaplace this sentences countdenominatorlaplace sentences probthisis countnumeratorlaplacethis is sentences countdenominatorlaplacethis sentences probisgood countnumeratorlaplaceis good sentences countdenominatorlaplaceis sentences probgoodeos countnumeratorlaplacegood sentences countdenominatorlaplacegood sentences printthisisgood probbosthis probthisis probisgood probgoodeos printthisisgood this is a good prove of concept for how the in the inner sum becomes a v in when you carry the out of the summation eg
76868251,how to load debertav properly,python nlp huggingfacetransformers,it seems v config structure is the same with v config i successfully ran the following code output in google colab or any linux os the model files would be stored here rootcachehuggingfacehubmodelsmicrosoftdebertavbase it also can be downloaded via v modules without error message in this case sentencepiece needs to be imported not sure whether the model will be usable
76826638,what does finetuning a multilingual checkpoint mean,python nlp huggingfacetransformers huggingface finetuning,what the point in the guide suggests is that multilingual models finetuned using setfit method generalize well even on languages they did not see during the setfit finetuning process this seems to be generally true for multilingual language models but it probably does not do any damage to mention it explicitly particularly when discussing setfit which is a method which usually works with a very small dataset ie the dataset that might not be multilingual the finding is supported by the paper mentioned in the guide where researchers show that model finetuned on english data using setfit performs well on variety of languages see table what i would take from it is this if you finetune multilingual checkpoint eg sentencetransformersparaphrasemultilingualmpnetbasev and finetune it on french it will perform well on french and probably will also perform well on other languages if you plan to use the finetuned model only on french texts you certainly can and try to finetune a specifically french model however its certainly not true that you must do this however if there exists a specifically french sentence transformer and you want to use your model only on french texts i would recommend using the french model not because you must but because it might perform better than the multilingual model
76825022,why nnembedding layer is used for positional encoding in bert,pytorch nlp huggingfacetransformers bertlanguagemodel wordembedding,nnembedding is just a table of vectors its input are indices to the table its output are the vectors associated to the indices from the input conceptually it is equivalent to having onehot vectors multiplied by a matrix because the result is just the vector within the matrix selected by the onehot input bert is based on the transformer architecture the transformer architecture needs positional information to be added to the normal tokens for it to distinguish where each token is at the positional information in the original formulation of the transformer architecture can be incorporated in different ways both with equal performance numbers static sinusoidal vectors the positional embeddings are precomputed trained positional embeddings the positional embeddings are learned the authors of the bert article decided to go with trained positional embeddings anyway in both cases the positional encodings are implemented with a normal embedding layer where each vector of the table is associated with a different position in the input sequence update positional embeddings are not essentially different from word embeddings the only difference is how they are trained in word embeddings you obtain the vectors so that they can be used to predict other words that appear close to the vectors word in the training data in positional embeddings each vector of the table is associated with an index representing a token position and you train the embeddings so that the vector associated with a specific position when added to the token embedding at that position is helpful for the task the model is trained on masked lm for bert machine translation for the original transformer therefore the positional embeddings end up with information that depends on the position because the positional embedding vector is selected based on the position of the token it will be used for and which has been trained to be useful for the task later the authors of the transformer article discovered that they could simply devise a static not trained version of the embeddings ie the sinusoidal embeddings which reduced the total size of the model to be stored in this case the information in the precomputed positional vectors together with the learned token embeddings is enough for the model to reach the same level of performance in the machine translation task at least
76802096,runtimeerror when trying to extract text features from a bert model then using knn for classification,python machinelearning nlp bertlanguagemodel knn,it seems that you are feeding all your data to the model at once and you dont have enough memory to do that instead of doing that you can invoke the model sentence by sentence or with small sentence batches so that you keep the needed memory within the available system resources
76783884,bertbaseuncased install with spacy is not working,python nlp spacy huggingfacetransformers spacytransformers,this is addressed in a discussion on the spacy github repo the explanation of the error is that entrfbertbaseuncasedlg is a spacy x model and you are using x instead of said model you can download and use encorewebtrf which contains transformer models for spacy x
76766594,is examples a default output variable for huggingface transformers library,nlp huggingfacetransformers nlpquestionanswering,to help you understand we first have to clarify that map does not have the called function preprocessfunction as an argument but the function itself preprocessfunction so basically what map does is for a given dataset here squad execute a function preprocessfunction on every batch batchedtrue with a default batch size of so what this means is that map will process the whole dataset squad in chunks batches of items at a time so for every batch a sublist of examples from the dataset the function is called very simplified under the hood map does something like this chop squad in sublists batches of examples each process each batch ie passing the examples to the function put the results of the function back into the resulting dataset as a reference point it might be useful for you to read up on the builtin map in python which works similarly on iterables it is also a concept that returns in other programming languages so it is good to know
76702377,indexerror index out of range in self while implementing transformer model for translation,python pytorch nlp huggingfacetransformers machinetranslation,i went through your code and found out that in the error trace of yours error in forward call of sentenceembedding encoder stage if you add printtorchmaxx before the line x selfembeddingx then you can see that the error is because x contains id that is if the value is greater than then pytorch will raise the error mentioned in the stack trace it means that while you are converting tokens to ids you are assigning a value greater than to prove my point when you are creating englishtoindex since there are three in your englishvocabulary starttoken paddingtoken endtoken are all you end up generating since this value is greater than the lenenglishtoindex length hence you are getting indexerror index out of range in self solution as a solution you can give unique tags to these tokens which is generally prescribed as this will make sure that the generated dictionaries will have the correct sizes please find the working google colaboratory file here with the solution section i added to the englishvocabulary since after a few iterations we get a keyerror hope it helps
76689997,i have trained a custom transformer model on language modeling now how do i make predictions with it,python tensorflow keras nlp tfkeras,i have changed the code so the transformer model can take inputs of varied length def transformer class embedkeraslayerslayer wordembedding positionalembedding def initself kwargs superinitkwargs selfwordembed keraslayersembeddingvocabsize dmodel b t vocabsize dmodel b t dmodel selfpositionembed keraslayersembeddingmaxlength dmodel b t maxlength dmodel b t dmodel def callself inputs b t inputsshape if training t maxlength tokembed selfwordembedinputs b t dmodel posembed selfpositionembedtfrangemaxlength maxlength dmodel t t dmodel return tokembed posembedt b t dmodel t dmodel b t dmodel def getconfigself baseconfig supergetconfig return baseconfig class multiheadattentionkeraslayerslayer def initself causal bool kwargs superinitkwargs selfcausal causal selflinear keraslayersdensedmodel usebiasfalse selflinearqkv keraslayersdensedk usebiasfalse keraslayersdensedk usebiasfalse keraslayersdensedv usebiasfalse selfdropout keraslayersdropout def attentionself q k v def masktensorx tril tfexperimentalnumpytriltfoneslikex return tfwheretril floatinf x scores q tftransposek perm kshape b t t scores masktensorscores if selfcausal else scores return tfnnsoftmaxscores axis v b t dv def headself x q k v selflinearqkvx selflinearqkvx selflinearqkvx return selfattentionq k v def callself x heads tfconcatselfheadx for in rangeh axis output selflinearheads return selfdropoutoutput def getconfigself baseconfig supergetconfig return baseconfig causal selfcausal def feedforward return kerassequential keraslayersdensedin keraslayersrelu keraslayersdensedmodel keraslayersdropout inputs kerasinputshapenone so can take inputs of varied length x embedinputs for in rangen transformers decoder z multiheadattentioncausaltruex x keraslayerslayernormalizationkeraslayersaddz x z feedforwardx x keraslayerslayernormalizationkeraslayersaddz x outputs keraslayersdensevocabsize activationsoftmaxx b t vocabsize model kerasmodelinputsinputs outputsoutputs nametransformer printnumber of parameters in the model modelcountparams return model dont use modelpredict for generation use model trainingfalse def generatefileprompt str numchar int temperature def nextcharseq return sentencetfargmaxmodelnparrayencodeseqnpnewaxis trainingfalsetemperature axisnumpytolist seq prompt for i in rangenumchar if lenseq maxlength seq nextcharseqmaxlength last maxlength characters so can predict char at maxlength elif lenseq maxlength seq nextcharseq printseq with openmachinegeneratedtexttxt w as f fwriteseq
76671494,how to get the embedding of any vocabulary token in gpt,machinelearning pytorch nlp huggingfacetransformers languagemodel,if i understand correctly you want an embedding representing a single token from the vocabulary they are two answers that i know for that depending on which embedding you want exactly st solution the first layer in the model is a torchnnembedding which is under the hood a linear layer with no bias so it has a weight parameter of shape v d where v is the vocab size for you and d is the dimension of the embedding you can access to the representation of a token k with modelbiogptembedtokensweightk this is the sized vector that directly represents the kth token nd solution you can feed the model with a created sequence containing just the token of which you want the representation this representation corresponds to the input of the first attention layer of the model for example to get the th token representation inp torchtensorlong output modelinp outputhiddenstatestrue printoutputhiddenstates these two representations are not exactly the same because the first one only represents a token while the second represents the token in its sentence which is a sequence of one single token it is up to you to decide which one suits to what you want to do after
76669635,get chatgpt to respond with a single direct answer,python nlp openaiapi langchain chatgpt,option prompt engineering in particular fewshot prompting eg providing examples prompt in was in liechtenstein where was he out liechtenstein in happened at am when did it happen out am in a really long text john came home last night at pm out option finetuning you provide samples like the one in option but usually significantly more say kk you can upload them and finetuning is done automatically on openais platform but this costs money
76634279,trying to save history in tokenizer for seqseq transformer chat model godel base,nlp chatbot huggingfacetransformers huggingfacetokenizers seqseq,here i was trying to iterate over a pandas series and considering it a list to resolve this use the tolist function on the pandas series before iterating over it
76509563,how do i fine tune berts self attention mechanism,pytorch nlp bertlanguagemodel finetuning,hugginface provides a model class that you can use for your task see also the small example provided in the link you can use the logits in the output to create your attention matrix this is just an inference task finetuning means to do further training on custom data
76416680,how to structure data for questionanswering task to finetune a model with huggingface runqapy example,python nlp huggingfacetransformers languagemodel nlpquestionanswering,the code snippet youre using with sagemaker and the huggingface example comes from the example uses the dataset formatted as how it is in the squad dataset each example should look like this the actual data file from squad would come from and looks something like breaking it down a little if you have a data in json format that looks like this then to train a model the easiest way out is to push to huggingface hub after that you can use loaddataset when you change the script on and save a local script on your machine eg on scriptsrunqapy finally instead of using the the gitconfig you can do this
76393971,bert ner model start and end position none after finetuning,nlp huggingfacetransformers bertlanguagemodel namedentityrecognition,the pipeline can not return positions when you pass a slowtokenizer use a fasttokenizer to get the positions as well from transformers import pipeline berttokenizer berttokenizerfast bertfortokenclassification fastt berttokenizerfastfrompretraineddslimbertbasener slowt berttokenizerfrompretraineddslimbertbasener model bertfortokenclassificationfrompretraineddslimbertbasener text this is a abc corp ltd slowp pipelinetaskner modelmodel tokenizerslowt devicecpu aggregationstrategysimple printslowptext fastp pipelinetaskner modelmodel tokenizerfastt devicecpu aggregationstrategysimple printfastptext output
76359515,hugging face transformers trainer perdevicetrainbatchsize vs autofindbatchsize,nlp artificialintelligence huggingfacetransformers,the autofindbatchsize argument is an optional argument which can be used in addition to the perdevicetrainbatchsize argument as you point out lowering the batch size is one way to resolve outofmemory errors the autofindbatchsize argument automates the lowering process enabling this will use findexecutablebatchsize from accelerate which operates with exponential decay decreasing the batch size in half after each failed run the perdevicetrainbatchsize is used as the initial batch size to start off with so if you use the default of it starts training with a batch size of on a single device if it fails it will restart the training procedure with a batch size of
76357536,how can i run some inference on the mptb language model,python nlp huggingfacetransformers largelanguagemodel,gives an example code for inference just replace mptb with mptb if you wish to use mptb
76337058,how to generate sentiment scores using predefined aspects with debertavbaseabsav huggingface model,python nlp huggingfacetransformers sentimentanalysis largelanguagemodel,specific to the yanghengdebertavbaseabsav model this is the usage and you have to loop through the model one time per aspect out to get the zeroshot classification scores in general try using pipeline out depending on what text generated aspect means perhaps its keyword extraction and if so doing a search on gives this as the top downloaded model out putting the extractor and classifier together out q but the extracted keywords is not right or doesnt match the predefined ones a no model is perfect and the model example above is a keyword extractor not a product aspect extractor ymmv q why isnt the zeroshot classifier giving me negative positive labels a the zeroshot classifier is labelling the data based on the extracted labels not a sentiment classifier
76299091,how to properly prompt the decoder of a transformer model,pytorch nlp artificialintelligence huggingfacetransformers summarization,a convention that pytorch loss functions use is that if you set a label to during training the loss function will ignore the token see the documentation for ease of mind heres a minimal code example
76275152,is it possible to build a text classifier using existing llm like chatgpt,nlp openaiapi largelanguagemodel,from the open ai cookbook there is classification using embeddings classification using finetuning embeddings can be easier with a smaller amount of data than finetuning but finetuning should work better once you have a lot of data neither use the big llm models as both chatgpt gptgpt and bard are more trained to answer questions and not as a text classifier so they are not that useful if you do try to use these big models for text classification using prompts you will find them inconsistent depending on what you specifically are trying to classify
76238212,bert model splits words by its own,python nlp huggingfacetransformers bertlanguagemodel,you are not doing anything wrong bert uses a socalled wordpiece subword tokenizer as a compromise for meaningful embeddings and acceptable memory consumption between a characterlevel small vocabulary and a wordlevel tokenizer large vocabulary a common approach to retrieve word embeddings from a subwordbased model is to take the mean of the respective tokens the code below shows you have you can retrieve the word embeddings noncontextualized and contextualized by taking the mean it uses a fasttokenizer to utilize the methods of the batchencoding object import torch from transformers import berttokenizerfast bertmodel t berttokenizerfastfrompretrainedbertbasemultilingualcased whole model m bertmodelfrompretrainedbertbasemultilingualcased token embedding layer embeddinglayer membeddingswordembeddings samplesentence this is an example with tokenembeddings and wordembeddings encoded tsamplesentence the batchencoding object allows us to map the token back to the string indices printtokenid encodedtokentocharsidx for idx tokenid in enumerateencodedinputids sepn and we can also check the mapping of word to token indices printword encodedwordtotokensidx for idx word in enumeratesamplesentencesplit sepn output to retrieve the word embeddings output
76235208,evaluating a bertopic model based on classification metrics,python nlp bertlanguagemodel topicmodeling,
76213873,how to finetune a zeroshot model for text classification,python nlp huggingfacetransformers,concept explanation before i answer your question it is crucial to understand how the entailment approach for zeroshot text classification works this approach requires a model that was trained for nli which means that it is able to determine if the hypothesis is supported not supported undetermined by a given premise you can verify that for the model you mentioned with the following code from transformers import automodelforsequenceclassification autotokenizer nlimodel automodelforsequenceclassificationfrompretrainedfacebookbartlargemnli it will output three logits printnlimodelclassificationheadoutproj each vector corresponds to the following labels printnlimodelconfigidlabel output the entailment approach proposed by yin et al utilizes these nli capabilities by using the text as premise and formulating a hypothesis for each possible class with the template the text is about that means when you have a text and three potential classes you will pass three sequences to the nli model and compare the entailment logits to classify the text finetuning to finetune an nli model on your annotated data you therefore need to formulate your text classification task as an nli task that means you need to generate premises and the labels need to be either contradiction or entailment the contradiction label is included to avoid the model only seeing hypotheses that are entailed by their respective premise ie the model needs to learn contraction to predict a low score for entailment for the zeroshot text classification task the following code shows you an example of how to prepare your dataset import random from datasets import loaddataset from transformers import autotokenizer yourdataset loaddatasetagnews splittest idlabels world sports business scitech yourdataset yourdatasetmaplambda x class idlabelsxlabel removecolumnslabel printyourdataset the relevant code t autotokenizerfrompretrainedfacebookbartlargemnli template this example is def createinputsequencesample text sampletext label sampleclass contradictionlabel randomchoicex for x in idlabels if xlabel encodedsequence ttext templateformatlabel templateformatcontradictionlabel encodedsequencelabels encodedsequenceinputsentence tbatchdecodeencodedsequenceinputids return encodedsequence traindataset yourdatasetmapcreateinputsequence batchedtrue batchsize removecolumnsclass text printtraindataset output robustness finetuning will obviously reduce the robustness ie the ability to provide decent results for classes that werent part of your finetuning dataset of your model to avoid that you could try to stop training before conversion and check if the performance is still sufficient for your needs wiseft proposed by wortsmann et al pseudocode is shown in appendix a
76185813,how to resolve error in seqeval in ner bert finetuning,nlp huggingfacetransformers namedentityrecognition evaluation huggingfaceevaluate,as indicated by the error message the expected predictions references should be lists of strings not integers for seqeval this makes sense since the seqeval metric is concerned with matching entity spans exactly as indicated by the b i prefixes of the tags so your labellist should map label identifiers to label tags such as o bper iper borg iorg bloc iloc
76102415,implementing a transformerbased span retrieval model for a documentgrounded dialogue system,python machinelearning deeplearning nlp,you interpreted your problem too complicated i think what you need is simple semantic search problem for the semantic search task i recommend using the sentencestransformers library here is a simple code using transformers library output using sentencestransformers is much more easier output
76102276,disable layers in gpt model,python nlp gpt,i cant think of a good way to lesion out eg just layer i and keep the layers after it since all output to layer i goes through layer i you can instead see what the model up to layer i would predict as follows i would create a new model which consists of the first i attention layers followed by a new final linear layer with a softmax mirroring the final output layer of the original model i would retrain finetune this model with the weights of the original i attention layers frozen and only the weights of the new linear layer are trained if youre not familiar with the underlying architecture for a transformer this is a good blog you can freeze layers by setting requiresgrad false for only those layers when creating your model
76079388,how to use crossencoder with huggingface transformers pipeline,python nlp huggingfacetransformers sentencetransformers largelanguagemodel,after much trial and error and code digging from this part of the code will contains the list of available precoded pipeline tasks this is where it document how you can input the text pairs to the model this is where the general usage of how the textclassification task was coded and the usages are in the docstrings of the functions and now here goes tldr out but the output isnt the same as using sentencetransformers yes it isnt because softmax was applied to the outputs theres a classification function that is applied post model inference at and particularly at tldr this time for real to replicate the results from rolling out your own tokenize forward function youll have to explicitly set the classification function and override the postprocess function ie out
76073565,gpt special tokens ignore words in input text when predicting next word,python nlp token predict gpt,solved this masking the tokens did the trick i used an attention mask and set all attention mask values of tokens i wanted to ignore to so their attention weights are on all layers
76015844,how to efficiently meanpool bert embeddings while excluding padding,pytorch nlp huggingfacetransformers,you can pad with nan and then use torchnanmean you can then change the values back to something less likely to cause gradient issues down the line alternatively take the sum of the row and divide by the number of nonzero assuming padding elements
75951190,sentence transformer use of evaluator,python nlp sentencetransformers,question how is trainsamples different from devsamples in the context of the embeddingsimilarityevaluator one needs to have a heldout split of data to be used for evaluation during training to avoid overfitting this heldout set is commonly referred to as the development set as it is the set of data that is used during development of the modelsystem a pedagogical analogy can be drawn between a traditional education curriculum and that of training deep learning models if one were to give students all the questions for a given topic and then use the same subset of questions for evaluation then eventually most students will learn to memorise the set of answers they repeatedly see while practicing instead of learning the procedures to solve the questions in general so if you are using your own custom data make sure that a subset of that data is allocated to devsamples in addition to trainsamples and testsamples alternatively if your own data is scarce you can use the original training data to supplement your own training development and test sets the test set is the one that is only used after training has completed to determine the final performance of the model ie all samples in the test set ideally havent been seen before question how is the model going to determine the best model that is saved to disc are we required to run devsamples against the model saved on the disc and then compare scores the previous answer alludes to how this will work but in brief once the evaluator has been instantiated it will measure the correlation against the gold labels and then return the similarity score depending on what mainsimilarity was initially set if the produced embeddings based on the development set offer a higher correlation with their gold labels and therefore a higher score overall then this better model is saved to disk hence there is no need for you to run devsamples against the model saved on the disc and then compare scores this process happens automatically provided everything has been set up appropriately question if my goal is to take a single model and then fine tune it is it okay to skip parameters evaluator and evaluationsteps based on the above answers you can understand why you cannot skip the evaluator and evaluationsteps the evaluator is an integral part of finetuning ie training the model question how to determine the total number of steps for the model i need to set evaluationsteps the evaluationsteps parameter sets the number of training steps that must occur before the model is evaluated using the evaluator if the authors have set this to then leave it as is unless you notice problems with training alternatively experiment with either increasing of decreasing it and select a value that works best for training followup questions question which metric is used to select the best epoch is it cosinepearson by default the maximum of the cosine spearman manhattan spearman euclidean spearman and dot product spearman is used question why are steps in the output the lets the user know that the evaluator was called after all training steps occurred for a particular epoch if the stepsperepoch was not set when calling the modelfit it defaults to none which sets the number of stepsperepoch to the size of the traindataloader which is passed to trainobjectives when modelfit is initially called ie modelfittrainobjectivestraindataloader trainloss in your case trainsamples is and trainbatchsize is so the size of traindataloader and therefore stepsperepoch will be if the stepsperepoch is less than the evaluationsteps then the number of training steps wont reach or exceed evaluationsteps and so additional calls to evalduringtraining on line wont occur this isnt a problem as the evaluation is forced to call at the end of each epoch anyway based on line question how do i find the number of evaluationsteps based on the size of my training data samples and batch size is too high the evaluationsteps is available to tell the model during the training process whether it should prematurely run an evaluation using the evaluator partway through an epoch otherwise the evaluation is forced to run at the end of the epoch after stepsperepoch have completed based on the numbers you provided you could for example set evaluationsteps to to get an evaluation to run approx halfway through an epoch assuming an epoch is trainingsteps see this answer and its question for more info on batch size vs epochs vs steps per epoch
75932605,getting the input text from transformers pipeline,nlp pipeline huggingfacetransformers bertlanguagemodel namedentityrecognition,solution datasets from datasets import loaddataset transformers torch cu from transformers import autotokenizer automodelfortokenclassification pipeline from transformerspipelinesptutils import keydataset model automodelfortokenclassificationfrompretraineddslimbertbasener tokenizer autotokenizerfrompretraineddslimbertbasener pipe pipelinetaskner modelmodel tokenizertokenizer dataset loaddatasetargillagutenbergspacyner splittrain results pipekeydatasetdataset text for idx extractedentities in enumerateresults printoriginal textnformatdatasetidxtext printextracted entities for entity in extractedentities printentity example output original text would i wish to send up my name now again i declined to the polite astonishment of the concierge who evidently considered me a queer sort of a friend he was called to his desk by a guest who wished to ask questions of course and i waited where i was at a quarter to eleven herbert bayliss emerged from the elevator his appearance almost shocked me out late the night before he looked as if he had been out all night for many nights extracted entities entity bper score index word herbert start end entity iper score index word bay start end entity iper score index word lis start end entity iper score index word s start end original text and you think our run will be better than five hundred and eighty it should be unless there is a remarkable change this ship makes over six hundred day after day in good weather she should do at least six hundred by tomorrow noon unless there is a sudden change as i said but six hundred would be it would be the high field by jove anything over five hundred and ninetyfour would be that the numbers are very low tonight extracted entities entity bmisc score index word jo start end brief explanation each sample in the dataset created by the loaddataset call can be accessed using an index and the associated dictionary key calls to the pipeline object with a keydataset as input returns pipelineiterator object that is iterable hence one can enumerate the pipelineiterator object to get both the result and the index for the particular result and then use that index to retrieve the associated sample in the dataset detailed explanation the huggingface pipeline abstraction is a wrapper for all available pipelines when one instantiates a pipeline object it will return the appropriate pipeline based on the task argument pipe pipelinetaskner modelmodel tokenizertokenizer given that the ner task is specified a tokenclassificationpipeline will be returned side note ner is an alias for tokenclassification this pipeline and all others inherits the base class pipeline the pipeline base class defines the call function which the tokenclassificationpipeline class relies on whenever the instantiated pipeline is called once a pipeline is instantiated see above it is called with data passed in as either a single string a list or when working with full datasets a huggingface dataset via the transformerspipelinesptutils keydataset class dataset loaddatasetargillagutenbergspacyner splittrain results pipekeydatasetdataset text pipeline call when the pipeline is called it checks whether the data passed in is iterable and then calls an appropriate function for huggingface dataset objects the getiterator function is called which returns a pipelineiterator object given the known behaviour of iterator objects one can enumerate the object to return a tuple containing a count from start which defaults to and the values obtained from iterating over iterable the values are the ner extractions for each sample in the dataset hence the following produces the desired results for idx extractedentities in enumerateresults printoriginal textnformatdatasetidxtext printextracted entities for entity in extractedentities printentity
75906407,how to interpret the modelmaxlen attribute of the pretrainedtokenizer object in huggingface transformers,python nlp huggingfacetransformers huggingfacetokenizers huggingface,this issue thread addresses a similar question according to that this is due to an error caused due to the max length not being specified in the tokenizer config file tokenizerconfigjson according to this a solution would be to modify the config file the docs also say this if no value is provided will default to verylargeinteger inte you can find similar issues related to this
75904923,i am getting error here torchembeddingweight input paddingidx scalegradbyfreq sparse when i call trainertrain function of gpt model,python nlp huggingfacetransformers torch gpt,the error you are experiencing is most likely due to the size of the vocabulary you have set in your gptconfig you have set the vocabsize to but the actual size of the vocabulary in the gpt model is therefore the model is expecting input token ids to be between and but some of the token ids in your training data are outside this range to fix this you should set the vocabsize in your gptconfig to also make sure that the tokenizer you are using is the same as the one used to tokenize your training data if the tokenizer is different the token ids in your training data may not match the expected token ids of the model
75886674,how to compute sentence level perplexity from hugging face language models,python nlp huggingfacetransformers largelanguagemodel huggingfaceevaluate,if the goal is to compute perplexity and then select the sentences theres a better way to do the perplexity computation without messing around with tokensmodels install then out q thats great but how do i use it for a custom model that cant be fetched with modelid a for that lets look under the hood this is how the code initialize the model argh theres no support for local models what if we do some simple changes to the code see load a pretrained model from disk with huggingface transformers technically if you could load a local model that you can load with you can should be able the modelid as such after the code change opened a pullrequest
75839825,how to prevent transformer generate function to produce certain words,python nlp huggingfacetransformers generativepretrainedtransformer,after looking at the docs found out there is a badwordsids parameter that you can pass in the generate given a bad word list you can create the id list using tokenizerbadwords addspecialtokensfalseinputids inputids tokenizerthe walks in park returntensorsptinputids badwords park offers badwordsids tokenizerbadwords addspecialtokensfalseinputids sequenceids modelgenerateinputids badwordsidsbadwordsids tensor sequences tokenizerbatchdecodesequenceids printsequences park is a short walk away from the park notice how the word park is appearing now this is because the tokenizer identifies park id and park id as different tokens this may depend on the tokenizer you use there are caseinsensitive tokenizers if you dont want this to happen you can add park into the bad word list as well colab demo
75813686,loading a pretrained fasttext model with gensim,python nlp gensim fasttext,thats the sort of error you might get from a file thats been truncated to not contain everything expected are you sure your ccdebin file is complete undamaged whats its size and can you try redowloading it to ensure you have a full copy separately theres no official support for finetuning fasttext vectors in gensim you can call usual training methods in atypical ways including on an alreadytrained model to attempt an effect like that but there are no guides for ways to do that effectively in gensim further ive never seen any good writeup explaining how finetuning a fasttext model could be attempted verified if you want confidence in the usual benefits of fasttext including its ability to synthesize useful vectors for outofvocabulary words its safest to usetrain it in the usual way via a single training session which includes representative training texts for all words of interest if improvising some other approach for patching in other words or differing word senses for existing words you should pay special attention to monitoring in what ways the novel steps are helping or hurting the overall model
75788612,use finetuned bert to train a new sentencetransformer,nlp bertlanguagemodel,should do
75780103,huggingface transformers trainermaybelogsaveevaluate indexerror invalid index to scalar variable,python pytorch nlp huggingfacetransformers huggingface,your issue comes from your computemetrics function as youre using a qa metric with a textgeneration model to fix it replace metric loadsquad with a textgeneration metric for example bleu metric loadbleu and adapt your computemetrics function in consequence def computemetricsevalpred predictions references evalpred predictions tokenizerbatchdecodepredictions references tokenizerbatchdecodereferences references ref for ref in references return metriccomputepredictionspredictions referencesreferences
75497996,hugging face transformer model bioclinicalbert not trained for any of the task,nlp huggingfacetransformers,this bioclinicalbert model is trained for masked language model mlm task this task basically used for learning the semantic relation of the token in the languagedomain for downstream tasks you can finetune the models header with your small dataset or you can use a finetuned model like bioclinicalbertfinetunedmedicalcondition which is the finetuned version of the same model you can find all the finetuned models in huggingface by searching bioclinicalbert as in the link
75491528,what does the embedding elements stand for in huggingface bert model,tensorflow nlp huggingfacetransformers bertlanguagemodel wordembedding,in bert model there is a postprocessing of the embedding tensor that uses layer normalization followed by dropout i think that those two arrays are the gamma and beta of the normalization layer they are learned parameters and will span the axes of inputs specified in param axis which defaults to corresponding to in embedding tensor
75459693,finetune t pretrained model on a specific domain for question answering,nlp huggingfacetransformers nlpquestionanswering,what i found is that it is not really feasible to finetune t llm word embeddings you can only use context or finetune the model on a dataset of qa but not retrain the model on a specific domain like finance which was my case i ended up building the qa system using haystack which is an opensource library offering project architecture to build nlp qa systems based on transformers you can specify
75266069,how to access berts inter layer,nlp bertlanguagemodel pretrainedmodel,if you want to use only the last layers of bert you can create a new model that only consists of these layers using the following code this will create a new model lastlayers that consists of only the last layers of the bert model you can then input your batchsize textlength tensor into this new model and use it for your specific task
75227030,getting an embedded output from huggingface transformers,nlp huggingfacetransformers transformermodel robertalanguagemodel,sound like you gave the model input of shape longestlengthinbatch which is huge i tried input and found even gb ram server also hits oom one solution is to break the huge input into smaller batches for example lines at a time
75179250,is splitting a long document of a dataset for bert considered bad practice,machinelearning nlp classification bertlanguagemodel textclassification,you have not mentioned if your intention is to classify but given that you refer to an article on classification i will refer to an approach where you classify the whole text the main question is which part of the text is the most informative for your purpose or in other words does it make sense to use more than the first last split of text when considering long passages of text frequently it is enough to consider the first or last tokens to correctly predict the class in substantial majority of cases say even though you may loose some precision you gain on speed and performance of the overall solution and you are getting rid of a nasty problem of figuring out the correct class out of a set of classifications why consider an example of text tokens long you split it by tokens obtaining pieces notice the small last piece should you even consider it your target class for this text is say a however you get the following predictions on the pieces a b a b c so you have now a headache to figure out the right method to determine the class you can use majority voting but it is not conclusive here weight the predictions by the length of the piece again non conclusive check that prediction of the last piece is class c but it is barely above the threshold and class c is kinda a so you are leaning towards a reclassify starting the split from the end in the same order as before you get a b c a a so clearly a you also get it when you majority vote combining all of the classifications forward and backward splits consider the confidence of the classifications eg a b a b c avg for a vs for b reconfirm the correction of labelling of the last piece manually if it turns out to be b then it changes all of the above then you can train an additional network to classify out of the raw classifications of pieces getting again into trouble of figuring out what to do with particularly long sequences or nonconclusive combinations of predictions resulting in poor confidence of the additional classification layer it turns out that there is no easy way and you will notice that text is a strange classification material exhibiting all of the above and more issues while typically the difference in agreement between the first piece prediction and the annotation vs the ultimate perfect classifier is slim at best so spare the effort and strive for simplicity performance and heuristic and clip it on details of the best practices you should probably refer to the article from this answer
75042153,cant load from autotokenizerfrompretrained typeerror duplicate file name sentencepiecemodelproto,python nlp protocolbuffers huggingface,ilyakam i ran into the same problem with mrmtbasefinetunedwikisql also in a notebook in a virtual environment your solution did almost work i had to add the line protocolbufferspythonimplementationpython so in case your solution does not work try adding the line in the notebook andreas python on ubuntu lts
74996994,do bert word embeddings change depending on context,nlp huggingfacetransformers bertlanguagemodel embedding transformermodel,this is a great question i had the same question but you asking it made me experiment a bit the answer is yes it changes based on the context you should not extract the embeddings and reuse them at least for most of the problems im checking the embedding for word bank in two cases when it comes separately and when it comes with a context river bank the embeddings that im getting are different from each other they have a cosine distance of
74978191,do i need to retrain bert for ner to create new labels,nlp bertlanguagemodel finetuning,yes you would have to use a model trained using the specific labels you require the ontonotes dataset may be better suited for what you are trying to do as it includes the entity names listed below see ontonotes release notes for further info the huggingface flairnerenglishontonoteslarge here and flairnerenglishontonotesfast here models are trained on this dataset and will likely produce results closer to what you desire as a demo make sure to pip install flair first from flairdata import sentence from flairmodels import sequencetagger tagger sequencetaggerloadflairnerenglishontonoteslarge load tagger sentence sentenceon september st george won dollar while watching game of thrones example sentence taggerpredictsentence predict ner tags print sentence and ner spans printsentence printthe following ner tags are found iterate over entities and print for entity in sentencegetspansner printentity output span september st labels date span george labels person span dollar labels money span game of thrones labels workofart ontonotes named entities person people including fictional norp nationalities or religious or political groups facility buildings airports highways bridges etc organization companies agencies institutions etc gpe countries cities states location nongpe locations mountain ranges bodies of water product vehicles weapons foods etc not services event named hurricanes battles wars sports events etc work of art titles of books songs etc law named documents made into laws language any named language date absolute or relative dates or periods time times smaller than a day percent percentage including money monetary values including unit quantity measurements as of weight or distance ordinal first second cardinal numerals that do not fall under another type
74876117,bert embeddings in lstm model error in fit function,python tensorflow keras nlp bertlanguagemodel,regenerating your error after running this code i am getting the same error remember if you are using tfdatadataset then encolse it then while making the dataset enclose the dataset within the set like this tfdatadatasetfromtensorsliceswordsid wordsmask second problem as you asked the warning you are getting because you should be aware that lstm doesnt run in cuda gpu it uses the cpu only therefore it is slow so tensorflow is just telling you that lstm will not run under gpu or parallel computing
74856703,finetuning distilbert takes hours,machinelearning nlp huggingfacetransformers bertlanguagemodel,are you using a gpu if not its normal that it would take this much time also i wouldnt be bothered by the accuracy if youre not able to run it for more epochs
74823070,can you create a custom model using gpt to answer questions only about a specific topic,nlp chatbot gpt,this has been successful for me you may wish to try it as well i want you to act as a javascript guide you are here to help answer any questions i may have about the language if i have any questions you will do your best to provide a helpful response please note that if my question is not related to javascript you have to write only error lets get started
74788816,text classification with a language model lm with class labels existing in text tokens,pytorch nlp classification bertlanguagemodel,first the label will be mapped to continuous integers so the model does not know that the text contains the label secondly you are right the dog label high probability model will pay more attention to the dog word in the text but not because the text contains the label but a feature finally if you want the model to learn more features that are not related to label words do not use mask and replace them with other label words it is a good solution at present refer to mabel attenuating gender bias using textual entailment data
74769552,classic king man woman queen example with pretrained wordembedding and wordvec package in r,r nlp wordvec wordembedding,an overview of using wordvec with r is available at which even shows an example of king man woman queen just following the instructions there and downloading the first english dim embedding wordvec model from ran on the british national corpus which i encountered downloaded and unzipped the modelbin on my drive and next inspecting the terms in the model words are there apparently appended with pos tags getting the word vectors displaying the vectors getting the king man woman and finding the closest vector to that vector gives queen do you prefer to build your own model based on your own text or a more larger corpus eg the text file follow the instructions shown at get a text file and use r package wordvec to build the model wait untill the model finished training and next interact with it
74656790,prepare json file for gpt,python nlp gpt,its more like writing n a new line character after each json so each line is json somehow the link jsonlines throw server not found error on me you can have these options write n after each line which have way to read file too its jsonlines install the module pip install jsonlines
74610298,translation with multilingual bert model,pandas nlp bertlanguagemodel generativepretrainedtransformer,the first one is using a string to tokenizer the second one you are trying to tokenizer an entire dataframe not a string
74595449,calculating embedding overload problems with bert,python pytorch nlp bertlanguagemodel embedding,i fixed the problem the reason for the memory overload was that i wasnt saving the tensor to the gpu so i made the following changes to the code
74591917,sbert gives same result no matter what,python nlp bertlanguagemodel,there was a couple of tiny modification to sort things out please bear in mind in order to cluster sentences you need to catch only the firstlast embedding for the sentence in addition kmeans expects to receive a d array for clustering output
74527928,how to get noncontextual word embeddings in bert,python pytorch nlp bertlanguagemodel,bert uses static subword embeddings in its first layer where they get summed with learned position embeddings you can get the embeddings layer by calling modelembeddingswordembeddings you should be able to pass the indices that you get from a berttokenizer to this layer and get the subword embeddings there are however several caveats with static embeddings these are not word embeddings but subwords that bert internally uses less frequent words get segmented into smaller units the embeddings are of much worse quality than standard word embeddings wordvec fasttext because they are trained to get combined with position embeddings and serve in the later layers not as standalone embeddings there are also methods for getting highquality word embeddings from bert and similar models those require training data and some computation afaik the best methods are interpreting pretrained contextualized representations via reductions to static embeddings bommasani et al acl obtaining better static word embeddings using contextual embedding models gupta jaggi acl with code on github
74448344,map bertopic topic ids back to the training dataframe,pythonx nlp bertlanguagemodel topicmodeling,there is no need to recalculate the topics as you already retrieved them when using fittransform there the topics that you retrieve are in the exact same order as the input documents therefore you can perform the following the that you get here are in the exact same order as belongs to to etc topicmodel bertopic topics probs topicmodelfittransformdocs topicmodelreducetopicsdocs nrtopics when you used df pddataframedocument docs topic topic for those using fit instead of fittransform you can also access the topics and their documents as follows when you used df pddataframedocument docs topic topicmodeltopics
74355660,i am new to pretrained language models in natural language processing could anyone give a hint on where should i start or the road maps to start,deeplearning nlp pretrainedmodel,i find these two courses very novicefriendly hugging face course deep lizard deep learning fundamentals classic edition
74286527,how to extend the vocabulary of a pretrained transformer model,nlp datascience huggingfacetransformers huggingfacetokenizers finetuning,if you resized the corresponding embedding weights with resizetokenembeddings they will be initialised randomly technically you can finetune the model on your target task nli in your case without touching the embedding weights in practice it will be harder for your model to learn anything meaningful about the newly added tokens since their embeddings are randomly initialised to learn the embedding weights you can do further pretraining before finetuning on the target task this is done by training the model on the pretraining objectives such as masked language modelling pretraining is more expensive than finetuning of course but remember that you arent pretraining from scratch since you start pretraining from the checkpoint of the already pretrained model therefore the number of epochssteps will be significantly less than what was used in the original pretraining setup when doing pretraining it will be beneficial to include indomain documents so that it can learn the newly added tokens depending on whether you want the model to be more domain specific or remain varied so as to not forget any previous domains you might also want to include documents from a variety of domains the dont stop pretraining paper might also be an interesting reference which delves into specifics regarding the type of data used as well as training steps
74244702,how to split input text into equal size of tokens not character length and then concatenate the summarization results for hugging face transformers,python nlp huggingfacetransformers huggingfacetokenizers huggingface,i like splitting text using nltk you can also do it with spacy and the quality is better but it takes a bit longer nltk and spacy allow you to cut text into sentences and this is better because the text pieces are more coherent you want to cut it less than to be on the safe side should be better and its what the original bert uses so it shouldnt be too bad you just summarize the summarizations in the end heres an example
74228567,error while using bertbasenlimeantokens bert model,python nlp bertlanguagemodel sentencetransformers sslerrorhandler,it was simply a proxy issue i just added and http and their relative proxy values into system environment in windows
74062240,using arabert model with spacy,nlp spacy bertlanguagemodel,spacy actually does support arabic though only at an alpha level which basically just means tokenization support see here thats enough for loading external models or training your own though so in this case you should be able to load this like any huggingface model see this faq in this case this would look like i dont speak arabic so i cant check the output thoroughly but that code ran and produced an embedding for me
73700165,how to use architecture of t without pretrained model hugging face,deeplearning nlp pytorch huggingfacetransformers,initializing with a config file does not load the weights associated with the model only the configuration for without weights create a tmodel with config file
73566299,xlmroberta tokenizer sticks all words together,python nlp huggingfacetransformers sentencetransformers robertalanguagemodel,the problem occurred because youve tried employing an average aggregation strategy since tokenizers unit of calculation is subword an aggregation strategy should be employed to reconstruct the original text in this case the strategy is determined as not appropriate for more information check out here another point for keeping in mind is aggregation between the model and tokenizer each model extracted the problem space using one particular tokenizer and it would function well using the same one output
73405232,transformer summariser pipeline giving different results on same model with fixed seed,deeplearning nlp huggingfacetransformers transformermodel summarization,you might reseed the program after bartlargecnn pipeline otherwise the seed generator would be used by the first pipeline and generate different outputs for your lidiya model across two scripts from transformers import autotokenizer automodelforseqseqlm pipeline import torch random text taken from uk news website text the veteran retailer stuart rose has urged the government to do more to shield the poorest from doubledigit inflation describing the lack of action as horrifying with a prime minister on shore leave leaving a situation where nobody is in charge responding to julys headline rate the conservative peer and asda chair said we have been very very slow in recognising this train coming down the tunnel and its run quite a lot of people over and we now have to deal with the aftermath attacking a lack of leadership while boris johnson is away on holiday he said weve got to have some action the captain of the ship is on shore leave right nobodys in charge at the moment lord rose who is a former boss of marks spencer said action was needed to kill pernicious inflation which he said erodes wealth over time he dismissed claims by the tory leadership candidate liz trusss camp that it would be possible for the uk to grow its way out of the crisis seed torchcudamanualseedallseed torchusedeterministicalgorithmstrue tokenizer autotokenizerfrompretrainedfacebookbartlargecnn model automodelforseqseqlmfrompretrainedfacebookbartlargecnn modeleval summarizer pipeline summarization modelmodel tokenizertokenizer numbeams dosampletrue norepeatngramsize device output summarizertext truncationtrue seed torchcudamanualseedallseed torchusedeterministicalgorithmstrue tokenizer autotokenizerfrompretrainedlidiyabartlargexsumsamsum model automodelforseqseqlmfrompretrainedlidiyabartlargexsumsamsum modeleval summarizer pipeline summarization modelmodel tokenizertokenizer numbeams dosampletrue norepeatngramsize device output summarizertext truncationtrue printoutput
73383418,how can we pass a list of strings to a fine tuned bert model,python nlp huggingfacetransformers bertlanguagemodel huggingfacetokenizers,different methods for one sentence vs batches there are different methods for encoding one sentence versus encoding a batch of sentences according to the documentation the encodeplus method expects the first parameter to be this can be a string a list of strings tokenized string using the tokenize method or a list of integers tokenized string ids using the converttokenstoids method emphasis mine so that if youre passing a list of strings to this particular method they are interpreted as a list of tokens not sentences and obviously all those very long tokens like what is gandhi commonly considered to befather of the nation in india do not match anything in the vocabulary so they get mapped to the outofvocabulary id if you want to encode a batch of sentences then you need to pass your list of strings to the batchencodeplus method
73370817,how to use gpt for fillmask tasks,python nlp gpt,first of all i dont think you can access properties like token or scores in gpt all you have is the generated text second of all in my experience gpt is all about the correct prompt you just have to give it instructions like you were talking to a human being in you specific case i would use a prompt like this prompt the sun is mask replace mask with the most probable words to replace and give me their probabilities result the sun is shining shining bright sunny hot beautiful if you want to do that programmatically heres the code
73334654,what is better custom training the bert model or use the model with pretrained data,python nlp huggingfacetransformers nlpquestionanswering,the stateoftheart approach is to take a pretrained model that was pretrained on tasks that are relevant to your problem and finetune the model on your dataset so assuming you have your dataset in english you should take a pretrained model on natural language english you can then finetune it this will most likely work better than training from scratch but you can experiment on your own you can also load a model without the pretrained weights in huggingface
73107703,issue when importing bloomtokenizer from transformers in python,python nlp huggingfacetransformers huggingfacetokenizers huggingface,bloom has no slow tokenizer class it only has a fast tokenizer the official documentation is wrong at this point use the following instead from transformers import bloomtokenizerfast tokenizer bloomtokenizerfastfrompretrained
72997028,how to understand the answerstart parameter of squad dataset for training bertqa model practical implications for creating custom dataset,nlp huggingfacetransformers bertlanguagemodel huggingfacedatasets squad,your question is a bit broad to give you a specific answer but i will try my best to point you in some directions the intuition behind how the model uses the answerstart when calculating the loss accuracy etc there are different types of qa tasksdatasets the ones you mentioned squad and adversarialqa belong to the field of extractive question answering there a model must select a span from a given context that answers the given question for example context second democrats have always elevated their minority floor leader to the speakership upon reclaiming majority status republicans have not always followed this leadership succession pattern in for instance republicans bypassed james r mann ril who had been minority leader for eight years and elected frederick gillett rma to be speaker mann had angered many republicans by objecting to their private bills on the floor also he was a protg of autocratic speaker joseph cannon ril and many members suspected that he would try to recentralize power in his hands if elected speaker more recently although robert h michel was the minority leader in when the republicans regained control of the house in the midterm elections he had already announced his retirement and had little or no involvement in the campaign including the contract with america which was unveiled six weeks before voting day questionhow did republicans feel about mann in answerangered starting at character a simple approach that is often used today is a linear layer that predicts the answer start and answer end from the last hidden state of a transformer encoder code example the last hidden state holds one vector for each input token token words and the linear layer is trained to assign high probabilities to tokens that could potentially be the start and end of the answer span to train a model with your data the loss function needs to know which tokens should get a high probability ie the answer and the start token if i need to go through the process of adding this to my custom dataset easier to run model evaluation code etc you should go through this process otherwise how should someone know where the answer starts in your context they can of course interfere with it programmatically but what if your answer string appears twice in the context providing an answer start position avoids confusion and allows your users to use it right away with one of the many extractive questions answering scripts that are already available out there if so is there a programmatic way to do this to avoid manual effort you could simply loop through your dataset and use strfind contextfindanswer output
72983056,can i remove some topics from a bertopic model,nlp bertlanguagemodel topicmodeling,i had the same question and i asked in github discussions for the package if you ask there the package author answers very quickly here is his answer to our question deleting topics is unlikely to help with speeding up the model in transform as it is not possible to do that easily in the underlying models instead i would either advise using the slower model and use mergetopics to merge all unwanted topics into a single topic so that it is easier to identify those or you can adjust the mintopicsize a bit lower to get a balance between helpful topics and speed of the transform function do note that the transform function can be speed up by a number of different ways for example if you have an older gpu then embedding the documents can be much slower in practice it is helpful to identify which steps of the algorithm are relatively slow for you by setting verbosetrue you have some indication of the time spent at each of those steps if umap is too slow for you then you can consider using pca instead c maarten grootendorst also note that you could improve the speed of transform by enabling the gpu acceleration for the latter two stages by default only the first stage is gpu accelerated you will find the info on that here
72854302,are the pretrained layers of the huggingface bert models frozen,nlp pytorch huggingfacetransformers bertlanguagemodel,they are not frozen all parameters are trainable by default you can also check that with for name param in modelnamedparameters printname paramrequiresgrad output
72801555,typeerror expected argument to be a boolean but got bert,machinelearning deeplearning nlp datascience bertlanguagemodel,i saw this question again after facing this issue in same code now i am writing an answer as i have solved it there is a keyword name missing in the above code i changed it to by just putting name bert and now it works
72776921,bert sentencetransformers list index out of range,python nlp datascience bertlanguagemodel sentencetransformers,had to tokenize texts with berttokenizer and not just use split
72747399,finetuning layoutlm on funsdlike dataset index out of range in self,neuralnetwork nlp pytorch huggingfacetransformers,after double checking the dataset and specifically the coordinates of the labels ive found that some rows bbox coordinates lead to zero width or height heres a simplified example after removing these labels from the dataset the issue was resolved
72724748,huggingface transformers padding vs padtomaxlength,python nlp huggingfacetransformers huggingfacetokenizers,it seems that the documentation is not complete enough you should add truncationtrue too to memic the padtomaxlength true like this
72723732,does ibm watson support using multiple language models at the same time,nlp ibmcloud multilingual ibmwatson speechtotext,you can configure the spanish node above the english node keep english as the default node the nodes can be distinguished with the lang context parameter also ibm watson follows the top to bottom approach
72690203,getting keyerrors when training hugging face transformer,python pandas nlp huggingfacetransformers huggingface,converting pandasseries into a simple python list and getting rid of some extra materials would fix the issue
72687276,evaluate bert model paramrequiresgrad,nlp pytorch huggingfacetransformers bertlanguagemodel,if you freeze your model then the parameter of the corresponding modules must not be updated ie they should not require gradient computation requiresgradfalse note nnmodule also has a requiresgrad method ideally freezebert would be a boolean and you would simply do
72673637,the decoder part in a transformer model,nlp transformermodel decoder,i get that ytrue is fed into the decoder during the training step to combine with the output of the encoder block well yes and no the job of the decoder block is to predict the next word the inputs to the decoder is the output of the encoder and the previous outputs of decoder block itself lets take a translation example english to spanish we have dogs nosotras tenemos perros the encoder will encode the english sentence and produce a attention vector as output at first step the decoder will be fed the attention vector and a token the decoder will should produce the first spanish word nosotras this is the yt in the next step the decoder will be fed again the attention vector as well as the token and the previous output yt nosotras tenemos will be the output and so on and so forth till the decoder spits out a token the decoder is thus an autoregressive model it relies on its own output to generate the next sequence
72673500,how can i use lstm with pretrained static word vectors on aclimdb dataset,keras deeplearning nlp lstm,if youre using bert for pretrained word vectors supplied as features to an lstm then you dont need to build a separate bert classification model you can use transformerembedding to generate word vectors for your dataset or use sentencetransformers in from ktraintext import transformerembedding in te transformerembeddingbertbasecased in teembedgeorge washington went to washington shape out this is what the included ner models in ktrain do underthehood also the input feature format for a bert model is completely different than input features for an lstm as the error message indicates to preprocess your texts for bert classification model youll need to supply preprocessmodebert to textsfromfolder
72503309,save a bert model with custom forward function and heads on hugginface,python nlp pytorch huggingfacetransformers bertlanguagemodel,maybe something is wrong with the configclass attribute inside your bertclassifier class according to the documentation you need to create an additional config class which inherits form pretrainedconfig and initialises the modeltype attribute with the name of your custom model the bertclassifiers configclass has to be consistent with your custom config class type afterwards you can register your config and model with the following calls and load your finetuned model with automodelfrompretrainedyourcustommodelname an incomplete example based on your code could look like this printing the model output should be similiar to this hope this helps
72489570,getting random output every time on running next sentence prediction code using bert,nlp pytorch huggingfacetransformers bertlanguagemodel attentionmodel,you need to put the model in evaluation mode if you use ie dropout layers while testing the model you should turn it off you can do this with if you dont use this you will get a different output and loss value because the dropout in your model will close different neurons each time
72338808,how to calculate per document probabilities under respective topics with bertopics,python nlp bertlanguagemodel topicmodeling,first to compute probabilities you have to add to your model definition calculateprobabilitiestrue this could slow down the extraction of topics if you have many documents then calling fittransform you should save the probabilities now you can create a pandas dataframe which shows probabilities under respective topics per document
72249052,how to use the pretrained model in an application,tensorflow nlp lstm,before you make predictions you have to convert your text to input format for example to make a prediction of he is a bad cat you have to tokenize the sentence then you have to add padding to it then you can make predictions on the padded sequence the output will be a float not exactly or because the model will give the value based on the given sentence close to the negative or positive side on the embedding for more details please refer to this working gist thank you
71909945,how to get the sentence embeddings with debertadebertapooling,python nlp spyder bertlanguagemodel,first you need to import pooler for deberta and then it is better to create a separate class to make it more convenient to work the details depend on your task a more precise implementation can be found in the model repository i hope it helps
71878447,how to use the deberta model by he et al on spyder,python nlp spyder,welcome to so when you call encode method it would tokenize the input then encode it to the tensors a transformer model expects then pass it through model architecture when youre using transformers you must do the steps manually lastly you must know what kind of output youre supposed to work with
71781813,how does the finetune on transformer t work,deeplearning nlp pytorch huggingfacetransformers seqseq,if you are using pytorch lightning then it wont freeze the head until you specify it do so lightning has a callback which you can use to freeze your backbone and training only the head module see backbone finetuning also checkout ligthningflash it allows you to quickly build model for various text tasks and uses transformers library for backbone you can use the trainer to specify which kind of finetuning you want to apply for your training thanks
71710186,creating word embedings from bert and feeding them to random forest for classification,machinelearning nlp datascience classification bertlanguagemodel,regarding the no improvements despite adding more features some researchers believe that the bert word embeddings already contain all the available information presented in text so then it doesnt matter how fancy a classification head you add to it doesnt matter if it is a linear model that uses the embeddings or a complicated ml algorithm with a number of other features they will not provide significant improvements in many tasks they argue that since bert is a contextaware bidirectional language model that is trained extensively on mlm and nsp tasks it already grasps most of the things that additional features for punctuation wordvec and tfidf could convey the lexicon could probably help a little in the sentiment task if it is relevant but the one or two extra variables that you likely use to represent it probably get drowned in all the other features other than that the accuracy of bertbased models depends on the dataset used sometimes the data is simply too diverse to obtain a perfect score eg if there are some instances of observations that are very similar but with different class labels etc you can see in the bert papers that the accuracy widely depends on the task eg in some tasks it is indeed but for some tasks eg masked language modeling where the model needs to choose a particular word from a vocab of over k words the accuracy of could be impressive in some cases so in order to obtain a reliable comparison with bert papers youd need to pick a dataset that theyve used and then compare regarding the dataset balance for deep learning models in general the rule of thumb is that the training set should be more or less balanced wrt the fraction of data covered by each class label so if you have labels should be if labels then each should be at around of training dataset etc that is because most nns work in batches where they update the model weights based on the feedback from each batch so if you have too many values of one class the batch updates will be dominated by that one class effectively worsening the quality of your training so if you want to improve the accuracy of your model balancing the dataset could be an easy fix and if you have eg ordered classes with differing sizes you may consider merging some of them eg reviews from as bad as neutral as good and then rebalancing if still necessary unless its a situation where eg class has of data and classes share the remaining in such a case you should probably consider some more advanced options such as partitioning the algo to two parts one predicting whether or not an instance is in class so a binary classifier the other to distinguish between the underrepresented classes
71708136,is it possible to access hugging face transformer embedding layer,python machinelearning nlp huggingfacetransformers transformermodel,taking bert as example if you load bertmodel if you load bert with other layers eg bertforpretraining or bertforsequenceclassification
71704422,combine camembert crf for token classification,python nlp huggingfacetransformers namedentityrecognition crf,you can ignore bertpretrainedmodel and initialize it as torch module import torch import torchnn as nn from torchcrf import crf from transformers import camembertmodel camemberttokenizerfast class camembertcrfnnmodule def initself numlabels supercamembertcrf selfinit selfencoder camembertmodelfrompretrainedcamembertbase selfconfig selfencoderconfig selfdropout nndropoutselfconfighiddendropoutprob selfclassifier nnlinearselfconfighiddensize numlabels selfcrf crfnumtagsnumlabels batchfirsttrue def forward self inputidsnone attentionmasknone tokentypeidsnone positionidsnone headmasknone inputsembedsnone labelsnone outputattentionsnone outputhiddenstatesnone r labels obj of shape obj labels for computing the token classification loss indices should be in confignumlabels outputs selfencoder inputids attentionmaskattentionmask tokentypeidstokentypeids positionidspositionids headmaskheadmask inputsembedsinputsembeds outputattentionsoutputattentions outputhiddenstatesoutputhiddenstates sequenceoutput outputslasthiddenstate sequenceoutput selfdropoutsequenceoutput logits selfclassifiersequenceoutput loss none if labels is not none loglikelihood tags selfcrflogits labels selfcrfdecodelogits loss loglikelihood else tags selfcrfdecodelogits tags torchtensortags output tags outputs return loss output if loss is not none else output m camembertcrf t camemberttokenizerfastfrompretrainedcamembertbase printmtthis is a test returntensorspt labelstorchtensor printmtthis is a test returntensorspt output
71691184,huggingface pretrained models tokenizer and model objects have different maximum input length,nlp huggingfacetransformers huggingfacetokenizers sentencetransformers,since you are using a sentencetransformer and load it to the sentencetransformer class it will truncate your input at tokens as stated by the documentation the relevant code is here property maxseqlength property to get the maximal input sequence length for the model longer inputs will be truncated you can also check this by yourself fifty modelencodethis converttotensortrue twohundered modelencodethis converttotensortrue fourhundered modelencodethis converttotensortrue printtorchallclosefifty twohundered printtorchallclosetwohunderedfourhundered output false true the underlying model xlmrobertabase is able to handle sequences with up to tokens but i assume symanto limited it to because they also used this limit during training ie the embeddings might be not good for sequences longer than tokens
71624150,is it possible to freeze some params in a single layer of tfbertmodel,machinelearning nlp bertlanguagemodel transferlearning,i dont know about training some weights inside a layers but i still suggest you to do the standard way freezing the layers is what is usually done in these cases to avoid retraining everything however you must not freeze all the layers since it would be useless what you want to do is to freeze everything except the last few layers and then train the neural network this works since the first layers usually learn very abstract features and therefore are transferrable across many problems on the other hand the last layers usually learn the features that really solves the task at hand based on the current dataset therefore if you want to retrain a pretrained model in another dataset you just need to retrain the last few layers you can also edit the last layers of the neural network by adding some dense layers and changing the output of the last layer which is useful if for example the number of classes to predict is different wrt the original dataset there are a lot of short and easy tutorials that you can follow online to do that to summarize freeze all the layers expect the last one optional create new layers and link them with the output of the secondlast layer train the network
71618602,finetuning gpt on windows,python machinelearning nlp openaiapi gpt,i would recommend you to install cygwin these commands are not for a windows based system but rather a linuxbased one
71617889,how to use metadata for document retrieval using sentence transformers,python search nlp informationretrieval sentencetransformers,it sounds like you need metadata filtering rather than placing the year within the query itself the faissdocumentstore doesnt support filtering id recommend switching to the pineconedocumentstore which haystack introduced in the v release a few days ago it supports the strongest filter functionality in the current set of document stores you will need to make sure you have the latest version of haystack installed and it needs an additional pineconeclient library too theres a guide here that may help it will go something like documentstore pineconedocumentstore apikey from environmentuswestgcp retriever embeddingretriever documentstore embeddingmodelallmpnetbasev modelformatsentencetransformers before you write the documents you need to convert the data to include your text in content as you have done above but no need to preappend the year and then include the year as a field in a meta dictionary so you would create a list of dictionaries that look like dicts content your text here meta year content another record text meta year i dont know the exact format of your df but assuming it is something like text year your text here another record here we could write the following to reformat it df dfrenamecolumnstext content you did this already create a new meta column that contains year data dfmeta dfyearapplylambda x year x we dont need the year column anymore so we drop it df dfdropyear axis now convert into the list of dictionaries format as you did before dicts dftodictorientrecords this data replaces the df dictionaries you write so we would continue as so documentstorewritedocumentsdicts documentstoreupdateembeddingsretrieverretriever now you can query with filters for example to search for docs with the publish year of we use the condition eq equals docs retrieverretrieve some query here topk filters year eq for published before we can use lt less than docs retrieverretrieve some query here topk filters year lt
71617394,bert vocabulary why every word has before,nlp bertlanguagemodel french,if i understand it correctly the camemberttokenizer uses this special character from sentencepiece see the source code sentencepiece on the other hand uses subword tokens splitting of words into smaller tokens but to internally always keep track of what is a real split between words where there was a whitespace and what is a subword splitting they use this character before the start of each real token followup subword tokens but not punctuation dont have this token see the explaination in the github repository basically the whitespace is always part of the tokenization but to avoid problems it is internally escaped as they use this example hello world becomes hello wor ld which can then be used by the model and later transformed back into the original string detokenized joinpiecesreplace hello world without ambiguity and without having to save the original string seperately
71603492,how to build a custom questionanswering head when using hugginface transformers,tensorflow nlp huggingfacetransformers nlpquestionanswering,for future reference i actually found a solution which is just editing the tfbertforquestionanswering class itself for example i added an additional layer in the following code and trained the model as usual and it worked
71581197,what is the loss function used in trainer from the transformers library of hugging face,python machinelearning nlp artificialintelligence huggingfacetransformers,it depends especially given your relatively vague setup description it is not clear what loss will be used but to start from the beginning lets first check how the default computeloss function in the trainer class looks like you can find the corresponding function here if you want to have a look for yourself current version at time of writing is the actual loss that will be returned with default parameters is taken from the models output values loss outputsloss if isinstanceoutputs dict else outputs which means that the model itself is by default responsible for computing some sort of loss and returning it in outputs following this we can then look into the actual model definitions for bert source here and in particular check out the model that will be used in your sentiment analysis task i assume a bertforsequenceclassification model the code relevant for defining a loss function looks like this if labels is not none if selfconfigproblemtype is none if selfnumlabels selfconfigproblemtype regression elif selfnumlabels and labelsdtype torchlong or labelsdtype torchint selfconfigproblemtype singlelabelclassification else selfconfigproblemtype multilabelclassification if selfconfigproblemtype regression lossfct mseloss if selfnumlabels loss lossfctlogitssqueeze labelssqueeze else loss lossfctlogits labels elif selfconfigproblemtype singlelabelclassification lossfct crossentropyloss loss lossfctlogitsview selfnumlabels labelsview elif selfconfigproblemtype multilabelclassification lossfct bcewithlogitsloss loss lossfctlogits labels based on this information you should be able to either set the correct loss function yourself by changing modelconfigproblemtype accordingly or otherwise at least be able to determine whichever loss will be chosen based on the hyperparameters of your task number of labels label scores etc
71493915,training camelbert model for token classification,deeplearning nlp bertlanguagemodel namedentityrecognition,the script you are using loads the labels from datadirtraintxt see for what the model expects it then tries to load the label list as first file file from the corpus even before loading the training data see and put it into labelmap but that fails for some reason my assumption would be that it doensnt find anything and labelmap is an empty dict so the first attempt to get the labels from it fails with keyerror probably either your input data is not there or not in the path as expected check if you have the right files and the right value for datadir from my experience relative paths in google drive can be tricky try something simple to see if it works like oslistdirdatadir to see if that is actually the directly you expect it to be if that is not the problem then probably something about the labels is actually wrong does anercorp use this exact way of writing labels bloc etc if it is different eg blocation or something it would fail too
71432983,runtimeerror error loading state dict for srlbert missing keys bertmodelembeddingspositionids unexpected keys,jupyternotebook nlp anaconda allennlp,if you are on the later versions of allennlpmodels you can use this archivefile instead the latest versions of the model archive files can be found on the demo page in the model card tab
71331411,why do pooler use tanh as a activation func in bert rather than gelu,nlp bertlanguagemodel,the author of the original bert paper answered it kind of in a comment on github the tanh thing was done early to try to make it more interpretable but it probably doesnt matter either way i agree it doesnt fully answer whether tanh is preferable but from the looks of it itll probably work with any activation
71215965,how to save checkpoints for thie transformer gpt to continue training,tensorflow nlp gpt,
71078218,valueerror no gradients provided for any variable tfcamembert,python tensorflow nlp huggingfacetransformers namedentityrecognition,try transforming your data into the correct format before feeding it to modelfit def mapfuncx y return inputids xinputids attentionmask xattentionmask labelsy traindataset traindatasetmapmapfunc the model seems to run after this step
70939904,can i finetune bert using only masked language model and next sentence prediction,nlp bertlanguagemodel,your first approach should be to try the pretrained weights generally it works well however if you are working on a different domain eg medicine then youll need to finetune on data from new domain again you might be able to find pretrained models on the domains eg biobert for adding layer there are slightly different approaches depending on your task eg for questionanswering have a look at tanda paper transfer and adapt pretrained transformer models for answer sentence selection it is a very nice easily readable paper which explains the transfer and adaptation strategy again huggingface has modified and pretrained models for most of the standard tasks
70913886,albert first word associations,nlp huggingfacetransformers nearestneighbor,all you need for that is to use alberts embedding and decoding layers transformers will provide you with these easily you can just use model to see a list of the layers comprising the model we want modelalbertembeddingswordembeddings that transforms a word actually a word token into an vector here a values one we also want modelpredictionsdecoder which will do the opposite operation by outputing a probability for each of the tokens thats inside the embeddings vocabulary and you get a critical look into the importance we give to natural products as compared to trademarks
70834489,choosing a good prompt for gpt,nlp openaiapi gpt,using gptj i tested the following input output giving an example of what you want in the input question answer can help gpt to understand the structure of the desired output explicitly explaining the gpt what the desired task is in the input see line can help it to understand the task in my experience it does not execute the task perfectly but using gpt might help and i hope this is a step in the right direction for you
70784728,gpt question answering based on keywords,nlp openaiapi gpt,i am currently working on something similar but im using gptj one thing i find helpful is describing in the input what you want gpt to do eg i want to generate a sentence containing all given keywords input output let me know if this helps you or if you find any other solutions which might help me as well
70716702,using sentence transformers with limited access to internet,python nlp huggingfacetransformers sentencetransformers,based on the things you mentioned i checked the source code of sentencetransformers on google colab after running the model and getting the files i check the directory and i saw the pytorchmodelbin there and according to sentencetransformers code link the flaxmodelmsgpack rustmodelot tfmodelh are getting ignored when the it is trying to download and these are the files that it downloads the only thing that you have to have to load the model is pytorchmodelbin file i tested with copying the modules to another directory and it worked and according to your question you havent downloaded this file so that is the problem all in all you should download the model using its command and then move the files to another directory and initialize the sentencetransformer class with that dir i wish it would be helpful
70682546,extract and concanate the last hidden states from bert model for each input,python deeplearning nlp featureextraction bertlanguagemodel,grab the last hidden states now a tuple of tensors of shape batchsize seqlen hiddensize and concatenate them here over the last dimension to a single tensor of shape batchsize seqlen hiddensize
70672460,hugging face efficient tokenization of unknown token in gpt,python nlp huggingfacetransformers huggingfacetokenizers gpt,for the importanttokens which contain several actual words like frankieandbennys you can replace underscore with the space and feed them normally or add them as a special token i prefer the first option because this way you can use pretrained embedding for their subtokens for the ones which arent actual words like cbdy you must add them as special tokens the output
70612932,can i make a transformer based chatbot which is pretrained on some other dataset,tensorflow nlp chatbot huggingfacetransformers medical,general solution is to design some dialog template that you can fill with question answers then use the qa model to generate synthetic dataset you should be able to use the generated data to trainfinetunesetup your chatbot the rest is about important details in a project you got there
70606666,solving cuda out of memory when finetuning gpt huggingface,python pytorch nlp huggingfacetransformers huggingface,if the memory problems still persist you could opt for distillgpt as it has a reduction in the parameters of the network the forward pass is also twice as fast particularly for a small gpu memory like gb vram it could be a solutionalternative to your problem at the same time it depends on how you preprocess the data indeed the model is capable of receiving a maximum length of n tokens could be for example depending on the models you choose i recently trained a named entity recognition model and the model had a maximum length of tokens however when i manually set the dimension of the padded tokens in my pytorch dataloader to a big number i also got oom memory even on gb vram as i reduced the dimension of the tokens to a much smaller one instead of for example the training started to work and i did not get any issues with the lack of memory tldr reducing the number of tokens in the preprocessing phase regardless of the max capacity of the network can also help to solve your memories problem note that reducing the number of tokens to process in a sequence is different from the dimension of a token
70578679,invalidconfigexception cant load class for name hftransformersnlp in rasa,nlp huggingfacetransformers bertlanguagemodel rasa,this error could be due to the rasa version youre using output of rasa version in the current versions hftransformersnlp and languagemodeltokenizer are deprecated using a bert model can be achieved with any tokenizer and see the documentation for further details
70563462,is there a way to use pretrained embedding with tfidf in tensorflow,python tensorflow keras deeplearning nlp,the most common approach is to multiply each word vector by its corresponding tfidf score one often sees this approach in academic papers you could do something like this create tfidf scores create tfidfweighted embeddings matrix
70496137,can we calculate feature importance in huggingface bert,nlp huggingfacetransformers bertlanguagemodel,captum is a prominent tool from pytorchfacebook for interpreting transformers and you can get a score for the attention the model pays to specific tokens at specific layers see a tutorial here or here
70464428,how to calculate perplexity of a sentence using huggingface masked language models,nlp pytorch huggingfacetransformers bertlanguagemodel transformermodel,there is a paper masked language model scoring that explores pseudoperplexity from masked language models and shows that pseudoperplexity while not being theoretically well justified still performs well for comparing naturalness of texts as for the code your snippet is perfectly correct but for one detail in recent implementations of huggingface bert maskedlmlabels are renamed to simply labels to make interfaces of various models more compatible i have also replaced the hardcoded with the generic tokenizermasktokenid so the snippet below should work from transformers import automodelformaskedlm autotokenizer import torch import numpy as np modelname cointegratedruberttiny model automodelformaskedlmfrompretrainedmodelname tokenizer autotokenizerfrompretrainedmodelname def scoremodel tokenizer sentence tensorinput tokenizerencodesentence returntensorspt repeatinput tensorinputrepeattensorinputsize mask torchonestensorinputsize diag maskedinput repeatinputmaskedfillmask tokenizermasktokenid labels repeatinputmaskedfill maskedinput tokenizermasktokenid with torchinferencemode loss modelmaskedinput labelslabelsloss return npexplossitem printscoresentencelondon is the capital of great britain modelmodel tokenizertokenizer printscoresentencelondon is the capital of south america modelmodel tokenizertokenizer you can try this code in google colab by running this gist
70330903,upload a pretrained spanish language word vectors and then retrain it with custom sentences gensim fasttext,python nlp gensim fasttext,the buildvocab method supports a step in the gensim library implementation of the fasttext algorithm not the original fastttext package from facebook that you seem to be loading youre mixing code meant for two different libraries if you switch to using gensim code rather than facebooks implementation you wont get that same error when trying to use buildvocab note though that what youre attempting incremental retraining of an existing model is an advancedexperimental technique that can easily backfire so its usually a bad idea to attempt without expertise rigorous checks as to whether the extra complications are helping
70276298,bertmodel and bertformaskedlm weights count,machinelearning deeplearning nlp pytorch bertlanguagemodel,using numel along with modelparameters is not a reliable method for counting the total number of parameters and may fail for recursive configuration of layers this is exactly what is happening in your case instead try following output output from the above outputs we can see that total number of trainable params for the two models are bertmodel bertformaskedlm in order to understand the difference lets have a look at the last module of both the models rest of the base model is exactly the same bertmodel bertformaskedlm only additions are the layernorm layer params for layer gammas and betas and the decoder layer using the yax b where a is of size nxm and b of nx with a total params of nxm params for bertformaskedlm
70255359,how to use xlmroberta in finetuning,python nlp,there are several things youre better to know before diving deep into huggingface transformers the preferred library for working with huggingfaces transformers is pytorch for several widely used models you may find the tensorflow version alongside but not for all fortunately there are ways to convert pt checkpoints to tf and vise versa finally how to fix the code
70201921,bert domain adaptation,python nlp pytorch huggingfacetransformers bertlanguagemodel,you saved a bert model with lm head attached now you are going to load the serialized file into a standalone bert structure without any extra element and the warning is issued this is pretty normal and there is no fatal error to do so you can check the list of unloaded params like below output
70146811,feed decoder input in transformers,python tensorflow machinelearning nlp transformermodel,this particular example actually uses teacherforcing but instead of feeding one gt token at a time it feeds the whole decoder input however because the decoder uses only autoregressive ie righttoleft attention it can attend only to tokens i when generating the ith token therefore such training is equivalent to teacherforcing one token at a time but is much faster because all these tokens are predicted in parallel
70005473,bert unable to reproduce sentencetoembedding operation,nlp bertlanguagemodel,maybe dropout works while inferencing you can try modeleval in addition transformers is longtimesupport stop using pytorchpretrainedbert import torch from transformers import berttokenizerfast bertmodel bertpath userscalebdesktopcodesptmsbertbase tokenizer berttokenizerfastfrompretrainedbertpath model bertmodelfrompretrainedbertpath maxlength teststr this is a sentence tokenized tokenizerteststr maxlengthmaxlength paddingmaxlength inputids tokenizedinputids inputids torchunsqueezetorchlongtensorinputids attentionmask tokenizedattentionmask attentionmask torchunsqueezetorchinttensorattentionmask res modelinputids attentionmaskattentionmask printreslasthiddenstate
69941156,the problem of the installation of transformers,python deeplearning nlp glibc huggingfacetransformers,i had the same issues and i downgraded to the following version
69931987,gpt davinci gives different results with the same prompt,text nlp autocomplete gpt,i just talked to openai and they said that their response is not deterministic its probabilistic so that it can be creative in order to make it deterministic or reduce the risk of being probabilistic they suggest adjusting the temperature parameter by default it is ie taking risks if we want to make it completely deterministic set it to another parameter is topp default that can be used to set the state of being deterministic but they dont recommend tweaking both temperature and topp only one of them would do the job
69923334,using gpu with simple transformer mt training,nlp pytorch machinetranslation simpletransformers,it jus out of memory cases the parameter and dataset werent loaded on my gpu memory so i changed my model mtbase to mtsmall delete save point reduce dataset
69866866,what is the best way to compute metrics for the transformers results,python nlp huggingfacetransformers,in my opinion there is something wrong with the model dslimbertlargener youre using according to documents they have introduced an argument named aggregationstrategy for the exact same purpose full explanation but for some reason this is not working properly here now there are two options for the quick fix first change the model to one which is working fine output second translate the output to a more comfortable format to do the rest of the process probably with the aid of a state machine
69852169,text preprocessing for fasttext pretrained models,nlp textprocessing textclassification fasttext,when the facebook engineers have been asked similar questions in their github repository issues theyve usually pointed to one or the other of two shell scripts in their public code especially the normalizetext functions within theyve also referenced this pages section on tokenization which names some libraries and the academic paper which describes the earlier work making individual language vectors none of these are guaranteed to exactly match what was used to create their pretrained classification models its a bit frustrating that each release of such models doesnt contain the exact code to reproduce but these sources seem to be as much detail as is available without getting direct answershelp from the team that created them
69836422,bert outputs explained,python tensorflow deeplearning nlp bertlanguagemodel,the tensorflow docs provide a very good explanation to the outputs you are asking about the bert models return a map with important keys pooledoutput sequenceoutput encoderoutputs pooledoutput represents each input sequence as a whole the shape is batchsize h you can think of this as an embedding for the entire movie review sequenceoutput represents each input token in the context the shape is batchsize seqlength h you can think of this as a contextual embedding for every token in the movie review encoderoutputs are the intermediate activations of the l transformer blocks outputsencoderoutputsi is a tensor of shape batchsize seqlength with the outputs of the ith transformer block for here is another interesting discussion on the difference between the pooledoutput and sequenceoutput if you are interested the default output is equal to the pooledoutput which you can confirm here import tensorflow as tf import tensorflowhub as hub tfhubhandlepreprocess tfhubhandleencoder def buildclassifiermodelname textinput tfkeraslayersinputshape dtypetfstring namefeatures bertpreprocessmodel hubkeraslayertfhubhandlepreprocess namepreprocessing encoderinputs bertpreprocessmodeltextinput encoder hubkeraslayertfhubhandleencoder outputs encoderencoderinputs net outputsname return tfkerasmodeltextinput net sentence tfconstant improve the physical fitness of your goldfish by getting him a bicycle classifiermodel buildclassifiermodelnamedefault defaultoutput classifiermodelsentence classifiermodel buildclassifiermodelnamepooledoutput pooledoutput classifiermodelsentence printdefaultoutput pooledoutput
69835532,dropping layers in transformer models pytorch huggingface,python nlp pytorch huggingfacetransformers,i think one of the safest ways would be simply to skip the given layers in the forward pass for example suppose you are using bert and that you added the following entry to the config configactivelayers false true using a layers model then you could modify the bertencoder class like the following class bertencodernnmodule def initself config superinit selfconfig config selflayer nnmodulelistbertlayerconfig for in rangeconfignumhiddenlayers selfgradientcheckpointing false def forward self hiddenstates attentionmasknone headmasknone encoderhiddenstatesnone encoderattentionmasknone pastkeyvaluesnone usecachenone outputattentionsfalse outputhiddenstatesfalse returndicttrue allhiddenstates if outputhiddenstates else none allselfattentions if outputattentions else none allcrossattentions if outputattentions and selfconfigaddcrossattention else none nextdecodercache if usecache else none for i layermodule in enumerateselflayer magic here if not selfconfigactivelayersi continue if outputhiddenstates allhiddenstates allhiddenstates hiddenstates layerheadmask headmaski if headmask is not none else none pastkeyvalue pastkeyvaluesi if pastkeyvalues is not none else none if selfgradientcheckpointing and selftraining if usecache loggerwarning is incompatible with gradient checkpointing setting usecache false def createcustomforwardmodule def customforwardinputs return moduleinputs pastkeyvalue outputattentions return customforward layeroutputs torchutilscheckpointcheckpoint createcustomforwardlayermodule hiddenstates attentionmask layerheadmask encoderhiddenstates encoderattentionmask else layeroutputs layermodule hiddenstates attentionmask layerheadmask encoderhiddenstates encoderattentionmask pastkeyvalue outputattentions hiddenstates layeroutputs if usecache nextdecodercache layeroutputs if outputattentions allselfattentions allselfattentions layeroutputs if selfconfigaddcrossattention allcrossattentions allcrossattentions layeroutputs if outputhiddenstates allhiddenstates allhiddenstates hiddenstates if not returndict return tuple v for v in hiddenstates nextdecodercache allhiddenstates allselfattentions allcrossattentions if v is not none return basemodeloutputwithpastandcrossattentions lasthiddenstatehiddenstates pastkeyvaluesnextdecodercache hiddenstatesallhiddenstates attentionsallselfattentions crossattentionsallcrossattentions at the moment you may need to write your special bert class using the new encoder layer however you should be able to load the weights from the pretrained models provided by huggingface bertencoder code taken from here
69802895,the inputs into bert are token ids how do i get the corresponding the input token vectors into bert,nlp huggingfacetransformers bertlanguagemodel wordembedding,in bert the input is a string itself then bert manages to convert it into a token and then create its vector lets see an example prepurl encurl bertpreprocess hubkeraslayerprepurl bertencoder hubkeraslayerencurl text hello im new to stack overflow first you need to preprocess the data preprocessedtext bertpreprocesstext this will give you a dict with a few keys such us inputwordids that is the tokenizer encoded bertencoderpreprocessedtext and this will give you the vector with the context value of the previous text the output is encodedpooledoutput you can play with both dicts printing its keys i recommend you to go to both links above and do a little of research to recap bert uses string as inputs and then tokenize it with its own tokenzer if you want to tokenize with the same values you need the same vocab file but for a fresh start like you are doing this should be enough
69720846,when should i train my own models and when should i use pretrained models,machinelearning nlp artificialintelligence,i would go like this try the pretrained model and see how it goes if results are non satisfactory you can fine tune it see this tutorial basically you are using your own examples to change the weights of the pretrained model this should improve the results but it depends on how your data is and how many examples you can provide the more you have the better it should be i would try to use k at least also how could i train my model on my data and then later use it on it too be careful to distinguish between pretrain and finetuning for pretraining you need a huge amount of text like billions of characters it is very resource demanding and tipically you dont want to do that unless for a very good reason for example a model for your target language does not exist finetuning requires much much less examples some tents of thousands it take tipycally less than a day on a single gpu and allow you to exploit pretrained model created by someone else from what you write i would go with finetune of course you can save the model for later as you can see in the tutorial i linked above
69720454,questions when training language models from scratch with huggingface,python nlp huggingfacetransformers transformermodel roberta,i think you are mixing two distinct actions the first guide you posted explains how to create a model from scratch the runmlmpy script is for finetuning see line of the script an already existing model so if you just want to create a model from scratch step should be enough if you want to finetune the model you just created you have to run step note that training a roberta model from scratch already implies a mlm phase so this step is useful only in case that you will have a different dataset in the future and you want to improve your model by further finetuning it however you are not loading the model you just created you are loading the robertabase model from the huggingface repository modelnameorpath robertabase coming to the warning it tells you that you loaded a model robertabase as cleared out that was pretrained for masked language modeling maskedlm task this means you loaded a checkpoint of a model so quoting if your task is similar to the task the model of the checkpoint was trained on you can already use robertaformaskedlm for predictions without further training this means that if you going to perform a maskedlm task the model is good to go if you want to use for another task for example question answering you should probably finetune it because the model as is would not provide satisfactory results concluding if you want to create a model from scratch to perform mlm follow step this will create a model that can perform mlm if you want to finetune in mlm an already existing model see the huggingface repository follow step
69677322,pretrained roberta relation extraction attribute error,nlp huggingfacetransformers roberta,you have to specify the type of tensor that you want in return for tokenizer if you dont it will return a dictionary with two lists inputids and attentionmask
69664125,how to download a huggingface model transformerstrainertrainer,python nlp huggingfacetransformers pretrainedmodel,what you have saved is the model which the trainer was going to tune and you should be aware that predicting training evaluation and etc are the utilities of transformerstrainertrainer object not transformersmodelsxlmrobertamodelingxlmrobertaxlmrobertaforquestionanswering based on what was mentioned the easiest way to keep things going is creating another instance of the trainer
69660201,inputting some data for bert model using tfdatadatasetfromtensorslices,tensorflow nlp bertlanguagemodel,when using tfdatadatasetfromtensorslices try providing a batchsize since the bert preprocessing layer expects a very specific shape here is a simplified working example based on the bert models used in this tutorial and your specific details
69585176,using sentencebert with other features in scikitlearn,python machinelearning nlp embedding bertlanguagemodel,lets assume this is your data and you are willing to use it with sklearn api crossvalidation pipeline gridsearch and so on there is a utility named columntransformer which can map pandas data frames to the desired data using userdefined arbitrary functions what you have to do is define a function and create an official sklearntransformer from it after that you would be able to use the transformer like any other transformer and map your text column into semantic space like xtrain would be the data you wanted its proper to use with sklearn ecosystem output gaussiannbpriorsnone varsmoothinge caveat numerical features and the tweets embeddings should belong to the same scale otherwise some would dominate others and degrade the performance
69544570,inputoutput format for fine tuning huggingface robertaforquestionanswering,nlp huggingfacetransformers bertlanguagemodel nlpquestionanswering robertalanguagemodel,a question answering ot is basically a dl model that creates an answer by extracting part of the context in you case what is called text this means that the goal of the qabot is to identify the start and the end of the answer basic functioning of a qabot first of all every word of the question and context is tokenized this means it is possibily divided into characterssubwords and then convertend into a number it really depends on the type of tokenizer which means it depends on the model you are using since you will be using the same tokenizer its what the third line of your code is doing i suggest this very useful guide then the tokenized question text are passed into the model which performs its internal operations remember when i told at the beginning that the model will identify the start and the end of the answer well it does so by calculating for every token of the question text the probability that that particular token is the start of the answer this probabilities are the softmaxed version of the startlogits after that the same operations are performed for the end token so this is what startscores and endscores are the presoftmax scores that every token is start and end of the answer respectively so what are startposition and stopposition as stated here they are startpositions torchlongtensor of shape batchsize optional labels for position index of the start of the labelled span for computing the token classification loss positions are clamped to the length of the sequence sequencelength position outside of the sequence are not taken into account for computing the loss endpositions torchlongtensor of shape batchsize optional labels for position index of the end of the labelled span for computing the token classification loss positions are clamped to the length of the sequence sequencelength position outside of the sequence are not taken into account for computing the loss moreover the model you are using robertabase see the model on the huggingface repository and the roberta official paper has not been finetuned for questionanswering it is just a model trained by using maskedlanguagemodeling which means that the model has a general understanding of the english language but it is not suitable for question asnwering you can use it of course but it would probably give non optimal results i suggest you use the same model inthe version specifically finetuned on questionanswering robertabasesquad see it on huggingface in practical terms you have to replace the lines where you load the model and the tokenizer with this will give much more accurate results bonus read what finetuning is and how it works
69517460,bert get sentence embedding,python nlp huggingfacetransformers bertlanguagemodel huggingfacetokenizers,one of the easiest methods which can accelerate your workflow is batch data processing in the current implementation you are feeding only one sentence at each iteration but there is a capability to use batched data now if you are willing to implement this part yourself i highly recommend using tokenizer in this way to prepare your data but there is a simpler approach using featureextractionpipeline with comprehensive documentation this would look like this update in fact you changed your code slightly but youre passing samples one at a time yet not in the batch form if we want to stick to your implementation batch processing would be something like this output update there are two questions about what has been mentioned about padding the batcheddata to maximum length one is it able to distrubting the transformer model with irrelevant information no because in the training phase the model has presented with variablelength input sentences in the batched form and designers have introduced a specific parameter to guide the model on where it should attention second how can you get rid of this garbage data using the attention mask parameter you can perform the mean operation only on relevant data so the code would be changed to something like this
69428811,how does bert word embedding preprocess work,nlp huggingfacetransformers bertlanguagemodel transformermodel,bert provides its own tokenizer because bert is a pretrained model that expects input data in a specific format following are required a special token sep to mark the end of a sentence or the separation between two sentences a special token cls at the beginning of our text this token is used for classification tasks but bert expects it no matter what your application is tokens that conform with the fixed vocabulary used in bert the token ids for the tokens from berts tokenizer mask ids to indicate which elements in the sequence are tokens and which are padding elements segment ids used to distinguish different sentences positional embeddings used to show token position within the sequence have a look at this excellent tutorial for more details
69406937,how to use scibert in the best manner,nlp pytorch textclassification huggingfacetransformers bertlanguagemodel,when i want to do tokenization and batching it only allows me to use maxlength of yes you are not using the complete text and this is one of the limitations of bert and t models which limit to using and tokens resp to the best of my knowledge i can suggest you to use longformer or bigbird or reformer models which can handle sequence lengths up to k k tokens respectively these are really good for processing longer texts like scientific documents i have tried to use this pretrained library with other models such as deberta or roberta but it doesnt let me i has only worked with bert is there anyway i can do that scibert is actually a pretrained bert model see this issue for more details where they mention the feasibility of converting bert to roberta since youre working with a bert model that was pretrained you unfortunately wont be able to change the tokenizer now from a wordpiece bert to a bytelevel bpe roberta i know this is a general question but any suggestion that i can improve my fine tuning from data to hyper parameter etc currently im getting accuracy i would first try to tune the most important hyperparameter learningrate i would then explore different values for hyperparameters of adamw optimizer and numwarmupsteps hyperparamter of the scheduler
69374271,how to use a language model for prediction after finetuning,tensorflow keras nlp huggingfacetransformers transferlearning,although this is an example for a specific model distilbert the following prediction code should work similarly small modifications according to your needs you just need to replace the distillbert according to your model tfautomodelforsequenceclassification and of course ensure the proper tokenizer is used
69249665,bert text clasisification using pytorch,python nlp pytorch classification bertlanguagemodel,you are using criterion nnbceloss binary cross entropy for a multi class classification problem the labels can have three values of use suitable loss function for multiclass classification
69199961,oom error when training the bert keras model,python keras nlp sentimentanalysis bertlanguagemodel,a quick google search led me to this discussion where he states the splitting of the data with validationsplit parameter can lead to this oom error and the resolution was to split the data before calling modelfit by using sklearnpreprocessingtraintestsplit or any other way you prefer
69181078,spacy how do you add custom ner labels to a pretrained model,python nlp spacy namedentityrecognition,for spacy i did it this way
69091576,string comparison with bert seems to ignore not in sentence,nlp bertlanguagemodel transformermodel sentencesimilarity sentencetransformers,tldr nli is all you need first the cosine similarity is reasonably high because the sentences are similar in the following sense they are about the same topic evaluation of a person they are about the same subject i and the same property being a good person they have similar syntactic structure they have almost the same vocabulary so from the formal point of view they should be considered similar moreover from the practical point of view they should often be considered similar for example if you google gmo are causing cancer you might find that the text with label gmo are not causing cancer is relevant second if you want to measure logical connection between sentences cosine similarity of embeddings is just not expressive enough this is because embeddings contain lots of semantic stylistic lexical and syntactic information but they are fixedsize dimensional in your case so they cannot contain complete information about the meaning of both sentences so you need another model with the following properties it encodes both texts simultaneously so it compares the texts themselves not just their fixedsize embeddings it is explicitly trained to evaluate logical connection between sentences the task of assesing logical connection between texts is called natural language inference nli and its most common formulation is recognizing textual entailment rte it is the problem of predicting whether the first sentence entails the second one there are lots of models trained for this task in the huggingface repo with robertalargemnli being a good one you can use it to evaluate equivalence of two texts if each text entails another they are equivalent so you can estimate the degree of equivalence as the product of the entailment scores in both directions
68946827,spacytransformers access gpt,machinelearning nlp spacy gpt,the encorewebtrf uses a specific transformers model but you can specify arbitrary ones using the transformermodel wrapper class from spacytransformers see the docs for that an example config
68821642,looping cosine similarity formula from one dataframe to another dataframe using pandas bert,python pandas nlp bertlanguagemodel cosinesimilarity,cosign similarity can perform well on two lists so you can pass the whole embeddings list as arguments and extract maximum similarities afterward output
68814074,how to save parameters just related to classifier layer of pretrained bert model due to the memory concerns,python nlp pytorch bertlanguagemodel transferlearning,you can do it like this
68760136,attributeerror caught attributeerror in dataloader worker process fine tuning pretrained transformer model,python machinelearning nlp bertlanguagemodel,updated i skimmed several lines of documentation here about how to use the fit method and i realized there is a simpler solution to do what you desired the only changes you need to consider are to define proper inputexample for constructing a dataloader create a loss
68742863,error while trying to finetune the reformermodelwithlmhead googlereformerenwik for ner,python nlp pytorch huggingfacetransformers namedentityrecognition,first of all you should note that googlereformerenwik is not a properly trained language model and that you will probably not get decent results from finetuning it enwik is a compression challenge and the reformer authors used this dataset for exactly that purpose to verify that the reformer can indeed fit large models on a single core and train fast on long sequences we train up to layer big reformers on enwik and imagenet this is also the reason why they havent trained a subword tokenizer and operate on character level you should also note that the lmhead is usually used for predicting the next token of a sequence clm you probably want to use a token classification head ie use an encoder reformermodel and add a linear layer with classes on topmaybe a dropout layer anyway in case you want to try it still you can do the following to reduce the memory footprint of the googlereformerenwik reformer reduce the number of hashes during training from transformers import reformerconfig reformermodel conf reformerconfigfrompretrainedgooglereformerenwik confnumhashes or maybe even to model transformersreformermodelfrompretrainedgooglereformerenwik config conf after you have finetuned your model you can increase the number of hashes again to increase the performance compare table of the reformer paper replace axialposition embeddings from transformers import reformerconfig reformermodel conf reformerconfigfrompretrainedgooglereformerenwik confaxialposembds false model transformersreformermodelfrompretrainedgooglereformerenwik config conf this will replace the learned axial positional embeddings with learnable position embeddings like berts and do not require the full sequence length of they are untrained and randomly initialized ie consider a longer training
68691450,how can i check a confusionmatrix after finetuning with custom datasets,pythonx machinelearning pytorch nlp huggingfacetransformers,what you could do in this situation is to iterate on the validation setor on the test set for that matter and manually create a list of ytrue and ypred finally observations the output of the model are the logits not the probabilities normalized as such we apply softmax on dimension one to transform to actual probabilities eg class class we apply the argmax operation to get the index of the class
68686272,how to increase dimensionvector size of bert sentencetransformers embedding,machinelearning nlp artificialintelligence bertlanguagemodel,unfortunately the only way to increase the dimension of the embedding in a meaningful way is retraining the model however maybe this is not what you needmaybe you should consider finetuning a model i suggest you take a look at sentencetransformers from ukplabs they have pretrained models for sentence embedding for over languages the best part is that you can fine tune those models good luck
68627093,bert problem with contextsemantic search in italian language,machinelearning nlp bertlanguagemodel,the problem is not with your code it is just the insufficient model performance there are a few things you can do first you can try universal sentence encoder use from my experience their embeddings are a little bit better at least in english second you can try a different model for example sentencetransformersxlmrdistilrobertabaseparaphrasev it is based on roberta and might give a better performance now you can combine together embeddings from several models just by concatenating the representations in some cases it helps on expense of much heavier compute and finally you can create your own model it is well known that single language models perform significantly better than multilingual ones you can follow the guide and train your own italian model
68605546,building a characterlevel ngram language model with nltk,python nlp nltk ngram nltkbook,you need to tokenize your input apart from this your approach works basic example n train vocab paddedeverygrampipelinen whatisgoingonheresplit model kneserneyinterpolatedn modelfittrain vocab modelgeneratenumwords randomseed i s g o n h e r e more realistic example how you transform your input depends on the kind of original source you use lets say for a more realistic case you input a sequence of words from a text from nltktokenize import wordtokenize n prep inputs text lorem ipsum dolor sit amet consetetur sadipscing elitr sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat sed diam voluptua at vero eos et accusam et justo duo dolores et ea rebum stet clita kasd gubergren no sea takimata sanctus est lorem ipsum dolor sit amet lorem ipsum dolor sit amet consetetur sadipscing elitr sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat sed diam voluptua at vero eos et accusam et justo duo dolores et ea rebum stet clita kasd gubergren no sea takimata sanctus est lorem ipsum dolor sit amet tokenized wordtokenizetext train vocab paddedeverygrampipelinen tokenized fit model generate word model kneserneyinterpolatedn modelfittrain vocab modelgeneratenumwords randomseed o r e s t
68461204,continual pretraining vs finetuning a language model with mlm,deeplearning nlp huggingfacetransformers bertlanguagemodel pretrainedmodel,the answer is a mere difference in the terminology used when the model is trained on a large generic corpus it is called pretraining when it is adapted to a particular task or dataset it is called as finetuning technically speaking in either cases pretraining or finetuning there are updates to the model weights for example usually you can just take the pretrained model and then finetune it for a specific task such as classification questionanswering etc however if you find that the target dataset is from a specific domain and you have a few unlabled data that might help the model to adapt to the particular domain then you can do a mlm or mlmnsp finetuning unsupervised learning some researchers do call this as pretraining especially when a huge corpus is used to train the model followed by using the target corpus with target task finetuning
68444252,multiple training with huggingface transformers will give exactly the same result except for the first time,python machinelearning deeplearning nlp huggingfacetransformers,sylvain gugger answered this question here you need to set the seed before instantiating your model otherwise the random head is not initialized the same way thats why the first run will always be different the subsequent runs are all the same because the seed has been set by the trainer in the train method to set the seed
68343073,seqseqmodeloutput object has no attribute logits bart transformers,nlp huggingfacetransformers,the issue here is the bartmodel line switch this for a bartforconditionalgeneration class and the problem will be solved in essence the generation utilities assume that it is a model that can be used for language generation and in this case the bartmodel is just the base without the lm head
68337487,what is the correct way of encoding a large batch of documents with sentence transformerspytorch,python numpy machinelearning nlp pytorch,this line here sorts the input by text length before doing the encode i have no idea why so either comment those lines out or copy them to your code like then use the corpussorted to encode and map the output back using lengthsortedidx or just encode it one by one and you wont need to care about which output is from which text
68212946,bert summarization for a column of texts,python nlp bertlanguagemodel summarize,you can simply apply summ to the column with summaries since summ can take a list as input it will also process a pandas series to provide an example with multiple rows summaries abstracts the nncsl offers nextlevel cooking convenience its four distinct cooking methods steaming baking grilling and microwaving ensure your meals are cooked or reheated to perfection its multifunction capabilities can be combined to save time without compromising taste texture or nutritional value its the allinone kitchen companion designed for people with a busy lifestyle the nncsl offers nextlevel cooking convenience these slim and stylish bodies are packed with high performance the attractive compact designs and energysaving functions help panasonic bluray products consume as little power as possible you can experience great movie quality with this ultrafast booting dmpbd full hd bluray disc player after starting the player the time it takes from launching the menu to playing a disc is much shorter than in conventional models the bd also allows for smart home networking dlna and provides access to video on demand so that home entertainment is more intuitive more comfortable and lots more fun panasonic dmpbd full hd bluray disc player
68127754,removal of stop words and stemminglemmatization for bertopic,python nlp bertlanguagemodel topicmodeling,no bertopic uses transformers that are based on real and clean text not on text without stopwords lemmas or tokens at the end of the calculation stop words have become noise noninformative and are all in topicid for the same reason you should not tokenize done internally or lemmatize somewhat subjective the text that will messup your topics a disadvantage of not lemmatizing is that the keywords of a topic have a lot of redundancy like topn hotel hotels resort resorts etc it also does not handle bigrams like new york or barack obama elegantly you cant have it all andreas ps you can ofcourse remove html tags they are not in your reference corpus either
67990545,im using bert pretrained model for question and answering its returning correct result but with lot of spaces between the text,deeplearning nlp bertlanguagemodel huggingfacetransformers nlpquestionanswering,you can just use the tokenizer decode function berttokenizerdecodeinputidsansstartlocansendloc output in case you do not want to use decode you can use resultreplace
67976977,use bert under spacy to get sentence embeddings,python nlp spacy bertlanguagemodel,transformers are a bit different than the other spacy models but you can use doctrfdatatensors the vectors for the individual bpe byte pair encoding tokenpieces are in doctrfdatatensors note that i use the term tokenpieces rather than tokens to prevent confusion between spacy tokens and the tokens that are produced by the bpe tokenizer eg in our case the spacytokens are and the tokenpieces are
67915131,spacy error in loading pretrained custom model with entity rulers and ner pipeline,python machinelearning nlp spacy namedentityrecognition,upgrading to spacy version resolved this error
67909030,how to prepare text for bert getting error,pythonx nlp bertlanguagemodel transferlearning,the error is because your x dfsentiments and y dfreviews lines where your x and y are still dataframe columns or dataframe series and not list a simplet way to change them is x dfsentimentsvalues and y dfreviewsvalues which returns numpy array and it works if notit can be further converted to python list using x dfsentimentsvaluestolist and y dfreviewsvaluestolist
67851322,how to test masked language model after training it,python nlp bertlanguagemodel huggingfacetransformers,this depends a lot of your task your task seems to be masked language modelling that is to predict one or more masked words today i ate pizza or pasta could be equally correct so you cannot use a metric such as accuray but water should be less correct than the other two so what you normally do is to check how surprised the language model is on an evaluation data set this metric is called perplexity therefore before and after you finetune a model on you specific dataset you would calculate the perplexity and you would expect it to be lower after finetuning the model should be more used to your specific vocabulary etc and that is how you test your model as you can see they calculate the perplexity in the tutorial you mentioned to predict samples you need to tokenize those samples and prepare the input for the model the fillmaskpipeline can do this for you which results in the following output
67829695,pretrained fasttext hyperparameters,nlp fasttext,from looking at the fasttext python model class in facebooks source it looks like at least when creating a model all the hyperparameters are added as attributes on the object have you checked if thats the case on your loaded model for example does ftdim report and other parameters like ftmincount report anything interesting update as that didnt seem to work it also looks like the fasttext model wraps an internal instance of a native notinpython fasttext model in its f attribute see a few lines up from the source code i pointed to earlier and that nativeinstance is set up by the module specified by fasttextpybindcc that code looks like it specified a bunch of readwrite class variable associated with the metaparameters see for example starting at so does ftfmincount or ftfdim return anything useful from a postloaded model ft
67706707,how to use seqeval classificationreport after having performed ner with huggingface transformers,nlp huggingfacetransformers namedentityrecognition,you can call the classificationreport on your training data first to check if the model trained correctly after that call it on the test data to check how your model is dealing with data that it didnt see before
67703260,xlmbert sequence outputs to pooled output with weighted average pooling,python nlp pytorch bertlanguagemodel attentionmodel,there are two simple ways to get a sentence representation get the vector for the cls token get the pooleroutput assuming the input is batchsize seqlength dmodel where batchsize is the number of sentences then to get the cls token for every sentence you will have a tensor with shape batchsize dmodel to get the pooleroutput again you get a tensor with shape batchsize dmodel
67635055,python bert model pooling error mean received an invalid combination of arguments got str int,python nlp pytorch kaggle huggingfacetransformers,since one of the x updates the models return now taskspecific output objects which are dictionaries instead of plain tuples you can either force the model to return a tuple by specifying returndictfalse o selfbert ids attentionmaskmask tokentypeidstokentypeids returndictfalse or by utilizing the basemodeloutputwithpoolingandcrossattentions object o selfbert ids attentionmaskmask tokentypeidstokentypeids you can view the other attributes with okeys o olasthiddenstate
67634995,retrain the multi language ner modelnerontonotesbertmult from deeppavlov with a dataset in a different language,nlp namedentityrecognition pretrainedmodel deeppavlov,yes you can finetune the model on any language that was used for multilingual bert training it is also possible to finetune on languages that are not from the list above if multilingual vocabulary has a good coverage for your language
67577227,indexerror list index out of range nlp bert tensorflow,tensorflow machinelearning keras deeplearning nlp,as shown in the ktrain tutorials and example notebooks like this one you need to use the predictor instance to make predictions on raw text inputs create a predictor instance predictor ktraingetpredictorlearnermodel preproc make prediction output predictorpredicti loved this movie printoutput save predictor to disk predictorsavetmpmypredictor reload predictor from disk reloadedpredictor ktrainloadpredictortmpmypredictor make another prediction output reloadedpredictorpredicti loved this movie printoutput
67567587,python bert tokenizer cannot be loaded,python nlp pytorch bertlanguagemodel huggingfacetransformers,i think this should work it will download the tokenizer from huggingface
67564014,bert to xlnet train model,python tensorflow nlp tfkeras transformermodel,to solve this lets first see what exactly hides behind the bertbert property for this we can inspect the source code of the library for the tfbertmodel class there we can see that it is defined as selfbert tfbertmainlayerconfig namebert where the tfbertmainlayer is exactly what the name suggests the main bert transformer component to be precise it is defined as follows selfembeddings tfbertembeddingsconfig nameembeddings selfencoder tfbertencoderconfig nameencoder selfpooler tfbertpoolerconfig namepooler if addpoolinglayer else none if we check the source code for tfxlnetmodel we can see that there is only one property as well which is defined as selftransformer tfxlnetmainlayerconfig nametransformer since this is similar enough in its name you should get the same result by simply calling xlnettransformer although i cant guarantee that all the input parameters work the same
67526697,bert tokenizer addtoken function not working properly,python nlp pytorch bertlanguagemodel,yes if a token already exists it is skipped by the way after changing the tokenizer you have to also update your model see the last line below
67429425,is there any tf implementation of the original bert other than google and huggingface,tensorflow keras deeplearning nlp bertlanguagemodel,as mentioned in the comment you can try the following implementation of mlpbert tensorflow its a simplified version and easy to follow comparatively
67356666,how to add tokens in vocabtxt which decoded as unk bert tokenizer,python nlp bertlanguagemodel huggingfacetransformers huggingfacetokenizers,use the addtokens function of the tokenizer to avoid unknown tokens output please keep in mind that you also need to resize your model to introduce this to the new token with resizetokenembeddings
67352831,tensorflow and bert what are they exactly and whats the difference between them,nlp tensorflowx,tensorflow is an opensource library for machine learning that will let you build a deep learning modelarchitecture but the bert is one of the architectures itself you can build many models using tensorflow including rnn lstm and even the bert the transformers like the bert are a good choice if you just want to deploy a model on your data and you dont care about the deep learning field itself for this purpose i recommended the huggingface library that provides a straightforward way to employ a transformer model in just a few lines of code but if you want to take a deeper look at these models i will suggest you to learns about the wellknown deep learning architectures for text data like rnn lstm cnn etc and try to implement them using an ml library like tensorflow or pytorch
67342988,verifying the implementation of multihead attention in transformer,tensorflow keras deeplearning nlp lstm,in your implementation in scaleddotproduct you scaled with query but according to the original paper they used key to normalize apart from that this implementation seems ok but not general the general transformer architecture can be demonstrated as follows where the first two linear layers represent query and key and responsible to produce attention weights maps and followed by weighted the value in matrix multiplication fashion i understand youre trying to minimize the original tf tutorial code but i think you should add reference first to your original question in the original implementation they also returned weighted probabilities or scores along with the weighted feature maps i think you shouldnt skip that the original code that youre following is more general and efficient optimized fyi in tf the tfkeraslayersmultiheadattention layer is officially added you can test these two as follows
67282155,what is the simplest way to continue training a pretrained bert model on a specific domain,nlp textclassification bertlanguagemodel huggingfacetransformers pytorchlightning,lets clarify a couple of points first to reduce some ambiguity bert uses two pretraining objectives masked language modeling mlm and next sentence prediction you mentioned having a large unannotated dataset which you plan on using to finetune your bert model this is not how finetuning works in order to finetune your pretrained model you would need an annotated dataset ie document class pair for sequence classification downstream task so what can you do first extend your general domain tokenizer with your unannotated dataset consisting of domainspecific vocabulary then using this extended tokenizer you can continue pretraining on mlm andor nsp objectives to modify your word embeddings finally finetune your model using an annotated dataset
67270272,why inputmask is all the same number in bert language model,python nlp bertlanguagemodel nlu,the input masks allows the model to cleanly differentiate between the content and the padding the mask has the same shape as the input ids and contains anywhere the the input ids is not padding
67136740,finetune a bert model for context specific embeddigns,python nlp bertlanguagemodel,here is an example from the transformers library on fine tuning a language model for masked token prediction the model that is used is one of the bertforlm familly the idea is to create a dataset using the textdataset that tokenizes and breaks the text into chunks then use a datacollatorforlanguagemodeling to randomly mask tokens in the chunks when traing and pass the model the data and the collator to the trainer to train and evaluate the results
67105996,how to use bert and elmo embedding with sklearn,python machinelearning nlp bertlanguagemodel elmo,sklearn offers the possibility to make custom data transformer unrelated to the machine learning model transformers i implemented a custom sklearn data transformer that uses the flair library that you use please note that i used transformerdocumentembeddings instead of transformerwordembeddings and one that works with the transformers library im adding a so question that discuss which transformer layer is interesting to use here im not familiar with elmo though i found this that uses tensorflow you may be able to modify the code i shared to make elmo work import torch import numpy as np from flairdata import sentence from flairembeddings import transformerdocumentembeddings from sklearnbase import baseestimator transformermixin class flairtransformerembeddingtransformermixin baseestimator def initself modelnamebertbaseuncased batchsizenone layersnone from for pickling reason you should not load models in init selfmodelname modelname selfmodelkwargs batchsize batchsize layers layers selfmodelkwargs k v for k v in selfmodelkwargsitems if v is not none def fitself x ynone return self def transformself x model transformerdocumentembeddings selfmodelname finetunefalse selfmodelkwargs sentences sentencetext for text in x embedded modelembedsentences embedded egetembeddingreshape for e in embedded return nparraytorchcatembeddedcpu import numpy as np from sklearnbase import baseestimator transformermixin from transformers import autotokenizer automodel from moreitertools import chunked class transformerembeddingtransformermixin baseestimator def initself modelnamebertbaseuncased batchsize layer from for pickling reason you should not load models in init selfmodelname modelname selflayer layer selfbatchsize batchsize def fitself x ynone return self def transformself x tokenizer autotokenizerfrompretrainedselfmodelname model automodelfrompretrainedselfmodelname res for batch in chunkedx selfbatchsize encodedinput tokenizerbatchencodeplus batch returntensorspt paddingtrue truncationtrue output modelencodedinput embed outputlasthiddenstateselflayerdetachnumpy resappendembed return npconcatenateres in your case replace your column transformer by this columntrans columntransformer embedding flairtransformerembedding text numberscaler minmaxscaler number
67097467,what is language modeling head in bertformaskedlm,nlp bertlanguagemodel huggingfacetransformers languagemodel,the bertformaskedlm as you have understood correctly uses a language modelinglm head generally as well as in this case lm head is a linear layer having input dimension of hidden state for bertbase it will be and output dimension of vocabulary size thus it maps to hidden state output of bert model to a specific token in the vocabulary the loss is calculated based on the scores obtained of a given token with respect to the target token
67058709,bert is that needed to add new tokens to be trained in a domain specific environment,nlp bertlanguagemodel huggingfacetransformers huggingfacetokenizers,yes you have to add them to the models vocabulary the last line is important and needed since you change the numbers of tokens in the models vocabulary you also need to update the model correspondingly
67052427,how to access a particular layer of huggingfaces pretrained bert model,tensorflow keras nlp,you can iterate over the bert model in the same way as any other model like so isinstance checks the type of the layer so really you can put any layer type here and change what you need i havent checked specifically whether embeddingsregularizer is available however if you want to see what methods are available to that particular layer run a debugger and call dirlayer inside the above function updated question the tfbertforsequenceclassification model has layers similarly calling modellayers gives we can access the layers inside tfbertmainlayer so from the above we can access the tfbertembeddings layer by if you check the documentation search for the tfbertembeddings class you can see that this inherits a standard tfkeraslayerslayer which means you have access to all the normal regularizer methods so you should be able to call something like or whatever argument regularizer you need to change see here for regularizer docs
67043468,unparsedflagaccesserror trying to access flag preserveunusedtokens before flags were parsed bert,python nlp bertlanguagemodel,based on this issue you have to downgrade berttensorflow to check this answer to find a solution if you are following this tutorial downgrade berttensorflow and use the wget quiet as suggested because inside the python code the author has made the change from tfgfilegfilevocabfile r to tfiogfilegfilevocabfile r after that code compiles successfully ping me if you want anything else
66911216,param poolinglayer does not exist error coming while loading bert embedding model in sparknlp,nlp johnsnowlabssparknlp,its likely you have mixed versions of models and library that parameter that the exception is complaining has been recently removed from the bert model so you should try a different pretrained bert model
66888011,create representation of questions using lstm via a pretrained word embedding such as glove,python nlp pytorch lstm embedding,please see torch embedding tutorial and use embedding with keras for knowledge about word embeddings basically nnembeddingvocabsize embedsize is a matrix vocabsizeembedsize where each row corresponds to a words representation in order to know which word correspond to which row you should define a vocabulary that can transform a word into an index for example a python dictionary hello word and before that to transform a sentence into word in order to compute the vocabulary you need to tokenize the sentence using nltkwordtokenize or strsplit for example though torchnnembedding expects a tensor so if you want to process multiple sentence into batches and the sentence have different length you will need to pad the sentence with empty tokens in order to fit them into a tensor this is pseudo code preprocess data documents first sentence the second sentence tokenizeddocuments dsplit for d in documents create vocabulary and add a special token for padding words w for d in documents for word in tokenizeddocuments vocabulary w i for i w in enumeratesetwords vocabularypad indexeddocuments vocabularyw for w in d for d in tokenizeddocuments indexeddocuments will look like paddeddocuments torchnnutilsrnnpadsequence indexeddocuments paddingvaluevocabularypad data can be fed to the neural network modelwordembeddingstorchtensorpaddeddocuments
66849433,interpreting the output tokenization of bert for a given word,python nlp pytorch bertlanguagemodel,from the code it seem like the input should be a list of sentences thus any and for the wordpiece i have never used this library but maybe try a longer word my assumption here is that any is a wordpiece
66821505,extracting features from bertforsequenceclassification,python nlp bertlanguagemodel huggingfacetransformers,you can use the pooling output contextualized embedding of the cls token fed to the pooling layers of the bert model
66821321,bert weights of input embeddings as part of the masked language model,nlp pytorch bertlanguagemodel transformermodel languagemodel,for those who are interested it is called weight tying or joint inputoutput embedding there are two papers that argue for the benefit of this approach beyond weight tying learning joint inputoutput embeddings for neural machine translation using the output embedding to improve language models
66810883,how to use a pretrained language model correctly,python nlp allennlp,the tokenindexer needs to be a dictionary it can be set as follows selftokenindexers tokens pretrainedtransformerindexermicrosoftdialogptsmall
66748030,using pretrained bert embeddings as input to textcat models in spacy,python nlp spacy spacy,try the following config g switches to a transformer and o accuracy switches to the textcat ensemble model spacy init config p textcat g o accuracy configcfg see
66707770,how to use finetuned bert model for sentence encoding,python nlp pytorch bertlanguagemodel huggingfacetransformers,to load a model with bertmodelfrompretrained you need to have saved it using savepretrained link any other storage method would require the corresponding load i am not familiar with s but i assume you can use getobject link to retrieve the model and then save it using the huggingface api from then on you should be able to use frompretrained normally
66703229,maximum input length of wordssentences of the pegasus model in the transformers library,python machinelearning nlp pytorch huggingfacetransformers,in the transformers library what is the maximum input length of words andor sentences of the pegasus model it actually depends on your pretraining you can create a pegagsus model that supports a length of tokens or tokens for example the model googlepegasuscnndailymail supports tokens while googlepegasusxsum supports from transformers import pegasustokenizerfast t pegasustokenizerfastfrompretrainedgooglepegasusxsum t pegasustokenizerfastfrompretrainedgooglepegasuscnndailymail printtmaxlensinglesentence printtmaxlensinglesentence output the numbers are reduced by one because of the special token that is added to each sequence i read in the pegasus research paper that the max was tokens but how many words andor sentences is that that depends on your vocabulary from transformers import pegasustokenizerfast t pegasustokenizerfastfrompretrainedgooglepegasusxsum printttokenizethis is a test sentence printi know tokensformatlent output a word can be a token but it can also be split into several tokens printttokenizeneuropsychiatric conditions output also can you increase the maximum number of tokens yes you can train a model with a pegasus architecture for a different input length but this is costly
66625389,attributeerror list object has no attribute size huggingface transformers,pythonx nlp huggingfacetransformers,the model requires pytorch tensors and not a python list simply add returntensorspt to prepareseqseq from transformers import autotokenizer automodelforseqseqlm tokenizer autotokenizerfrompretrainedhelsinkinlpopusmtenhi model automodelforseqseqlmfrompretrainedhelsinkinlpopusmtenhi text hello my friends how are you doing today tokenizedtext tokenizerprepareseqseqbatchtext returntensorspt perform translation and decode the output translation modelgeneratetokenizedtext translatedtext tokenizerbatchdecodetranslation skipspecialtokenstrue print translated text printtranslatedtext output
66596142,bertmodel or bertforpretraining,deeplearning nlp bertlanguagemodel huggingfacetransformers transformermodel,you should be using bertmodel instead of bertforpretraining bertforpretraining is used to train bert on masked language model mlm and next sentence prediction nsp tasks they are not meant for classification bert model simply gives the output of the bert model you can then finetune the bert model along with the classifier that you build on top of it for classification if its just a single layer on top of bert model you can directly go with bertforsequenceclassification in anycase if you just want to take the output of bert model and learn your classifier without finetuning bert model then you can freeze the bert model weights using the above code is borrowed from here
66579324,error running runseqseqpy transformers training script,python tensorflow machinelearning nlp huggingfacetransformers,the problem is that you clone the master branch of the repository and try to run the runseqseqpy script with a transformers version that is behind that master branch runseqseqpy was updated to import isofflinemode on the th of march with this merge all you need to do is to clone the branch that was used for your used transformers version git clone branch vrelease ps i do not think you need to clone the dataset library
66561880,weights of pretrained bert model not initialized,tensorflow nlp pytorch bertlanguagemodel huggingfacetransformers,this actually seems to be expected behaviour in the documentation of the gpt models the huggingface team writes this will issue a warning about some of the pretrained weights not being used and some weights being randomly initialized thats because we are throwing away the pretraining head of the bert model to replace it with a classification head which is randomly initialized so it seems to not be a problem for the finetuning in my use case described above it worked despite the warning as well
66518375,how is transformers loss calculated for blank token predictions,machinelearning nlp transformermodel languagemodel,you need to mask out the padding what you call is is more often called create a mask saying where the valid tokens are pseudocode mask target when computing the categorical crossentropy do not automatically reduce the loss and keep the value multiply the loss values with the mask ie positions corresponding to the tokens get zero out and sum the losses at the valid positions pseudocode losssum loss masksum divide the losssum by the number of valid position ie the sum of the mask pseudocode loss losssum masksum
66421258,iterating through multiple files with bert for qa returns nothing,python nlp bertlanguagemodel elmo,for some reason when looping through all files print actually does return the answer it is weird because usually you do not need to call print to make it work working code
66315926,split a sentence by words just as bert tokenizer would do,python nlp tokenize bertlanguagemodel huggingfacetransformers,the problem is solved with tokentochars method as cronoik proposed in comments it gives me the exact position and it is universal not like words i used before which depends on how is splited of any token even unk
66302371,how to specify the loss function when finetuning a model using the huggingface tftrainer class,pythonx tensorflow nlp huggingfacetransformers,trainer has this capability to use computeloss for more you can look into the documentation here is an example of how to customize trainer to use a weighted loss useful when you have an unbalanced training set
66294710,bert embeddings for entire sentences vs verbs,nlp wordembedding bertlanguagemodel,to answer your question if i were you i would try to do a practical test for an easy way to use bert for sentence embeddings check this repo it is summarily simple to use once you have the embedding vectors you can use any similarity function to validate your hypothesis however for what is my limited experience i think that the vector of make is more similar than that of eat also only because make is present in the other sentence and therefore contributes to the ambedding of the sentence
66284360,how to get biobert embeddings,python nlp datascience biopython bertlanguagemodel,try to install it as follows i extended your sample dataframe to illustrate how you can now calculate the sentence vectors for your problem assessments and use these to calculate for example the cosine similarity between similar visit codes visit code problem assessment ge reflux working diagnosis well other reflux diagnosis poor medication refill order working diagnosis note called in brand benicar mg qd prn refill medication must be refilled diagnosis note called in brand olmesartan mg qd prn refill visit code problem assessment sentence embedding ge reflux working diagnosis well tensor e e e e e other reflux diagnosis poor tensor e e e e e medication refill order working diagnosis note called in brand benicar mg qd prn refill tensor e e e e e medication must be refilled diagnosis note called in brand olmesartan mg qd prn refill tensor e e e e e we can see that as expected the similar sentences lie very close together
66279882,how does masking work in the scaleddotproductattention of language understanding transformers,tensorflow deeplearning neuralnetwork nlp,ok so the value e resembles negative infinity therefor the softmax function will produce a probability of to such elements and will be ignored when calculating the attention values
66250618,how does pretrained fasttext handle multiword queries,nlp fasttext,fasttext can synthesize a guessvector from wordfragments for any string it can work fairly well for typo or variant wordform of a word that was wellrepresented in training for your word get up it might not work so well there may have been no or nomeaningful characterngrams in the training set of substrings of your word like get et u or t up but as fasttext uses a collision and presence oblivious hashtable for storing the ngram vectors these will still return essentiallyrandom vectors if you want instead something based on the perword vectors for get and up i think youd want to use the getsentencevector method instead
66244123,why use multiheaded attention in transformers,nlp transformermodel attentionmodel,transformers were originally proposed as the title of attention is all you need implies as a more efficient seqseq model ablating the rnn structure commonly used til that point however in pursuing this efficiency a single headed attention had reduced descriptive power compared to rnn based models multiple heads were proposed to mitigate this allowing the model to learn multiple lowerscale feature maps as opposed to one allencompasing map in these models the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions this makes it more difficult to learn dependencies between distant positions in the transformer this is reduced to a constant number of operations albeit at the cost of reduced effective resolution due to averaging attentionweighted positions an effect we counteract with multihead attention attention is all you need as such multiple attention heads in a single layer in a transformer is analogous to multiple kernels in a single layer in a cnn they have the same architecture and operate on the same featurespace but since they are separate copies with different sets of weights they are hence free to learn different functions in a cnn this may correspond to different definitions of visual features and in a transformer this may correspond to different definitions of relevance for example architecture input layer kernelhead layer kernelhead cnn image diagonal edgedetection horizontal edgedetection transformer sentence attends to next word attends from verbs to their direct objects notes there is no guarantee that these are human interpretable but in many popular architectures they do map accurately onto linguistic concepts while no single head performs well at many relations we find that particular heads correspond remarkably well to particular relations for example we find heads that find direct objects of verbs determiners of nouns objects of prepositions and objects of possesive pronouns what does bert look at an analysis of berts attention
66219090,how to get the hidden states of a finetuned tfbertmodel,python tensorflow deeplearning nlp huggingfacetransformers,you can retrieve the embeddings with the getinputembeddings function model buildcustomemodel modellayersgetinputembeddingsinputids
66110901,finetuned albert question and answering with huggingface,python nlp artificialintelligence torch bertlanguagemodel,turns out i just needed to grab an additional identifier when trying to request the model for future reference this information can be grabbed from the transformers use button seem in the
66096703,running huggingface bert tokenizer on gpu,deeplearning nlp huggingfacetransformers huggingfacetokenizers,tokenization is string manipulation it is basically a for loop over a string with a bunch of ifelse conditions and dictionary lookups there is no way this could speed up using a gpu basically the only thing a gpu can do is tensor multiplication and addition only problems that can be formulated using tensor operations can be accelerated using a gpu the default tokenizers in huggingface transformers are implemented in python there is a faster version that is implemented in rust you can get it either from the standalone package huggingface tokenziers or in newer versions of transformers they should be available under distilberttokenizerfast
66032426,backpropagation in bert,nlp bertlanguagemodel transformermodel,during pretraining there is a complete training if the model updation of weights moreover bert is trained on masked language model objective and not classification objective in pretraining you usually train a model with huge amount of generic data thus it has to be finetuned with the taskspecific data and taskspecific objective so if your task is classification on a dataset x you finetune bert accordingly and now you will be adding a taskspecific layer classification layer in bert they have used dense layer over cls token while finetuning you update the pretrained model weights as well as the new taskspecific layer
66015068,is the pretrained model selected at random when not specified from transformers,python nlp huggingfacetransformers nlg,the model is not chosen randomly ever task in the pipeline selects the appropriate model whichever is close to the task a model which is closely trained on the objective of your desired task and dataset is chosen for example sentimentanalysis pipeline can chose the model trained on sst task likewise for questionanswering it chooses automodelforquestionanswering class with distilbertbasecaseddistilledsquad as the default model as squad dataset is associated with question answering task to get the list you can look at the variable supportedtasks here
65924090,simpletransformers error versionconflict tokenizers how do i fix this,tensorflow nlp bertlanguagemodel simpletransformers sentencetransformers,i am putting this here incase someone faces the same problem i was helped by the creator himself
65698304,spacy language model load issues from encodewebsm to encodeweblg,python nlp spacy,are you sure you downloaded the model encoreweblg to disk you can do this by running this in the command line or in the script
65646925,how to train bert from scratch on a new domain for both mlm and nsp,deeplearning nlp bertlanguagemodel huggingfacetransformers transformermodel,you can easily train bert from scratch both on mlm nsp tasks using combination of bertforpretraining textdatasetfornextsentenceprediction datacollatorforlanguagemodeling and trainer i wouldnt suggest you to first train your model mlm then nsp which might lead to catastrophic forgetting its basically forgetting what youve learnt from previous training load your pretrained tokenizer initialize your model with bertforpretraining from transformers import bertconfig bertforpretraining config bertconfig model bertforpretrainingconfig create dataset for nsp task textdatasetfornextsentenceprediction will tokenize and creates labels for sentences your dataset should in the following format or you could just modify the existing code one sentence per line these should ideally be actual sentences blank lines between documents from transformers import textdatasetfornextsentenceprediction dataset textdatasetfornextsentenceprediction tokenizerbertcasedtokenizer filepathpathtoyourdataset blocksize use datacollatorforlanguagemodeling for masking and passing the labels that are generated from textdatasetfornextsentenceprediction datacollatorfornextsentenceprediction has been removed since it was doing the same thing with datacollatorforlanguagemodeling from transformers import datacollatorforlanguagemodeling datacollator datacollatorforlanguagemodeling tokenizerbertcasedtokenizer mlmtrue mlmprobability train save from transformers import trainer trainingarguments trainingargs trainingarguments outputdir pathtooutputdirfortrainingarguments overwriteoutputdirtrue numtrainepochs pergputrainbatchsize savesteps savetotallimit predictionlossonlytrue trainer trainer modelmodel argstrainingargs datacollatordatacollator traindatasetdataset trainertrain trainersavemodelpathtoyourmodel
65484081,translating using pretrained hugging face transformers not working,pythonx nlp translation huggingfacetransformers huggingfacetokenizers,this is not how an mt model is supposed to be used it is not a gptlike experiment to test if the model can understand instruction it is a translation model that only can translate there is no need to add the instruction translate english to dutch dont you want to translate the other way round also the translation models are trained to translate sentence by sentence if you concatenate all sentences from the column it will be treated as a single sentence you need to either iterate over the column and translate each sentence independently split the column into batches so you can parallelize the translation note that in that case you need to pad the sentences in the batches to have the same length the easiest way to do it is by using the batchencodeplus method of the tokenizer
65431837,transformers vx convert slow tokenizer to fast tokenizer,python nlp huggingfacetransformers huggingfacetokenizers,according to transformers v release sentencepiece was removed as a required dependency this means that the tokenizers that depend on the sentencepiece library will not be available with a standard transformers installation including the xlmrobertatokenizer however sentencepiece can be installed as an extra dependency or if you have transformers already installed
65424676,pytorchs nntransformerencoder srckeypaddingmask not functioning as expected,python nlp pytorch transformermodel,i got the same issue which is not a bug pytorchs transformer implementation requires the input x to be seqlen x batchsize x embdim while yours seems to be batchsize x seqlen x embdim
65400543,smoothing ngram language models with zr nr t q what to do with the final frequency q r and t are successive nonzero frequences,nlp statistics,i found several code examples that have solutions but no explanation to go along with them the question refers to the frequencies as q r and t the sample code there uses i j and k basically for the last t that would be an index out of bounds use nr q
65372032,deal with out of vocabulary word with gensim pretrained glove,nlp stanfordnlp gensim wordembedding,load the model import gensimdownloader as api model apiloadglovetwitter load glove vectors modelmostsimilarcat show words that similar to word cat there is a very simple way to find out if the words exist in the models vocabulary result printword exists if word in modelwvvocab else printword does not exist apart from that i had used the following logic to create sentence embedding dim with n tokens from future import printfunction division import os import re import sys import regex import numpy as np from functools import partial from fuzzywuzzy import process from levenshtein import ratio as levratio import gensim import tempfile def vocabcheckmodel word similarwords modelmostsimilarword matchratio matchword for simword simscore in similarwords ratio levratioword simword if ratio matchratio matchword simword if matchword return similarwords return modelsimilarityword matchword def sentencevectormodel sent dim words sentsplit emb modelwstrip for w in words weights if w in modelwvvocab else vocabcheckmodel w for w in words if lenemb sentvec npzerosdim dtypenpfloat else sentvec npdotweights emb sentvec sentvecastypefloat return sentvec
65221079,what do the logits and probabilities from robertaforsequenceclassification represent,python nlp pytorch textclassification huggingfacetransformers,you have initialized a robertaforsequenceclassification model that per default in case of robertabase and robertalarge which have no trained output layers for sequence classification tries to classify if a sequence belongs to one class or another i used the expression belongs to one class or another because these classes have no meaning yet the output layer is untrained and it requires a finetuning to give these classes a meaning class could be x and class could be y or the other way around for example the tutorial for finetuning a sequence classification model for the imdb review dataset defines negative reviews as class and positive reviews as class link you can check the number of supported classes with modelnumlabels output the output you get is the nonnormalized probability for each class ie logits you applied the softmax function to normalize these probabilities which leads to for the first class and for the second class maybe you got confused because the values are close to each other lets try a model with a pretrained output layer model card sentimodel robertaforsequenceclassificationfrompretrainedcardiffnlptwitterrobertabasesentiment printsentimodelnumlabels outputs sentimodelinputs printoutputslogitssoftmaxdimtolist output these values represent the probabilities for the sentence hello my dog is cute to be negative neutral or positive we know what these classes are because the authors provided mapping that clarifies it in case the authors of the model do not provide such a mapping via a readme or the original training code we can only guess what each class represents by testing it with random samples the model card you have mentioned does not provide any useful information regarding the mapping of the classes to what they represent but the model is provided by huggingface itself and they provide a link to the code used for training the model the datasetpy indicates that fake is represented by class and real by class
65034771,how to truncate a bert tokenizer in transformers library,python nlp huggingfacetransformers,truncation is not a parameter of the class constructor class reference but a parameter of the call method therefore you should use tokenizer autotokenizerfrompretrainedallenaiscibertscivocabuncased modelmaxlength lentokenizertext truncationtrueinputids output
65017564,train n last layers of bert in pytorch using huggingface library train last bertlayer out of,deeplearning nlp pytorch torch huggingfacetransformers,you are probably looking for namedparameters for name param in bertnamedparameters printname output namedparameters will also show you that you have not frozen the first but the last for name param in bertnamedparameters if paramrequiresgrad true printname output you can freeze the first with for name param in listbertnamedparameters printi will be frozen formatname paramrequiresgrad false
64896922,how do i truncate long document for bert,nlp,well the notion is you have to do the truncation before prepending and appending the cls and sep tokens and you can indeed choose the number max tokens of for bertbase model alone so the idea is first you choose the max tokens less than if you are using bertbase then split the sentence to its list of wordpieces then truncate the sentence to maxtokens with this when you add cls and sep tokens it would have a number of tokens equal to maxtokens for any sentence having the number of tokens including cls and sep less than maxtokens you can always append with zeros
64799622,how is the gpts maskedselfattention is utilized on finetuninginference,nlp transformermodel largelanguagemodel,in the standard transformer the target sentence is provided to the decoder only once you might confuse that with the masked languagemodel objective for bert the purpose of the masking is to make sure that the states do not attend to tokens that are in the future but only to those in the past the mask looks like this queries are on the vertical axis keys and values on the horizontal axis this means that for generating the first token you cannot attend to anything when generating the second token you can attend to states of the first token at the third one you can attend the first and to the second the masking thus emulates the inference time when you can only attend to states corresponding to tokens that have already been generated it is the same for the standard transformer for sequencetosequence learning and for decoderonly transformer such as gpt if you implement the inference efficiently you do not need the masking you keep all the previous states in memory do the attention only with the last query which corresponds to the newly generated token and thus get the new states and predict what the next token is this is done in a loop until you generate the endofsentence token
64712375,finetune bert for specific domain unsupervised,python deeplearning neuralnetwork nlp bertlanguagemodel,what you in fact want to is continue pretraining bert on text from your specific domain what you do in this case is to continue training the model as masked language model but on your domainspecific data you can use the runmlmpy script from the huggingfaces transformers
64704461,open source pretrained models for taxonomygeneral word classification,machinelearning nlp classification wordembedding,sounds like wordnet would be a good fit for this task wordnet is a lexical database that organises words in a hierarchical tree structure like a taxonomy and contains additional semantic information for many words see eg wordnet for cat here for a browserbased demo a word thats one hierarchy level above another word is a so called hypernym the hypernym for cat is eg feline with wordnet in nltk you can get the hypernyms of two words until you get the same hypernym for cat and dog the common hypernym is animal see example code here you ask for a machine learning solution in your question a classical approach would be word vectors via gensim but they will not give you a clear common category based on a database created by experts like wordnet but just give you words that often occur next to your target words cat dog in the training data i think that machine learning is not necessarily the best tool here see example
64684506,transformers get named entity prediction for words instead of tokens,nlp pytorch huggingfacetransformers,there are two questions here annotating token classification a common sequential tagging especially in named entity recognition follows the scheme that a sequence to tokens with tag x at the beginning gets bx and on reset of the labels it gets ix the problem is that most annotated datasets are tokenized with space for example where o indicates that it is not a namedentity bartist is the beginning of the sequence of tokens labelled as artist and iartist is inside the sequence similar pattern for medium at the moment i posted this answer there is an example of ner in huggingface documentation here the example doesnt exactly answer the question here but it can add some clarification the similar style of named entity labels in that example could be as follows adapt tokenizations with all that said about annotation schema bert and several other models have different tokenization model so we have to adapt these two tokenizations in this case with bertbaseuncased the expected outcome is like this in order to get this done you can go through each token in original annotation then tokenize it and add its label again when you add cls and sep in the tokens their labels o must be added to labels with the code above it is possible to get into a situation that a beginning tag like bartist get repeated when the beginning word splits into pieces according to the description in huggingface documentation you can encode these labels with to be ignored something like this should work
64646890,train bert with cli commands,python machinelearning nlp bertlanguagemodel huggingfacetransformers,found out the problem it had to do with the cuda driver not being compatible with the installed version of pytorch for anyone with nvidia gpu encountering the same problem going to the nvidia control panel help system information components there is a setting called nvcudadll with a driver number in the names column choosing the corresponding cuda version in the installation builder on pytorchorg should do the trick also there is a good readme in the transformers repository explaining all steps to train the bert model with cli commands here
64646867,downloading huggingface pretrained models,python nlp googlecolaboratory huggingfacetransformers,mount your google drive from googlecolab import drive drivemountcontentdrive do your stuff and save your models from transformers import berttokenizer tokenizer berttokenizerfrompretrainedbertbaseuncased tokenizersavepretrainedcontentdrivemy drivetokenizer reload it in a new session tokenizer berttokenizerfrompretrainedcontentdrivemy drivetokenizer
64622833,interpreting attention in keras transformer official example,tensorflow keras nlp attentionmodel,edit in case you want to interpret the classification output using attention from what i know it is impossible to fully interpret what transformer does in classification what transformer does is just to see how each input is related to each other not how each word contributes to the label if you wish to find the model that is interpretable try looking at lstm attention for classification ok so ive read your code and spotted some mistakes when you calling modellayers first you need to understand that the model is processing the data in a batch therefore your input should be in a format of batchsize seqlen however your input shape seems to drop the first dimension which is batch which makes your model think that you are giving a model sentences with a sequence length of therefore the output shape is looking strange as seen in the image the correct method is to add an extra dimension to the first dimension using tfexpanddims now for interpreting the results you need to know that the transformer block does selfattention which finds the scores for each word to other words in the sentences and weighted sum it thus the output would be the same as the embedding layer and you wouldnt be able to explain it as it is a hidden vector generated by the network however you can see the attention scores for each head by using the following codes import seaborn as sns import matplotlibpyplot as plt headnum inp tfexpanddimsxtrain axis emb modellayersmodellayersinp selfattn modellayersatt compute q k v query selfattnquerydenseemb key selfattnkeydenseemb value selfattnvaluedenseemb separate heads query selfattnseparateheadsquery batchsize key selfattnseparateheadskey batchsize value selfattnseparateheadsvalue batchsize compute attention scores qkt attention weights selfattnattentionquery key value idxword v k for k v in kerasdatasetsimdbgetwordindexitems pltfigurefigsize snsheatmap weightsnumpyheadnum xticklabelsidxwordidx for idx in inpnumpy yticklabelsidxwordidx for idx in inpnumpy heres an example output
64606333,bert embeddings in sparknlp or bert for token classification in huggingface,nlp bertlanguagemodel huggingfacetransformers johnsnowlabssparknlp,to answer your question no hugging face uses different head for different tasks this is almost the same as what the authors of bert did with their model they added taskspecific layer on top of the existing model to finetune for a particular task one thing that must be noted here is that when you add task specific layer a new layer you jointly learn the new layer and update the existing learnt weights of the bert model so basically your bert model is part of gradient updates this is quite different from obtaining the embeddings and then using it as input to neural nets question when you obtain the embeddings and use it for another complex model i am not sure how to quantify in terms of loosing the information because you are still using the information obtained using bert from your data to build another model so we cannot attribute to loosing the information but the performance need not be the best when compared with learning another model on top of bert and along with bert often people would obtain the embeddings and then as input to another the classifier due to resource constraint where it may not be feasible to train or finetune bert
64485777,how is the number of parameters be calculated in bert model,neuralnetwork nlp bertlanguagemodel,transformer encoderdecoder architecture the bert model contains only the encoder block of the transformer architecture lets look at individual elements of an encoder block for bert to visualize the number weight matrices as well as the bias vectors the given configuration l means there will be layers of self attention h means that the embedding dimension of individual tokens will be of dimensions a means there will be attention heads in one layer of self attention the encoder block performs the following sequence of operations the input will be the sequence of tokens as a matrix of s d dimension where s is the sequence length and d is the embedding dimension the resultant input sequence will be the sum of token embeddings token type embeddings as well as position embedding as a ddimensional vector for each token in the bert model the first set of parameters is the vocabulary embeddings bert uses wordpiece embeddings that has tokens each token is of dimensions embedding layer normalization one weight matrix and one bias vector multihead self attention there will be h number of heads and for each head there will be three matrices which will correspond to query matrix key matrix and the value matrix the first dimension of these matrices will be the embedding dimension and the second dimension will be the embedding dimension divided by the number of attention heads apart from this there will be one more matrix to transform the concatenated values generated by attention heads to the final token representation residual connection and layer normalization one weight matrix and one bias vector positionwise feedforward network will have one hidden layer that will correspond to two weight matrices and two bias vectors in the paper it is mentioned that the number of units in the hidden layer will be four times the embedding dimension residual connection and layer normalization one weight matrix and one bias vector lets calculate the actual number of parameters by associating the right dimensions to the weight matrices and bias vectors for the bert base model embedding matrices word embedding matrix size vocabulary size embedding dimension position embedding matrix size maximum sequence length embedding dimension token type embedding matrix size embedding layer normalization weight and bias total embedding parameters attention head query weight matrix size and bias key weight matrix size and bias value weight matrix size and bias total parameters for one layer attention with heads dense weight for projection after concatenation of heads and bias layer normalization weight and bias position wise feedforward network weight matrices and bias and layer normalization weight and bias total parameters for one complete attention layer total parameters for layers of attention output layer of bert encoder dense weight matrix and bias total parameters in ase
64445784,how can i apply pruning on a bert model,python tensorflow nlp bertlanguagemodel huggingfacetransformers,the distilbert model in ktrain is created using hugging face transformers which means you can use that library to prune the model see this link for more information and the example script you may need to convert the model to pytorch before using the script in addition to making some modifications to the script itself the approach is based on the paper are sixteen heads really better than one
64236859,my checkpoint albert files does not change when training,nlp trainingdata bertlanguagemodel checkpoint nlpquestionanswering,while training your model you can store the weights in a collection of files formatted as checkpoints that contain only the weights trained in a binary format in particular the checkpoints contain one or more blocks that contain the weights of our model an index file indicating which weights are stored in a particular block so the fact that the size of the checkpoint file is always the same depends on the fact that the model used is always the same so the number of model parameters is always the same so the size of the weights you are going to save is always the same while the suffix dataof indicates that you are training the model on a single machine the size of the dataset in my opinion has nothing to do with it
64130834,build a model that answers question from dataset using gpt,nlp nlpquestionanswering gpt,sure if you got a beta access to the openai gpt api youre easily able to do so in case you dont you can apply for it you should get accepted fairly quickly in my specific case it took about hours depending whether you look for speed or precision you should choose between davinci cushman or curie list of engines whereas davinci is the best precisionwise you can use the playground to enter a text corpus and a question here is an example i used davinciinstructbeta with a temperature of and response length of a pretty basic setup for demonstration purposes here is the api request made via python response returns anna hates doing research the most
64023547,inconsistent vector representation using transformers bertmodel and berttokenizer,python nlp bertlanguagemodel huggingfacetransformers,when you do it in single sentence per batch the maximum length of the sentence is maximum number of tokens however when you do it in batch the maximum length of the sentences remains the same across the batch which defaults to the maximum number of tokens in the longest sentence the max values of in this case indicates its not a token and indicates a token the best way to control this is to define the maximum sequence length and truncate the sentences longer than the maximum sequence length this can be done using an alternative method to tokenize the text in batches a single sentence can be considered as batchsize of
64013808,why bert model have to keep mask token unchanged,deeplearning nlp bertlanguagemodel,this is done because they want to pretrain a bidirectional model most of the time the network will see a sentence with a mask token and its trained to predict the word that is supposed to be there but in finetuning which is done after pretraining finetuning is the training done by everyone who wants to use bert on their task there are no mask tokens unless you specifically do masked lm this mismatch between pretraining and training sudden disappearence of the mask token is softened by them with a probability of the word is not replaced by mask the task is still there the network has to predict the token but it actually gets the answer already as input this might seem counterintuitive but makes sense when combined with the mask training
63924567,gpt on hugging facepytorch transformers runtimeerror grad can be implicitly created only for scalar outputs,python nlp pytorch huggingfacetransformers huggingfacetokenizers,i finally figured it out the problem was that the data samples did not contain a target output even tough gpt is selfsupervised this has to be explicitly told to the model you have to add the line to the getitem function of the dataset class and then it runs okay
63920887,whitelist tokens for text generation xlnet gpt in huggingfacetransformers,pythonx nlp pytorch huggingfacetransformers,id also suggest to do what sahar mills said you can do it in the following way you get the whole vocab of the model you are using eg define words you do want in the model define function to create the badwordsids that is the whole model vocab minus the words you want in the model hope it helps cheers
63867124,pytorch nlp sequence length of target in transformer,nlp pytorch mask transformermodel,thats because the entire aim is to generate the next token based on the tokens weve seen so far take a look at the input into the model when we get our predictions were not just feeding the source sequence but also the target sequence up until our current step the model inside modelspy looks like so you can see that the forward method receives src and trg which are each fed into the encoder and decoder this is a bit easier to grasp if you take a look at the model architecture from the original paper the outputs shifted right corresponds to trg in the code
63822730,why do we need the custom dataset class and use of getitem method in nlp bert fine tuning etc,nlp pytorch bertlanguagemodel,it is a recommended abstraction in pytorch to define datasets by inheriting torchutilsdatadataset those objects define how many elements are there len method and how to get a single item via specified index getitemindex its source code so its basically a thin wrapper which adds possibility to concatenate two dataset objects for readability and api compatibility you should inherit from it unlike the one provided in kaggle you can read more about pytorchs data functionality here
63672169,huggingface transformers model for german news classification,python nlp textclassification bertlanguagemodel huggingfacetransformers,you dont need to look for a specific text classification model when your classes are completely different because most listed models used one of the base models and finetuned the base layers and trained the output layers for their needs in your case you will remove the output layers and their finetuning of the base layers will not benefit or hurt you much sometimes they have extended the vocabulary which could be beneficial for your task but you have to check description which is often sparse and the vocabulary by yourself to get more details about the respective model in general i recommend you to work with one of the base models right away and only look for other models in case of insufficient results the following is an example for bert with classes from transformers import bertforsequenceclassification berttokenizer tokenizer berttokenizerfrompretrainedbertbasegermandbmdzuncased model bertforsequenceclassificationfrompretrainedbertbasegermandbmdzuncased numlabels
63651263,what is the state of gpt for text classification in spanish,nlp textclassification,gpt is only available via an api and only to people who apply for the access the model is too big to run it locally on any reasonable hardware and finetuning is thus hardly an option given how well gpt works machine translation my guess is that it will work reasonably well for spanish by default however if your task is text classification you can do a much better job when using a pretrained bertlike model hugginfaces transformers already have several models for spanish
63607919,tokens returned in transformers bert model from encode,python machinelearning nlp bertlanguagemodel huggingfacetransformers,you can call tokenizerconvertidstotokens to get the actual token for an id from transformers import berttokenizer tokenizer berttokenizerfrompretrainedbertbaseuncased tokens tokensappendtokenizerencodehello my dog is cute he is really nice tokensappendtokenizerencodehello my dog is cute he is really nice tokensappendtokenizerencodehello my dog is cute tokensappendtokenizerencodehello my dog is cute for t in tokens printtokenizerconvertidstotokenst output as you can see here each of your inputs was tokenized and special tokens were added according your model bert the encode function hasnt processed your lists properly which could be a bug or intended beheaviour depending on how you define it because their is a method for batch processing batchencodeplus tokenizerbatchencodeplushello my dog is cute he is really nice returntokentypeidsfalse returnattentionmaskfalse output im not sure why the encode method is not documented but it could be the case that huggingface wants us to use the call method directly tokens tokensappendtokenizerhello my dog is cute he is really nice returntokentypeidsfalse returnattentionmaskfalse tokensappendtokenizerhello my dog is cute he is really nice returntokentypeidsfalse returnattentionmaskfalse tokensappendtokenizerhello my dog is cute returntokentypeidsfalse returnattentionmaskfalse tokensappendtokenizerhello my dog is cute returntokentypeidsfalse returnattentionmaskfalse printtokens output
63491247,does bert and other language attention model only share crossword information in the initial embedding stage,deeplearning nlp bertlanguagemodel,the embeddings dont contain any information about the other embeddings around them bert and other models like opengptgpt dont have context dependent inputs the context related part comes later what they do in attention based models is use these input embeddings to create other vectors which then interact with each other and using various matrix multiplications summing normalizing and this helps the model understand the context which in turn helps it do interesting things including language generation etc when you say i would have expected to see a point in the model where the embedding for cat is combined with the embedding for dog in order to create the attention mask you are right that does happen just not at the embedding level we make more vectors by matrix multiplying the embeddings with learned matrices that then interact with each other
63380543,how many characters can be input into the prompt for gpt,python nlp openaiapi gpt,gpt does not work on characterlevel but on the subword level the maximum length of text segments in was trained on was subwords it uses a vocabulary based on bytepairencoding under such encoding frequent words remain intact infrequent words get split into several units eventually down to the byte level in practice the segmentation looks like this characters subwords at the training time there is no difference between the prompt and the answer so the only limitation is that the prompt and answer cannot be longer than subwords in total in theory you can continue generating beyond this but the history model considers can never be longer the selection of topk only influences memory requirements a long query also needs more memory but it is probably not the main limitation
63348829,wordsense disambiguation based on sets of words using pretrained embeddings,python nlp pytorch nltk spacy,transfer learning particularly models like allen ais elmo openais opengpt and googles bert allowed researchers to smash multiple benchmarks with minimal taskspecific finetuning and provided the rest of the nlp community with pretrained models that could easily with less data and less compute time be finetuned and implemented to produce state of the art results these representations will help you accuratley retrieve results matching the customers intent and contextual meaning even if theres no keyword or phrase overlap to start off embeddings are simply moderately low dimensional representations of a point in a higher dimensional vector space by translating a word to an embedding it becomes possible to model the semantic importance of a word in a numeric form and thus perform mathematical operations on it when this was first possible by the wordvec model it was an amazing breakthrough from there many more advanced models surfaced which not only captured a static semantic meaning but also a contextualized meaning for instance consider the two sentences below note that the word apple has a different semantic meaning in each sentence now with a contextualized language model the embedding of the word apple would have a different vector representation which makes it even more powerful for nlp tasks contextual embeddings like bert offers an advantage over models like wordvec because while each word has a fixed representation under wordvec regardless of the context within which the word appears bert produces word representations that are dynamically informed by the words around them
63216550,positional embedding in the transformer model does it change the words meaning,nlp transformermodel,wordvec and transformer treat tokens completely different wordvec is contextfree which means bank is always some fixed vector from the wordvec matrix in other words the vector of bank doesnt depend on the tokens position in the sentence on the other hand transformer as a input receives the tokes embeddings and positional embeddings to add a sense of position to the tokens otherwise it relates to the text as a bagofwords and not as a sequence
63201036,add additional layers to the huggingface transformers,python tensorflow keras nlp huggingfacetransformers,it looks like pooleroutput is a roberta and bert specific output but instead of using pooleroutput we can use a few hiddenstatesso not only last hidden state with all models we want to use them because papers report that hiddenstates can give more accuracy than just one lasthiddenstate import the needed modelbert roberta or distilbert with outputhiddenstatestrue transformermodel tfbertforsequenceclassificationfrompretrainedbertlargecased outputhiddenstatestrue inputids tfkerasinputshape dtypeint attentionmask tfkerasinputshape dtypeint transformer transformermodelinputids attentionmask hiddenstates transformer get outputhiddenstates hiddenstatessize count of the last states hiddesstatesind listrangehiddenstatessize selectedhiddesstates tfkeraslayersconcatenatetuplehiddenstatesi for i in hiddesstatesind now we can use selectedhiddesstates as we want output tfkeraslayersdense activationreluselectedhiddesstates output tfkeraslayersdense activationsigmoidoutput model tfkerasmodelsmodelinputs inputids attentionmask outputs output modelcompiletfkerasoptimizersadamlre lossbinarycrossentropy metricsaccuracy
63178631,should the queries keys and values of the transformer be split before or after being passed through the linear layers,deeplearning nlp pytorch transformermodel attentionmodel,yes i think the paper is quite confusing but according to the tutorial of pytorch lightning you can see its first linear then split i think in this way the linear layer will help you to decide how to distribute the values in each head edit i got more proves that is done before just look at the gpt architecture here you can see clearly it is done before
63152188,huggingface transformers berttokenizer changing characters,nlp huggingfacetransformers huggingfacetokenizers,it worked by using the berttokenizerfast and setting stripaccents false it appears as the error was in unicodenormalize in the strip accents function naturally one has to alter the vocabtxt file to make it match the bert tokenizer format
63112907,how to normalize output from bert classifier,pythonx nlp classification tfkeras huggingfacetransformers,yes you can use softmax to be more precise use an argmax over softmax to get label predictions like or this blog was helpful for me when i had the same query to answer your second question i would ask you to focus on what test instances that your classification model had misclassified than trying to find where the model went indecisive because argmax is always going to return or and never and i would say that a label will be the appropriate value for claiming your model to be indecisive
63040954,how to extract and use bert encodings of sentences for text similarity among sentences pytorchtensorflow,tensorflow deeplearning nlp pytorch bertlanguagemodel,when you want to compare the embeddings of sentences the recommended way to do this with bert is to use the value of the cls token this corresponds to the first token of the output after the batch dimension this will give you one embedding for the entire sentence as you will have the same size embedding for each sentence you can then easily compute the cosine similarity if you do not get satisfactory results using the cls token you can also try averaging the output embedding for each word in the sentence
63030692,how do i use bertformaskedlm or bertmodel to calculate perplexity of a sentence,nlp pytorch transformermodel huggingfacetransformers bertlanguagemodel,yes you can use the parameter labels or maskedlmlabels i think the param name varies in versions of huggingface transformers whatever to specify the masked token position and use to ignore the tokens that you dont want to include in the loss computing for example sentence from transformers import berttokenizer bertformaskedlm import torch import numpy as np tokenizer berttokenizervocabfilevocabtxt model bertformaskedlmfrompretrainedbertbasechinese tensorinput tokenizersentence returntensorspt tensor repeatinput tensorinputrepeattensorinputsize tensor mask torchonestensorinputsize diag tensor maskedinput repeatinputmaskedfillmask tensor labels repeatinputmaskedfill maskedinput tensor loss modelmaskedinput maskedlmlabelslabels score npexplossitem the function def scoremodel tokenizer sentence masktokenid tensorinput tokenizerencodesentence returntensorspt repeatinput tensorinputrepeattensorinputsize mask torchonestensorinputsize diag maskedinput repeatinputmaskedfillmask labels repeatinputmaskedfill maskedinput loss modelmaskedinput maskedlmlabelslabels result npexplossitem return result scoremodel tokenizer returns
62978957,sliding window for long text in bert for question answering,nlp textclassification huggingfacetransformers nlpquestionanswering bertlanguagemodel,i think there is a problem with the examples you pick both squadconvertexamplestofeatures and squadconvertexampletofeatures have a sliding window approach implemented because squadconvertexamplestofeatures is just a parallelization wrapper for squadconvertexampletofeatures but lets look at the single example function first of all you need to call squadconvertexampletofeaturesinit to make the tokenizer global this is done automatically for you in squadconvertexamplestofeatures from transformersdataprocessorssquad import squadresult squadvprocessor squadvprocessor squadconvertexamplestofeatures squadconvertexampletofeaturesinit from transformers import autotokenizer autoconfig squadconvertexamplestofeatures filedir tokenizer autotokenizerfrompretrainedbertbaseuncased squadconvertexampletofeaturesinittokenizer processor squadvprocessor examples processorgettrainexamplesfiledir features squadconvertexampletofeatures exampleexamples maxseqlength docstride maxquerylength istrainingtrue printlenfeatures output you might say that this function is not using a sliding window approach but this is wrong because your example doesnt needed to be split printlenexamplesquestiontextsplit lenexamplesdoctokens output which is less as the maxseqlength which you have set to now lets try a different one output which you can now compare with the original sample printcls examplesquestiontext sep joinexamplesdoctokens sep for idx f in enumeratefeatures printsplit formatidx print joinftokens output if my questions were when did kaggle make the announcement and how many registered users i can use chunk and chunk and not use chunk at all in the model not quiet sure if i should still use chunk to train the model yes you should also use chunk to train your model because when you try to predict the same sequence you hope that your model predicts as answer span for chunk ie you can easily select the chunk which contains the answer
62782001,using hugging face transformers library how can you postag french text,python nlp huggingfacetransformers bertlanguagemodel,we have ended up training a model for pos tagging part of speech tagging with the hugging face transformers library the resulting model is available here you can basically see how it assigns pos tags on the webpage mentioned above if you have the hugging face transformers library installed you can try it out in a jupyter notebook with this code this is the result on the console
62677651,openai gpt model use with tensorflow js,tensorflow machinelearning nlp tensorflowjs gpt,i dont see any reason as to why not other than maybe some operation that is in gpt that is not supported by tensorflowjs i dont know how to do it but heres a nice starting point installsh savepy that will give you a savedmodel file now you can try figure out the input and output nodes and use tensorflowjsconverter to try and convert it pointer
62595908,how to obtain contextual embedding for a phrase in a sentence using bert,nlp bertlanguagemodel,bert returns one vector per input subword so you need to get the vectors that correspond to the phrase you are interested in what is usually called a sentence embeddings is either the embedding of the technical symbol cls that is prepended to the sentence before processing it with bert or an average of the contextual subword vectors because the cls vector necessarily covers the entire sentence you cannot get it just for a subphrase but you can use the average of the subword embeddings of the phrase the package you are using sentencetransformers has a very simple userfriendly api but i am afraid it is not strong enough to do this job id suggest using huggingfaces transormers this package allows you to view how the sentence got tokenized and thus obtain the corresponding vectors
62525680,save only best weights with huggingface transformers,deeplearning nlp pytorch huggingfacetransformers,you may try the following parameters from trainer in the huggingface there may be better ways to avoid too many checkpoints and selecting the best model so far you can not save only the best model but you check when the evaluation yields better results than the previous one
62487267,fine tuning bert on medical dataset,python nlp huggingfacetransformers,yes this question is too general to be on stack overflow but ill try to give some helpful pointers try to look for any existing medical pretrained models otherwise finetune bertroberta on your domain or whatever downstream task classificationquestion answering youre working on such that it captures the unknown medical terms in your corpus
62466514,shall we lower case input data for pre training a bert uncased model using huggingface,deeplearning nlp pytorch huggingfacetransformers,tokenizer will take care of that a simple example out but in case of cased
62462878,customize the encode module in huggingface bert model,nlp textclassification huggingfacetransformers bertlanguagemodel,i think you can use the tokens in the bert vocab and add your custom tokens there so now you can easily refer to them with a valid token id
62405867,error running config robertaconfigfrompretrained absolutepathtobertweetbasetransformersconfigjson,nlp googlecolaboratory bertlanguagemodel huggingfacetransformers robertalanguagemodel,first of all you have to download the proper package as described in the github readme after that you can click on the directory icon left side of your screen and list the downloaded data right click on bertweetbasetransformers choose copy path and insert the content from your clipboard to your code config robertaconfigfrompretrained contentbertweetbasetransformersconfigjson bertweet robertamodelfrompretrained contentbertweetbasetransformersmodelbin configconfig
62405155,bertwordpiecetokenizer vs berttokenizer from huggingface,nlp huggingfacetransformers bertlanguagemodel huggingfacetokenizers,they should produce the same output when you use the same vocabulary in your example you have used bertbaseuncasedvocabtxt and bertbasecasedvocabtxt the main difference is that the tokenizers from the tokenizers package are faster as the tokenizers from transformers because they are implemented in rust when you modify your example you will see that they produce the same ids and other attributes encoding object while the transformers tokenizer only have produced the a list of ids from tokenizers import bertwordpiecetokenizer sequence hello yall how are you tokenizer tokenizerbw bertwordpiecetokenizercontentbertbaseuncasedvocabtxt tokenizedsequencebw tokenizerbwencodesequence printtokenizedsequencebw printtypetokenizedsequencebw printtokenizedsequencebwids output from transformers import berttokenizer tokenizerbt berttokenizercontentbertbaseuncasedvocabtxt tokenizedsequencebt tokenizerbtencodesequence printtokenizedsequencebt printtypetokenizedsequencebt output you mentioned in the comments that your questions is more about why the produced output is different as far as i can tell this was a design decision made by the developers and there is no specific reason for that it is also not a the case that bertwordpiecetokenizer from tokenizers is an inplace replacement for the berttokenizer from transformers they still use a wrapper to make it compatible with with the transformers tokenizer api there is a berttokenizerfast class which has a clean up method convertencoding to make the bertwordpiecetokenizer fully compatible therefore you have to compare the berttokenizer example above with the following from transformers import berttokenizerfast sequence hello yall how are you tokenizer tokenizerbw berttokenizerfastfrompretrainedbertbaseuncased tokenizedsequencebw tokenizerbwencodesequence printtokenizedsequencebw printtypetokenizedsequencebw output from my perspective they have build the tokenizers library independently from the transformers library with the objective to be fast and useful
62386631,cannot import bertmodel from transformers,python nlp pytorch huggingfacetransformers bertlanguagemodel,fixed the error this is the code
62385092,how to improve code to speed up word embedding with transformer models,nlp pytorch wordembedding huggingfacetransformers bertlanguagemodel,i dont think that there is a trivial way to significantly improve the speed without using a gpu some of the ways i could think of include smart batching which is used by sentencetransformers where you basically sort inputs of similar length together to avoid padding to the full token limit im not sure how much of a speedup this is going to get you but the only way that you can improve it significantly in a short period of time otherwise if you have access to google colab you can also utilize their gpu environment if the processing can be completed in reasonable time
62302499,huggingface bert which bert flavor is the fastest to train for debugging,machinelearning nlp huggingfacetransformers bertlanguagemodel,i think generally using a specific model for debugging can be critical and depends entirely on the kind of debugging you want to perform specifically consider the aspect of tokenization since each model also carries their own derivation of the basetokenizer class therefore any specifics of the respective model will only show up if you also use this specific tokenizer say eg you want to debug a later roberta implementation by using distilbert for debugging anything specific to robertas tokenization will not be the same in distilbert which uses berts tokenizer similarly any specifics to the training process might completely screw up the training from anecdotal evidence i had models train to completion and convergence with roberta but not on bert which makes the proposed solution of using different models for debugging a potentially dangerous substitution albert again has properties different from any of the above mentioned models but analogously the mentioned aspects still hold if you want to prototype services and simply require a model for in between i think both of the models suggested by you would do just fine and there should be only a minor difference in loadingsaving depending on the exact number of model parameters but keep in mind that inference time for applications is also something that is worth considering unless you are absolutely sure that there will not be any noticeable difference in the execution time at least make sure that you are testing with the full model as well
62206826,huggingface bert output printing,python nlp torch huggingfacetransformers spacytransformers,so first thing that you have to understand is the tokenised output given by bert if you look at the output it is already spaced i have written some print statements that will make it clear if you just want perfect output change the lines where i have added comments
62109957,why does the bert nsp head linear layer have two outputs,nlp pytorch transformermodel huggingfacetransformers bertlanguagemodel,the two scores are meant to represent unnormalized probabilities logits from the model if we softmax them we get our predictions where index indicates next sentence and index indicates random this is just a stylistic choice on the huggingface authors behalf probably to keep the loss function consistent heres the forward method of bertforpretraining where selfcls is bertonlynsphead its convenient to use the same crossentropyloss for both mlm and nsp as you describe it would be equivalent to have nsp produce a single output then feed that number through a sigmoid to get probability of the next sentence we can then train with bcewithlogitsloss where bce is just the special binary case of cross entropy loss
62072536,how to do language model training on bert,nlp pytorch huggingfacetransformers bertlanguagemodel,the raw only indicates that they use the raw version of the wikitext they are regular text files containing the raw text were using the raw wikitext no tokens were replaced before the tokenization the description of the data files options also says that they are text files from runlanguagemodelingpy ll traindatafile optionalstr field defaultnone metadatahelp the input training data file a text file therefore you can just specify your text files
61988776,how to calculate perplexity for a language model using pytorch,nlp pytorch huggingfacetransformers,as shown in wikipedia perplexity of a probability model the formula to calculate the perplexity of a probability model is the exponent is the crossentropy while logarithm base b is traditionally used in crossentropy deep learning frameworks such as pytorch use the natural logarithm b e therefore to get the perplexity from the crossentropy loss you only need to apply torchexp to the loss perplexity torchexploss the mean loss is used in this case the n part of the exponent and if you were to use the sum of the losses instead of the mean the perplexity would get out of hand exceedingly large which can easily surpass the maximum floating point number resulting in infinity
61826824,can you train a bert model from scratch with task specific architecture,machinelearning nlp bertlanguagemodel,bert can be viewed as a language encoder which is trained on a humongous amount of data to learn the language well as we know the original bert model was trained on the entire english wikipedia and book corpus which sums to m words bertbase has m model parameters so if you think you have large enough data to train bert then the answer to your question is yes however when you said still achieve a good result i assume you are comparing against the original bert model in that case the answer lies in the size of the training data i am wondering why do you prefer to train bert from scratch instead of finetuning it is it because you are afraid of the domain adaptation issue if not pretrained bert is perhaps a better starting point please note if you want to train bert from scratch you may consider a smaller architecture you may find the following papers useful wellread students learn better on the importance of pretraining compact models albert a lite bert for selfsupervised learning of language representations
61806293,deep learning nlp efficient bertlike implementations,keras deeplearning nlp pytorch multilabelclassification,if you want to use your current setup it will have no problem running a transformer model you can reduce memory use by reducing the batch size but at the cost of slower runs alternatively test your algorithm on google colab which is free then open a gcp account google will provide dollars of free credits use this to create a gpu cloud instance and then run your algorithm there you probably want to use albert or distilbert from huggingface transformers albert and distilbert are both compute and memory optimized huggingface has lots of excellent examples rule of thumb you want to avoid language model training from scratch if possible fine tune the language model or better yet skip it and go straight to the training the classifier also huggingface and others have medicalbert sciencebert and other specialized pretrained models
61736874,how to compare cosine similarities across three pretrained models,nlp gensim wordvec wordembedding glove,its certainly possible whether its meaningful given a certain amount of data is harder to answer note that in separate training sessions a given word a wont necessarily wind up in the same coordinates due to inherent randomness used by the algorithm thats even the case when training on the exact same data its just the case that in general the distancesdirections to other words b c etc should be of similar overall usefulness when theres sufficient datatraining and wellchosen parameters so a b c etc may be in different places with slightlydifferent distancesdirections but the relative relationships are still similar in terms of neighborhoodsofwords or the ab direction still be predictive of certain humanperceptible meaningdifferences if applied to other words c etc so you should avoid making direct cosinesimilarity comparisons between words from different trainingruns or corpuses but you may find meaning in differences in similarities ab vs a b or topn lists or relativerankings this could also be how to compare against rd corpora to what extent is there variance or correlation in certain pairwisesimilarities or topn lists or ordinal ranks of relevant words in each other words most similar results you might want to perform a sanity check on your measures by seeing to what extent they imply meaningful differences in comparisons where they logically shouldnt for example multiple runs against the exact same corpus thats just bee reshuffled or against random subsets of the exact same corpus im not aware of anything as formal as a ttest in checking the significance of differences between wordvec models but checking whether some differences are enough to distinguish a trulydifferent corpus from just a nth random subset of the same corpus to a certain confidence level might be a grounded way to assert meaningful differences to the extent such oughtta be very similar runs instead show end vector results that are tangibly different it could be suggestive that either the corpus is too small with too few varied usage examples per word wordvec benefits from lots of data and political speech collections may be quite small compared to the sometimes hundredsofbillions training words used for large wordvec models the model is misparameterized an oversized and thus prone to overfitting model or insufficient training passes or other suboptimal parameters may yield models that vary more for the same training data youd also want to watch out for mismatches in trainingcorpus size a corpus thats x as large means many more words would pass a fixed mincount threshold and any chosen n epochs of training will involve x as many examples of commonwords and support stable results in a larger vectorsize model whereas the same model parameters with a smaller corpus would give more volatile results another technique you could consider would be combining corpuses into one training set but munging the tokens of key wordsofinterest to be different depending on the relevant speaker for example youd replace the word family with ffamily or mfamily depending on the gender of the speaker you might do this for every occurrence or some fraction of the occurrences you might also enter each speech into your corpus more than once sometimes with the actual words and sometimes with someorall replaced with the contextlabeled alternates in that case youd wind up with one final model and all wordscontexttokens in the same coordinate space for direct comparison but the pseudowords ffamily and mfamily would have been more influenced by their contextspecific usages and thus their vectors might vary from each other and from the original family if youve also retained unmunged instances of its use in interestingly suggestive ways also note if using the analogysolving methods of the original google wordvec code release or other libraries that have followed its example like gensim note that it specifically wont return as an answer any of the words supplied as input so when solving the genderfraught analogy man doctor woman via the call modelmostsimilarpositivedoctor woman negativeman even if the underlying model still has doctor as the closest word to the targetcoordinates it is automatically skipped as one of the input words yielding the secondclosest word instead some early biasinwordvectors writeups ignored this detail and thus tended to imply larger biases due to this implementation artifact even where such biases smalltononexistent you can supply raw vectors instead of stringtokens to mostsimilar and then get full results without any filtering of inputtokens
61700506,what does theta mean in a language model,nlp stanfordnlp informationretrieval ngram languagemodel,theta is a conventionalstandard machine learning notation indicating strictly speaking a set of parameter values often more commonly known as the parameter vector the notation pyxtheta is to read as the yvalues eg mnist digit labels are predicted from the xvalues eg input images of mnist digits with the help of a trained model that is trained on annotated xy pairs this model is parameterized by theta obviously if the training algorithm changes so will the parameter vector theta the structure of these parameter vectors are usually interpreted from the model they are associated with eg for multilayered neural networks they indicate realvalued vectors initially randomly assigned and then updated by gradient descent at each iteration for word generation based language models they refer to the probability of a word v following a word u meaning that each element is an entry in a hashtable of the form u v countuvcountu these probabilities are learned from a training collection c of documents as a result of which they essentially become a function of the training set for a different collection these probability values will be different hence the usual convention is to write pwnpwntheta which basically indicates that these word succession probabilities are parameterized by theta a similar argument applies for documentlevel language models in information retrieval where the weights essentially indicate probabilities of sampling terms from documents
61656822,tensorflow hugging face transformers tfbertforsequenceclassification unexpected output dimensions in inference,python tensorflow machinelearning nlp huggingfacetransformers,i found the problem if you get unexpected dimensions when using tensorflow datasets tfdatadataset it might be because of not running batch so in my example adding makes this work as i would expect so this is not a problem related to tfbertforsequenceclassification and only due to my input being incorrect i also want to add a reference to this answer which made me find the problem
61588381,speed up embedding of m sentences with roberta,python nlp wordembedding transformermodel,i found a ridiculous speedup using this package by feeding in the utterances as a list instead of looping over the list i assume there is some nice internal vectorisation going on the full code would be as follows
61569900,getting embedding lookup result from bert,python tensorflow nlp huggingfacetransformers bertlanguagemodel,it is in fact incorrect to treat the first output result as the result of an embedding lookup the raw embedding lookup is given by while result gives the embedding lookup plus positional embeddings and token type embeddings the above code does not require a full pass through bert and the result can be processed prior to feeding into the remaining layers of bert edit to get the result of addition positional and token type embeddings to an arbitrary inputsembeds one can use here the call method for the embeddings object accepts a list which is fed into the embeddings method the first value is inputids the second positionids the third tokentypeids and the fourth inputsembeds see here for more details if you have multiple sentences in one input you may need to set positionids
61567599,huggingface bert giving unexpected result,python tensorflow nlp huggingfacetransformers bertlanguagemodel,my intuition about positional and token type embeddings being added in turned out to be correct after looking closely at the code i replaced the line with the lines now the difference is as expected
61550968,implementation details of positional encoding in transformer model,encoding deeplearning nlp transformermodel attentionmodel,your implementation is basically correct the typical implementation is precomputing the embedding matrix make a nontrainable embedding layer and do an embedding lookup of a range see eg the implementation in huggingfaces transformers some hints about the intuition behind the equations are in these threads on crossvalidated on reddit but it seems to me that pretty much all decisions about the position encoding were empirical choices by cyclic properties they imho mean that given a dimension of the embedding the difference of the embedding values between positions with a constant offset is the same regardless of the position in the sequence for that using either only sine or cosine might be enough but some positions would have a much larger norm that the others therefore they alternate sine and cosine i think the scaling factors are empirically estimated to cover the usual length of sentences with padding you indeed consider also the positional encoding of the padded positions but since they are precomputed it does mean higher computation load because you get the embeddings for the padding symbols anyway
61350737,is it possible to finetune bert to do retweet prediction,machinelearning nlp bertlanguagemodel,you can finetune bert and you can use bert to do retweet prediction but you need more architecture in order to predict if user i will retweet tweet j here is an architecture off the top of my head at a high level create a dense vector representation embedding of user i perhaps containing something about the users interests such as sports create an embedding of tweet j create an embedding of the combination of the first two embeddings together such as with concatenation or hadamard product feed this embedding through a nn that performs binary classification to predict retweet or nonretweet lets break this architecture down by item to create an embedding of user i you will need to create some kind of neural network that accepts whatever features you have about the user and produces a dense vector this part is the most difficult component of the architecture this area is not in my wheelhouse but a quick google search for user interest embedding brings up this research paper on an algorithm called starspace it suggests that it can obtain highly informative user embeddings according to user behaviors which is what you want to create an embedding of tweet j you can use any type of neural network that takes tokens and produces a vector research prior to would have suggested using an lstm or a cnn to produce the vector however bert as you mentioned in your post is the current stateoftheart it takes in text or text indices and produces a vector for each token one of those tokens should have been the prepended cls token which commonly is taken to be the representation of the whole sentence this article provides a conceptual overview of the process it is in this part of the architecture that you can finetune bert this webpage provides concrete code using pytorch and the huggingface implementation of bert to do this step ive gone through the steps and can vouch for it in the future youll want to google for bert single sentence classification to create an embedding representing the combination of user i and tweet j you can do one of many things you can simply concatenate them together into one vector so if user i is an mdimensional vector and tweet j is an ndimensional vector then the concatenation produces an mndimensional vector an alternative approach is to compute the hadamard product elementwise multiplication in this case both vectors must have the same dimension to make the final classification of retweet or notretweet build a simple nn that takes the combination vector and produces a single value here since you are doing binary classification a nn with a logistic sigmoid function would be appropriate you can interpret the output as the probability of retweeting so a value above would be to retweet see this webpage for basic details on building a nn for binary classification in order to get this whole system to work you need to train it all together endtoend that is you have to get all the pieces hooked up first and train it rather than training the components separately your input dataset would look something like this if you want an easier route where there is no user personalization then just leave out the components that create an embedding of user i you can use bert to build a model to determine if the tweet is retweeted without regard to user you can again follow the links i mentioned above
61221810,confusion in preprocessing text for roberta model,nlp pytorch huggingfacetransformers bertlanguagemodel,the easiest way is probably to directly use the provided function by huggingfaces tokenizers themselves namely the textpair argument in the encode function see here this allows you to directly feed in two sentences which will be giving you the desired output from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedrobertabase sequence tokenizerencodetextvery severe pain in hands textpairnumbness of upper limb addspecialtokenstrue this is especially convenient if you are dealing with very long sequences as the encode function automatically reduces your lengths according to the truncactionstrategy argument you obviously dont have to worry about this if it is only short sequences alternatively you can also make use of the more explicit buildinputswithspecialtokens function of the robertatokenizer specifically which could be added to your example like so from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedrobertabase list tokenizerencodevery severe pain in hands addspecialtokensfalse list tokenizerencodenumbness of upper limb addspecialtokensfalse sequence tokenizerbuildinputswithspecialtokenslist list note that in that case you have to generate the sequences list and list still without any special tokens as you have already done correctly
61157314,runtimeerror unknown device when trying to run albertformaskedlm on colab tpu,nlp pytorch tpu huggingfacetransformers tensorflowxla,solution is here before calling modeltodev you need to call xmsendcpudatatodevicemodel xmxladevice there are also some issues with getting the gelu activation function albert uses to work on the tpu so you need to use the following branch of transformers when working on tpu see the following colab notebook by for full solution
61134980,trying to adapt pretrained bert to another use case of semantic separation of sentences,python nlp huggingfacetransformers,i think the better question is to refine how you are framing the task if in fact the constituents are nonoverlapping this might be a case for bertfortokenclassification essentially you are trying to predict the labels of each individual token in your case either something like no label subject or object a great example for this kind of task is named entity recognition ner which is generally framed in a similar fashion specifically huggingfaces transformer repository has a very extensive example available for you that can serve as inspiration on how to format inputs and how to train properly
61134275,difficulty in understanding the tokenizer used in roberta model,nlp pytorch huggingfacetransformers bertlanguagemodel,this question is extremely broad so im trying to give an answer that focuses on the main problem at hand if you feel the need to have other questions answered please open another question focusing on one question at a time see the helpontopic rules for stackoverflow essentially as youve correctly identified bpe is central to any tokenization in modern deep networks i highly recommend you to read the original bpe paper by sennrich et al in which they also highlight a bit more of the history of bpes in any case the tokenizers for any of the huggingface models are pretrained meaning that they are usually generated from the training set of the algorithm beforehand common implementations such as sentencepiece also give a bit better understanding of it but essentially the task is framed as a constrained optimization problem where you specify a maximum number of k allowed vocabulary words the constraint and the algorithm tries to then keep as many words intact without exceeding k if there are not enough words to cover the whole vocabulary smaller units are used to approximate the vocabulary which results in the splits observed in the example you gave roberta uses a variant called bytelevel bpe the best explanation is probably given in this study by wang et al the main benefit is that it results in a smaller vocabulary while maintaining the quality of splits from what i understand the second part of your question is easier to explain while bert highlights the merging of two subsequent tokens with robertas tokenizer instead highlights the start of a new token with a specific unicode character in this case u the g with a dot the best reason i could find for this was this thread which argues that it basically avoids the use of whitespaces in training
61121982,asking gpt to finish sentence with huggingface transformers,nlp pytorch huggingfacetransformers gpt,unfortunately there is no way to do so you can set the length parameter to a greater value and then just discard the incomplete part at the end even gpt doesnt support completing a sentence before a specific length gpt support sequences though sequences force the model to stop when certain condition is fulfilled you can find more information about in thi article
61072673,reduce the number of hidden units in hugging face transformers bert,python pythonx nlp pytorch huggingfacetransformers,changing the dimensionality would mean changing all the model parameters ie retraining the model this could be achievable by knowledge distillation but it will be probably still quite computationally demanding you can also use some dimensionality reduction techniques on the bert outputs like pca available eg in scikitlearn in that case i would suggest taking several thousand bert vectors fit the pca and then apply the pca on all the remaining vectors
60937617,how to reconstruct text entities with hugging faces transformers pipelines without iob tags,nlp tokenize transformermodel namedentityrecognition huggingfacetransformers,edit as pointed out the groupedentities parameter has been deprecated the correct way is to use the aggregationstrategy parameters as pointed in the source code for instance gives the following output original answer the th of may a new pull request with what you are asking for has been merged therefore now our life is way easier you can you it in the pipeline like and your output will be as expected at the moment you have to install from the master branch since there is no new release yet you can do it via
60923314,i fine tuned a pretrained bert for sentence classification but i cant get it to predict for new sentences,python machinelearning nlp pytorch huggingfacetransformers,the output of the models are logits ie the probability distribution before normalization using softmax if you take your output and run softmax over it in import torch in import torchnnfunctional as f in fsoftmaxtorchtensor out tensore e you get get probability score for label when you have the logits as a d tensor you can easily get the classes by calling logitsargmax the nans values in your truelabels are probably a bug in how you load the data it has nothing to do with the bert model
60914449,define and use new smoothing method in nltk language models,nlp nltk smoothing languagemodel,here you can see the definition of mle as you can see there is no option of a smoothing function but there are others in the same file probably some of them fits your needs the interpolatedlanguagemodel see same file above does accept a smoothing classifier which needs to implement alphagammaword context and unigramscoreword and be a subclass of smoothing so if you really need to add functionality to the mle class you could do something like that but i am not sure if this is a good idea
60897514,how to load bertforsequenceclassification models weights into bertfortokenclassification model,nlp pytorch namedentityrecognition bertlanguagemodel,you can get weights from the bert inside the first model and load into the bert inside the second
60847291,confusion in understanding the output of bertfortokenclassification class from transformers library,nlp pytorch huggingfacetransformers bertlanguagemodel,if you check the source code specifically bertencoder you can see that the returned states are initialized as an empty tuple and then simply appended per iteration of each layer the final layer is appended as the last element after this loop see here so we can safely assume that hiddenstates is the final vectors
60839967,keyword arguments in bert call function,tensorflow nlp arguments huggingfacetransformers,it seems that internally they are interpreting the inputs as inputids if you do not put more than just a single tensor as the first argument you can see this in tfbertmodel and then looking for tfbertmainlayers call function for me i get exactly the same results as result and result if i do the following alternatively you can also just drop the inputs works as well
60833301,train huggingfaces gpt from scratch assert nstate confignhead error,python nlp huggingfacetransformers transformermodel gpt,i think the error message is pretty clear assert nstate confignhead tracing it back through the code we can see nstate nx in attention nstate which indicates that nstate represents the embedding dimension which is generally by default in bertlike models when we then look at the gpt documentation it seems the parameter specifying this is nembd which you are setting to as the error indicates the embedding dimension has to be evenly divisible through the number of attention heads which were specified as so choosing a different embedding dimension as a multiple of should solve the problem of course you can also change the number of heads to begin with but it seems that odd embedding dimensions are not supported
60780181,access the output of several layers of pretrained distilbert model,python nlp pytorch bertlanguagemodel huggingfacetransformers,if you want to get the output of all the hidden layers you need to add the outputhiddenstatestrue kwarg to your config your code will look something like the hidden layers will be made available as bertoutput
60539758,biobert for keras version of huggingface transformers,keras nlp keraslayer huggingfacetransformers,might be a bit late but i have found a not so elegant fix to this problem the tf bert models in the transformers library can be loaded with a pytorch save file step convert the tf checkpoint to a pytorch save file with the following command more here step make sure to combine the following files in a directory configjson bert config file must be renamed from bertconfigjson pytorchmodelbin the one we just converted vocabtxt bert vocab file step load model from the directory we just created there is actually also an argument fromtf which according to the documentation should work with tf style checkpoints but i cant get it to work see
60505798,why can berts three embeddings be added,vector nlp embedding transformermodel bertlanguagemodel,firstly these vectors are added elementwise the size of the embeddings stays the same secondly position plays a significant role in the meaning of a token so it should somehow be part of the embedding attention the token embeddinng does not necessarily hold semantic information as we now it from wordvec all those embeddingstoken segment and position are learned together in pretraining so that they best accomplish the tasks together in pretraining they are already added together so they are trained especially for this case direction of vectors do change with this addition but the new direction gives important information to the model packed in just one vector note each vector is huge dimensions in the base model
60492839,how to compare sentence similarities using embeddings from bert,python vector nlp cosinesimilarity huggingfacetransformers,in addition to an already great accepted answer i want to point you to sentencebert which discusses the similarity aspect and implications of specific metrics like cosine similarity in greater detail they also have a very convenient implementation online the main advantage here is that they seemingly gain a lot of processing speed compared to a naive sentence embedding comparison but i am not familiar enough with the implementation itself importantly there is also generally a more finegrained distinction in what kind of similarity you want to look at specifically for that there is also a great discussion in one of the task papers from semeval sick dataset which goes into more detail about this from your task description i am assuming that you are already using data from one of the later semeval tasks which also extended this to multilingual similarity
60486655,need to fine tune a bert model to predict missing words,python nlp bertlanguagemodel,bert is a masked language model meaning it is trained on exactly this task that is why it can do it so in that sense no fine tuning is needed however if the text you will see at runtime is different than the text bert was trained on your performance may be much better if you fine tune on the type of text you expect to see
60418179,bert fine tuning,nlp bertlanguagemodel,fine tuning is adopting refining the pretrained bert model to two things domain task eg classification entity extraction etc you can use pretrained models asis at first and if the performance is sufficient fine tuning for your use case may not be needed
60390971,how to give one sample text input to a pretrained lstm model,python machinelearning deeplearning nlp lstm,prediction can be done using predict function as below ypredict modelpredictxte batchsizebatchsize where xte is the preprocessed testset the preprocessing is generally same for trainingset and testset in case if you want to predict for a single instance from the test set the input has to be reshaped as given below ypred modelpredictxtereshape
60382793,what are the inputs to the transformer encoder and decoder in bert,python deeplearning nlp huggingfacetransformers,ah but you see bert does not include a transformer decoder it is only the encoder part with a classifier added on top for masked word prediction the classifier acts as a decoder of sorts trying to reconstruct the true identities of the masked words classifying nonmasked is not included in the classification task and does not effect loss bert is also trained on predicting whether a pair of sentences really does precedes one another or not i do not remember how the two losses are weighted i hope this draws a clearer picture
60286735,gensims docvec how to use pretrained wordvec word similarities,python nlp gensim docvec,just documents is way too small to meaningfully train a docvec or wordvec model published docvec work tends to use tensofthousands to millions of documents to the extent you may be able to get slightly meaningful results from smaller datasets youll usually need to reduce the vectorsizes a lot to far smaller than the number of wordsexamples and increase the training epochs your toy data has texts unique words even to get dimensional vectors you probably want something like constrasting documents also gensims docvec doesnt offer any official option to import wordvectors from elsewhere the internal docvec training is not a process where wordvectors are trained st then docvectors calculated rather docvectors wordvectors are trained in a simultaneous process gradually improving together some modes like the fast often highly effective dbow that can be enabled with dm dont create or use wordvectors at all theres not really anything bizarre about your sentence results when looking at the data as if we were the docvec or wordvec algorithms which have no prior knowledge about words only whats in the training data in your training data the token love and the token hate are used in nearly exactly the same way with the same surrounding words only by seeing many subtly varied alternative uses of words alongside many contrasting surrounding words can these dense embedding models move the wordvectors to useful relative positions where they are closer to related words farther from other words and since youve provided no training data with the token adore the model knows nothing about that word and if its provided inside a test document as if to the models infervector method it will be ignored so the test document it sees is only the known words i hot chocolate but also even if you did manage to train on a larger dataset or somehow inject the knowledge from other wordvectors that love and adore are somewhat similar its important to note that antonyms are typically quite similar in sets of wordvectors too as they are used in the same contexts and often syntactically interchangeable and of the same general category these models often arent very good at detecting the flipinhumanperceived meaning from the swapping of a word for its antonym or insertion of a single not or other reversingintent words ultimately if you want to use gensims docvec you should train it with far more data if you were willing to grab some other pretrainined wordvectors why not grab some other source of somewhatsimilar bulk sentences the effect of using data that isnt exactly like your actual problem will be similar whether you leverage that outside data via bulk text or a pretrained model finally its a bad errorprone pattern to be calling train more than once in your own loop with your own alpha adjustments you can just call it once with the right number of epochs and the model will perform the multiple training passes manage the internal alpha smoothly over the right number of epochs
60220842,how should properly formatted data for ner in bert look like,python nlp format bertlanguagemodel huggingfacetransformers,update the tutorial link points to a legacy tutorial which i dont fully recommend anymore since it does not use huggingfaces convenience library datasets there is actually a great tutorial for the ner example on the huggingface documentation page specifically it also goes into detail how the provided script does the preprocessing specifically there is a link to an external contributors preprocesspy script that basically takes the data from the conll format to whatever is required by the huggingface library i found this to be the easiest way to assert i have proper formatting and unless you have some specific changes that you might want to incorporate this gets you started super quick without worrying about implementation details the linked example script also provides more than enough detail on how to feed the respective inputs into the model itself but generally you are correct in your abovementioned input pattern
60142937,huggingface transformers for text generation with ctrl with google colabs free gpu,python deeplearning nlp pytorch huggingfacetransformers,the solution was to increase the ram since i was using the google colabs free gpu i was going through this github issue and found this useful solution the following piece of code will crash the session in colab and select get more ram which will increase the ram up to gb
60120043,optimizer and scheduler for bert finetuning,nlp pytorch huggingfacetransformers,here you can see a visualization of learning rate changes using getlinearschedulerwithwarmup referring to this comment warm up steps is a parameter which is used to lower the learning rate in order to reduce the impact of deviating the model from learning on sudden new data set exposure by default number of warm up steps is then you make bigger steps because you are probably not near the minima but as you are approaching the minima you make smaller steps to converge to it also note that number of training steps is number of batches number of epochs but not just number of epochs so basically numtrainingsteps nepochs is not correct unless your batchsize is equal to the training set size you call schedulerstep every batch right after optimizerstep to update the learning rate
60029211,how can i get the logit values as probabilities from gpt,python tensorflow keras nlp tensor,get list of decoded tokens get probabilities
59944537,is there a gpt implementation that allows me to finetune and prompt for text completion,pythonx deeplearning nlp openaigym gpt,huggingfaces transformers package has a gpt implementation including pretrained models for pytorch and tensorflow you can easily work with them in python finetuning of gpt however requires a lot of memory and i am not sure is you will be able to do the full backpropagation on that in that case you finetune just a few highest layers
59590993,where can i download a pretrained wordvec map,python nlp wordvec wordembedding,you can try out googles wordvec model trained with about billion words from various news articles an interesting fact about word vectors wvking wvman wvwoman wvqueen
59497734,how can i get an alignment for two different tokenizations eg bert vs spacy,algorithm nlp,i came up with an algorithm based on shortest edit script for this question and created a python library tokenizations written in rust repository
59384146,why do transformers in natural language processing need a stack of encoders,machinelearning deeplearning nlp transformermodel,stacking layer is what makes any deep learning architecture powerful using a single encoderdecoder with attention wouldnt be able to capture the complexity needed to model an entire language or archive high accuracy on tasks as complex as language translation the use of stacks of encoderdecoders allows the network to extract hierarchical features and model complex problems
59327637,how do i train gpt from scratch,python machinelearning nlp nlg,i found the answer in if you want to not use the released model at all for instance because you want to train a model with incompatible hyperparameters it should be sufficient to just skip the restore from the released model checkpoint around trainpy on your first run so the parameters will all be randomly initialized
59315138,how to get words from output of xlnet using transformers library,nlp masking transformermodel languagemodel huggingfacetransformers,the output you have is a tensor of size by by vocabulary size the meaning of the nth number in this tensor is the estimated logodds of the nth vocabulary item so if you want to get out the word that the model predicts to be most likely to come in the final position the position you specified with targetmapping all you need to do is find the word in the vocabulary with the maximum predicted logodds just add the following to the code you have so predictedtoken is the token the model predicts as most likely in that position note by default behaviour of xlnettokenizerencoder adds special tokens and to the end of a string of tokens when it encodes it the code you have given masks and predicts the final word which after running though tokenizerencoder is the special character which is probably not what you want that is when you run tokenizerencodei went to york and saw the building the result is a list of token ids which if you convert back to tokens by calling tokenizerconvertidstotokens on the above id list you will see has two extra tokens added at the end i went to york and saw the building so if the word you are meaning to predict is building you should use permmask and targetmapping
59282572,memory efficiently loading of pretrained word embeddings from fasttext library with gensim,python nlp gensim wordembedding fasttext,as vectors will typically take at least as much addressablememory as their ondisk storage it will be challenging to load fullyfunctional versions of those vectors into a machine with only gb ram in particular once you start doing the most common operation on such vectors finding lists of the mostsimilar words to a target wordvector the gensim implementation will also want to cache a set of the wordvectors thats been normalized to unitlength which nearly doubles the required memory current versions of gensims fasttext support through at least also waste a bit of memory on some unnecessary allocations especially in the fullmodel case if youll only be using the vectors not doing further training youll definitely want to use only the loadfacebookvectors option if youre willing to give up the models ability to synthesize new vectors for outofvocabulary words not seen during training then you could choose to load just a subset of the fullword vectors from the plaintext vec file for example to load just the st k vectors because such vectors are typically sorted to put the morefrequentlyoccurring words first often discarding the long tail of lowfrequency words isnt a big loss
59172532,bert finetuned for semantic similarity,nlp cosinesimilarity pearsoncorrelation sentencesimilarity,in addition if youre after a binary verdict yesno for semantically similar bert was actually benchmarked on this task using the mrpc microsoft research paraphrase corpus the google github repo includes some example calls for this see tasknamemrpc in section sentence and sentencepair classification tasks
59030907,nlp transformers best way to get a fixed sentence embeddingvector shape,machinelearning deeplearning nlp pytorch wordembedding,this is quite a general question as there is no one specific right answer as you found out of course the shapes differ because you get one output per token depending on the tokenizer those can be subword units in other words you have encoded all tokens into their own vector what you want is a sentence embedding and there are a number of ways to get those with not one specifically right answer particularly for sentence classification wed often use the output of the special classification token when the language model has been trained on it camembert uses note that depending on the model this can be the first mostly bert and children also camembert or the last token ctrl gpt openai xlnet i would suggest to use this option when available because that token is trained exactly for this purpose if a cls or or similar token is not available there are some other options that fall under the term pooling max and mean pooling are often used what this means is that you take the max value token or the mean over all tokens as you say the danger is that you then reduce the vector value of the whole sentence to some average or some max that might not be very representative of the sentence however literature shows that this works quite well as well as another answer suggests the layer whose output you use can play a difference as well iirc the google paper on bert suggests that they got the best score when concatenating the last four layers this is more advanced and i will not go into it here unless requested i have no experience with fairseq but using the transformers library id write something like this camembert is available in the library from v import torch from transformers import camembertmodel camemberttokenizer text salut comment vastu tokenizer camemberttokenizerfrompretrainedcamembertbase encode automatically adds the classification token tokenids tokenizerencodetext tokens tokenizerconvertidtotokenidx for idx in tokenids printtokens unsqueeze tokenids because batchsize tokenids torchtensortokenidsunsqueeze printtokenids load model model camembertmodelfrompretrainedcamembertbase forward method returns a tuple we only want the logits squeeze because batchsize output modeltokenidssqueeze only grab output of cls token which is the first token clsout output printclsoutsize printed output is in order the tokens after tokenisation the token ids and the final size
58812949,tf transformer model valueerror shapes and are incompatible,python nlp tensorflow,the problem was with loading the tokenizers there were punctuations which were getting saved while using the save function but on load some of these failed to load dont know the root cause and that caused the mismatch in shapes once i cleaned the text to remove punctuation and extra spaces it worked fine
58794613,train ner spacy using entrfbertbaseuncasedlg model,nlp spacy namedentityrecognition,according to the documentation of this model on spacy here this model doesnt support namedentity recognition yet it only supports sentencizer trfwordpiecer trftokvec you can get the available pipe for a given model like so
58541811,how to use bert just for entity extraction from a sequence without classification in the ner task,nlp pytorch namedentityrecognition namedentityextraction bertlanguagemodel,regardless bert ner tagging is usually done by tagging with the iob format inside outside beginning or something similar often the end is also explicitly tagged the inside and beggining tags contain the entity type something like this if you modify your training data such that there will be only one entity type the model will only learn to detect the entities without knowing what type the entity is
58518980,extracting fixed vectors from biobert without using terminal command,pythonx nlp pytorch,you can get the contextual embeddings on the fly but the total time spend on getting the embeddings will always be the same there are two options how to do it import biobert into the transformers package and treat use it in pytorch which i would do or use the original codebase import biobert into the transformers package the most convenient way of using pretrained bert models is the transformers package it was primarily written for pytorch but works also with tensorflow it does not have biobert out of the box so you need to convert it from tensorflow format yourself there is converttfcheckpointtopytorchpy script that does that people had some issues with this script and biobert seems to be resolved after you convert the model you can load it like this import torch from transformers import load dataset tokenizer model from pretrained modelvocabulary tokenizer berttokenizerfrompretraineddirectorywithconvertedmodel model bertmodelfrompretraineddirectorywithconvertedmodel call the model in a standard pytorch way embeddings modeltokenizerencodecool biomedical tetrahydrosentence addspecialtokenstrue use directly biobert codebase you can get the embeddings on the go basically using the code that is exctractfeautrespy on lines they initialize the model you get the embeddings by calling estimatorpredict for that you need to format your format the input first you need to format the string using code on line and then apply and call convertexamplestofeatures on it
58334235,what is currently the best way to add a custom dictionary to a neural machine translator that uses the transformer architecture,neuralnetwork nlp transformermodel machinetranslation seqseq,i am afraid you cannot easily do that you cannot easily add new words to the vocabulary because you dont know what embedding it would get during training you can try to remove some words or alternatively you can manually change the bias in the final softmax layer to prevent some words from appearing in the translation anything else would be pretty difficult to do what you want to do is called domain adaptation to get an idea of how domain adaptation is usually done you can have a look at a survey paper the most commonly used approaches are probably model finetuning or ensembling with a language model if you want to have parallel data in your domain you can try to finetune your model on that parallel data with simple sgd small learning rate if you only have monolingual data in the target language you train a language model on that data during the decoding you can mix the probabilities from the domainspecific language and the translation model unfortunately i dont know of any tool that could do this out of the box
58309837,how does ulmfits language model work when applied on a text classification problem,nlp lstm textclassification languagemodel fastai,ulmfits model is a regular lstm which is a special case of a recurrent neural network rnn rnns eat the input text word by word sometimes character by character and after every bite they produce an output update an internal hidden state in text classification the output is discarded until the very end the updated hidden state is instead added to the next word to bite after the rnn ate the last word you can check the output layer typically a softmax layer with as many neurons as your labels compute the loss against the true label then update the weights accordingly after the training phase suppose you want to classify a new document the rnn eats the input again and updates its hidden state after each word you disregard the output layer until you see the last word at that point the max element of the output softmax layer will be your predicted label i found particularly helpful this pytorch tutorial
58308257,bert sentence embedding by summing last layers,python neuralnetwork nlp pytorch,you create a list using a list comprehension that iterates over tokenembeddings it is a list that contains one tensor per token not one tensor per layer as you probably thought judging from your for layer in tokenembeddings you thus get a list with a length equal to the number of tokens for each token you have a vector that is a sum of bert embeddings from the last layers more efficient would be avoiding the explicit for loops and list comprehenions summedlastlayers torchstackencodedlayerssum now variable summedlastlayers contains the same data but in the form of a single tensor of dimension length of the sentence to get a single ie pooled vector you can do pooling over the first dimension of the tensor maxpooling or averagepooling might make much more sense in this case than summing all the token embeddings when summing the values vectors of differently long sentences are in different ranges and are not really comparable
58244540,my current finetuned bert model saved on physical space takes gb of space is it normal for the model to take such large amount of space,tensorflow neuralnetwork nlp,lets count the parameters of the model for the bert base model it is embeddings k wordpiece embeddings with dimensions m transformer layers each with feedforward layer with projection m projections queries keys and values in selfattention for heads m projection from head context vector into a single context vector m this is m parameters per layer m this is m parameters for the entire model standard floats in tensorflow have bits ie bytes this gives you roughly gib only bert parameters you need some space for the model definition you classifier also has some parameters my estimate is that your saved model should be slightly over gib adam optimizer keeps momenta for all parameters ie in the end you save three numbers per parameter which makes the saved model three times bigger this might be your case as well
58084661,how are token vectors calculated in spacypytorchtransformers,python nlp pytorch spacy spacytransformers,it seems that there is a more elaborate weighting scheme behind this which also accounts for the cls and sep token outputs in each sequence this has also been confirmed by an issue post from the spacy developers unfortunately it seems that this part of the code has since moved with the renaming to spacytransformers
57960995,how are the tokenembeddings in bert created,machinelearning nlp wordembedding,the wordpieces are trained separately such the most frequent words remain together and the less frequent words get split eventually down to characters the embeddings are trained jointly with the rest of bert the backpropagation is done through all the layers up to the embeddings which get updated just like any other parameters in the network note that only the embeddings of tokens which are actually present in the training batch get updated and the rest remain unchanged this also a reason why you need to have relatively small wordpiece vocabulary such that all embeddings get updated frequently enough during the training
57837315,how to install a language model,python nlp spacy,make sure to activate your environment using virtualenv or conda and install spacy as aris mentioned to install spacy to install a specific model run the following command with the model name for example encoreweblg to load a model use spacyload with the model name a shortcut link or a path to the model data directory you can also import a model directly via its full name and then call its load method with no arguments this should also work for older models in previous versions of spacy
57782409,set the number of iterations gpt,python tensorflow nlp gpt,all you have to do is to modify the while true loop to a for loop try replaced while true for i in range if counter argssaveevery save if counter argssampleevery generatesamples if argsvalevery and counter argsvalevery or counter validation if argsaccumulategradients sessrunoptreset for in rangeargsaccumulategradients sessrun optcompute feeddictcontext samplebatch vloss vsummary sessrunoptapply summaries else vloss vsummary sessrun optapply loss summaries feeddictcontext samplebatch summarylogaddsummaryvsummary counter avgloss avgloss vloss avgloss print counter timef losslossf avgavgf format countercounter timetimetime starttime lossvloss avgavgloss avgloss counter except keyboardinterrupt printinterrupted save
57500159,how to get sentence embeddings from encoder in fastai learner language model,machinelearning nlp pytorch fastai,this should give you the encoderwhich is an embedding layer learnmodelencoder
57481599,how to build input to predict with saved model for bert squad with tensorflow,python tensorflow deeplearning nlp nlpquestionanswering,i figured it out i need to use tftrainexample function def createintfeaturevalues f tftrainfeatureintlisttftrainintlistvaluelistvalues return f inputs collectionsordereddict inputsinputids createintfeaturefeaturesinputids inputsinputmask createintfeaturefeaturesinputmask inputssegmentids createintfeaturefeaturessegmentids inputsuniqueids createintfeaturefeaturesuniqueid tfexample tftrainexamplefeaturestftrainfeaturesfeatureinputs out predictfnexamplestfexampleserializetostring
57475889,could i use bert to cluster phrases with pretrained model,tensorflow nlp pytorch gensim wordvec,you can feed a phrase into the pretrained bert model and get an embedding ie a fixeddimension vector so bert can embed your phrases in a space then you can use a clustering algorithm such as kmeans to cluster the phrases the phrases do not need to occur in the training corpus of bert as long as the words they consist of are in the vocabulary you will have to try to see if the embeddings give you relevant results
57378005,how to transform berts network output to readable text,python nlp pytorch,i think you are trying to use bert for qa where the answer is a span of the original text the original paper uses this for the squad dataset where this is the case startlogits and endlogits have the logits for a token being the startend of the answer so you can take argmax and it would be the index of the token in the text there is this naacl tutorial on tranfer learning from the author of the repo you linked its using classification as the target task but you might still find it useful
57248098,using huggingfaces pytorch transformers gpt for classifcation tasks,python nlp pytorch languagemodel huggingfacetransformers,so i can not do what as the paper said for a classification task just add a fully connected layer in the tail this is the answer to your question usually transformers like bert and roberta have bidirectional selfattention and they have the cls token where we feed in to the classfier since gpt is leftright you need to feed the final token of the embeddings sequence ps can you put the link to the paper
56746191,why are the matrices in bert called query key and value,python tensorflow deeplearning nlp bertlanguagemodel,for your first question bert is based on the encoder of the transformer model from the vaswani et al attention is all you need paper the queries keys and values metaphor appears already in that paper although i have learned it is not the source of this idea since the comments above however the metaphor actually works best for the other part of the transformer namely the decoder this is because as you say the encoder uses self attention and it seems to me that the queries and keys play a symmetric role in bert so perhaps it would be easier to understand this metaphor for the transformers decoder rather than for bert to my understanding in the vaswani et al transformer model the queries and keys allow all positions of the decoder layer j to attend to all positions of the encoder layer j via the attention scores the values are then selected by the queries and keys the result of the attention layer is the sum of values weighted by the attention scores the projections of queries and keys determine where the attention for each position is placed for example an extreme case could be that the queries are projected by the identity function and the keys are projected to a permutation which moves position i to position i the dot product of the keys and queries would allow each position of decoder layer j to attend to the position before it in encoder layer j so the decoder layer j is referred to as the queries when together with the keys it decides how much each position in decoder layer j again but not referred to as the values will contribute
56723111,using bert in order to detect language of a given word,deeplearning nlp classification bertlanguagemodel,bert is intended to work with words in context without context a bertlike model is equivalent to simple wordvec lookup there is fancy tokenization but i dont know how it works with hebrew probably not very efficiently so if you really really want to use distributional features in your classifier you can take a pretrained wordvec model instead its simpler than bert and no less powerful but im not sure it will work anyway wordvec and its equivalents like bert without context dont know much about inner structure of a word only about contexts it is used in in your problem however word structure is more important than possible contexts for example words gland or blood or sugar often occur in the same context as insulin but and are hebrew whereas is english okay originally arabic but we are probably not interested in too ancient origins you just cannot predict it from context only so why not start with some simple model eg logistic regression or even naive bayes over simple features eg character ngrams distributional features i mean wv may be added as well because they tell about topic and topics may be informative eg in medicine and technology in general there are probably relatively more english words than in other domains
56675662,how does beam search operate on the output of the transformer,machinelearning nlp beamsearch,the beam search works exactly in the same as with the recurrent models the decoder is not recurrent its selfattentive but it is still autoregressive ie generating a token is conditioned on previously generated tokens at the training time the selfattention is masked such that in only attend to words to the left from the word that is currently generated it simulates the setup you have at inference time when you indeed only have the left context because the right context has not been generated yet the only difference is that in the rnn decoder you only use the last rnn state in every beam search step with the transformer you always need to keep the entire hypothesis and do the selfattention over the entire left context
56639938,bert output not deterministic,deeplearning nlp transformermodel bertlanguagemodel,please try to set the seed i faced the same issue and set the seed to make sure we get same values every time one of the possible reasons could be dropout taking place in bert
56555066,cant import berttokenization,pythonx deeplearning nlp,i found it
56554380,why cant i import functions in bert after pip install bert,python nlp bertlanguagemodel,the package youre looking for is berttensorflow not bert berttensorflow is the python package for googles bert implementation bert is a serialization library
56468865,sentence iterator to pass to gensim language model,python nlp gensim wordvec wordembedding,your illustrated class mysentences assumes one sentence per line that might not be the case for your data one thing to note is calling wordvecsentences iter will run two passes over the sentences iterator or in general iter passes default iter the first pass collects words and their frequencies to build an internal dictionary tree structure the second and subsequent passes train the neural model these two or iter passes can also be initiated manually in case your input stream is nonrepeatable you can only afford one pass and youre able to initialize the vocabulary some other way for example if you are trying to read dataset stored in a database your generator function to stream text directly from a database will throw typeerror a generator can be consumed only once and then its forgotten so you can write a wrapper which has an iterator interface but uses the generator under the hood the generator function is stored as well so it can reset and be used in gensim like this
56298584,access spacy masked language model,python nlp spacy languagemodel,this is basically the disadvantage of the lmao approximation i actually hadnt realised this until it was pointed out to me by someone on the rmachinelearning subreddit because were predicting a vector we really only get to predict one point in the vectorspace this is really different from predicting a distribution over the words imagine we had a gap like the of corn lets say a good distribution of fillers for that would be kernel ear piece the vectors for these words arent especially close as the wordvec algorithm is constructing a vector space based on all contexts of the words and the words are only interchangeable in this context in the vast majority of uses of piece the word ear would be a really bad substitution if the likely fillers arent close together in the vectorspace there will be no way for the lmao model to return you an answer that corresponds to that set of words if you only need the best answer the algorithm in spacy pretrain has a good chance of giving it to you but if you need the distribution the approximation breaks down and you should use something like bert
56286510,how to handle unseen words for pretrained glove wordembedding to avoid keyerror,python nlp wordembedding,i would suggest below all missing words assigned to some unique vector say all zeros find words similar to it and use their embedding try ngrams prefix or suffux of the words and check if it is in vocab stem the word and check if it is in vocab simplest solution use fasttext it assembles word vectors from subword ngrams which allows it to handle out of vocabulary words
56201147,how to access the predictions of pytorch classification model bert,python deeplearning pytorch pretrainedmodel nlp,after looking at this part of the runclassifierpy code you are just missing then they just use preds to compute the accuracy as
56076874,read glove pretrained embeddings into r as a matrix,r nlp wordembedding textvec glove,the text file is already in a tabular form just use readcsvpathtoglovebdtxt sep note that the fieldcell separator in this case is a space not a comma
55910635,why is a throwaway column required in bert format,machinelearning deeplearning nlp bertlanguagemodel,bert was pretrained on two tasks masked language modelling next sentence prediction the third column as you refer to it as is used only in next sentence prediction and downstream tasks that require multiple sentences such as question answering in these cases the value of the column wont just be a or for everything sentence will be all while sentence will be all indicating that the former is sentence a and the latter is sentence b
55756841,where to find a pretrained docvec model on wikipedia or large article dataset like google news,python nlp gensim wordvec docvec,im not aware of any publiclyavailable standard gensim docvec models trained on wikipedia
55693826,manage keyerror with gensim and pretrained wordvec model,python nlp gensim,adding a synthetic token would just let you look up that token like modelthe model would still give key errors for absent keys like kjklk theres no builtin support for adding any such catchall mapping often ignoring unknown tokens is better than using some plug value such as a zerovector or randomvector its fairly idiomatic in python to explicitly check if a key is present via the in keyword if you want to do something different for absent keys for example notably the expr if expr else expr defers evaluation of the initial expr avoiding keyerror python also has the defaultdict variant dictionary which can have a default value returned for any unknown key see itd be possible to try replacing the keyedvectors vocab dictionary with one of those if the behavior is really important but there could be side effects on other code
55619176,how to cluster similar sentences using bert,python nlp artificialintelligence wordembedding bertlanguagemodel,you can use sentence transformers to generate the sentence embeddings these embeddings are much more meaningful as compared to the one obtained from bertasservice as they have been finetuned such that semantically similar sentences have higher similarity score you can use faiss based clustering algorithm if number of sentences to be clustered are in millions or more as vanilla kmeans like clustering algorithm takes quadratic time
55575681,use bert for feature extraction of a unique word,python tensorflow nlp languagemodel,bert is not a contextfree transformer which means that you dont want to use it for a single word as you would use wordvec its kind of the point really you want to contextualise your input i mean you can have a oneword sentence input but then why not just use wordvec heres what the readme says pretrained representations can also either be contextfree or contextual and contextual representations can further be unidirectional or bidirectional contextfree models such as wordvec or glove generate a single word embedding representation for each word in the vocabulary so bank would have the same representation in bank deposit and river bank contextual models instead generate a representation of each word that is based on the other words in the sentence hope that makes sense
55540541,is there a way to increase dimensionality of pretrained word embeddings,machinelearning neuralnetwork nlp wordembedding,i understand that you want to feed a d embedding to a network that takes an input of dimension to do this you need to project up the embedding vectors to a higher dimension you can use a simple feedforwardlinear layer that takes in the input of size make the hidden size of the layer to be the desired size which is in this case also note that this should be part of the entire network that is being trained ie the feedforward layer should be trainable
55531061,how can i create and fit vocabbpe file gpt and gpt openai models with my own corpus text,python encoding nlp gpt,check out here you can easily create the same vocabbpe using the following command
55230575,is there an alternative to fully loading pretrained word embeddings in memory,python machinelearning memorymanagement nlp wordembedding,what task do you have in mind if this is a similarity based task you could simply use the loadwordvecformat method in gensim this allows you to pass in a limit to the number of vectors loaded the vectors in something like the googlenews set are ordered by frequency this will give you the critical vectors this also makes sense theoretically as the words with low frequency will usually have relatively bad representations
55114128,unidirectional transformer vs bidirectional bert,nlp transformermodel pretrainedmodel bertlanguagemodel,to clarify the original transformer model from vaswani et al is an encoderdecoder architecture therefore the statement transformer is unidirectional is misleading in fact the transformer encoder is bidirectional which means that the selfattention can attend to tokens both on the left and right in contrast the decoder is unidirectional since while generating text one token at a time you cannot allow the decoder to attend to the right of the current token the transformer decoder constrains the selfattention by masking the tokens to the right bert uses the transformer encoder architecture and can therefore attend both to the left and right resulting in bidirectionality from the bert paper itself we note that in the literature the bidirectional transformer is often referred to as a transformer encoder while the leftcontextonly version is referred to as a transformer decoder since it can be used for text generation recommended reading this article
55108636,document similarity with word mover distance and bertembedding,python nlp similarity wordembedding,bertasservice outputs shape is batchsize sequencelen embeddingdimension in your case sequencelen is since you are pooling the results now you can transpose the other one to match with this using the transpose method of the numpyndarray
55066010,masked language model processing deeper explanation,nlp stanfordnlp,if you dont use random replacement during training your network wont learn to extract useful features from nonmasked tokens in other words if you only use masking and try to predict them it will be a waste of resources for your network to extract good features for the nonmasked tokensremember that your network is as good as your task and it will try to find the easiest way to solve your task
54959340,nltk language modeling confusion,python machinelearning nlp nltk,the paddedeverygrampipeline function expects a list of list of ngrams you should fix your first code snippet as follows also python generators are lazy sequences you cant iterate them more than once
54890488,textlmdatabunch memory issue language model fastai,nlp outofmemory pytorch languagemodel fastai,when you use this function your dataframe is loaded in memory since you have a very big dataframe this causes your memory error fastai handles tokenization with a chunksize so you should still be able to tokenize your text here are two things you should try add a chunksize argument the default value is k to your textlmdatabunchfromdf so that the tokenization process needs less memory if this is not enough i would suggest not to load your whole dataframe into memory unfortunately even if you use textlmdatabunchfromfolder it just loads the full dataframe and pass it to textlmdatabunchfromdf you might have to create your own databunch constructor feel free to comment if you need help on that
54847574,combining tfidf with pretrained word embeddings,nlp spacy tfidf wordembedding tfidfvectorizer,you should try to train embeding on your own corpus there are many package gensim glove you can use embeding from bert without retraining on your own corpus you should know that the probability distribution on different corpus is always different for example the count of basketball in posts about food is very different from the count of the term in news about sport so the gap of word embeding of basketball in those corpus is huge
54762478,how to use pretrained wordvec vectors in docvec model,python machinelearning nlp wordvec docvec,you might think that docvec aka the paragraph vector algorithm of mikolovle requires wordvectors as a st step thats a common belief and perhaps somewhat intuitive by analogy to how humans learn a new language understand the smaller units before the larger then compose the meaning of the larger from the smaller but thats a common misconception and docvec doesnt do that one mode pure pvdbow dm in gensim doesnt use conventional perword input vectors at all and this mode is often one of the fastesttraining and bestperforming options the other mode pvdm dm in gensim the default does make use of neighboring wordvectors in combination with docvectors in a manner analgous to wordvecs cbow mode but any wordvectors it needs will be trainedup simultaneously with docvectors they are not trained st in a separate step so theres not a easy splicein point where you could provide wordvectors from elsewhere you can mix skipgram wordtraining into the pvdbow with dbowwords in gensim but that will train wordvectors from scratch in an interleaved sharedmodel process to the extent you could preseed a model with wordvectors from elsewhere it wouldnt necessarily improve results it could easily send their quality sideways or worse it might in some lucky wellmanaged cases speed model convergence or be a way to enforce vectorspacecompatibility with an earlier vectorset but not without extra gotchas and caveats that arent a part of the original algorithms or welldescribed practices
54753658,finetune text embeddings using bert,nlp embedding,if you are using the original bert repository published by google all layers are trainable meaning no freezing at all you can check that by printing tftrainablevariables
54262318,how to use pretrained bert model for next sentence labeling,tensorflow artificialintelligence nlp,the answer is to use weights what was used nor next sentence trainings and logits from there so to use bert for nextsentence input two sentences in a format used for training def convertsingleexampleexindex example labellist maxseqlength tokenizer converts a single into a single labelmap for i label in enumeratelabellist labelmaplabel i tokensa tokenizertokenizeexampletexta tokensb none if exampletextb tokensb tokenizertokenizeexampletextb if tokensb modifies and in place so that the total length is less than the specified length account for cls sep sep with truncateseqpairtokensa tokensb maxseqlength else account for cls and sep with if lentokensa maxseqlength tokensa tokensamaxseqlength the convention in bert is a for sequence pairs tokens cls is this jack son ville sep no it is not sep typeids b for single sequences tokens cls the dog is hairy sep typeids where typeids are used to indicate whether this is the first sequence or the second sequence the embedding vectors for and were learned during pretraining and are added to the wordpiece embedding vector and position vector this is not strictly necessary since the sep token unambiguously separates the sequences but it makes it easier for the model to learn the concept of sequences for classification tasks the first vector corresponding to cls is used as as the sentence vector note that this only makes sense because the entire model is finetuned tokens segmentids tokensappendcls segmentidsappend for token in tokensa tokensappendtoken segmentidsappend tokensappendsep segmentidsappend if tokensb for token in tokensb tokensappendtoken segmentidsappend tokensappendsep segmentidsappend inputids tokenizerconverttokenstoidstokens the mask has for real tokens and for padding tokens only real tokens are attended to inputmask leninputids zeropad up to the sequence length while leninputids maxseqlength inputidsappend inputmaskappend segmentidsappend assert leninputids maxseqlength assert leninputmask maxseqlength assert lensegmentids maxseqlength labelid labelmapexamplelabel if exindex tflogginginfo example tflogginginfoguid s exampleguid tflogginginfotokens s join tokenizationprintabletextx for x in tokens tflogginginfoinputids s joinstrx for x in inputids tflogginginfoinputmask s joinstrx for x in inputmask tflogginginfosegmentids s joinstrx for x in segmentids tflogginginfolabel s id d examplelabel labelid feature inputfeatures inputidsinputids inputmaskinputmask segmentidssegmentids labelidlabelid return feature and then extend bert model with next code def createmodelbertconfig istraining inputids inputmask segmentids labels numlabels useonehotembeddings creates a classification model model modelingbertmodel configbertconfig istrainingistraining inputidsinputids inputmaskinputmask tokentypeidssegmentids useonehotembeddingsuseonehotembeddings in the demo we are doing a simple classification task on the entire segment if you want to use the tokenlevel output use modelgetsequenceoutput instead outputlayer modelgetpooledoutput hiddensize outputlayershapevalue with tfvariablescopeclsseqrelationship outputweights tfgetvariable outputweights numlabels hiddensize outputbias tfgetvariable outputbias numlabels with tfvariablescopeloss if istraining ie dropout outputlayer tfnndropoutoutputlayer keepprob logits tfmatmuloutputlayer outputweights transposebtrue logits tfnnbiasaddlogits outputbias probabilities tfnnsoftmaxlogits axis logprobs tfnnlogsoftmaxlogits axis onehotlabels tfonehotlabels depthnumlabels dtypetffloat perexampleloss tfreducesumonehotlabels logprobs axis loss tfreducemeanperexampleloss return loss perexampleloss logits probabilities probabilities is what you need its nextsentence preditions
54219851,bertservingstart is not recognized as an internal or external command,python nlp conda,the cli doesnt found the application bertservingstart take a look in your pythonscriptsfolder the application bertservingstart should be there besides pip etc open the cli in this folder and try it again
54215456,gensim pretrained model similarity,python vector nlp gensim wordvec,you cannot exclude the words from already trained model i dont know in which framework youre working on but ill give you the examples in keras as its simple to understand the intentions what you could do is use embedding layer populate it with glove knowledge and then resume training with your corpus so that layer will learn the words and fit them for your specific domain you can read more about it in keras blog
54202559,how to find a context of paragraph with a help of a bert,nlp,they have already trained it on squad dataset for question answering task which is a sequence to sequence task like yours so you can just finetune it with a proper dataset text summarization perhaps
54000564,how to use elmo word embedding with the original pretrained model b in interactive mode,python machinelearning nlp artificialintelligence allennlp,by default elmoembedder uses the original weights and options from the pretrained models on the bil word benchmark about million tokens to ensure youre using the largest model look at the arguments of the elmoembedder class from here you could probably figure out that you can set the options and weights of the model i got these links from the pretrained models table provided by allennlp assert is a convenient way to test and ensure specific values of variables this looks like a good resource to read more for example the first assert statement ensure the embedding has three output matrices going off of that we index with ij because the model outputs layer matrices where we choose the ith and each matrix has n tokens where we choose the jth each of length notice how the code compares the similarity of apple and carrot both of which are the th token at index j from the example documentation i represents one of the first layer corresponds to the context insensitive token representation followed by the two lstm layers see the elmo paper or follow up work at emnlp for a description of what types of information is captured in each layer the paper provides the details on those two lstm layers lastly if you have a set of sentences with elmo you dont need to average the token vectors the model is a characterwise lstm which works perfectly fine on tokenized whole sentences use one of the methods designed for working with sets of sentences embedsentences embedbatch etc more in the code
53923344,pretrained wordvec embedding in neural networks,python tensorflow nlp artificialintelligence wordvec,one easy popup in mind is we can use the another digitnumberofwordsinvocab to pad but wouldnt that take more size nope thats the same size edit typo
53565271,wordvec userlevel documentlevel embeddings with pretrained model,python twitter nlp wordvec wordembedding,averaging the vectors of all the words in a short text is one way to get a summary vector for the text it often works ok as a quick baseline and if all you have is wordvectors may be your main option such a representation might sometimes improve if you did a weighted average based on some other measure of relative term importance such as tfidf or used raw wordvectors before normalization to unitlength as prenormalization raw magnitudes can sometimes hints at strengthofmeaning you could create userlevel vectors by averaging all their texts or by roughly equivalently placing all their authored words into a pseudodocument and averaging all those words together you might retain more of the variety of a users posts especially if their interests span many areas by first clustering their tweets into n clusters then modeling the user as the n centroid vectors of the clusters maybe even the n varies per user based on how much they tweet or how farranging in topics their tweets seem to be with the original tweets you could also train up pertweet vectors using an algorithm like paragraph vector aka docvec in a library like python gensim but that can have challenging ram requirements with million distinct documents if you have a smaller number of users perhaps they can be the documents or they could be the predicted classes of a fasttextinclassificationmode training session
53376459,gensim docvec model how to compute similarity on a corpus obtained using a pretrained docvec model,python nlp gensim docvec,a dirty solution i used about a month ago in gensim the syntax might have changed you can save your inferred vectors in keyedvectors format then you can load and use mostsimilar function another solution esp if the number of questions is not so high would be just to convert questions to a nparray and get cosine distance eg
53316174,using pretrained word embeddings how to create vector for unknown oov token,neuralnetwork deeplearning nlp pytorch wordembedding,there are multiple ways you can deal with it i dont think i can cite references about which works better nontrainable option random vector as embedding you can use an allzero vector for oov you can use the mean of all the embedding vectors that way you avoid the risk of being away from the actual distribution also embeddings generally come with unk vectors learned during the training you can use that trainable option you can declare a separate embedding vector for oov and make it trainable keeping other embedding fixed you might have to overwrite the forward method of embedding lookup for this you can declare a new trainable variable and in the forward pass use this vector as embedding for oov instead of doing a lookup addressing the comments of op i am not sure which of the three nontrainable methods may work better and i am not sure if there is some work about this but method should be working better for trainable option you can create a new embedding layer as below usage
52224555,use pretrained embedding in spanish with torchtext,nlp deeplearning pytorch wordembedding torchtext,it turns out there is a relatively simple way to do this without changing torchtexts source code inspiration from this github thread create numpy wordvector tensor you need to load your embedding so you end up with a numpy array with dimensions numberofwords wordvectorlength myvecsarraywordindex should return your corresponding word vector important the indices wordindex for this array array must be taken from torchtexts wordtoindex dictionary fieldvocabstoi otherwise torchtext will point to the wrong vectors dont forget to convert to tensor load array to torchtext i dont think this step is really necessary because of the next one but it allows to have the torchtext field with both the dictionary and vectors in one place pass weights to model in your model you will declare the embedding like this then you can load your weights using use requiresgradtrue if you want to train the embedding use false if you want to freeze it edit it looks like there is another way that looks a bit easier the improvement is that apparently you can pass the pretrained word vectors directly during the vocabularybuilding step so that takes care of steps here
50909726,finetuning glove embeddings,machinelearning nlp wordvec wordembedding,i myself am trying to do the exact same thing you can try mittens they have succesfully built a framework for it christopher d manningcoauthor of glove is associated with it
50545596,using pretrained word embeddings to classify pools of words,python nlp keras deeplearning convneuralnetwork,the key point in creating that classifier would be to avoid any bias from the order of words in list a naive lstm solution would just look at first or last few words and try to classify this effect could reduced by giving permutations of lists every time perhaps a simpler approach might be where the reduced sum would avoid any ordering bias if a majority of words express similar features of a certain class then the sum would also lean towards that
50233608,how to implement supervised class based language model in srilm,nlp speechrecognition srilm,ive done this before but it was several years ago let me see if i can retrace the steps for you the first step is to create the file that specifies the classes it should have three columns first is the class id then the probability of that word given the class and lastly the word next step is to replace all the words in the training data with their class ids you can use the srilm replacewordswithclasses script or you can write your own script to do it now you train a language model using ngramcount just like you would for a regular nonclass ngram model for evaluation you just specify the language model and also the class file
50152856,spacy create new language model with data from corpus,python windows nlp spacy,there are three main components of a language model in spacy the static languagespecific data shipped in python tokenizer exceptions stop words rules for mapping finegrained to coarsegrained partofspeech tags the statistical model trained to predict partofspeech tags dependencies and named entities trained on a large labelled corpus and included as binary weights and optional word vectors that can be converted and added before or after training you can also train your own vectors on your raw text using a library like gensim and then add them to spacy spacy vx allows you to train all pipeline components independently or in on go so you can train the tagger parser and entity recognizer on your data all of this requires labelled data if youre training a new language from scratch you normally use an existing treebank heres an example of the universal dependencies corpus for spanish which is also the one that was used to train spacys spanish model you can then convert the data to spacys json format and use the spacy train command to train a model for example i dont know whats in your corpustxt and whether its fully labelled or only raw text i also dont know of any existing resources for luxembourgish sounds like thats potentially quite hard to find if your data is labelled you can convert it to spacys format using one of the builtin converters or your own little script if your corpus consists of only raw text you need to label it first and see if its suitable to train a general language model ultimately this comes down to experimenting but here are some strategies label your entire corpus manually for each component eg partofspeech tags if you want to train the tagger dependency labels if you want to train the parser and entity spans if you want to train the entity recognizer youll need a lot of data though ideally a corpus of a similar size to the universal dependencies ones experiment with teaching an existing pretrained model luxembourgish for example the german model this might sound strange but its not an uncommon strategy instead of training from scratch you posttrain the existing model with examples of luxembourgish ideally until its predictions on your luxembourgish text are good enough you can also create more training data by running the german model over your luxembourgish text and extracting and correcting its mistakes see here for details remember that you always need evaluation data too also referred to as development data in the docs this is usually a random portion of your labelled data that you hold back during training and use to determine whether your model is improving
49451160,how to combine two pretrained wordvec models,java nlp emoji wordvec deeplearningj,combining two models trained from different corpuses is not a simple supported operation in the wordvec libraries with which im most familiar in particular even if the same word appears in both corpuses and even in similar contexts the randomization thats used by this algorithm during initialization and training and extra randomization injected by multithreaded training mean that word may appear in wildly different places its only the relative distancesorientation with respect to other words that should be roughly similar not the specific coordinatesrotations so to merge two models requires translating ones coordinates to the other that in itself will typically involve learningaprojection from one space to the other then moving unique words from a source space to the surviving space i dont know if dlj has a builtin routine for this the python gensim library has a translationmatrix example class in recent versions which can do this as motivated by the use of wordvectors for languagetolanguage translations
49346922,does pretrained embedding matrix has word vector,nlp deeplearning,the pretrained embedding has a specific vocabulary defined the words which are not in vocabulary are called words also called oov out of vocabulary words the pretrained embedding matrix will not provide any embedding for unk there are various methods to deal with the unk words ignore the unk word use some random vector use fasttext as pretrained model because it solves the oov problem by constructing vector for the unk word from ngram vectors that constitutes a word if the number of unk is low the accuracy wont get affected a lot if the number is higher better to train embedding or use fast text eos token can also be taken initialized as a random vector make sure the both random vectors are not the same
49239941,what is unk in the pretrained glove vector files eg glovebdtxt,neuralnetwork deeplearning nlp wordembedding glove,the unk token in the pretrained glove files is not an unknown token see this google groups thread where jeffrey pennington glove author writes the pretrained vectors do not have an unknown token and currently the code just ignores outofvocabulary words when producing the cooccurrence counts its an embedding learned like any other on occurrences of unk in the corpus which appears to happen occasionally instead pennington suggests in the same post ive found that just taking an average of all or a subset of the word vectors produces a good unknown vector you can do that with the following code should work with any pretrained glove file import numpy as np glovefile glovebdtxt get number of vectors and hidden dim with openglovefile r as f for i line in enumeratef pass nvec i hiddendim lenlinesplit vecs npzerosnvec hiddendim dtypenpfloat with openglovefile r as f for i line in enumeratef vecsi nparrayfloatn for n in linesplit dtypenpfloat averagevec npmeanvecs axis printaveragevec for glovebdtxt this gives and because it is fairly compute intensive to do this with the larger glove files i went ahead and computed the vector for glovebdtxt for you
47692906,fasttext using pretrained word vector for text classification,nlp wordvec textclassification fasttext,fasttext supervised training has pretrainedvectors argument which can be used like this few things to consider chosen dimension of embeddings must fit the one used in pretrained vectors eg for wiki word vectors is must be it is set by dim argument as of midfebruary python api v doesnt support training using pretrained vectors the corresponding parameter is ignored so you must use cli command line interface version for training however a model trained by cli with pretrained vectors can be loaded by python api and used for predictions for large number of classes in my case there were of them even cli may break with an exception so you will need to use hierarchical softmax loss function loss hs hierarchical softmax is worse in performance than normal softmax so it can give up all the gain youve got from pretrained embeddings the model trained with pretrained vectors can be several times larger than one trained without in my observation the model trained with pretrained vectors gets overfitted faster than one trained without
47305633,language modeling in tensorflow how to tie embedding and softmax weights,tensorflow nlp languagemodel,i have figures out how to implement weight sharing correctly
47297321,how to learn language model,machinelearning nlp lstm languagemodel penntreebank,so what exactly is it to train a language model i think you dont need to train with every bigram in the corpus just use a sequence to sequence model and when you predict the next word given previous words you just choose the one with the highest probability so i have resulting matrices of k entries parameters yes per step of decoding is this a right implementation im getting perplexity of around that hardly changes over iterations which is definitely not in a right range of what it usually is say around you can first read some open source code as a reference for instance wordrnntensorflow and charrnntensorflow the perplexity is at large log which is around per wordwhich means the model is not trained at all and selects the words totally randomly as the model being tuned the complexity will decrease so is reasonable i think in your statement may mean the complexity per sentence for example if tfcontribseqseqsequenceloss is employed to calculate the complexity the result will be less than if you set both averageacrosstimesteps and averageacrossbatch to be true as default but if you set the averageacrosstimesteps to be false and the average length of the sequence is about it will be about
42502605,difference between pretrained word embedding and training word embedding in keras,python python nlp deeplearning,when training wordvec with gensim the result you achieve is a representation of the words in your vocabulary as vectors the dimension of these vectors is the size of the neural network the pretrained wordvec models simply contain a list of those vectors that were pretrained on a large corpus you will find pretrained vectors of various sizes how to use those vector representations that depends on what you want to do some interesting properties have been shown for these vectors it has been shown that the vector for man king woman will often result in the closest match to the vector woman you may also consider using the word vectors as input for another neural networkcomputation model gensim is a very optimized library to perform the cbow and skipgram algorithms but if you really want to set up your neural network yourself you will first have to learn about the structure of cbow and skipgram and learn how to code it in keras for example this should not be particularly complex and a google search for these subjects should provide you with many results to help you along
42119237,load pretrained wordvec model for docvec,machinelearning nlp gensim wordvec docvec,a set of word vectors such as googlenewsvectorsnegativebin is neither necessary nor sufficient for the kind of text vectors lemikolov paragraph vectors created by the docvec class it instead expects to be trained with example texts to learn perdocument vectors then also the trained model can be used to infer vectors for other new documents the docvec class only supports the loadwordvecformat method because it inherits from the wordvec class not because it needs that functionality theres another simple kind of text vector that can be created by simply averaging all the words in the document perhaps also according to some perword significance weighting but thats not what docvec provides
38264636,train a language model using google ngrams,python nlp nltk ngram,you ought to check out this slick code base from uc berkley in the examples folder you will find a bash script makebinaryfromgooglesh that creates a compact language model from the raw google ngrams the resulting lm implements stupid backoff and utilizes a fast and efficient data structure described in the following paper if you are just interested in the final trained lm you can download it in a variety of languages from the berkleyhosted website
37793118,load pretrained glove vectors in python,python vector nlp,glove model files are in a word vector format you can open the textfile to verify this here is a small snippet of code you can use to load a pretrained glove file you can then access the word vectors by simply using the glovemodel variable printglovemodelhello
36338668,language model with srilm,nlp ngram languagemodel srilm,this is a flagged problem of srilm see kenneth heafields thesis footnote on page and his website notes on srilm the way the mass is allocated to unknown word can assign them a higher probability compared to the seen rare words in the training data you can have a look at kenlm package which has only the implementation of modified kneserney generally performs better than kneserney smoothing but does the mass allocation to unknown words in a way that prevents this from happening
36297422,incremental language model training with lingpipe,java nlp languagemodel lingpipe,yes you can train these classifiers incrementally you just need to write your own data handler that doesnt try to read all the data in at once the above doesnt buffer all the data but reads it in once per training item so that should work if youre still running out of memory its probably just because it takes a lot of memory to build a language model over gb if you have long contexts or dont explicitly prune as you go i wrote a paper on how lingpipes scaling works for language models and the incremental classifiers just build a bunch of parallel language models an even more extreme alternative that can save memory is to train each category separately and then combine them later into a classifier which is also supported by the lingpipe api
35828037,training a cnn with pretrained word embeddings is very slow tensorflow,nlp neuralnetwork tensorflow,so there were two issues here as mrry pointed out in his comment to the question the warning was not a result of a conversion during the updates rather i was calculating summary statistics sparsity and histogram on the embeddings gradient and that caused the conversion interestingly removing the summaries made the message go away but the code remained slow per the tensorflow issue referenced in the question i had to also replace the adamoptimizer with the adagradoptimizer and once i did that the runtime was back on par with the one obtained from a small vocabulary
29915993,what is the most efficient way of storing language models in nlp applications,nlp ngram languagemodel,the most common data structures in language models are tries and hash tables you can take a look at kenneth heafields paper on his own language model toolkit kenlm for more detailed information about the data structures used by his own software and related packages
23384206,encryption decryption of natural language model,python encryption cryptography nlp aes,to use utf as your encoding use unicodeencodeutf to convert from a unicode string to a utf encoded string and stringdecodeutf to convert from a utf encoded string to a unicode string yes they are different
22933412,why can we use entropy to measure the quality of language model,machinelearning nlp datamining textmining,if you had a larger sample of data its very likely that the model that assigns to a and to b will do worse than the true model which gives to each the problem is that your training set is too small so you were mislead into thinking the wrong model was better i encourage you to experiment generate a random string of length where each character equally likely then measure the cross entropy of the model vs the model on that much longer string i am sure you will see the latter performs better here is some sample python code demonstrating the fact notice that if you add an extra a to the choice then the second model which is closer to the true distribution gets lower cross entropy than the first however one other thing to consider is that you really want to measure the likelihood on held out data that you didnt observe during training if you do not do this more complicated models that memorize the noise in the training data will have an advantage over smallersimpler models that dont have as much ability to memorize noise one real problem with likelihood as measuring language model quality is that it sometimes doesnt perfectly predict the actual higher level application error rate for example language models are often used in speech recognition systems there have been improved language models in terms of entropy that didnt drive down the overall systems word error rate which is what the designers really care about this can happen if the language model improves predictions where the entire recognition system is already confident enough to get the right answer
21604403,how does generative language model work in the natural language processing,nlp nltk probability probabilitytheory,first of all you dont pick the word with highest probability you pick a random word but no uniformly with the probability in the model so if you have words in a model yes and no and the probability distribution is yes no than the generated text may look like this ie youll have approximately yes in the text and no edit heres a simple way to sample from the distribution generate a random number from to iterate over all words in the model summing their probability weights as soon as the sum is larger than the generated number emit the current word heres an example suppose youve generated you start from yes and the accumulated probability weight will be so you take next word no and get the accumulated weight which is greater than so you emit no suppose next time you have then you need to emit yes
16408163,arpa language model documentation,nlp speechrecognition cmusphinx sphinx languagemodel,there is actually not much more to say about the format than is said in those docs besides youll probably want to prepare a text file with sample sentences and generate the language file based on it there is an online version which can do it for you lmtool
16225667,what does word count refer to when calculating unigram probabilities in an unigram language model,nlp,divide by the total number of tokens ie word occurrences in the training set the reason is quite easy to see if you divide by the number of distinct words the probabilities for all words will not necessarily sum to one so they wont form a probability distribution
6462709,nltk language model ngram calculate the prob of a word from context,python nlp nltk,i know this question is old but it pops up every time i google nltks ngrammodel class ngrammodels prob implementation is a little unintuitive the asker is confused as far as i can tell the answers arent great since i dont use ngrammodel often this means i get confused no more the source code lives here here is the definition of ngrammodels prob method note selfcontextprobword is equivalent to selfmodelcontextprobword okay now at least we know what to look for what does context need to be lets look at an excerpt from the constructor alright the constructor creates a conditional probability distribution selfmodel out of a conditional frequency distribution whose context is tuples of unigrams this tells us context should not be a string or a list with a single multiword string context must be something iterable containing unigrams in fact the requirement is a little more strict these tuples or lists must be of size n think of it this way you told it to be a trigram model you better give it the appropriate context for trigrams lets see this in action with a simpler example as a side note actually trying to do anything with mle as your estimator in ngrammodel is a bad idea things will fall apart i guarantee it as for the original question i suppose my best guess at what op wants is this but there are so many misunderstandings going on here that i cant possible tell what he was actually trying to do
3673329,how to use the pretrained maltparser parsing models for english,java nlp textparsing,you can try to use version from the version release note fixed a problem introduced in with path separator in microsoft windows environment
2236858,build a natural language model that fixes misspellings,java parsing nlp linguistics,peter norvig has written a terrific spell checker maybe that can help you
79469627,unable to export custom language model data speech framework,ios swift speechrecognition mobileapplication languagemodel,so i learned about vartmp and that its path on macos fixed the issue by creating command line tool project in xcode and exported bin file on my macos and imported it in my speech recognition project here is the sample code i found from apple doing this
76655508,how to get the vector embedding of a token in gpt,machinelearning pytorch huggingfacetransformers languagemodel,you are right with outputhiddenstatetrue and watching outhiddenstates this element is a tuple of length as you mentioned according to biogpt paper and huggingface doc your model contains transformer layers and the elements in the tuple are the first embedding layer output and the outputs of each of the layers the shape of each of these tensors is b l e where b is your batch size l is the length of the input and e is the dimension of your embedding it seems that you are padding your input to regarding the shape you indicated so the representation of your first token in the first batched sentence would be outhiddenstatesk which is of shape here k denotes the layer you want to use and it is up to you to decide which one you want depending on what you will do with it
76546004,openai finetuning api whywould i use llamaindexorlangchain instead of finetuning a model,openaiapi langchain chatgptapi languagemodel llamaindex,tldr use llamaindex or langchain to get an exact answer ie a fact to a specific question from existing data sources why choose llamaindex or langchain over finetuning a model the answer is simple but you couldnt answer it yourself because you were only looking at the costs there are other aspects as well not just costs take a look at the usability side of the question finetuning a model will give the model additional general knowledge but the finetuned model will not give you an exact answer ie a fact to a specific question people train an openai model with some data but when they ask it something related to the finetuning data they are surprised that the model doesnt answer with the knowledge gained by finetuning see an example explanation on the official openai forum by juanolano i finetuned a kword book my initial expectation was to have the desired qa and at that point i didnt know any better but this finetuning showed me the limits of this approach it just learned the style and stayed more or less within the corpus but hallucinated a lot then i split the book into sentences and worked my way through embeddings and now i have a very decent qa system for the book but for narrow questions it is not as good for questions that need the context of the entire book also see the official openai documentation some common use cases where finetuning can improve results setting the style tone format or other qualitative aspects improving reliability at producing a desired output correcting failures to follow complex prompts handling many edge cases in specific ways performing a new skill or task thats hard to articulate in a prompt llamaindex or langchain enable you to connect openai models with your existing data sources for example a company has a bunch of internal documents with various instructions guidelines rules etc llamaindex or langchain can be used to query all those documents and give an exact answer to an employee who needs an answer openai models gpt gpt gpt etc cant query their knowledge querying requires calculating embedding vectors from a resource and then calculating cosine similarity which openai models cant do an openai model simply gives an answer based on the statistical probability of which word should follow the previous one i strongly suggest you read my previous answer regarding semantic search youll understand this answer better
76048714,finetuning a lm vs promptengineering an llm,languagemodel robertalanguagemodel roberta gpt largelanguagemodel,i found a medium piece which goes a long way in clarifying this here quoting from the conclusion in the above in the low data domain prompting shows superior performance to the respective finetuning method to beat the sota benchmarks in finetuning leveraging large frozen language models in combination with tuning a soft prompt seems to be the way forward it appears prompting an llm may outperform fine tuning a smaller model on domainspecific tasks if the training data is small and vice versa if otherwise additionally in my own personal anecdotal experience with chatgpt bard bing vicunab dollyvb and illamab it appears models of the size of chatgpt bard and bing have learned to mimic human understanding of language well enough to be able to extract meaningful answers from context provided at inference time it seems to me the smaller models do not have that mimicrymastery and might not perform as well with incontext learning at inference time they might also be too large to be well suited for finetuning in a very limited domain my hunch is that for very limited domains if one is going the finetuning route finetuning on much smaller models like bert or roberta or smaller variants of gpt or gptj for generative tasks rather than on these mediumsized models might be the more prudent approach resourcewise another approach to fine tuning the smaller models on domain data could be to use more carefully and rigorously crafted prompts with the mediumsized models this could be a viable alternative to using the apis provided by the owners of the very large proprietary models
75664012,i want to make an ai text classifier using openai api based on gpt but i cannot find the api documentation for the gpt,machinelearning artificialintelligence openaiapi languagemodel gpt,gpt is not available through the openai api only gpt and above so far i would recommend accessing the model through the huggingface transformers library and they have some documentation out there but it is sparse there are some tutorials you can google and find but they are a bit old which is to be expected since the model came out years ago now also what do you mean by i wanted to use gpt api for the same as it is more reliable to catch the content generated by gpt by all accounts gpt should be much better than gpt at text classification and by signing up for a free trial with openai you can use their api for free with provided credits for only three months if you want to train the gpt xl model you will probably get better results than gpts ada but then you have compute resources you have to worry about
72467610,oom while finetuning medium sized model with dialogpt on colab,googlecolaboratory huggingfacetransformers languagemodel gpt,just reduce the tokenizer input maxlen from to it worked perfectly for me
72032858,training a ff neural language model,python tensorflow keras neuralnetwork languagemodel,your code runs fine when using categoricalcrossentropy as your loss function since you are using onehot encoded labels sparsecategoricalcrossentropy only works with sparse integer values
69489265,ngram language model returns nothing,python dictionary prediction ngram languagemodel,i had to give the model the last two words from the list not the entire thing even if its two words like so
69380237,why is my transformer implementation losing to a bilstm,deeplearning pytorch lstm transformermodel languagemodel,this may be surprising but transformers dont always beat lstms for example language models with transformers states transformer architectures are suboptimal for language model itself neither selfattention nor the positional encoding in the transformer is able to efficiently incorporate the wordlevel sequential context crucial to language modeling if you run the transformer tutorial code itself on which your code is based youll also see lstm do better there see this thread on statsse for more discussion on this topic disclaimer both the question and the answer there are mine
68907519,bert with padding and masked token predicton,tensorflow keras bertlanguagemodel huggingfacetransformers languagemodel,as already mentioned in the comments you forgot to pass the attentionmask to bert and it therefore treated the added padding tokens like ordinary tokens you also asked in the comments how you can rid of the padding token prediction there are several ways to do it depending on your actual task one of them is removing them with booleanmask and the attentionmask as shown below import tensorflow as tf from transformers import tfbertlmheadmodel berttokenizerfast ckpt bertlargecasedwholewordmasking t berttokenizerfastfrompretrainedckpt m tfbertlmheadmodelfrompretrainedckpt e thello world mask like itreturntensorstf epadded thello world mask like itreturntensorstf paddingmaxlength maxlength def predictionencoding logits mencodinglogits tokenmapping tfargmaxtfkerasactivationssoftmaxlogitsaxis return tfbooleanmasktokenmapping encodingattentionmask tokenpredictions predictione tokenpredictionspadded predictionepadded printtokenpredictions printtokenpredictionspadded output
66276186,huggingface gpt tokenizer configuration in configjson,pytorch huggingfacetransformers languagemodel huggingfacetokenizers gpt,your repository does not contain the required files to create a tokenizer it seems like you have only uploaded the files for your model create an object of your tokenizer that you have used for training the model and save the required files with savepretrained from transformers import gpttokenizer t gpttokenizerfrompretrainedgpt tsavepretrainedsomefolder output
65925640,assigning weights during testing the bert model,bertlanguagemodel huggingfacetransformers transformermodel languagemodel,the vector representation of a token keep in mind that token word is stored in an embedding layer when we load the bertbaseuncased model we can see that it knows tokens and that the vector representation of each token consists of elements from transformers import bertmodel bert bertmodelfrompretrainedbertbaseuncased printbertembeddingswordembeddings output this embedding layer is not aware of any strings but of ids for example the vector representation of the id is printbertembeddingswordembeddingsweight output everything that is outside of the known ids is not processable by bert to answer your question we need to look at the component that maps a string to the ids this component is called a tokenizer there are different tokenization approaches bert uses a wordpiece tokenizer which is a subword algorithm this algorithm replaces everything that can not be created from its vocabulary with an unknown token that is part of the vocabulary unk in the original implementation id please have a look at the following small example in which a wordpiece tokenizer is trained from scratch to confirm that beheaviour from tokenizers import bertwordpiecetokenizer path filewithyourtrainingssentencetxt tokenizer bertwordpiecetokenizer tokenizertrainfilespath vocabsize specialtokensunk sep pad cls mask otrain tokenizerencodewent to get loan from bank otest tokenizerencodereceived education loan from bank printvocabulary size formattokenizergetvocabsize printtrain tokens formatotraintokens printtest tokens formatotesttokens output
65529156,huggingface transformer gpt resume training from saved checkpoint,python pytorch huggingfacetransformers languagemodel gpt,to resume training from checkpoint you use the modelnameorpath parameter so instead of giving the default gpt you direct this to your latest checkpoint folder so your command becomes
62830783,scripts missing for gpt fine tune and inference in huggingface github,python huggingfacetransformers languagemodel gpt,it looks like theyve been moved around a couple times and the docs are indeed out of date the current version can be found in runlanguagemodelingpy here
62458671,bertquestionanswering total number of permissible wordstokens for training,pytorch recurrentneuralnetwork languagemodel bertlanguagemodel,together and actually its together they should be since there are two sep one after question and another after answer where qword refers to words in the question and aword refers to words in the answer
62069350,transformerxl input and labels for language modeling,huggingfacetransformers languagemodel,that does sound like a typo from another models convention you do have to pass data twice once to inputids and once to labels in your case for both the model will then attempt to predict from i am not sure adding something at the beginning of the target tensor would work as that would probably cause size mismatches later down the line passing twice is the default way to do this in transformers before the aforementioned pr transfoxl did not shift labels internally and you had to shift the labels yourself the pr changed it to be consistent with the library and the documentation where you have to pass the same data twice
61598029,size of input and output layers in keras implementation of an rnn language model,tensorflow keras neuralnetwork wordembedding languagemodel,the reason why we add leads to the possibility that we can encounter a chance to see an unseen wordout of our vocabulary during testing or in production it is common to consider a generic term for those unknown and that is why we add a oov word in front which resembles all out of vocabulary words check this issue on github which explains it in detail
61482810,fine tuning a pretrained language model with simple transformers,pythonx huggingfacetransformers languagemodel simpletransformers,question yes the input to the trainmodel and evalmodel methods need to be a single file dynamically loading from multiple files will likely be supported in the future question yes you can use bertbasemultilingualcased model you will find a much more detailed updated guide on language model training here disclaimer i am the creator of the above library
61440281,is positional encoding necessary for transformer in language modeling,transformermodel languagemodel,this research group claims positional encoding is not necessary
60173639,size of the training data of gptxl pretrained model,pytorch languagemodel huggingfacetransformers,the gptxl model is the biggest of the four architectures detailed in the paper you linked m parameters it is trained on the same data as the other three which is the webtext youre mentioning
60137162,pretraining bertroberta language model using domain text how long it gonna take estimately which is faster,languagemodel bertlanguagemodel huggingfacetransformers,k words it too few to train such a large model as bert or roberta the main claim of the roberta paper is that bert is actually undertrained whereas bert was trained on gb of text data roberta used gb of plain text for a small domainspecific data as you describe you can try finetuning an existing model in this case i would go for roberta because it seems to be better pretrained does not have the nextsentenceobjective which is a hassle to preprocess data for it and it uses sentencepiece for tokenization which allows lossless detokenization
60121768,while running huggingface gptxl model embedding index getting out of range,pythonx languagemodel huggingfacetransformers,this was actually an issue i reported and they fixed it
51638889,how word association mining is generalization of ngram language model,datamining textmining languagemodel,association rule mining will try to cover frequent concurrences of arbitrary length if you apply this not just two term correlations to text you would indeed find ngrams without a fixed n
45612636,character level bidirectional language model in tensorflow,tensorflow languagemodel,looks like you set selflr tfvariable trainablefalse try changing this to a nonzero value if you are reading probabilities from selfprobs during the testing phase this should be normalized appropriately
44873156,how can the perplexity of a language model be between and,python tensorflow languagemodel sequencetosequence perplexity,this does not make a lot of sense to me perplexity is calculated as entropy and the entropy is from to so your results which are i would suggest you to take a look at how your model calculate the perplexity because i suspect there might be an error
43423112,statistical text analysis language modeling and information retrieval program rainbow,c compilererrors classification textanalysis languagemodel,have you edited the rainbowc file looks like line has the include line you should read and note that the anglebrackets search for files on the standard include path which can be modified likely you need to do some installation take a look for the readme and make sure you have followed the install instructions i downloaded the bow package you linked and looked inside there is a rainbowc which you are apparently trying to compile read the included readme file and follow the compilationinstallation instructions included therein
43359882,how to build tensorflow speech recognition integrated with language model,python tensorflow speechrecognition languagemodel,lm scoring is just an additional rescoring step simply a spelling correction with a language model it can be applied on any system output mozilla has it spellpy for example
39242569,get the probability distribution of next word given a sequence using tensorflows rnn lstm language model,tensorflow lstm languagemodel,im new to tensorflow and rnn too so heres my thinking about your questions assuming you have a corpus consisting words too small the output of the ith lstm cell is a vector having elements each corresponding to a probability and this vector is the predicted probability distribution for the ith word back to your question you just need to feed the input wwww to rnn and you get four vectors each having elements the number of words in corpus and then you pick up the last output vector and thats the predicted probability distribution of the th word and you also can do an argmax on this vector to find the most probable word for th position i have no idea about this question even i can assign a probability to any given sequences also considering your input wwww after calculating rnn you have four output vectors denoted as vvvv and then you just need to find the probabilities of w in v w in v w in v and multiply these probabilities and thats the probability of your input v is not used because it is used to predict the next word of this sequence w is also not used because it is usually the starting token edit once you have trained your model you should get a embedding matrix embedding a rnn cell cell and a softmax weightsbiases softmaxw softmanxb you can generate outputs using these three things python the final output is a list containg leninputs vectors tensors you can use sessruntensor to get the value of a tensor in the form of numpyarray this is a just simple function i wrote and should give you a general idea about how to generate outputs when you finish training
37947619,using custom beam scorer in tensorflow ctc language model,tensorflow languagemodel,theres currently no api for python to use language model with a custom scorer contributions are welcome but theres some difficulty in making this possible in the python api as it would require running the tf lm subgraph in an independent session inside the decoder op and those wouldnt blend nicely together the easiest way of doing this is in c and requires extending the basebeamscorer class along with a beamstate similar to what can be seen in tests and further run ctcbeamsearchdecoderdecode on top of the outputs from the tensorflow graph that would normally go in the ctcbeamsearchdecoder op by doing this your beamscorer implementation could make use of any language model you have at hand and simply needs to return the appropriate scores when expanding the beam from one state to another
35403070,unable to open cube language model params for hindi language in tesseract,ocr tesseract hindi languagemodel,i fixed this error by installing the correct versions of the below files hincubebigrams hincubefold hincubelm hincubenn hincubeparams hincubewordfreq hintesseractcubenn along with the correct versions of the hindi and english training data all above files are available at i put these files under usrlocalsharetessdata this is on centos
34502351,what is the next procedure after creating a cmusphinx language model with my own dictionary,java dictionary cmusphinx languagemodel,procedure for training acoustic model is described in tutorial for acoustic model training you need to create fileids and transcription files manually in a text editor or with a script if you want to convert existing transcription in any custom form to required format fileids must list the file names transcription file must list transcription for each of the files in a special format for example of acoustic model training database you can check inside an database
33026460,cmu sphinx custom language model,cmusphinx sphinx languagemodel,you dont need a new acoustic model for this but rather a custom grammar see and to learn more sphinx recognizes characters just fine if you put them spaceseparated in the grammar as to accuracy there are two ways to increase it if the numbers of employees isnt too large you can just make the grammar with all possible employee ids if this is not your case than to have a generic grammar is your only option although its possible to make a custom scorer which will use the context information to predict the employee id better than the generic algorithm this way requires some knowledge in both asr and cmu sphinx code
5743390,creating arpa language model file with words,speechrecognition cmusphinx ngram languagemodel,i thought id answer this one since it has a few votes although based on christinas other questions i dont think this will be a usable answer for her since a word language model almost certainly wont have an acceptable word error rate or recognition speed or most likely even function for long with inapp recognition systems for ios that use this format of language model currently due to hardware constraints i figured it was worth documenting it because i think it may be helpful to others who are using a platform where keeping a vocabulary this size in memory is more of a viable thing and maybe it will be a possibility for future device models as well there is no webbased tool im aware of like the sphinx knowledge base tool that will munge a word plaintext corpus and return an arpa language model but you can obtain an alreadycomplete word dmp language model which can be used with sphinx at the command line or in other platform implementations in the same way as an arpa lm file with the following steps download this language model from the cmu speech site in that folder is a file called languagemodelarpaformatdmp which will be your language model download this file from the cmu speech site which will become your pronunciation dictionary convert the contents of cmuadic to all uppercase letters if you want you could also trim down the pronunciation dictionary by removing any words from it which arent found in the corpus languagemodelvocabulary this would be a regex problem these files are intended for use with one of the sphinx englishlanguage acoustic models if the desire to use a word english language model is driven by the idea of doing some kind of generalized large vocabulary speech recognition and not by the need to use a very specific words for instance something specialized like a medical dictionary or entry contact list this approach should give those results if the hardware can handle it there are probably going to be some sphinx or pocketsphinx settings that will need to be changed which will optimize searches through this size of model
5220661,building openears compatible language model,iphone speechrecognition languagemodel,regarding lm formats afaik most language models use the arpa standard for language models sphinx cmu language models are compiled into binary format youd need the source format to convert a sphinx lm into another format most other language models are in text format id recommend using the htk speech recognition toolkit detailed documentation here heres also a description of cmus slm toolkit heres an example of a language model in arpa format i found on the net you probably want to create an arpa lm first then convert it into any binary format if needed in general to build a language model you need lots and lots of training data to determine what the probability of any other word in your vocabulary is after observing the current input to this point in time you cant just make a language model by just adding the words you want to recognize you also need a lot of training data typical input you observe when running your speech recognition application a language model is not just a word list it estimates the probability of the next token word in the input to estimate those probabilities you need to run a training process which goes over training data eg historic data and observes word frequencies there to estimate above mentioned probabilities for your problem maybe as a quick solution just assume all words have the same frequency probability create a dictionary with the words you want to recognize n words in dictionary create a language model which has n as the probability for each word unigram language model you can then interpolate that unigram language model lm with another lm for a bigger corpus using htk toolkit
77785423,shap value for binary classification using pretrain bert how to extract summary graph,python machinelearning bertlanguagemodel textclassification shap,will you try ive got a grey plot this is because your data is nonnumeric
77322066,how to change evaluation metric from roc auc to accuracy in hugging face transformers finetuning,python machinelearning huggingfacetransformers textclassification,you can modify the computemetrics function to calculate prediction accuracy
76170604,huggingface pipeline with a finetuned pretrained model errors,python pipeline huggingfacetransformers textclassification huggingface,after the model training your model seems to be still placed on your gpu the error message you receive runtimeerror expected all tensors to be on the same device but found at least two devices cuda and cpu when checking argument for argument index in method wrapperindexselect is thrown because the input tensors that are generated from the pipeline are still on cpu that is also the reason why the pipeline works as expected when you move the model to cpu with modeltocpu per default the pipeline will perform its actions on cpu you change that behavior by specifying the device parameter cuda classifier pipelinezeroshotclassification modelmodel tokenizertokenizer device cpu classifier pipelinezeroshotclassification modelmodel tokenizertokenizer devicecpu
76099140,hugging face transformers bart cuda error cublasstatusnotinitialize,python pytorch huggingfacetransformers textclassification huggingface,i was able to reproduce your problem here is how i solved it on both of the clusters you provided in order to solve it i used at the frompretrained call ignoremismatchedsizestrue because the model you use has fewer labels than what you have numlabels insert the number of your labels i used just to be sure its based on both your errors but mainly the second one i suspect it was also the source of the second error on the gpu please test and confirm it i also used the following at the trainingarguments call for memory optimizations fptrue gradientcheckpointingtrue i tested it with up until numtrainepochs perdevicetrainbatchsize perdeviceevalbatchsize warmupsteps and it worked just fine hopefully it will help you get the desired results you can look at the final links i provided for more details about how to optimize the speed and memory while training on both gpu and cpu for reference huggingface models search for ignoremismatchedsizes huggingface configuration search for numlabels finetuning with custom datasets efficient training on a single gpu efficient training on cpu
73567055,extend bert or any transformer model using manual features,huggingfacetransformers textclassification bertlanguagemodel,the are several ways to achieve that i will explain just two in the following answer add category as a token the idea of this approach is rather simple when transformer models like bert are able to produce contextualized embeddings for a given sentence why cant we incorporate categorical features as text as well for example you use the title of a cited paper as input and also want to incorporate the research area of the paper to provide more context to do that i would add the categories of your new feature as separate tokens to bert that is not required but reduces the sequence length and finetune it for a few epochs from transformers import berttokenizer bertforsequenceclassification mycategories computer science machine translation sentenceattention is all you need computer science machine translation t berttokenizerfrompretrainedbertbasecased mbertforsequenceclassificationfrompretrainedbertbasecased tokenized without separate tokens printlentsentenceinputids tokenized without separate tokens taddtokensmycategories printlentsentenceinputids extend embedding layer of model mresizetokenembeddingslentgetvocab training output separate embedding layer a more traditional way is to hold an embedding for each category and concatenate or any other method to combine features it with the contextualized output of bert before you feed it to the classification layer for this approach you can simply copy the code from huggingfaces bertforsequenceclassification class or whatever class you are using and make the required changes import torch from torchnn import bcewithlogitsloss crossentropyloss mseloss from transformers import bertpretrainedmodel bertmodel from typing import optional class mybertforsequenceclassificationbertpretrainedmodel def initself config superinitconfig selfnumlabels confignumlabels selfconfig config selfbert bertmodelconfig classifierdropout configclassifierdropout if configclassifierdropout is not none else confighiddendropoutprob selfdropout torchnndropoutclassifierdropout modified selfclassifier torchnnlinearconfighiddensize confignumlabels modified different categories embedding dimension selfmycategoricalfeature torchnnembedding initialize weights and apply final processing selfpostinit modified new parameter categoricalfeatureids def forward self inputids optionaltorchtensor none attentionmask optionaltorchtensor none tokentypeids optionaltorchtensor none positionids optionaltorchtensor none headmask optionaltorchtensor none inputsembeds optionaltorchtensor none labels optionaltorchtensor none outputattentions optionalbool none outputhiddenstates optionalbool none returndict optionalbool none categoricalfeatureids none returndict returndict if returndict is not none else selfconfigusereturndict outputs selfbert inputids attentionmaskattentionmask tokentypeidstokentypeids positionidspositionids headmaskheadmask inputsembedsinputsembeds outputattentionsoutputattentions outputhiddenstatesoutputhiddenstates returndictreturndict pooledoutput outputs modified get embeddings mycategoricalembedding selfmycategoricalfeaturecategoricalfeatureids mycategoricalembedding selfdropoutmycategoricalembedding pooledoutput selfdropoutpooledoutput modified concatenate contextualized embeddings from bert and your categorical embedding pooledoutput torchcatpooledoutput mycategoricalembedding dim logits selfclassifierpooledoutput loss none if labels is not none if selfconfigproblemtype is none if selfnumlabels selfconfigproblemtype regression elif selfnumlabels and labelsdtype torchlong or labelsdtype torchint selfconfigproblemtype singlelabelclassification else selfconfigproblemtype multilabelclassification if selfconfigproblemtype regression lossfct mseloss if selfnumlabels loss lossfctlogitssqueeze labelssqueeze else loss lossfctlogits labels elif selfconfigproblemtype singlelabelclassification lossfct crossentropyloss loss lossfctlogitsview selfnumlabels labelsview elif selfconfigproblemtype multilabelclassification lossfct bcewithlogitsloss loss lossfctlogits labels if not returndict output logits outputs return loss output if loss is not none else output return lossloss logitslogits hiddenstatesoutputshiddenstates attentionsoutputsattentions you can use this class just as the bertforserquenceclassification class the only difference is that it expects categoricalfeatureids as additional input from transformers import berttokenizer bertforsequenceclassification t berttokenizerfrompretrainedbertbasecased m mybertforsequenceclassificationfrompretrainedbertbasecased batch with two sentences ie the citation text you have already used i tpaper title paper title paddingtrue returntensorspt we assume that the first sentence ie paper title belongs to category and the second sentence to category you probably want to use a dictionary in your own code icategoricalfeatureids torchtensor printmi output
72784032,how to classify new data using a pretrained model python text classification nltk and scikit,python scikitlearn nltk textclassification nltktrainer,it seems than after training you just have to do as for your validation step using directly the gridsearcher which in sklearn library is also used after training as a model taking the best found hyperparameters so take a x which is what you want to evaluate and run predsmnnb should contain what you expect
72515966,implement metrics using xlmroberta model for text classification,python textclassification,xlmrobertaforsequenceclassification and other classes of the forsequenceclassification family assume classification into multiple classes and use categorical crossentropy as the loss function the class is just a lightweight wrapper of the xlmroberta class if you want to use specifically binary crossentropy you can either make your own wrapper with a single class output and binary crossentropy or you can do the loss computation in the training loop in your code snippet ie instead of using outputs use the logits outputs as an input to the loss function regarding other metrics you have the logits in the outputs variable it should be enough to compute whatever metric you find useful for your task
71465239,cant backward pass two losses in classification transformer model,python neuralnetwork pytorch textclassification,there is nothing wrong with having a loss that is the sum of two individual losses here is a small proof of principle adapted from the docs there must be a real second time that you call directly or indirectly backward on some varaible that then traverses through your graph it is a bit too much to ask for the complete code here only you can check this or at least reduce it to a minimal example while doing so you might already find the issue apart from that i would start checking does it already occur in the first iteration of training if not are you reusing any calculation results for the second iteration without a detach when you do backward on your losses individually llossbackward followed by dlossbackward this has the same effect as adding them together first as gradients are accumulated what happens this will let you track down for which of the two losses the error occurs
67398812,how to add extra dense layer on top of bertforsequenceclassification,textclassification bertlanguagemodel pytorchlightning,the class bertforsequenceclassification that comes from the huggingface transformers when using pytorch lightning implements a fixed architecture if you want to change it eg by adding layers you need to inherit your own module this is actually quite simple you can copy the code of bertforsequenceclassification and modify the code between getting the pooled bert output and getting the logits note however that adding a hidden layer to a classifier does not make much difference when finetuning bert the capacity of the additional hidden layer is negligible compared to the entire stack of bert layers even if you cannot finetune the entire model finetuning just the last bert layer is probably better than adding an extra layer to the classifier
63377198,what is the difference between pooled output and sequence output in bert layer,pythonx tensorflow neuralnetwork textclassification bertlanguagemodel,sequence output is the sequence of hiddenstates embeddings at the output of the last layer of the bert model it includes the embedding of the cls token hence for the sentence you are on stackoverflow it gives embeddings one embedding for each of the four words assuming the word stackoverflow was tokenized into a single token along with the embedding of the cls token pooled output is the embedding of the cls token from sequence output further processed by a linear layer and a tanh activation function the linear layer weights are trained from the next sentence prediction classification objective during pretraining for further details please refer to the bert original paper
63321892,how can i use gpt for my text classification,keras textclassification transferlearning openaiapi gpt,i substituted hateful language with in the following samples given samples like gpt can indeed then decide if a given input is hateful or not gpt actually is implementing filters that will very effectively tell if an arbitrary comment is hatefull or not you would just enter the msg and let gpt autcomplete the truefalse part at the end setting tokens to about and temperature setting booleanish classification that also relies on more complex context you can insult someone without using foullanguage id doeable with gpt and can also be done with gpt
62319735,bert text classification loss is nan,python tensorflow sentimentanalysis textclassification bertlanguagemodel,the label classes index should start from not tfbertforsequenceclassification requires labels in the range labels tftensor of shape batchsize optional defaults to none labels for computing the sequence classificationregression loss indices should be in confignumlabels if confignumlabels a regression loss is computed meansquare loss if confignumlabels a classification loss is computed crossentropy source
61943409,spacys bert model doesnt learn,python spacy textclassification multiclassclassification bertlanguagemodel,received an answer to my question on github and it looks like there must be some optimizer parameters specified just like in this example
61467389,how to create a bert layer with keras,tensorflow deeplearning textclassification,here is how i ultimately integrated a bert layer
60888020,difference between blank and pretrained models in spacy,python spacy textclassification,if you are using spacys text classifier then it is fine to start with a blank model the textcategorizer doesnt use features from any other pipeline components if youre using spacy to preprocess data for another text classifier then you would need to decide which components make sense for your task the pretrained models load a tagger parser and ner model by default the lemmatizer which isnt implemented as a separate component is the most complicated part of this it tries to provide the best results with the available data and models if you dont have the package spacylookupsdata installed and you create a blank model youll get the lowercase form as a defaultdummy lemma if you have the package spacylookupsdata installed and you create a blank model it will automatically load lookup lemmas if theyre available for that language if you load a provided model and the pipeline includes a tagger the lemmatizer switches to a better rulebase lemmatizer if one is available in spacy for that language currently greek english french norwegian bokml dutch swedish the provided models also always include the lookup data for that language so they can be used when the tagger isnt run if you want to get the lookup lemmas from a provided model you can see them by loading the model without the tagger import spacy nlp spacyloadencorewebsm disabletagger in general the lookup lemma quality is not great theres no information to help with ambiguous cases and the rulebased lemmas will be a lot better however it does take additional time to run the tagger so you can choose lookup lemmas to speed things up if the quality is good enough for your task and if youre not using the parser or ner model for preprocessing you can speed things up by disabling them nlp spacyloadencorewebsm disablener parser
56846266,no batchsize while making inference with bert model,python tensorflow machinelearning deeplearning textclassification,you are using savedmodelestimator which does not provide an option to pass in runconfig or params arguments since savedmodelestimator is a subclass of estimator the params is merely a dictionary that stores hyperparameters i think you could modify params by passing the desired keyvalue pair to it before you call getprediction for example
55561060,error multiclass text classification with pretrained bert model,python textclassification multiclassclassification transferlearning,solved just by replacing with strx for x in range in the getlabel function the two are actually equivalent but for some unknown reason this solved the issue
37660555,text categorization python with pretrained data,pythonx scikitlearn tfidf textclassification,it looks like you already vectorised the text ie already converted the text to numbers so that you can use scinkitlearns classifiers now the next step is to train a classifier you can follow this link it looks like this vectorization train classifier predict on new docs
33899867,using language models for term weighting,python machinelearning scikitlearn ngram textclassification,it depends what do you mean by term if as usual term is just a word then a probability model will work the same as simple tf weighting even without idf why beacause empirical estimator of pword is just word allwords and as allwords is constant then the weight becomes just word which is simple term frequency so in this sense scikit does what you need ok so maybe you want to consider context then what kind of context do you want to analyze independently ppreword word and use it as a weighted sum for word then why not pword postword why not ppreword preword word postword postword etc why not to include some reweighting based on unigrams when bigrams are not available the answer is quite simple once you go into using language models as a weighting schemes amount of possible introductions grows exponentialy and there is no typical approach which is worth implementing as a standard for a library which is not a nlp library
78271828,tensor size error when generating embeddings for documents using huggingface pretrained models,huggingfacetransformers largelanguagemodel wordembedding huggingface pretrainedmodel,the length of the text variable is while the pipeline accepts a maximum length of you can fix this by splitting your text into chunks of or you can use a model that accepts larger sequences
76575021,mlnet stuck on pretrained model fit method,c vectorization wordembedding pretrainedmodel microsoftml,im an idiot to resolve this issue you should download word embeddings to your appdatalocalmlnetresourceswordvectors folder as example adding downloaded file glovebdtxt to that folder succeed the debug completely and fit works correctly without stuck process so i close this question with my own answer
75723102,how does gensim calculate sentence embeddings when using a pretrained fasttext model,gensim wordembedding fasttext,in gensim you should use getsentencevector method which was recently added please read the docs and notice that this method expects a list of words specified by string or int ids
74860397,use three transformations average max min of pretrained embeddings to a single output layer in pytorch,python machinelearning pytorch neuralnetwork wordembedding,regarding you can use torchconcat to concatenate the outputs along the appropriate dimension and then eg map them to a single output using another linear layer regarding you will have to try it yourself and see whether this is useful
74021562,bert without positional embeddings,huggingfacetransformers bertlanguagemodel wordembedding,you can do a workaround by setting the position embedding layer to zeros when you check the embeddings part of bert you can see that the position embeddings are there as a separate pytorch module from transformers import automodel bert automodelfrompretrainedbertbasecased printbertembeddings you can assign the position embedding parameters whatever value you want including zeros which will effectively disable the position embeddings bertembeddingspositionembeddingsweightdata torchzeros if you plan to finetune the modified model make sure the zeroed parameters do not get updated by setting bertembeddingspositionembeddingsrequiresgrad false this sort of bypassing the position embeddings might work well when you train a model from scratch when you work with a pretrained model such removal of some parameters might confuse the models quite a bit so more finetuning data might be needed in this case there might be better strategies on how to replace the position embeddings eg using the average value for all positions
71969499,pytorch loading pretrained weights from json file,pytorch wordembedding,
71863257,how to replace keras embedding with pretrained word embedding to cnn,tensorflow keras deeplearning wordembedding,this reads the text file containing the weights stores the words and their weights in a dictionary then maps them into a new matrix using the vocabulary of your fit tokenizer
71802729,keras semantic similarity model from pretrained embeddings,keras neuralnetwork wordvec similarity wordembedding,its unclear what youre intending to predict do you want your keras nn to report the same value as the precise cosinesimilarity calculation between the two text summary vectors would report if so why not just do the calculation its not something id necessarily expect a neuralarchitecture to approxmate better alternatively if your tiny pair dataset is the target your existing gold standard answers dont seem obviously correct to me superficially olive plant black olives seem nearly as similar as tomato tomato substance similarly watering plants cornsalad plant aboutassimilar as sweet potatoes potato chip a mere examples maybe after traintest split is both inadequate to usefully train a larger neural classifier and to the extent the classifer might be easily trained indeed overfit to the training examples it wont necessarily have learned anything generalizable to the one holdout example which is using vectors quite far from the training texts with such a paucity of training data testing using inputs that might be arbitrarily different than the training data very poor performance would be expected neural nets require lots of varied training examples finally the strategy of creating combinedembeddingsbyaveraging as investigated by your linked paper is another atypical practice that seems fishy to me even if it could offer some benefits theres no reason to mix that atypical somewhat nonintuitive extra practice into your experiment before even having things work with a more typical and simple baseline approach for comparison to be sure the extra metaaveraging is worth the complication the paper itself doesnt really show any advantage over concatenation which has a stronger theoretical basis preserving each models full independent spaces than averaging except by a tiny amount in of tests further average of glove cbow performs the same or worse than glove alone on on their evaluations and pretty minimally better on the other evaluations that implies to me the outperformance might be mainly random jitter introduced by the extra steps and the averaging is at best a cheap option to consider as a tiny boost not a generallybetter approach the paper also leaves many natural related questions unaddressed is averaging better than say just picking a random half of each models dimensions for concatenation thatd be even cheaper might some of the slight lift in some tasks be due not to the averaging but the other transformations theyve applied the lnormalization applied to each source model or across the whole of each dimension for the glove model its unclear if this modelpostprocessing was only applied before dualmodel averaging or also to glove in its solo evaluation theres other work suggesting posttraining transformations of wordvector spaces may improve performance on downstream tasks see for example all but the top so which steps exactly get which advantages is important to distinguish
70057975,how to get cosine similarity of word embedding from bert model,python bertlanguagemodel wordembedding transformermodel,okay lets do this first you need to understand that bert has layers the first layer is basically just the embedding layer that bert gets passed during the initial training you can use it but probably dont want to since thats essentially a static embedding and youre after a dynamic embedding for simplicity im going to only use the last hidden layer of bert here youre using two words new and york you could treat this as one during preprocessing and combine it into newyork or something if you really wanted in this case im going to treat it as two separate words and average the embedding that bert produces this can be described in a few steps tokenize the inputs determine where the tokenizer has wordids for new and york suuuuper important pass through bert average cosine similarity first what you need to import from transformers import autotokenizer automodel now we can create our tokenizer and our model make sure to use the model in evaluation mode unless youre trying to fine tune next we need to tokenize step step need to determine where the index of the words match the above code checks where the wordids produced by the tokenizer overlap the word indices from the original sentence this is necessary because the tokenizer splits rare words so if you have something like aardvark when you tokenize it and look at it you actually get this step pass through bert now we grab the embeddings that bert produces across the token ids that weve produced now you will have two embeddings each is shape the first size is because you have two words were looking at new and york the second size is the embedding size of bert step average okay so this isnt necessarily what you want to do but its going to depend on how you treat these embeddings what we have is two shaped embeddings you can either compare new to new and york to york or you can combine new york into an average ill just do that but you can easily do the other one if it works better for your task step cosine sim cosine similarity is pretty easy using torch this is good they point in the same direction theyre not exactly but that can be improved in several ways you can fine tune on a training set you can experiment with averaging different layers rather than just the last hidden layer like i did you can try to be creative in combining new and york i took the average but maybe theres a better way for your exact needs
68817989,bert model output interpretation,translation bertlanguagemodel huggingfacetransformers wordembedding,i think one possible answer to your dilemma is provided in this question practically with the output of bert you get a vectorized representation for each of your words in essence it is easier to use the output for other tasks but trickier in the case of machine translation a good starting point of using a seqseq model from the transformers library in the context of machine translation is the following the example above provides how to translate from english to romanian
68815926,how to combine embeddins vectors of bert with other features,python pythonx bertlanguagemodel wordembedding,there is not one perfect way to tackle this problem but a simple solution will be to concat the bert embeddings with hardcoded features the bert embeddings sentence embeddings will be of dimension if you have used bert base these embeddings can be treated as features of the sentence itself the additional features can be concatenated to form a higher dimensional vector if the features are categorical it will be ideal to convert to onehot vectors and concatenate them for example if you want to use level in your example as set of input features it will be best to convert it into onehot feature vector and then concatenate with bert embeddings however in some cases your hard coded features can be dominant feature to bias the classifier and in some other cases it can have no influence at all it all depends on the data that you have
68111122,how to concatenate new vectors into existing bert vector,tensorflow embedding bertlanguagemodel wordembedding,you first need to ensure that the vector with whom you want to concatenate has the same dimension on the axis you want to concatenate eg then you could use tfkeraslayersconcatenatepooledoutputentitylayeraxisaxis on the desired axis you can also have a look here for more details
67199914,can you integrate your pretrained word embeddings in a custom spacy model,spacy wordembedding namedentityrecognition,yes convert your vectors from wordvec text format with spacy init vectors and then specify that model as initializevectors in your config along with includestaticvectors true for the relevant tokvec models a config excerpt you can also use spacy init config o accuracy configcfg to generate a sample config including vectors that you can edit and adjust as you need see
67099706,when should i consider to use pretrainmodel wordvec model weights,python gensim wordvec wordembedding pretrainedmodel,the general answer to this type of question is you should try them both and see which works better for your purposes no one without your exact data project goals can be sure which will work better in your situation and youll need to exact same kind of abilitytoevaluate alterante choices to do all sorts of very basic necessary tuning of your work separately finetuning wordvecvectors can mean many things and can introduce a number of expertleve thorny tradeoffdecisions the sorts of tradeoffs that can only be navigated if youve got a robust way to test different choices against each other the specific simple tuning approach your code shows which relies on an experimental method intersectwordvecformat that might not work in the latest gensim is pretty limited and since it discards all the words in the outside vectors that arent already in your own corpus also discards one of the major reasons people often want to mix older vectors in to cover more words not in their training data i doubt that approach will be useful in many cases but as per above to be sure youd want to try it with respect to your datagoals its almost always a bad idea to use mincount with wordvec similar algorithms if such rare words are truly important find more training examples so good vectors can be trained for them but without enough training examples theyre usually better to ignore keeping them even makes the vectors for surrounding words worse
66021131,how to load pretrained glove model with gensim loadwordvecformat,stanfordnlp gensim wordvec wordembedding,the glove format is slightly different missing a stline declaration of vectorcount dimensions than the format that loadwordvecformat supports theres a glovewordvec utility script included you can run once to convert the file also starting in gensim currentlyu in prerelease testing the loadwordvecformat method gets a new optional noheader parameter if set as noheadertrue the method will deduce the countdimensions from a preliminary scan of the file so it can read a glove file with that option but at the cost of two fullfile reads instead of one so you may still want to resave the object with savewordvecformat or use the glovewordvec script to make future loads faster
65304058,how to use my own corpus on word embedding model bert,wordembedding bertlanguagemodel huggingfacetransformers,got it the solution was really easy i assumed that the variable lines was already a str but that wasnt the case just by casting to a string the questionanswering model accepted my testtxt file so from to
64949799,can the gensim pretrained models be used for docvec models,python gensim wordembedding docvec,most of the models currently listed there as of are just sets of wordvectors allowing lookup of vectors by individual word but not the full algorithmic model that would allow for followup training the only exception i see is the fasttext model which might be a full fasttext model im not sure but even there the model only reports wordvectors for known words or synthesizes a vector for outofvocabulary words with no native method of creating vectors for larger texts from any set of wordvectors there are some crude ways to either create a simple vector for larger texts such as averaging all the wordvectors for the words of the text together or do other comparisons between sets of words using the wordvectors to influence the similarity such as the word movers distance algorithm available on gensim wordvector sets as wmdistance but none of those models availabe via the gensimdownloader utility are for algorithms that inherently create vectors for larger texts such as docvec separately i would strongly recommend downloading models explicitly as data from their original locations rather than using the gensimdownloader utility it obscures key aspects of the process including running extra shim code for each dataset that is downloaded outside of normal codeversioning packageinstallation processes a practice that i consider recklessly insecure
64377890,is there a pretrained gensim phrase model,python machinelearning gensim wordembedding phrase,im not aware of anyone sharing a phrases model any such model would be very sensitive to the preprocessingtokenization step and the specific parameters the creator used other than the highlevel algorithm description i havent seen googles exact choices for tokenizationcanonicalizationphrasecombination done to the data that fed into the googlenews wordvectors have been documented anywhere some guesses about preprocessing can be made by reviewing the tokens present but im unaware of any code to apply similar choices to other text you could try to mimic their unigram tokenization then speculatively combine strings of unigrams into everlonger multigrams up to some maximum check if those combinations are present and when not present revert to the unigrams or largest combination present this might be expensive if done naively but be amenable to optimizations if really important especially for some subset of the morefrequent words as the googlenews set appears to obey the convention of listing words in descending frequency in general though its a quick easy starting set of wordvectors i think googlenews is a bit overrelied upon it will lack wordsphrases and new senses that have developed since and any meanings it does capture are determined by news articles in the years leading up to which may not match the dominant senses of words in other domains if your domain isnt specifically news and you have sufficient data deciding your own domainspecific tokenizationcombination will likely perform better
64365639,extracting word features from bert model,wordembedding bertlanguagemodel latentsemanticanalysis,the very first layer of bert is a static embeddings table so you can use it as any other embeddings table and embeddings for words or more frequently subwords that bert uses input to the first selfattentive layer the static embeddings are only comparable with each other not with the standard contextual embeddings if need them comparable embeddings you can try passing singleword sentences to bert but note that this will be an embeddings of a singleword sentenece not the word in general however bert is a sentencelevel model that is supposed to get embeddings of words in context it is not designed for static word embeddings and methods specifically designed for static word embeddings such as fasttext would certainly get better results
63002081,download pretrained bert model locally,pythonx wordembedding,after instantiating the sentencetransformer via download you can then save it to any path of your choosing with the save method the accepted answer doesnt work as it doesnt have the encapsulating folder and configjson that sentencetransformer is looking for
62669261,how to encode multiple sentences using transformersberttokenizer,wordembedding huggingfacetransformers huggingfacetokenizers,transformers use call method of the tokenizer it will generate a dictionary which contains the inputids tokentypeids and the attentionmask as list for each input sentence tokenizerthis is the first sentence another setence output transformers use tokenizerbatchencodeplus documentation it will generate a dictionary which contains the inputids tokentypeids and the attentionmask as list for each input sentence tokenizerbatchencodeplusthis is the first sentence another setence output applies to call and batchencodeplus in case you only want to generate the inputids you have to set returntokentypeids and returnattentionmask to false tokenizerbatchencodeplusthis is the first sentence another setence returntokentypeidsfalse returnattentionmaskfalse output
62485231,why pytorch transformer srcmask doesnt block positions from attending,pytorch wordembedding transformermodel,as far as i understand the model doesnt prevent each word to indirectly see itself in multylayer context i tried to use one layer it looks like the model works but training is too slow
62308418,word embedding with gensim and fasttext training on pretrained vectors,python gensim wordembedding fasttext,i believe but am not certain that in this particular case youre getting this error because youre trying to load a set of justplain vectors which fasttext projects tend to name as files ending vec with a method thats designed for use on the fasttextspecific format that includes subwordmodel info as a result its misinterpreting the files leading bytes as declaring the model as one using fasttexts supervised mode gensim truly doesnt support such full models in that lesscommon mode but it could load the endvectors from such a model and in any case your file isnt truly from that mode released files that will work with loadfacebookvectors typically end with bin see the docs for this method for more details so you could either supply an alternate binnamed facebookfasttextformatted set of vectors with subword info to this method from a quick look at their download options i believe their file analogous to your st try would be named crawldmsubwordbin be about gb in size load the file you have with just its fullword vectors via from gensimmodels import keyedvectors model keyedvectorsloadwordvecformatfasttextcrawldmvec binaryfalse in this latter case no fasttextspecific features like the synthesis of guessvectors for outofvocabulary words using subword vectors will be available but that info isnt in the crawldmvec file anyway those features would be available if you used the larger bin file loadfacebookvectors method above
57685633,should the vocabulary be restricted to the trainingset vocabulary when training an nn model with pretrained wordvec like glove,keras neuralnetwork wordembedding glove,yes it is better to restrict your vocab size because pretrained embeddings like glove have many words in them that are not very useful and so wordvec and the bigger vocab size the more ram you need and other problems select your tokens from all of your data it wont lead to a limited nongeneralizable model if your data is big enough if you think that your data does not have as many tokens as are needed then you should know things your data is not good enough and you have to gather more your model cant generate well on the tokens that it hasnt seen at training so it has no point to having many unused words on your embedding and better to gather more data to cover those words i have an answer to show how you can select a minor set of word vectors from a pretrained model in here
53348473,using gloves pretrained glovebtxt as a basis for word embeddings r,r wordembedding textvec glove,i eventually figured it out the embeddings matrix is all i needed it already has the words in their vocab as rownames so i use those to determine the vector of each word now i need to figure out how to update those vectors
53180173,keras autoencoder with pretrained embeddings returning incorrect number of dimensions,python machinelearning keras deeplearning wordembedding,embedding layer takes as input a sequence of integers ie word indices with a shape of numwords and gives the corresponding embeddings as output with a shape of numwords embddim so after fitting the tokenizer instance on the given texts you need to use its textstosequences method to transform each text to a sequence of integers further since after padding encodedtraintext it would have a shape of numsamples maxlength the output shape of the network must also have the same shape ie since we are creating an autoencoder and therefore you need to remove the returnsequencestrue argument of last layer otherwise it would give us a d tensor as output which does not make sense as a side note the following line is redundant as paddedtraintext is already a numpy array and by the way you have not used paddingtraintext at all
52444089,word embeddings in tensorflow no pretrained,tensorflow deeplearning embedding wordembedding,yes those embeddings are trained further just like weights and biases otherwise representing words with some random values wouldnt make any sense those embeddings are updated while training like you would update a weight matrix that is by using optimization methods like gradient descent or adam optimizer etc when we use pretrained embeddings like wordvec theyre already trained on very large datasets and are quite accurate representations of words already hence they dont need any further training if you are asking how those are trained there are two main training algorithms that can be used to learn the embedding from the text they are continuous bag of words cbow and skip grams explaining them completely is not possible here but i would suggest taking help from google this article might get you started
52126539,using pretrained gensim wordvec embedding in keras,python keras gensim wordvec wordembedding,with the new gensim version this is pretty easy there you have your keras embedding layer
51382664,how to correctly use maskzerotrue for keras embedding with pretrained weights,python tensorflow keras wordembedding,youre second approach is correct you will want to construct your embedding layer in the following way where embeddingmatrix is the second matrix you provided you can see this by looking at the implementation of keras embedding layer notably how maskzero is only used to literally mask the inputs thus the entire kernel is still multiplied by the input meaning all indexes are shifted up by one
50914729,gensim wordvec select minor set of word vectors from pretrained model,python keras wordvec gensim wordembedding,thanks to this answer ive changed the code a little bit to make it better you can use this code for solving your problem we have all our minor set of words in restrictedwordsetit can be either list or set and wv is our model so here is the function import numpy as np def restrictwvwv restrictedwordset newvectors newvocab newindexentity newvectorsnorm for i in rangelenwvvocab word wvindexentityi vec wvvectorsi vocab wvvocabword vecnorm wvvectorsnormi if word in restrictedwordset vocabindex lennewindexentity newindexentityappendword newvocabword vocab newvectorsappendvec newvectorsnormappendvecnorm wvvocab newvocab wvvectors nparraynewvectors wvindexentity nparraynewindexentity wvindexword nparraynewindexentity wvvectorsnorm nparraynewvectorsnorm warning when you first create the model the vectorsnorm none so you will get an error if you use this function there vectorsnorm will get a value of the type numpyndarray after the first use so before using the function try something like mostsimilarcat so that vectorsnorm not be equal to none it rewrites all of the variables which are related to the words based on the wordveckeyedvectors usage beers lager beer drinks lagers yuenglinglager microbrew brooklynlager suds brewedbeer lagers wine bash computer python it can be used for removing some words either
49710537,pytorch gensim how do i load pretrained word embeddings,python pytorch neuralnetwork gensim wordembedding,i just wanted to report my findings about loading a gensim embedding with pytorch solution for pytorch and newer from v there is a new function frompretrained which makes loading an embedding very comfortable here is an example from the documentation the weights from gensim can easily be obtained by as noted by guglie in newer gensim versions the weights can be obtained by modelwv solution for pytorch version and older im using version and frompretrained isnt available in this version therefore i created my own frompretrained so i can also use it with code for frompretrained for pytorch versions or lower the embedding can be loaded then just like this i hope this is helpful for someone
48677077,how do i create a keras embedding layer from a pretrained word embedding dataset,python tensorflow keras wordvec wordembedding,you will need to pass an embeddingmatrix to the embedding layer as follows embeddingvocablen embdim weightsembeddingmatrix trainableistrainable vocablen number of tokens in your vocabulary embdim embedding vectors dimension in your example embeddingmatrix embedding matrix built from glovebdtxt istrainable whether you want the embeddings to be trainable or froze the layer the glovebdtxt is a list of whitespaceseparated values word token embedding values eg the to create a pretrainedembeddinglayer from a glove file
44060592,weights update in tensorflow embedding layer with pretrained fasttext weights,tensorflow wordembedding,you are correct about the problem when using pretrained vector and finetuning them in your final model the words that are infrequent or hasnt appear in your training set wont get any updates now usually one can test how much of the issue for your particular case this is eg if you have a validation set try finetuning and not finetuning the weights and see whats the difference in model performance on validation set if you see a big difference in performance on validation set when you are not finetuning here is a few ways to handle this a add a linear transformation layer after nottrainable embeddings finetuning embeddings in many cases does affine transformations to the space so one can capture this in a separate layer that can be applied at test time eg a is pretrained embedding matrix embeds tfnnembeddinglookupa tokens x tfgetvariablex embedsize embedsize b tfgetvairableb embedsize embeds tfmulembeds x b b keep pretrained embeddings in the nottrainable embedding matrix a add trainable embedding matrix b that has a smaller vocab of popular words in your training set and embedding size lookup words both in a and b and if word is out of vocab use id for example concat results and use it input to your model this way you will teach your model to use mostly a and sometimes rely on b for popular words in your training set fixedembeds tfnnembeddinglookupa tokens b tfgetvariableb smallervocabsize embedsize oovtokens tfwheretflesstokens smallervocabsize tokens tfzerostfshapetokens dtypetokensdtype dynembeds tfnnembeddinglookupb oovtokens embeds tfconcatfixedembeds dynembeds
40345607,how does finetuning word embeddings work,machinelearning deeplearning wordembedding,yes if you feed the embedding vector as your input you cant finetune the embeddings at least easily however all the frameworks provide some sort of an embeddinglayer that takes as input an integer that is the class ordinal of the wordcharacterother input token and performs a embedding lookup such an embedding layer is very similar to a fully connected layer that is fed a onehot encoded class but is way more efficient as it only needs to fetchchange one row from the matrix on both front and back passes more importantly it allows the weights of the embedding to be learned so the classic way would be to feed the actual classes to the network instead of embeddings and prepend the entire network with a embedding layer that is initialized with wordvec glove and which continues learning the weights it might also be reasonable to freeze them for several iterations at the beginning until the rest of the network starts doing something reasonable with them before you start fine tuning them
78506114,spacy transformer ner training zero loss on transformer not trained,python machinelearning spacy spacy spacytransformers,transformers are touchy when it comes to exactness of settings i ended up using command to generate configcfg for me that way i know where exactly i deviated from working version if something goes wrong as long as i change params one by one this worked right away
77360174,map bert token indices to spacy token indices,python mapping spacy tokenize bertlanguagemodel,use a fast tokenizer to get the character offsets directly from the transformer tokenizer with returnoffsetsmappingtrue and then map those to the spacy tokens however youd like from transformers import autotokenizer tokenizer autotokenizerfrompretrainedbertbaseuncased text britains railways cost bn output tokenizertext returnoffsetsmappingtrue printoutputinputids printtokenizerconvertidstotokensoutputinputids cls britain s railways cost bn sep printoutputoffsetmapping
77354502,attributeerror module transformers has no attribute berttokenizerfast,python pip spacy,try downgrading spacy to with and then try
76158903,installing spacy for gpu training of transformer,python gpu spacy spacytransformers,after reading the installation documentation and seeing the warning that you got it seems that spacy does not currently support cuda the installation guide is misleading because you can select the version of cuda and there is a version for cuda which suggests that anything above is supported however that is not the case here is what i mean by the installation guide being misleading your best bet would be to downgrade your version of cuda to cuda if youre looking for the cuda links on the nvidia website here is a github discussion that talks about which version of cuda should be used with both spacy and pytorch after the release of cuda
73856995,how can i use a model trained with the transformers trainer in the spacy pipeline,spacy huggingfacetransformers spacytransformers,spacy uses spacytransformers to wrap huggingface transformers but it only allows using the models as a source of features it doesnt allow using taskspecific heads like ner so theres not an easy way to use this in spacy if you want to load your model as a source of features see the guide here on how to specify your filename another option is to load your huggingface model separately and wrap it as a spacy component but thats kind of involved and usually not very useful
73024546,why dont spacy transformer models do ner for nonenglish models,spacy namedentityrecognition spacytransformers,the spacy models vary with regards to which nlp features they provide this is just a result of how the respective authors createdtrained them ie lists ner in its components but does not the spanish as well the two smaller variants does list ner in its components so they show named entities
72519750,installing spacy language models directly from conda environmentyml,python pip anaconda spacy,as the spacy documentation of download command states downloading best practices the download command is mostly intended as a convenient interactive wrapper it performs compatibility checks and prints detailed messages in case things go wrong its not recommended to use this command as part of an automated process if you know which package your project needs you should consider a direct download via pip or uploading the package to a local pypi installation and fetching it straight from there this will also allow you to add it as a versioned package dependency to your project while it is possible to include pypi dependencies in a conda environment yaml conda forge also publishes spacy models as packages via the spacymodelsfeedstock in the example from op this would mean adding the package spacymodelencorewebsm
71635441,retrain custom language model with the current spacy version compatibility issues,python pythonx spacy spacy,in nearly all cases spacy v models are forwardscompatible with newer versions of spacy v so download jacorenewstrf and then install the vietnamese model with pip install nodeps so that pip doesnt install an older version of spacy as a dependency youll get a warning on load that an older model might be incompatible but test it on your data and as long as the performance is the same as with the older version of spacy it should be fine to use see you can only retrain the model if you have access to the original training data
70772641,how to resume training in spacy transformers for ner,deeplearning spacy namedentityrecognition transformermodel,the vectors setting is not related to the transformer or what youre trying to do in the new config you want to use the source option to load the components from the existing pipeline you would modify the component blocks to contain only the source setting and no other settings see
69738938,how to use existing huggingfacetransformers model into spacy,spacy huggingfacetransformers bertlanguagemodel spacytransformers,what you do is add a transformer component to your pipeline and give the name of your huggingface model as a parameter to that this is covered in the docs though people do have trouble finding it its important to understand that a transformer is only one piece of a spacy pipeline and you should understand how it all fits together to pull from the docs this is how you specify a custom model in a config going back to why you need to understand spacys structure its very important to understand that in spacy transformers are only sources of features if your huggingface model has an ner head or something it will not work so if you use a custom model youll need to train other components like ner on top of it also note that spacy has a variety of nontransformers builtin models these are very fast to train and in many situations will give performance comparable to transformers even if they arent as accurate you can use the builtin models to get your pipeline configured and then just swap in a transformer all guides examples and posts i found start from a spacy structured model like spacyencorewebsm but how did that model was created in the first place did you see the quickstart the pretrained models are created using configs similar to what you get from that
69459301,ner using spacy transformers different result when running inside and outside of a loop,python spacy huggingfacetransformers namedentityrecognition,you are using the csv module to read your file and then trying to convert each row aka line of the file to a string with strrow if your file just has one sentence per line then you do not need the csv module at all you could just do with opensynthdatasetrawtxt r as fd for line in fd remove the trailing newline line linerstrip sent nlpline displacyrendersent style ent if you in fact have a csv with presumably multiple columns and a header you could do opensynthdatasetrawtxt r as fd reader csvreaderfd header nextreader textcolumnindex for row in reader sent nlprowtextcolumnindex displacyrendersent style ent
68061849,get spacy ner to search only for company name and not waste computing power on anything else using existing language models,python spacy,you cant change the models to only tag one named entity you can ignore entities you dont care about trivially you cant cut out the other entities because they arent like separate functions the model uses its knowledge about all the different types to help it figure out ambiguous cases like knowing john smith is a person but john deere is probably a company the good news is that it is not processing useless information or wasting computing power if you trained a model to recognize just org entities it wouldnt be faster or anything
67720450,keyerror packaging autofill the config file spacy bert model,python config googlecolaboratory spacy,sorry you ran into that weve had one report of that error before it seems like something is weird with cupy on colab specifically based on the previous report you should start with a clean python environment and should not install cupy directly i think colab uses a special version or something
65030816,downloading language model in spacy breaks in docker,docker spacy,before you install python packages add the following line run pip install upgrade pip
64800623,spacybertspacycake cannot perform reduction function max on tensor,python spacy huggingfacetransformers,i think you are using wrong model this model is for other purpose huggingface transformer model is pretrained bertbaseuncased here you are using wrong model see this
64047426,find out all the language models installed in spacy,python spacy,i think this would do the job list spacys languages models
63405961,valueerror when using columntransformer in an sklearn pipeline using custom class of spacy for glovevectorizer,python pandas machinelearning scikitlearn spacy,the error message tells you what you need to fix valueerror the output of the titleglove transformer should be d scipy matrix array or pandas dataframe but what you are returning with your current transformer spacyvectortransformer is a list you can fix it by turning the list into a pandas dataframe for instance like this next time please also provide a minimal reproducible example in your provided code there are no imports as well as no dataframe called df
62038216,spacy different language models,spacy detect namedentityrecognition,here is a link that you might find useful when it comes to detecting a language there are multiple options including langdetect how to detect language you can create a dictionary with the languages you plan to detect and match it with langdetects output i guess you have the rest sorted out
61902426,cased vs uncased bert models in spacy and train data,python spacy bertlanguagemodel,as a nongermanspeaker your comment about nouns being uppercase does make it seem like case is more relevant for german than it might be for english but that doesnt obviously mean that a cased model will give better performance on all tasks for something like partofspeech detection case would probably be enormously helpful for the reason you describe but for something like sentiment analysis its less clear whether the added complexity of having a much larger vocabulary is worth the benefits as a human you could probably imagine doing sentiment analysis with all lowercase text just as easily given that the only model available is the cased version i would just go with that im sure it will still be one of the best pretrained german models you can get your hands on cased models have separate vocab entries for differentlycased words eg in english the and the will be different tokens so yes during preprocessing you wouldnt want to remove that information by calling lower just leave the casing asis
61899118,cannot load german bert model in spacy,python spacy transformermodel bertlanguagemodel,its probably a problem with your installation of torch start in a clean virtual environment and install torch using the instructions here with cuda as none then install spacytransformers with pip
61643370,spacy english language model take too long to load,python chatbot spacy namedentityrecognition,this is really slow because it loads the model for every sentence this is not slow because it loads the model once and reuses it for every function call you should change your application to look like the second example this is not specific to spacy but will be the case with any kind of model you choose to use
60823095,pretrained vectors not loading in spacy,spacy namedentityrecognition,you could try to pass all vectors at once instead of using a for loop so youre else statement would become like this
60759015,spacytransformers regression output,machinelearning pytorch spacy spacytransformers,i have figured out a workaround extract vector representations from the nlp pipeline as after doing so for all the text entries you end up with a fixeddimensional representation of the texts assuming you have the continuous output values feel free to use any estimator including neural network with a linear output note that the vector representation is an average of the vector embeddings of all the words in the text it might be a suboptimal solution for your case
59413468,how to use the pretrained transformer model entrfbertbaseuncasedlg in spacy,python spacy transformermodel,the offical documentation explains that you can use the bert spacy model entrfbertbaseuncasedlg model to get word embeddings for sentence tokens install the spacy bert model and spacytransformers module pip install spacytransformers python m spacy download entrfbertbaseuncasedlg below is a slightly adapted example from the official documentation the word embeddings could be used in further downstream nlpmachine learning task
57697374,list most similar words in spacy in pretrained model,python spacy,i used andys response and it worked correctly but slowly to resolve that i took the approach below spacy uses the cosine similarity in the backend to compute similarity therefore i decided to replace wordsimilarityw with its optimized counterpart the optimized method that i worked with was cosinesimilaritynumbawvector wordvector shown below that uses the numba library to speed up computations you should replace line in the mostsimilar method with the line below the method became times faster which was essential for me i explained it in more details in this article how to build a fast mostsimilar words method in spacy
57652054,loading pretrained word embeddings,pythonx wordvec spacy,spacy expects the vectors to be in the text format rather than the binary format for how to convert the binary model see
57476279,model got multiple values for argument nrclass spacy multiclassification model bert integration,python pytorch spacy multiclassclassification spacytransformers,this is a regression in the most recent version we released of spacypytorchtransformers sorry about this the root cause is this is another case of the evils of kwargs im looking forward to refining the spacy api to prevent these issues in future you can see the offending line here we provide the nrclass positional argument which overlaps with the explicit argument you passed in during the config in order to workaround the problem you can simply remove the nrclass key from your the config dict youre passing into spacycreatepipe
56615584,reload spacy language model on a running script,python spacy,if you only use nlp in spacyclases then there is one way really not recommend but can work the above way to change nlp in fact is using a global writable variable in many many files which is very very bad thing
55280666,spacy language model installation in python returns importerror from mklinit importerror dll load failed the specified module could not be found,python pip conda importerror spacy,thanks to ines montani for pointing this out it did seem that a quick reinstallation of numpy helps solve the problem however what i did realize is that using pip uninstall numpy and simple pip install numpy solves the problem however using conda remove force numpy and conda install numpy does not solve the problem for me
51045347,installation of spaccy language model in python not working,python spacy rasanlu rasacore,latest spacy has a requirement of thinc as double check the requirementstxt and check which modules are installed with pip list if the issue is with incorrect version of thinc then run
54745482,what is the difference between tfidf vectorizer and tfidf transformer,python scikitlearn nltk tfidf tfidfvectorizer,tfidfvectorizer is used on raw documents while tfidftransformer is used on an existing count matrix such as one returned by countvectorizer
32331848,create a custom transformer in pyspark ml,python apachespark nltk pyspark apachesparkml,can i extend the default one not really default tokenizer is a subclass of pysparkmlwrapperjavatransformer and same as other transfromers and estimators from pysparkmlfeature delegates actual processing to its scala counterpart since you want to use python you should extend pysparkmlpipelinetransformer directly example usage data from ml features for custom python estimator see how to roll a custom estimator in pyspark mllib this answer depends on internal api and is compatible with spark or later spark for code compatible with previous spark versions please see revision
71200243,simple transformers producing nothing,python pythonx seqseq simpletransformers,use this model instead roberta is not a good option for your task i have rewritten your code on this colab notebook results
63959319,bert sentencetransformers stopsquits during fine tuning,python machinelearning bertlanguagemodel sentencesimilarity,my solution was to set batch and worker to one and its very slow
75445494,bert tokenizer punctuation for named entity recognition task,huggingfacetransformers bertlanguagemodel namedentityrecognition punctuation,not sure whether this might be a viable solution for you but heres a possible hack indeed from the documentation neversplit iterable optional collection of tokens which will never be split during tokenization only has an effect when dobasictokenizetrue
74192948,attributeerror list object has no attribute ents in building ner using bert,python pandas bertlanguagemodel namedentityrecognition,it seems you mix code from different modules ents exists in module spacy but not in transformers in transformers you should use directly nlpv but it gives directory with ententity entscore entindex entword entstart entend
70799226,ner classification deberta tokenizer error you need to instantiate debertatokenizerfast,python tokenize bertlanguagemodel namedentityrecognition roberta,lets try this
69694277,could not find function spacytransformerstransformermodelv in function registry architectures,namedentityrecognition bertlanguagemodel spacy spacytransformers,this happened since spacy had a new update recently and the baseconfig file have the architecture mentioned as spacytransformerstransformermodelv change it into spacytransformerstransformermodelv
67740759,how to apply a pretrained transformer model from huggingface,huggingfacetransformers namedentityrecognition transformermodel,in transformers ner is done with the tokenclassificationpipeline from transformers import autotokenizer pipeline automodelfortokenclassification tokenizer autotokenizerfrompretrainedemilyalsentzerbioclinicalbert model automodelfortokenclassificationfrompretrainedemilyalsentzerbioclinicalbert nerpipeline pipelinener modelmodel tokenizertokenizer text my text for named entity recognition here nerpipelinetext output please note that you need to use automodelfortokenclassification instead of automodel and that not all models have a trained head for token classification ie you will get random weights for the token classification head
64128864,are special tokens cls sep absolutely necessary while fine tuning bert,bertlanguagemodel namedentityrecognition cls,im also following this tutorial it worked for me without adding these tokens however i found in another tutorial that it is better to add them because the model was trained in this format update actually just checked it it turned out that the accuracy improved by after adding the tokens but note that i am using it on a different dataset
61107371,transformer pipeline for ner returns partial words with s,python pytorch namedentityrecognition huggingfacetransformers,pytorch transformers and bert make tokens the regular words as tokens and words subwords as tokens which divide words by their base meaning their complement addin at the start lets say you have the phrease i like hugging animals the first set of tokens would be and the second list with the subwords would be you can learn more here
79249787,how can one obtain the correct embedding layer in bert,pytorch sentimentanalysis similarity bertlanguagemodel,st approach is not a good choice because leveraging the cls token embedding directly might not be the best approach in case if the bert was fine tuned for a task other than similarity matching taskspecific embeddings the cls token embedding is affected by the task the bert model was trained on averaging taking the mean of all token embeddings we can get a more general representation of the input this method balances out the representation by considering the contextual embeddings of all tokens consider taking average or pooling passing through another dense layer will work
75172022,token indices sequence length warning while using pretrained roberta model for sentiment analysis,python sentimentanalysis robertalanguagemodel roberta,you have not shared the code where you use tokenizer to encodetokenize the inputs so im taking my own example to explain how you can achieve this example usage these above parameters will tokenize any string into maxlength tokens by padding if number of tokens is maxlength or truncating for tokens count maxlength note maxlength cannot be greater than for roberta model
73963008,problem completing bert model for sentiment classification,python tensorflow keras sentimentanalysis bertlanguagemodel,op was using bertbasecased for their model and bertbaseuncased for their tokenizer causing issues during training when the vocab size of the model and the tokenized data differed
72955752,finding the scores for each tweet with a bertbased sentiment analysis model,python twitter sentimentanalysis huggingfacetransformers bertlanguagemodel,you can inherit from the models class and define a function to output the scores
72396420,kernel keeps dying while using bertbased sentiment analysis model,python jupyternotebook sentimentanalysis,for me a similar issue was solved by just moving import torch before the transformer imports could you try editing your imports to
72288839,and must have the same shape received none vs none when using transformers,tensorflow machinelearning keras sentimentanalysis bertlanguagemodel,as described in this kaggle notebook you must build a custom keras model around the pretrained bert model to perform classification the bare bert model transformer outputing raw hiddenstates without any specific head on top here is a copy of a piece of code note you might have to adapt this code and in particular modify the input shape to seemingly from the error message your tokenizer maximum length load bert model and build the classifier summary
71338524,how can i show label output only from transformers pipeline sentiment analysis,python sentimentanalysis huggingfacetransformers,i have no idea how works your pipe but you have list with dict and pandas has str to work with string but strindex works also with list or dict minimal working example result
69820318,predicting sentiment of raw text using trained bert model hugging face,pytorch sentimentanalysis huggingfacetransformers pytorchdataloader,you can use the same code to predict texts from the dataframe column
69655995,is it necessary to retrain bert models specifically roberta model,python tensorflow sentimentanalysis bertlanguagemodel robertalanguagemodel,you can use pretrained models from huggingface there are plenty to choose from search for emotion or sentiment models here is an example of a model with emotions the current implementation works but is very slow for large datasets import pandas as pd from transformers import robertatokenizerfast tfrobertaforsequenceclassification pipeline tokenizer robertatokenizerfastfrompretrainedarpanghoshalemoroberta model tfrobertaforsequenceclassificationfrompretrainedarpanghoshalemoroberta emotion pipelinesentimentanalysis modelarpanghoshalemoroberta example data datauri dataf pdreadcsvdatauri usecolsreview text this is super slow i will find a better optimization asap dataf dataf head comment this out for the whole dataset assignemotion lambda d dreview text fillna maplambda x emotionxgetlabel none we could also refactor it a bit a bit faster than the previous but still slow def emotionfunctextstr str if not text return none return emotiontextgetlabel none dataf dataf head comment this out for the whole dataset assignemotion lambda d dreview text mapemotionfunc results review text emotion absolutely wonderful silky and sexy and comf admiration love this dress its sooo pretty i happene love i had such high hopes for this dress and reall fear i love love love this jumpsuit its fun fl love i aded this in my basket at hte last mintue to admiration i ordered this in carbon for store pick up an neutral i love this dress i usually get an xs but it love im and lbs i ordered the s petite t love material and color is nice the leg opening i neutral took a chance on this blouse and so glad i did admiration i have been waiting for this sweater coat to s excitement the colors werent what i expected either the disapproval i never would have given these pants a second love these pants are even better in person the onl disapproval i ordered this months ago and it finally ca disappointment this is such a neat dress the color is great admiration wouldnt have given them a second look but tri love this is a comfortable skirt that can span seas approval pretty and unique great with jeans or i have admiration this is a beautiful top its unique and not s admiration this poncho is so cute i love the plaid check love first this is thermal so naturally i didnt love
68383634,cuda error cublasstatusinvalidvalue error when training bert model using huggingface,python pytorch sentimentanalysis bertlanguagemodel,i suggest trying out couple of things that can possibly solve the error as shown in this forum one possible solution is to lower the batch size of how you load data since it might be a memory error if that does not work then i suggest as shown in this github issue to update to a new version of pytorch cuda that fixes a matrix multiplication bug that releases this same error that your code could be doing hence as shown in this forum you can update pytorch to the nightly pip wheel or use the cuda or conda binaries you can find information on such installations on the pytorch home page where it mentions how to install pytorch if none of that works then the best thing to do is to run a smaller version of the process on cpu and recreate the error when running it on cpu instead of cuda you will get a more useful traceback that can solve your error edit based on comments you have a matrix error in your model the problem stems in your forward func then the model bert outputs a tensor that has torchsize which means if you put it in the linear layer you have it will error since that linear layer requires input of bc you initialized it as nnlinear in order to make the error disappear you need to either do some transformation on the tensor or initialize another linear layer as shown below sarthak jain
67771257,why does transformers bert for sequence classification output depend heavily on maximum sequence length padding,sentimentanalysis bertlanguagemodel huggingfacetransformers huggingfacetokenizers,this is caused because your comparison isnt correct the sentence de samenwerking gaat de laatste tijd beter has actually tokens for the specialtokens and not you only counted the words which are not necessarily the tokens printtokenizertokenizesent printlentokenizertokenizesent output when you set the sequence length to you are truncating the sentence to tokenizerdecodetokenizersent maxlength padding maxlength truncation true returntensors pt addspecialtokensfalse inputids output and as final prove the output when you set maxlength to is also
79458703,how can i load a pretrained transformers model that was manually downloaded,python huggingfacetransformers,it looks like the provided tensorflow statedict is missing some weights when you load the pytorch variant the results look pretty decent from transformers import automodelforsequenceclassification autotokenizer pipeline localpath contenttwitterrobertabasesentimentlatest tokenizer autotokenizerfrompretrainedlocalpath model automodelforsequenceclassificationfrompretrainedlocalpath analyze pipelinetasksentimentanalysis modelmodel tokenizertokenizer printanalyzethis is good printanalyzethis is bad output label positive score label negative score but when you execute the same code with the tensorflow equivalent ie tfautomodelforsequenceclassification instead of automodelforsequenceclassification you receive a warning that the classifier important component is randomly initialized and the result you receive are not useful at all there was an issue reported addressing that in link but no action so far from the model creators you can fix the tensorflow statedict if you have to use this framework please open a separate question if you need assistance to answer your original question on how to load locally stored models from huggingface you usually dont need to figure out which classes you need because the pipeline will already take care of that for you you can simplify your code in the following way from transformers import pipeline localpath contenttwitterrobertabasesentimentlatest analyze pipelinetasksentimentanalysis modellocalpath you can also control with the framework parameter values tf or pt which framework you want to use in general something you experienced with cardiffnlptwitterrobertabasesentimentlatest can happen again therefore always verify the results and read the warning messages
79375287,gpu utilization almost always during training hugging face transformer,python machinelearning huggingfacetransformers,you seem to have an io bottleneck it means the data cannot be transfered fast enough and your gpu ends up waiting for the data most of the time you can verify that claim by checking the status of the python workers in htop you do not seem to have a cpu bottleneck because your cpu isnt fully used this often happens on vms when the data is being transfered using old protocols like nfs if the vm youre using has a local disk you can try copying the data there before the training and point your huggingface dataset to that local path this could also be due to a suboptimal configuration of the data loading process you might want to give this a read you might not be seeing this issue on your pc because your gpu is slower than an h hence takes more time to process a single batch as a result your system has more time to load the next batch your data is stored in your local disk and therefore the time to load the data is much smaller and yes please increase your number of workers it can drastically improve the performance
79355967,huggingface pretrained model for finetuning has trainable parameters,pytorch huggingfacetransformers finetuning,this is the expected behavior the library cant freeze the layers for you you can freeze them yourself by setting requiresgrad to false for certain layers as shown below from transformers import automodelforsemanticsegmentation modelname nvidiasegformerbfinetunedcityscapes model automodelforsemanticsegmentationfrompretrainedmodelname printtrainableparametersmodel freezing everything except the decoder head for name param in modelnamedparameters if not namestartswithdecodehead paramrequiresgrad false printtrainableparametersmodel output trainable params all params trainable trainable params all params trainable
79354534,how to load a finetuned vision llm model moondream model case,python artificialintelligence huggingfacetransformers largelanguagemodel,the issue probably arises due to significant structural changes made to the moondream models config and renaming in the hf repository as seen in this commit if youve saved your checkpoint using savepretrained it no longer align with the new structure of the repository heres how i solved the problem temporarly maybe there needs to be a new legacy moondream model to solve these changes in the long term workaround copy the repository content from huggingface a commit prior to the breaking changes into the directory where your saved model resides remove any remote references in configjson for example replace with if the model complains about missing py files paste them into the local directory in my case this was in the hugginface cache folder now load the model with the updated files i know this isnt a perfect solution but maybe it helps let me know if you have a better one
79338718,do i have to write custom automodel transformers class in case typeerror nvembedmodelforward got an unexpected keyword argument inputsembeds,deeplearning huggingfacetransformers largelanguagemodel peft,this is similar to this issue the reason is that tasktypefeatureextraction expects an additional inputembeds input which this model does not provide neither does clip for example remove the tasktypefeatureextraction basically setting tasktypenone so that you receive a plain peftmodel instance instead of a peftmodelforfeatureextraction and this problem should vanish
79309854,valueerror exception encountered when calling layer tfbertmodel type tfbertmodel,tensorflow tensorflow huggingfacetransformers bertlanguagemodel transformermodel,tldr use this explanation transformers package uses keras objects current version is keras packed in tensorflow since version fastest fix without downgrading tensorflow is to set legacy keras usage flag as above more info can be found here
79296215,how can i measure gender or racial bias in a transformerbased language model,pythonx huggingfacetransformers mlm,the mlmbias package should work for evaluating biases in pretrained mlms available through huggingface along with finetunedretrained mlms you can also compute the relative bias between two mlms or evaluate retrained mlms versus their pretrained base you can use the package to compute bias scores across various bias types gender racial socioeconomic etc with benchmark datasets like crowspairs cps and stereoset ss intrasentence or custom datasets after installing it with pip install mlmbias the following code works for me python import mlmbias load sample from the crowspairs cps benchmark dataset cpsdataset mlmbiasbiasbenchmarkdatasetcps cpsdatasetsampleindiceslistrange specify the model name or path model bertbaseuncased initialize the biasmlm evaluator mlmbias mlmbiasbiasmlmmodel cpsdataset evaluate the model result mlmbiasevaluateincattentiontrue save the results resultsavebertbaseuncasedresults print the bias scores printresultbiasscores print the eval results printresultevalresults for examples on how to load custom or locally saved models check out the hugging face documentation
79241735,unexpected transformers dataset structure after settransform or withtransform,python machinelearning neuralnetwork huggingfacetransformers huggingfacedatasets,this is not the way how you pick up the dataset items first you need to indicate the slice by using indexing then you can use the key labels regarding the second issue with saving the data you are not able to save it because of the known issue with transform functions you might however save the dataset as prepareddswithformatnonesavetodisktestpath but after loading it again from disk you need to launch again the transform function edited you cannot use prepareddstrainlabels as labels is expected to be integers representing indices of the items
79184146,error bus error running the simplest example on hugging face transformers pipeline macos m,huggingfacetransformers huggingface,using device the first gpu solved this classifier pipelinesentimentanalysis device
78999652,error during the compilation of the tokenizers package when trying to install transformers,artificialintelligence huggingfacetransformers largelanguagemodel,chatgpt suggested you can try using python or to see if the issue is resolved since my python version was i downgraded to and reran pip install this successfully resolved the problem ive noticed that gemini flash only suggests me to update rust and cargo while gpto mini additionally mentions the issue of python version i have been using gemini before it seems i should compare these two models more in the future
78992237,berttokenizerfrompretrained raises unicodedecodeerror,python huggingfacetransformers,frompretrained take as input the path to the directory containing model weights saved using savepretrained not the bin file you can save your model modelsavepretrainedmymodeldirectory then you can load it berttokenizerfrompretrainedmymodeldirectory
78938529,loss becomes nan after attentionmask is added to the model while finetuning gemma,python nan huggingfacetransformers pytorchlightning loss,this is the problem of the default attention applying flash attention could solve this
78902301,why doesnt permuting positional encodings in gpt affect the output as expected,pytorch huggingfacetransformers transformermodel gpt,you have to consider the effect of the causal attention mask clms like gpt use an attention mask to prevent tokens from attending to tokens that come later in the sequence this is important because if this wasnt the case the model could look ahead and cheat at the next token prediction task the attention mask restricts the model such that token i can only attend to tokens j say we have tokens token attends to itself token attends to and so on now consider your permutations if we permute the positional embeddings we give the model slightly different signal but the token order and attention order is still the same token still attends to token token still attends to and so on as a result the output is mostly similar to the base case now we permute the token order say to token which used to attend to can now only attend to itself token which used to attend to can now only attend to token permutation substantially changes what information is routed to what tokens resulting in a substantial difference in model output if you want to look at the effect of token order and positional embeddings in isolation you should use a bertstyle masked language model that does not use a causal attention mask
78863932,runtimeerror numpy is not available transformers,python pythonx numpy huggingfacetransformers,try then restart the kernel
78856532,how to make huggingface transformer for translation return n translation inferences,python huggingfacetransformers transformermodel,check out the documentation of the generate method the parameter to use is numreturnsequences but t by default does a greedy search meaning it generates word by word and discards the options on its path there to generate multiple options you need a selection of alternative paths there are basically two ways to do this my guess would be that for your case the first option works better if you activate dosample the model will not just pick the highest probability token at each time but instead take a weighted sample from the distribution of next word probabilities if you set numbeams to anything larger than you switch to beam search where for each further token the model follows multiple alternatives of next tokens to also get the scores of generated outputs you can additionally use the arguments outputscorestrue and returndictingeneratetrue although you should note that these will return the logits of all individual tokens which you then would have to put together to the overall probability yourself check out in general t might not be the best model for code synthesis as of my knowledge it wasnt pretrained or finetuned on it in its multitask instruction finetuning there is however flant which was finetuned on a wider range of tasks including code synthesis there are also codet and many other models relating to code synthesis
78827482,cant suppress warning from transformerssrctransformersmodelingutilspy,python machinelearning pytorch huggingfacetransformers tokenize,before loading from pretrained model set transformers logger level to error as shown below it sure is really frustrating not being able to leverage the warnings library filter
78805522,panicexception addedvocabulary bad split after adding tokens to berttokenizer,huggingfacetransformers huggingfacetokenizers huggingfacetrainer,the panicexception is resolved when changing the pipeline from to the pipeline function uses autotokenizer instead of berttokenizer which leads to the panicexception from the source code if not provided the default tokenizer for the given model will be loaded if it is a string if model is not specified or not a string then the default tokenizer for config is loaded if it is a string however if config is also not given or not a string then the default tokenizer for the given task will be loaded from the actual code it uses autotokenizer which caused the problem
78748344,gpt model from hugging face always generate same result,deeplearning pytorch huggingfacetransformers largelanguagemodel gpt,the reason is that you got the ouput of shape batch hiddensize which is i guess you cannot fit it into a argmax and do tokenization as is the dimension of vector space instead of vocab try using gptlmheadmodel it will give you the with a means want liken hellohellohellohellohello hello hello hello hello writewrite write write the i
78689702,different embeddings for same sentences with torch transformer,python pytorch huggingfacetransformers bertlanguagemodel,you are correct the model layer weights for bertpoolerdensebias and bertpoolerdenseweight are initialized randomly you can initialize these layers always the same way for a reproducible output but i doubt the inference code that you have copied from there readme is correct as already mentioned by you the pooling layers are not initialized and their model class also makes sure that the poolinglayer is not added selfbert bertmodelconfig addpoolinglayerfalse the evaluation script of the repo should be called according to the readme with the following command python evaluationpy modelnameorpath qiyuwpclbertbaseuncased mode test pooler clsbeforepooler when you look into it your inference code for qiyuwpclbertbaseuncased should be the following way import torch from scipyspatialdistance import cosine from transformers import automodel autotokenizer import our models the package will take care of downloading the models automatically tokenizer autotokenizerfrompretrainedqiyuwpclbertbaseuncased model automodelfrompretrainedqiyuwpclbertbaseuncased tokenize input texts texts theres a kid on a skateboard a kid is skateboarding a kid is inside the house inputs tokenizertexts paddingtrue truncationtrue returntensorspt get the embeddings with torchinferencemode embeddings modelinputs embeddings embeddingslasthiddenstate calculate cosine similarities cosine similarities are in higher means more similar cosinesim cosineembeddings embeddings cosinesim cosineembeddings embeddings printcosine similarity between s and s is f texts texts cosinesim printcosine similarity between s and s is f texts texts cosinesim output can i make the output reproducible by initialisingseeding the model differently yes you can use torchmaunalseed import torch from transformers import automodel autotokenizer modelrandom automodelfrompretrainedqiyuwpclbertbaseuncased torchmanualseed modelrepoducible automodelfrompretrainedqiyuwpclbertbaseuncased torchmanualseed modelrepoducible automodelfrompretrainedqiyuwpclbertbaseuncased printtorchallclosemodelrandompoolerdenseweight modelrepoduciblepoolerdenseweight printtorchallclosemodelrandompoolerdenseweight modelrepoduciblepoolerdenseweight printtorchallclosemodelrepoduciblepoolerdenseweight modelrepoduciblepoolerdenseweight output
78660117,how can i export a tokenizer from huggingface transformers to coreml,python huggingfacetransformers coreml,this is the bert tokenizer i used and it works well a lot of this is from zach nagengast and julien chaumond hope it helps all you need is a vocabtxt file of the tokenizers vocab which can be found here
78631255,how to extract states in llavas transformers huggingface implementation,huggingfacetransformers transformermodel multimodal,ok i will try to answer my own question the solution was quite not available directly with the transformers library i do not know why the functionality which is mentioned in their documentation doesnt work however i found a workaround by making use of the pytorch prehooks and getting the values of the hiddenunits
78542429,running out of ram when finetuning model,machinelearning pytorch artificialintelligence huggingfacetransformers,the sound inputs were too long after resampling the audio into chunks the problem was resolved
78518971,can i dynamically add or remove lora weights in the transformer library like diffusers,python huggingfacetransformers huggingface peft,in peft when you create or load an adapter you give it a name then you can enable the adapters of your choice dynamically by name with see the example of how to do this here
78505175,cannot start weaviate server getting transformer remote inference service not ready,huggingfacetransformers weaviate,thats because of enablemodules textvectransformers if you enable the transformers module docs weaviate expects that it can reach that service in this specific example here transformersinferenceapi in your dockerfile tvtransformers isnt created hence weaviate cant find it to solve your issue you can do two things run weaviate without the transformers module you want this if you bring your own vector embeddings you simply run it without any modules as can be found in the docs here version services weaviate command host port scheme http image crweaviateiosemitechnologiesweaviate ports volumes weaviatedatavarlibweaviate restart onfailure environment querydefaultslimit authenticationanonymousaccessenabled true persistencedatapath varlibweaviate defaultvectorizermodule none enablemodules clusterhostname node volumes weaviatedata run weaviate with the transformers module you should use this one if you want to locally vectorize your data using the transformers container as can be found in the docs here version services weaviate image crweaviateiosemitechnologiesweaviate restart onfailure ports environment querydefaultslimit authenticationanonymousaccessenabled true persistencedatapath data defaultvectorizermodule textvectransformers enablemodules textvectransformers transformersinferenceapi clusterhostname node tvtransformers image crweaviateiosemitechnologiestransformersinferencesentencetransformersmultiqaminilmlcosv environment enablecuda set to to enable nvidiavisibledevices all enable if running with cuda
78490151,autotokenizerfrompretrained took forever to load,python huggingfacetransformers huggingfacetokenizers,the problem is resolved when downgrading transformers version to from both pipeline and frompretrained load the tokenizer successfully in seconds
78474215,how to load pretrained model to transformers pipeline and specify multigpu,pytorch huggingfacetransformers,i guess the easiest way to achieve what you want is exporting cudavisibledevices import os osenvironcudavisibledevices or osenvironcudavisibledevices import torch from transformers import llamaforcausallm modeldir modelsllamabchathf model llamaforcausallmfrompretrainedmodeldir devicemapauto if you want to use the devicemap you have to map each layer by yourself distillroberta because it is smaller from transformers import automodelformaskedlm model automodelformaskedlmfrompretraineddistilbertdistilrobertabase parameter names printx for x in modelnamedparameters output you dont need to map each weight it is enough when you map the layers device map example for distillroberta from transformers import autotokenizer automodelformaskedlm devicemap robertaembeddingscpu robertaencoder lmheadcpu model automodelformaskedlmfrompretraineddistilbertdistilrobertabase devicemap devicemap
78451428,python accelerate package thrown error when using trainer from transformers,python huggingfacetransformers,seems like you have to force update accelerate with the specific version simply installing accelerate wont work as it will pick the latest to be as listed below you will have to force install by specifying it as the version
78430960,presidio transformers package not available despite being installed,python huggingfacetransformers pii presidio,you will need to install the presidio analyzer package with the transformers extra dependency specifier this will install the extra dependencies needed for the transformers based nlp engine
78382913,how to know which words are encoded with unknown tokens in huggingface berttokenizer,huggingfacetransformers huggingfacetokenizers,when you use the berttokenizerfast instead of the slow version you will get a batchencoding object that gives you access to several convenient methods that allow you to map a token back to the original string the following code uses the tokentochars method from transformers import berttokenizerfast just an example paragraphchinese koka koka tokenizerbart berttokenizerfastfrompretrainedfnlpbartbasechinese encodedchinesebart tokenizerbartparagraphchinese unktokenidbart tokenizerbartunktokenid lenparagraphchinese lenparagraphchinese unktokencntchinesebart encodedchinesebartinputidscountunktokenidbart printfbart unknown token count in chinese paragraph unktokencntchinesebart unktokencntchinesebart lenparagraphchinese find all indices unkindices i for i x in enumerateencodedchinesebartinputids if x unktokenidbart for unki in unkindices start stop encodedchinesebarttokentocharsunki printfat startstop paragraphchinesestartstop original
78280443,google colab error when importing tfbertmodel,tensorflow keras googlecolaboratory huggingfacetransformers,for the runtime python cpu created today i was able to run the following tfbertmodel just fine in google colab today i noticed that our tensorflow versions differ other than that i dont believe that tfbertmodel is being deprecated transformers version tensorflow version example working tfbertmodel code output
78267762,quantization and torchdtype in huggingface transformer,huggingfacetransformers huggingface quantization,gptq is a posttraining quantization method this means a gptq model was created in full precision and then compressed not all values will be in bits unless every weight and activation layer has been quantized the gptq method does not do this specifically gptq adopts a mixed intfp quantization scheme where weights are quantized as int while activations remain in float as these values need to be multiplied together this means that during inference weights are dequantized on the fly and the actual compute is performed in float in a hugging face quantization blog post from aug they talk about the possibility of quantizing activations as well in the room for improvement section however at that time there were no open source implementations since then they have released quanto this does support quantizing activations it looks promising but it is not yet quicker than other quantization methods it is in beta and the docs say to expect breaking changes in the api and serialization there are some accuracy and perplexity benchmarks which look pretty good with most models surprisingly at the moment it is slower than bit models due to lack of optimized kernels but that seems to be something theyre working on so this does not just apply to gptq you will find yourself using float with any of the popular quantization methods at the moment for example activationaware weight quantization awq also preserves in full precision a small percentage of the weights that are important for performance this is a useful blog post comparing gptq with other quantization methods
78216628,how can i finetune a language model with negative examples using sfttrainer,python huggingfacetransformers finetuning,sfttrainer is designed for supervised finetuning maximizing likelihood of indistribution samples so there is no straightforward way to utilize negative samples may be other alignment algorithms like ktoalso implemented in trl would do the job in your case another possible way is to modify prompt to include negative label in it for example question this is the wrong answer answer
78210297,how to convert pretrained hugging face model to pt and run it fully locally,machinelearning pytorch huggingfacetransformers transformermodel,so the solution was to save model and its weights by using savepretrained not by torchsave
78019134,how to properly save the finetuned transformer model in safetensors without losing frozen parameters,python pytorch huggingfacetransformers accelerate safetensors,i met the same problem with you i fixed it with this makes the savepretrained function saves all parameters instead of saving only safe tensors hope it is helpful
78016973,check the difference in pretrained and finetuned model,pytorch huggingfacetransformers llama,just loop over the parameters and compare them with torchallclose i used distilbertmodel for the answer please use the respective classes from your example that are also mentioned in the comments output
77948682,how to stop at tokens when sending text to pipeline huggingface and transformers,deeplearning huggingfacetransformers huggingface huggingfacetokenizers,only add the tokenizer maximum length and truncation to the pipe as well and it will work well
77910495,what are my options for running llms locally from pretrained weights,python huggingfacetransformers langchain largelanguagemodel,you dont really have to install ollama instead you can directly run the llm for example for mistral model locally llm gptall modelhomejeffcachehuggingfacehubgptallmistralbopenorcaqgguf devicegpu nthreads callbackscallbacks verbosetrue or for falcon from transformers import autotokenizer automodelforcausallm pipeline import torch modelid tiiuaefalconbinstruct tokenizer autotokenizerfrompretrainedmodelid pipeline pipeline textgeneration modelmodelid tokenizertokenizer torchdtypetorchbfloat trustremotecodetrue devicemapauto maxnewtokens maxlength from langchaincommunityllmshuggingfacepipeline import huggingfacepipeline llm huggingfacepipelinepipelinepipeline i have a g ram nvidia installed on my laptop which can support the above models running locally
77877783,running through this error attributeerror cant set attribute when finetuning llama,huggingfacetransformers llama,the error attributeerror cant set attribute is raised when you attempt to change a property see more here from the error message this line is the cause of the problem selfcansaveslowtokenizer false if not selfvocabfile else true the cansaveslowtokenizer is updated to be a property in this commit the line does not exist in transformers it was replaced with property def cansaveslowtokenizerself bool return ospathisfileselfvocabfile if selfvocabfile else false as you mentioned you may have dependencies conflict consider creating a new virtual environment and install transformers
77811002,how to make a trained torch model transformerescompatible,deeplearning pytorch huggingfacetransformers huggingface,root cause your local path needs to be registered on the paths models registry solution i suggest you to use the savepretrained method it registers the local directory for further use with frompretrained method import torch from transformers import gptlmheadmodel gptconfig yourmodel torchloadpathtoyourmodelpth statedict yourmodelstatedict config gptconfigfrompretrainedgpt model gptlmheadmodelconfig modelloadstatedictstatedict save local directory modelsavepretrainedpathtosavetransformersmodel now you can load the transformers model using frompretrained loadedmodel gptlmheadmodelfrompretrainedpathtosavetransformersmodel extra ball similar discussion on github
77799262,why doesnt bert give me back my original sentence,python huggingfacetransformers bertlanguagemodel huggingface,bert bidirectional encoder representations from transformers is a transformerbased machine learning model trained on a large corpus in selflearning fashion it is devleoped to understand the context of a word based on its surrounding words in a sentence however bert is not an autoencoder as the prediction of nonmasked words is ignored during training and does not reconstruct the input sentence ratherit generates a highdimensional vector representation for each word in the sentence these representations are contextdependent means the same word can have different representations based on its context when user inputs a sentence to the bert it does not return the original sentence because it is not developed to generate text but to understand the context and semantic meaning of the given text bert returns a set of vectors representing the contextualized meaning of each word in the sentence
77788451,i have rectangular in vision transformers i set imagesize but what could be the patch size,python machinelearning pytorch typeerror huggingfacetransformers,i dont know the architecture you are using but it is common to work with squared input images even though it looks strange to humans a machine that is trained with distorted images and in inference time is fed with the same distorted images wont make a difference i assume your framework does not support rectangular input you can make the input square by padding your images with black bars this can be easily done in your data loader of course you need to preprocess images the same way at inference also this makes the model unnecessary larger so dont worry and use your distorted images
77656467,attention mask error when finetuning mistral b using transformers trainer,python huggingfacetransformers mistralb,experiencing the same issue downgrading transformers to instead of latest version seems to work fine
77628127,transformers crossentropy loss masked label issue,python huggingfacetransformers gpt,you get the same result because you do not actually modify targetids this set all values to except the last seqlen that mean you exclude all the result is an empty tensor tensor size dtypetorchint to get different result use a value less than seqlen using for example output
77614213,transformerjs model fails to parse json in clientside nextjs example,javascript nextjs huggingfacetransformers brave,looking through the github issues for transformersjs i found in which one user suggested adding the following line to the worker file envusebrowsercache false turning the browser cache option off i no longer receive this error in the brave browser and in fact the nextjs clientside example code hugging face provides in their tutorial works without any further issues for me that said browser caching is a significant feature so this could indicate an underlying bug that needs to be fixed in transformersjs see the link to the transformersjs github issue above
77555030,how to resolve bert hf model valueerror too many values to unpack expected,python tensorflow machinelearning huggingfacetransformers bertlanguagemodel,the preparetfdataset function does not require the dataframe to have tensor value types removing returntensorstf should solve the problem def tokenizedatasetdf keys of the returned dictionary will be added to the dataset as columns return tokenizer dftextcolumn dftextcolumn paddingtrue truncationtrue maxlength
77505283,crossencoder transformer converges every input to the same cls embedding,pytorch huggingfacetransformers,okay after a lot of debugging i tried changing my optimizer i was using adam which worked well when i was using a dualencoder architecture changing to sgd fixed the issue and the model learns correctly now not super sure why adam wasnt working will update if i figure it out
77282461,huggingface bettertransformer in context cannot disable after context,python huggingfacetransformers withstatement contextmanager huggingfacetrainer,i found a solution by using a custom context manager on the trainer object as opposed to applying it on a model object the custom context manager is as follows class bettertransformertrainercontext context manager to wrap trainermodel with bettertransformer def initself trainer selftrainer trainer def enterself selftrainermodel bettertransformertransform selftrainermodel keeporiginalmodeltrue return selftrainer def exitself exctype excval exctb selftrainermodel bettertransformerreverseselftrainermodel it can be used as follows print with optimum should be fast with bettertransformertrainercontexttrainer as optimumtrainer evalaccuracy optimumtrainerevaluateevalaccuracy printevalaccuracy i hope this might be helpful to someone else
77249578,how to create a dataset with huggingface from a list of strings to finetune llama with the transformers library,python dataset huggingfacetransformers huggingfacedatasets finetuning,this is what worked for me in the end and then to tokenize the data
77174289,multiprocessing with hfs transformers uses all cpu cores despite being limited numworkers,python pytorch multiprocessing huggingfacetransformers,apparently pytorch uses a parallelization library called openmp link and according to this answer openmp does multithreading within a process and the default number of threads is typically the number that the cpu can actually run simultaneously so what happens on that quadcore cpu if you run a multiprocessing program that runs python processes and each calls an openmp function runs threads you end up running threads on cores so when i set osenvironopenmpnumthreads it performs as expected other resources hf discussion
77159136,efficiently using hugging face transformers pipelines on gpu with large datasets,python gpu huggingfacetransformers huggingfacedatasets,i think you can ignore this message i found it being reported on different websites this year but if i get it correctly this github issue on the huggingface transformers shows that the warning can be safely ignored in addition batching or using datasets might not remove the warning or automatically use the resources in the best way you can do callcount in here to ignore the warning as explained by martin weyssow above how can i modify my code to batch my data and use parallel computing to make better use of my gpu resources you can add batching like this and most importantly you can experiment with the batch size that will result to the highest gpu usage possible on your device and particular task huggingface provides here some rules to help users figure out how to batch making the best resourcegpu usage possible might take some experimentation and it depends on the use case you work on every time what does this warning mean and why should i use a dataset for efficiency this means the gpu utilization is not optimal because the data is not grouped together and it is thus not processed efficiently using a dataset from the huggingface library datasets will utilize your resources more efficiently however it is not so easy to tell what exactly is going on especially considering that we dont know exactly how the data looks like what the device is and how the model deals with the data internally the warning might go away by using the datasets library but that does not necessarily mean that the resources are optimally used what code or function or library should be used with hugging face transformers here is a code example with pipelines and the datasets library it mentions that using iterables will fill your gpu as fast as possible and batching might also help with computational time improvements in your case it seems you are doing a relatively small poc doing inference for under documents with a medium size model so i dont think you need to use pipelines i assume the sentiment analysis model is a classifier and you want to keep using pandas as shown in the post so here is how you can combine both this is usually fast enough for my experiments and prints no warnings about the resources as soon as your inference starts either with this snippet or with the datasets library code you can run nvidiasmi in a terminal and check what the gpu usage is and play around with the parameters to optimize it beware that running the code on your local machine with a gpu vs running it on a larger machine eg a linux server with perhaps a more powerful gpu might lead to different performance and might need different tuning if you wish to run the code for larger document collections you can split the data in order to avoid gpu memory errors locally or in order to speed up the inference with concurrent runs in a server
77061667,resume from checkpoint gives device error in huggingface transformers trainer,pytorch huggingfacetransformers,as pointed in huggingface github this is a known issue of loading optimizer changing the line in trainerpy from to will fix the issue huggingface transformer v pytorch v
77047800,transformersjs in reactjs,javascript reactjs huggingfacetransformers huggingface,there are many reported issues on github but it seems that you need to specifically define that you do not want to use local models if that does not work you need to further explicitly prevent the lib from using browser cache it varies depending on the runtime bundler and environment thus for more specific error explanation try searching for unexpected token on issues section of the lib
76982260,huggingface transformer evaluation process is too slow,machinelearning huggingfacetransformers bertlanguagemodel trainingdata huggingface,so i finally got the problem its related to evaluateload calls inside the computemetrics function it seems this method has a significant overhead in time so it shouldnt be inside some functions eg computemetrics which are called many times i moved out two load methods of computemetrics function and it works quickly now
76883181,running sentence transformers at pythonanywhere,python pytorch huggingfacetransformers pythonanywhere sentencetransformers,as mentioned in the comments the meta device was added in the pytorch version and the pythonanywhere comes with pytorch version downgrading transformers library to which was released in may before torch was released solved this issue
76847246,how to save and load a peftlora finetune starchat,python pytorch huggingfacetransformers torch,you need to set istrainabletrue next time you load the model and adapter
76740367,how to resolve the error importerror cannot import name generationconfig from transformers,pytorch huggingfacetransformers,it looks like torchvision and torchaudio are not compatible with your current torch version try to install pytorchs stable version using this command or generate the desired combination from see also my answer to a similar question
76735605,cudapytorch transformer model import error,pytorch huggingfacetransformers,you should install the function from the transformers package you could use the following code this should load the model i used this version of the package
76663419,how to generate text using gpt model with huggingface transformers,python huggingfacetransformers huggingface gpt largelanguagemodel,to generate text using transformers and gpt model if youre not particular about modifying different generation features you can use the pipeline function eg out if you have somehow have to use gpttokenizer and automodelforcausallm instead of using pipeline you can try autotokenizer instead of gpttokenizer eg out to use the computetransitionscores function implemented in first make sure you really have the update version of transformers by doing if the version is after the feature have been implemented this should give no error out if you see the attributeerror most probably your current python kernel maybe inside jupyter isnt the right one that you have with your pip if so check your executable then you should see something like after that instead of simple pip install u transformers reuse that above python binary and do see also whats the difference between pip install and python m pip install why is m needed for python m pip install
76633368,how does one set the pad token correctly not to eos during finetuning to avoid model not predicting eos,machinelearning pytorch huggingfacetransformers huggingface huggingfacetokenizers,for falcon you can use already existing special tokens available for the model tokenizeraddspecialtokenspadtoken suffix modelconfigpadtokenid tokenizerpadtokenid this way you dont have to extend the embedding of the model like its done here for other models like llama you can set the tokenizerpadtoken tokenizerunktoken
76589840,cant run transformer fine tuning with m mac cpu,macos gpu cpu huggingfacetransformers finetuning,i can verify the op and the response trainer is not working properly on my m mps but does work painfully slowly with the mps turned off with the nocudatrue option under trainingarguments fwiw the errors that i am getting with the mps set to be on seem to have to do with storage allocation on the mps
76474907,how to visulaize or print the output from each layer of pytorch saved model specifically for visiontransformer,python pytorch huggingfacetransformers,as shown in this other question the order of the elements in modelnamedchildren is given by the order of appearance in the init method of the model which means that without access to this method the order of appearance of the different layers is not guaranteed moreover even if you are certain that modules will appear in the correct order your loop will still probably cause an error because some parts of the model will be used twice in your loop when the module named encoder appears the current value of output is processed and saved to the same variable with output encoderoutput after children layers of the encoder are applied to the current value of output which have already been processed by the encoder the effect of that will be in the best case if the shapes and layers are not compatible to raise an error like you got and in the worst case if shapes and layers are compatible to give you an answer as a tensor but which would not be what you wanted since the data would have gone through the encoder once and then through some parts of the encoder again resulting on uninterpretable results update first about the convproj output this is the opposite you have patches each of which being summarized by a sized representation this representation is not really vizualizable since each feature could be independant from the other if you want something that can be seen as an image you could check the kernel of the convolution layer this will give you a k k n tensor with k being the kernel size of the convolution and n the number of filters likely to be unless there are other layers after the convolutions for interpretation you could visualize each k k array as how important is each pixel for this filter values can be positive or negative depending on whether the pixel has a positive or negative effect on this filter then about the transformer layer the dimension you get is b t n where b is batch size in your case t is some kind of time dimension and n is the size of the representation since an not have a time dimension one is constructed by defining an order between the patches so you find again the patches you already had from the convolution layer with one patch added by the transformer layer which gives the t dimension finally n is the size of the embedding which is kept at as for the previous one this representation is not really meant to be visualized something that can be interesting to watch is the attention maps from the transformer layers it is a tthsized tensor where t is still the time dimension and h is the number of heads in this layer each of these tt tensors are describing the importance of each patch relatively to the other to construct the output of the layer and so for each attention head you have you can find some details in this link from pytorch doc the output is named attnoutputweights and you might have to tweak a bit your implementation to get these
76465343,huggingface transformers model config reported this is a deprecated strategy to control generation and will be removed soon,python huggingfacetransformers,rootcause this is a warning about using the api in the outdated manner unsupported soon however as of now the code is fixing this on its own hence only a warning not a breaking error see these lines in the source code remedy the transformers library encourages the use of config files in this case we need to pass a generationconfig object early rather than to set attributes i will first share a clean simple example from transformers import autotokenizer bartforconditionalgeneration model bartforconditionalgenerationfrompretrainedfacebookbartlargecnn tokenizer autotokenizerfrompretrainedfacebookbartlargecnn articletosummarize pge stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions the aim is to reduce the risk of wildfires nearly thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow inputs tokenizerarticletosummarize maxlength returntensorspt change config and generate summary from transformersgeneration import generationconfig modelconfigmaxnewtokens modelconfigminlength gencfg generationconfigfrommodelconfigmodelconfig gencfgmaxnewtokens gencfgminlength summaryids modelgenerateinputsinputids generationconfiggencfg tokenizerbatchdecodesummaryids skipspecialtokenstrue cleanuptokenizationspacesfalse if you try to manipulate the config attributes directly and pass no config you get a warning if you pass a generationconfig you are all good this example is reproducible as a colab notebook here now to the original question note that in general changing architecture configs of pretrained models is not recommended for incompatibility reasons this is sometimes possible with extra effort however certain config changes are possible upon initialization model bartforconditionalgenerationfrompretrained facebookbartlargecnn attentiondropout here is the fullyworking code corrected for reproducibility and see also this notebook from transformers import autotokenizer bartforconditionalgeneration from transformersgeneration import generationconfig from transformers import trainer trainingarguments from transformersmodelsbartmodelingbart import shifttokensright from transformers import datacollatorforseqseq model bartforconditionalgenerationfrompretrainedfacebookbartlargecnn attentiondropout tokenizer autotokenizerfrompretrainedfacebookbartlargecnn seqseqdatacollator datacollatorforseqseqtokenizer modelmodel def getfeaturesbatch inputencodings tokenizerbatchtext maxlength truncationtrue with tokenizerastargettokenizer targetencodings tokenizerbatchsummary maxlength truncationtrue return inputids inputencodingsinputids attentionmask inputencodingsattentionmask labels targetencodingsinputids datasetftrs datasetmapgetfeatures batchedtrue columns inputids labels inputidsattentionmask datasetftrssetformattypetorch columnscolumns trainingargs trainingarguments outputdirmodelsbartsummarizer numtrainepochs perdevicetrainbatchsize perdeviceevalbatchsize warmupsteps weightdecay loggingdirlogs modelconfigoutputattentions true modelconfigoutputhiddenstates true trainingargs trainingarguments outputdirmodelsbartsummarizer numtrainepochs warmupsteps perdevicetrainbatchsize perdeviceevalbatchsize weightdecay loggingsteps pushtohubfalse evaluationstrategysteps evalsteps savestepse gradientaccumulationsteps trainer trainer modelmodel argstrainingargs tokenizertokenizer datacollatorseqseqdatacollator traindatasetdatasetftrstrain evaldatasetdatasetftrstest assert modelconfigattentiondropout trainertrain
76459034,how to load a finetuned peftlora model based on llama with huggingface transformers,python huggingfacetransformers llamaindex peft,to load a finetuned peftlora model take a look at the guanco example you will need an ag gpu runtime minimally to load the model properly for more details see inference notebook training notebook
76403814,what is the best approach to creating a question generation model using gpt and bert architectures,python opensource huggingfacetransformers huggingface gpt,when dealing with text generation it is more straightforward to work with transformer decoder models such as gpt models although bertlike models are also capable of text generation it is a quite convoluted process and not something that follows naturally from the tasks for which these models have been pretrained i assume you are comparing gpt and wizardlm b the performance of the model on this task is expected to improve as you scale up the number of parameters by using larger models i would recommend you to try llms such as alpacalora dolly or gptj see here how to run gptj on colab pro
76376455,transformers tokenizer attention mask for pytorch,python pytorch huggingfacetransformers huggingface,pytorchs tgtmask is not the same as hf attentionmask the latter indicates which tokens are padded from transformers import berttokenizer t berttokenizerfrompretrainedbertbasecased encoded tthis is a test maxlength paddingmaxlength printtpadtokenid printencodedinputids printencodedattentionmask output pytorchs equivalent to that is tgtkeypaddingmask the tgtmask on the other hand serves a different purpose it defines which token should attend to other tokens for an nlp transformer decoder this is usually used to prevent tokens to attend to future tokens causal mask in case this is your use case you could also simply pass tgtiscausaltrue and pytorch will create the tgtmask for you
76363706,got exception eagertensor object has no attribute size when generating bert embeddings,tensorflow huggingfacetransformers huggingface,oh i found the solution the solution is to import a different set of classes tfautomodel autotokenizer to do the job
76300212,problems combining tensorflow with hugginggpt transformers on a python project,python tensorflow huggingfacetransformers,i recommend using the latest stable version and not a release candidate in this case install tf and keras also i dont recommend using brew python install conda and create a virtual env to experiment with installations there if you are not very experienced with packages and versions final the error that you mention tfbertforsequenceclassification requires the tensorflow library but it was not found in your environment can be easily solved google is your friend so to wrap up install conda setup a virtual env install everything in that virtual env make sure to activate it before install stuff with conda and not with pip recommendation not mandatory make sure you are using gpu tf pip install tensorflowgpu for example the last error is related to this conda handles quite good gpu support conda getting started
76205193,sentence transformers no more on huggingface,huggingfacetransformers sentencetransformers huggingfacehub,it was an error on huggingface its back again here the postmortem through a combination of human error and technical issues the hugging face hub was experiencing some intermittent difficulties today for hours and about users and organizations were incorrectly displayed as deleted which led to downtime for some great downstream libraries like sentencetransformers nils reimers
76191862,how can i finetune mbart for machine translation in the transformers python library so that it learns a new word,python huggingfacetransformers pretrainedmodel machinetranslation finetuning,one could add the following to finetune mbart full code it outputs the correct made up translation plorizatizzzon i reported the documentation issue on contains two more advanced scripts to finetune mbart and t thanks sgugger for pointing me to it here is how to use the script to finetune mbart create a new conda environment command note the readme seems to have missed dopredict with finetuningtranslationtrainjson finetuningtranslationvalidationjson and finetuningtranslationtestjson formatted as follows with the json lines format note one must use double quotes in the json files single quotes eg en will make the script crash i run the code on ubuntu lts with an nvidia t tensor core gpu gb memory and cuda the mbart model takes around gb of gpu memory
76188888,how do i get a pretrained model from hugging face running on my own data,python huggingfacetransformers,i recommend you read the documentation provided in the hugging face website to answer your question for auto tokenizers pathtosavedmodel stands for pretrainedmodelnameorpath str or ospathlike can be either a string the model id of a predefined tokenizer hosted inside a model repo on huggingfaceco valid model ids can be located at the rootlevel like bertbaseuncased or namespaced under a user or organization name like dbmdzbertbasegermancased a path to a directory containing vocabulary files required by the tokenizer for instance saved using the savepretrained method eg mymodeldirectory a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file like bert or xlnet eg mymodeldirectoryvocabtxt not applicable to all derived classes same thing for automodelforsequenceclassification
76015442,how to generate sentence embeddings with sentence transformers using pyspark in an optimized way,pyspark amazonemr huggingfacetransformers sentencetransformers,even with the distributed computing and more cpus generating embeddings using sentence transformers is slow there are p ec gpu instances that provides gpus for large computation in parallel using gpus and batch processing i am able to generate sentence transformers embeddings efficiently in my case a single gpu ec instance is at least times faster than cpu instances batch processing is necessary to utilize gpu efficiently otherwise its same as to generate a single sentence embedding for a sentence at a time
76014701,how to avoid adding double start of token in trocr finetune model,python deeplearning pytorch huggingfacetransformers huggingface,the problem comes from passed token ids i am adding start token from tokenizer another start token from the trocr model so the duplication happens the solution is super easy by just skipping start token coming from the tokenizer by using labels labels
75961596,where are the different projection matrices for huggingface transformer model,pytorch huggingfacetransformers,the weights for the heads are concatenated not only that but it is also customary to concatenate the weights for q k and v into a single matrix for selfattention the easiest most common way to make the split is along the last dimension as the resulting matrix would still be contiguous in memory but ultimately that depends on the implementation take for instance this code from mingpt on init cattn nnlinearnembed nembed on forward b t c xsize batch size time sequence length nembed q k v cattnxsplitnembed dim q qviewb t nhead nembednhead transpose as you can understand both the qkv and heads are concatenated along the last dimension and were splitreshaped accordingly for your distilbert you can look at the source code of hugginface transformers multiheadattention def shapex torchtensor torchtensor separate heads return xviewbs selfnheads dimperheadtranspose query torchtensorbs seqlength dim selfqlinquery still has shape bs seqlength dim q shapeselfqlinquery bs nheads qlength dimperhead the result is already transpose for the next matrix product is this really x projection matrices concatenated yes the splitconcat is along the last dimension the best documentation is the source
75960984,where does automodelforseqseqlmfrompretrained store the model to disk,python huggingfacetransformers,in most cases the loaded models are saved in the transformers cache directory on windows the default directory is given by cusersusername cachehuggingfacetransformers you can specify the cache directory every time you load a model by setting the parameter cachedir for python
75948679,deberta onnx export does not work for tokentypeids,pytorch huggingfacetransformers onnx huggingface,i have the same error on way to solve it was removing tokentypeids while tokenizing the text but keep only inputids attentionmask
75945735,xlnet or bert chinese for huggingface automodelforseqseqlm training,python pytorch huggingfacetransformers huggingface,xlnetbasecased bertbasechinese can not be loaded directly with automodelforseqseqlm because it expects a model that can perform seqseq tasks but you can leverage these checkpoints for a seqseq model thanks to this paper and the encoderdecodermodel class from transformers import encoderdecodermodel automodelforseqseqlm this will use the weights of bertbasechinese for the encoder and the decoder model encoderdecodermodelfromencoderdecoderpretrainedbertbasechinese bertbasechinese modelconfigdecoderstarttokenid tokenizerclstokenid you can later load it as automodelforseqseqlm modelsavepretrainedmyseqseqbert model automodelforseqseqlmfrompretrainedmyseqseqbert i havent tested it but it seems xlnet can not be used as a decoder according to this issue in this case you can try to use a decoder like gpt with crossattention model encoderdecodermodelfromencoderdecoderpretrainedxlnetbasecased gpt modelsavepretrainedmyseqseqxlnetbasecased model automodelforseqseqlmfrompretrainedmyseqseqxlnetbasecased
75931144,change the number of layers of a pretrained huggingface pegasus model used for conditional generation,pytorch huggingfacetransformers huggingface,from what i understand you are trying to use a pretrained model from huggingface for inference this model contains different layers encoder layers decoder layers by default for the pretrained model you use if you want to use internal states of the model which is equivalent to ignoring some of the final layers you can create the model with model pegasusforconditionalgenerationfrompretrainedgooglepegasuspubmed and then use the parameter outputhiddenstatestrue of the model inference call and use the embedding from any internal layer but you cannot bypass only intermediate layers since the following layers are dependant on it if you want to change the structure of the network by adding layers you cannot use a pretrained model since the layers you are trying to add would have random weights so you would have to create your new model from the config and train it on data that you have access to you can try to initialize some of the layers of your model with weights from the pretrained one but you wont have good results before training since the model still has some completely random weights
75904748,is it possible to train a deep learning model using low precision and subsequently finetune it with high precision,tensorflow deeplearning pytorch huggingfacetransformers bertlanguagemodel,what youre referring to is called mixedprecision training and its basically training the model with lowprecision floatingpoint numbers eg fp for most of the layers and using highprecision numbers eg fp only for certain layers that require more accuracy finetuning a lowprecision model with high precision on accuracy can vary depending on the specific model and task in some cases finetuning can result in an increase in accuracy while in other cases it may not here is a quick and dirty script to try for mixed precision to see if it is worth trying as a temperature check
75873428,how to install diff version of a package transformers without internet in kaggle notebook wo killing the kernel while keeping variables in memory,python jupyternotebook pip huggingfacetransformers pythonimportlib,when you initially import a module in a python environment it is cached in sysmodules subsequent imports are not read from the disk but from the cache for this reason you are not seeing the new version of the module being loaded a possible solution is to attempt to reload the module using importlibreload read the documentation so that you are aware of the caveats of using this method
75845842,is the default class in huggingface transformers using pytorch or tensorflow under the hood,python tensorflow pytorch huggingfacetransformers,it depends on how the model is trained and how you load the model most popular models on transformers supports both pytorch and tensorflow and sometimes also jax out maybe something like q so if i use trainer its pytorch a yes most probably the model has pytorch backend and the training loop optimizer loss etc uses pytorch but the trainer isnt the model its the wrapper object q and if i want to use trainer for tensorflow backend models i should use tftrainer not really in the latest version of transformers the tftrainer object is deprecated see it is recommended that you use keras sklearnstyle fit training if you are using a model with tensorflow backend q why does my script keep printing out tensorflow related errors shouldnt trainer be using pytorch only try checking your transformers version most probably you are using an outdated version that uses some deprecated objects eg textdataset see how to resolve only integer tensors of a single element can be converted to an index error when creating a dataset to finetune gpt model in the later versions most probably pip install transformers the trainer shouldnt be activating tf warnings and using tftrainer would have raised warnings to suggest users to use keras instead
75799722,how to deal with stack expects each tensor to be equal size eror while fine tuning gpt model,python tensorflow artificialintelligence huggingfacetransformers gpt,yes seems like you didnt pad your inputs the model expects the size to be the same for each text so if its too short you pad it and if its too long it should be truncated see also how does maxlength padding and truncation arguments work in huggingface berttokenizerfastfrompretrainedbertbaseuncased try changing how the tokenizer process the inputs
75772288,how to read a bert attention weight matrix,huggingfacetransformers bertlanguagemodel attentionmodel selfattention multiheadattention,the attention matrix is asymmetric because query and key matrices differ at its core leaving normalization constants and the multihead trick aside dotproduct selfattention is computed as follows compute keyquery affinities eij given t being the sequence length qi and kj being query and key vectors compute attention weights from affinities alphaij as you can see you get the normalization of the affinities by summing over all keys given a query said differently in the denominator youre summing affinities by row thus probabilities sum to over rows the way you should read the attention matrix is the following row tokens queries attend to column tokens keys and the matrix weights represent a way to probabilistically measure where attention is directed to when querying over keys ie to which key and so to which token of the sentence each query token mainly focuses to such interaction is unidirectional you might look at each query as looking for information somewhere in the keys the opposite interaction being irrelevant i found the interpretation of the attention matrix as a directed graph within this blogpost very effective eventually id also suggest the first bertviz medium post which distinguishes different attention patterns and according to which your example would fall in the case where attention is mostly directed to the delimiter token cls
75762087,trying to finetune gpt in vertex ai but it just freezes,python pytorch huggingfacetransformers googlecloudvertexai gpt,i got around this by using a workbook with these settings zone uscentralb environment numpyscipyscikitlearn when making the workbook i chose the python cuda option machine type vcpus gb ram gpus nvidia v x and in the workbook itself i used this command to install pytorch after that everything worked just fine just like in google colab
75734019,how to load a smaller gpt model on huggingface,machinelearning pytorch huggingfacetransformers huggingface,in order to stack or decoder layers rather than the default number of layers gpt has it is sufficient to pass either nlayer or nlayer as an additional parameter to frompretrained method of the autoconfig class gptconfig under the hood alternatively you can also pass numhiddenlayers or numhiddenlayers indeed due to the two are interchangeable
75713161,finetuning vision encoder decoder models with huggingface causes valueerror expected sequence of length at dim got,python huggingfacetransformers,update in my case i solved it by passing trunction true inside the tokenizer for more check
75710776,huggingface gpt loss understanding,pytorch huggingfacetransformers gpt,the default loss function is negative loglikelihood the actual model output is not the token city but a categorical distribution over the entire k vocabulary depending on the generation strategy you either sample from these distributions or take the most probable token the token city apparently the most probable one gets some probability and the loss is then minus the logarithm of this probability loss close to zero would mean the token would get a probability close to one however the token distribution also considers many plausible but less likely followups loss corresponds to the probability of exp approximately it seems small but in a k vocabulary it is approximately times more probable than a random guess you can try to finetune the model to be absolutely sure that city will follow with probability but it would probably break other language modeling capabilities
75679788,how is an object of type basemodeloutput in huggingface transformer library subscriptable,huggingfacetransformers,the basemodeloutput class inherits from modeloutput which implements getitem from typing import list from dataclasses import dataclass dataclass class x blaliststr none def getitemself iint return selfblai x xtldr no yes printx output
75595699,huggingfaces berttokenizerfast is between and times slower than expected,performance huggingfacetransformers huggingfacetokenizers huggingfacedatasets,turns out the log message about berttokenizerfast had nothing to do with the progress bar that appeared right after which i thought was the tokenization progress bar but was in fact the training progress bar the actual problem was that the model was training on cpu instead of gpu i thought i had ruled this out because i had verified that torchcudaisavailable true and huggingface trainers are supposed to use cuda if available however the installed version of pytorch was incorrect for my version of cuda and despite cuda being available pytorch refused to use the gpu making huggingface default back to cpu training all of this was silent and caused no warnings or error messages
75585069,why is bert storing cache even after caching is disabled,caching huggingfacetransformers torch bertlanguagemodel transformermodel,the code snippet has several issues it does not use gpu at all you need to send both model and the data to a gpu explicitly eg by doing the following device torchdevicecuda if torchcudaisavailable else cpu modeltodevice inputstodevice pytorch automatically prepares for computing the gradients which requires storing intermediate results you can wrap the call in with torchnograd to ensure no gradients are collected k sentences is too much regardless of your gpu memory in any case you need to split it into more batches you can use the dataset interface from transformers for that
75560424,transformers refinetune with different classes,python classification huggingfacetransformers bertlanguagemodel,for further references you need to edit the final layer in my case as i was using tensorflow
75548317,transformers always only use a single linear layer for classification head,huggingfacetransformers finetuning,to add onto the previous answer embedding layers selfbert bertmodelconfig in your case transform the original data a sentence an into some semanticaware vector spaces this is where all the architecture designs come in eg attention cnn lstm etc which are all far more superior than a simple fc for their chosen tasks so if you have the capacity of adding multiple fcs why not just add another attention block on the other hand the embeddings from a decent model should have large interclass distance and small intraclass variance which could easily be projected to their corresponding classes in a linear fashion and a fc is more than enough it would be ideal to have the pretrained portion as big as possible such that as a downstream user i just have to trainfinetune a tiny bit of the model eg the fc classification layer
75531115,bert model for word similarity,python deeplearning huggingfacetransformers bertlanguagemodel,first of all the similarity is a tricky word because there are different types of similarities especially semantic and sentimental similarities are very different concepts for example while good and bad are sentimental opposite words they are semantically similar words the basic bert model is trained to capture the semantic similarity of the language therefore if you want to measure sentimental similarity you can use bert models for sentiment analysis i suggest other similarity techniques for your task like gloveembedding regarding of your question there are a couple of errors in your implementation output of the models is a dict when you accessed the first item you already accessed the lasthiddenstate you dont need the before the lasthiddenstate berttype transformers use tokenizers that can split the word to multiple tokens one solution for this issue you can take the average of the tokens which is basically the average of the output except for first and last elements your cosine similarity function will give an error when you run the code
75437911,i would like to finetune the blip model on roco data set for of chest xrays,pythonx pytorch huggingfacetransformers,there are two issues here youre not providing the labels during training your capts are passed as the models question there is an example on how to do that in the link below finetuning hfs blipforconditionalgeneration is not supported at the moment see where they just fixed blipforquestionanswering if you create a dataset based on this link you will also get the error valueerror expected input batchsize to match target batchsize which can be solved if you put the effort to reproduce the changes made on blipforquestionanswering to blipforconditionalgeneration
75237628,tokenizersavepretrained typeerror object of type property is not json serializable,python huggingfacetransformers gpt,the problem is on the line tokenizerpadtoken gpttokenizereostoken here the initializer is wrong thats why this error occurred a simple solution is to modify this line to tokenizerpadtoken tokenizereostoken for the reference purpose your final code will look like this from transformers import gpttokenizer gptlmheadmodel tokenizer gpttokenizerfrompretrainedgpt tokenizerpadtoken tokenizereostoken datasetfile xcsv df pdreadcsvdatasetfile sep inputids tokenizerbatchencodepluslistdfx maxlengthpaddingmaxlengthtruncationtrueinputids saving the tokenizer tokenizersavepretrainedtokenfile
75158430,error img when applying increment with keras and transformers for,python image keras classification huggingfacetransformers,i guess the error is here you are trying to access examples dictionary using img key from some code above it looks like the key should be image
75154742,bert model conversion from deeppavlov to huggingface format,pythonx huggingfacetransformers bertlanguagemodel huggingface deeppavlov,there is no need to convert the tf checkpoints into huggingface format the deeppavlovs pretrained language models are already available as huggingface models
75050748,why is positional encoding needed while input ids already represent the order of words in bert,huggingfacetransformers bertlanguagemodel,the reason is the design of the neural architecture bert consists of selfattention and feedforward sublayers and neither of them is sequential the feedforward layers process each token independently of others the selfattention views the input states as an unordered set of states attention can be interpreted as soft probabilistic retrieval from a set of values according to some keys the position embeddings are there so the keys can contain information about their relative order
74926252,how to replace the tokenize and padsequence functions from transformers,python huggingfacetransformers huggingfacetokenizers gpt,edit this happend because transformers version to old for this please update transformers with pip install u transformers
74678703,how to disable neptune callback in transformers trainer runs,python pytorch callback huggingfacetransformers neptune,the reason neptune is included is because the default value of reportto in trainingarguments is all which implicitly includes all installed loggers from the officially supported list of loggers you should either uninstall neptune from the environment you use for the project or pass reporttonone to the trainingarguments instance you use to initialize the trainer nb thats the string literal none not a python none the other answers here including the accepted answer are either poor workarounds for this problem or simply do not work at all the proper way to handle this issue is as above
74657367,how do i know which parameters to use with a pretrained tokenizer,deeplearning huggingfacetransformers huggingfacetokenizers,the choice on whether to use padding and truncation depends on the model you are finetuning and on your training process and not on the pretrained tokenizer tranformerbased models have a constraint on the number of tokens the model can process so generally yes thats it yes when maxlength is none then the maximum acceptable input length for the model is considered see docs yes you should not pad the input sequence if you use datacollatorwithpadding more about it in this video as you already noticed you have to specify them yourself when you pass your input text to the pipeline
74021237,if i train a custom tokenizer on my dataset i would still be able to leverage a pretrained model weight,huggingfacetransformers huggingfacetokenizers mlmodel,in short no you cannot use your own pretrained tokenizer for a pretrained model the reason is that the vocabulary for your tokenizer and the vocabulary of the tokenizer that was used to pretrain the model that later you will use it as pretrained model are different thus a wordpiece token which is present in tokenizerss vocabulary may not be present in pretrained models vocabulary detailed answers can be found here
74014379,how to finetune gptj using huggingface trainer,python machinelearning pytorch huggingfacetransformers huggingface,i found what appears to work though now im running low on memory and working through ways of handling it the datacollator parameter seems to take care of the exact issue that i was having
73975817,how do a put a different classifier on top of bertforsequenceclassification,machinelearning pytorch huggingfacetransformers huggingface,by looking at the source code of bertforsequenceclassification here you can see that the classifier is simply a linear layer that project the bert output from hiddensize dimension to numlabels dimension suppose you want to change the linear classifier to a two layer mlp with relu activation you can do the following the requirement of the structure of your new classifier is its input dimension and output dimension need to be confighiddensize dimension and confignumlabels accordingly the structure of the classifier doesnt rely on the batch size and module like nnlinear takes hdimension dimension as input so you dont need to specify the batch size when creating the new classifier
73851375,use sentence transformers models with apache beam,googleclouddataflow apachebeam huggingfacetransformers sentencetransformers,you are correct in the assumption that the pipeline file is too largethe direct runner doesnt have that limitation but i believe dataflow limits the json to something like mb im guessing youre embedding the model into that json youll probably be better off loading it from an external source for example runinference in the python sdk allows for loading custom models
73850035,what does permutation invariant mean in the context of transformers doing language modelling,pytorch huggingfacetransformers,since all tokens in the sequence are treated equally in transformers changing the order of the input tokens permutation would result in the same output invariance to avoid this one adds positional embeddings which are just numbers in each token that represent its position in the sequence eg in language modelling i traveled from france to england and saw should result in something like london as the next word but without the positional embeddings the transformer cant differentiate between the correct sentence and i traveled from england to france and saw so it might as well respond with paris the order of words matters thereby permutation invariance is bad in language modelling
73763538,how to set outputshape of bert preprocessing layer from tensorflow hub,tensorflow keras deeplearning huggingfacetransformers bertlanguagemodel,you need to go lower levels in order to achieve this your goal was shown in the page of preprocess layer however not properly introduced you can wrap your intention into a custom tf layer basically you will tokenize and prepare your inputs by yourself preprocessor has a method named bertpackinputs which will let you the specify maxlen of the inputs for some reason selftokenizer expects the inputs in a list format mostly likely this will allow it to accept multiple inputs your model should look like this note that textinput layer is now inside in a list as selftokenizers input signatures expects a list heres the model summary when calling the custom preprocessing layer notice the inputs should be in a list calling the model can be done with both ways or
73645084,create hugging face transformers tokenizer using amazon sagemaker in a distributed way,amazonsagemaker huggingfacetransformers huggingfacetokenizers amzsagemakerdistributedtraining,considering the following example code forhuggingfaceprocessor if you have large files in s and use a processinginput withsdatadistributiontypeshardedbyskey instead of fullyreplicated the objects in your s prefix will be sharded and distributed to your instances for example if you have large files and want to filter records from them using huggingface on instances the sdatadistributiontypeshardedbyskey will put objects on each instance and each instance can read the files from its own path filter out records and write uniquely named files to the output paths and sagemaker processing will put the filtered files in s however if your filtering criteria is stateful or depends on doing a full pass over the dataset first such as filtering outliers based on mean and standard deviation on a feature in case of using sklean processor for example youll need to pass that information in to the job so each instance can know how to filter to send information to the instances launched you have to use theoptmlconfigresourceconfigjsonfile currenthost algo hosts algoalgoalgo
73643066,how to get hidden layerstate outputs from a bert model,huggingfacetransformers bertlanguagemodel,the basemodeloutputwithpoolingandcrossattentions you retrieve is class that inherits from ordereddict code that holds pytorch tensors you can access the keys of the ordereddict like properties of a class and in case you do not want to work with tensors you can them to python lists or numpy please have a look at the example below from transformers import berttokenizer bertmodel t berttokenizerfrompretrainedbertbasecased m bertmodelfrompretrainedbertbasecased i tthis is a test returntensorspt o mi outputhiddenstatestrue printokeys printtypeolasthiddenstate printolasthiddenstatetolist printolasthiddenstatedetachnumpy output
73546673,understanding vocabtxt of vinaibertweetbase,python pytorch huggingfacetransformers,each number denotes the frequency count that the corresponding word appears in the pretraining corpus only top k words are included in the vocab
73537733,how to prepare custom training data for donut document understanding transformer,python huggingfacetransformers,please visit roboflow annotate your images via rob flow annotation tools than export the file in coco json format as the model of donut is expecting the input as json file use script available at hugging face for training the model
73530622,the size of logits of roberta model is weird,pytorch huggingfacetransformers,the modelclassifier object you have replaced used to be an instance of a robertaclassificationhead if you take a look at its source code the layer is hardcoded into indexing the first item of the second dimension of its input which is supposed to be the cls token by replacing it with an identity you miss out on the indexing operation hence your output shape long story short dont assume functionality you havent verified when it comes to nonown code huggingface in particular lots of adhoc classes and spaghetti interfaces least as far as im concerned source
73433868,systemerror googleprotobufpyextdescriptorcc bad argument to internal function while using audio transformers in hugging face,python tensorflow pytorch huggingfacetransformers,import tensorflow lib first even if you are not using it before importing any torch libraries dont know the exact reason but after importing the lib code is working on the notebook you have shared refer to these links torchvision and tensorflowgpu import error
73428120,runtimeerror msecuda not implemented for long when training a transformertrainer,python pytorch huggingfacetransformers,changing the datatype of the labels column from int to float solved this issue for me if your dataset is from a pandas dataframe you can change the datatype of the column before passing the dataframe to a dataset
73409904,is it possible to get the meaning of each word using bert,huggingfacetransformers bertlanguagemodel,no you can not get the meaning of the word in plain english the whole idea of the bert is to convert plain english into meaningful numerical representations unfortunately these vectors are not interpretable it is a general limitation of deep learning compared to other traditional ml models that use selfextracted features but note that you can use these representation to find out certain relationships between words for example the words that are close to each other in terms of some distance measure have similar meanings have a look at this link for more information
73358850,valueerror no gradients provided for any variable tfdebertavforsequenceclassificationdebertaembeddingswordembeddings,python pandas tensorflow keras huggingfacetransformers,i would say its probably due to the fact that you are not adding a loss to the compilation thus no gradient can be computed wrt it
73247922,runtimeerror failed to import transformerspipelines because of the following error look up to see its traceback initialization failed,pythonx tensorflow jupyternotebook pytorch huggingfacetransformers,changed kernel condatensorflowp
73204460,how to pretrain bart using custom datasetnot fine tuning,huggingfacetransformers pretrainedmodel huggingface bart,you need to initialize a random model with the architecture of your choice from transformers import bartconfig bartmodel configuration bartconfig default bart config model bartmodelconfiguration default randomly initialised bart then you need to train said model the easiest way being using a trainer doc to which you provide your model training sets evaluation sets etc
73201858,no module named keras error in transformers,python keras tensorflow huggingfacetransformers,turns out a new version of the huggingfacetransformers library was released a few days ago so setting the transformers version to solved the issue maybe upgrading tensorflow to might work as well
73143613,how can i get metrics per label displayed in the transformers trainer,huggingfacetransformers,you can print the sklear classification report during the training phase by adjusting the computemetrics function and pass it to the trainer for a little demo you can change the function in the official huggingface example to the following from sklearnmetrics import classificationreport def computemetricsevalpred predictions labels evalpred if task stsb predictions npargmaxpredictions axis else predictions predictions printclassificationreportlabels predictions return metriccomputepredictionspredictions referenceslabels after each epoch you get the following output for a more fine grained control during your training phase you can also define callback to customise the behaviour of the training loop during different states class printclassificationcallbacktrainercallback def onevaluateself args state control logsnone kwargs printcalled after evaluation phase trainer trainer model args traindatasettraindataset evaldatasetevaldataset callbacksprintclassificationcallback after your training phase you can also use your trained model in a classification pipeline to pass one or more samples to your model and get the corresponding prediction labels for example from transformers import pipeline from sklearnmetrics import classificationreport textclassificationpipeline pipelinetextclassification modelmyfinetunedmodel x this is a cat sentence this is a dog sentence this is a fish sentence yact label label label labels label label label ypred resultlabel for result in textclassificationpipelinex printclassificationreportypred yact labelslabels output hope it helps
73127139,equivalent to tokenizer in transformers,pytorch tokenize huggingfacetransformers bertlanguagemodel huggingfacetokenizers,sadly their documentation for the old versions is broken but you can use encodeplus as shown in the following he oldest available documentation of encodeplus is from import torch from transformers import berttokenizer t berttokenizerfrompretrainedtextattackbertbaseuncasedyelppolarity tokenized tencodeplushello my dog is cute returntensorspt printtokenized output
73089823,how to find the actual sentence from sentence transformer,python numpy huggingfacetransformers sentencetransformers faiss,to retrieve the query results try something like this using the variables from your code but if you have corpus as a nparray object you can do some cool slicing like this and it can get a little cooler if your indices are already nparray like output of faiss and if you have queries with xxk results additional notes the faissindexflatl object returns these through the search function labels output labels of the nns size nk ie i in your code snippet refers to indices of the topk results distances output pairwise distances size nk ie d in your code snippet referring to the distance of the topk results from your query string since you have only query the n therefore your i and d matrice are of size xxk see also
73063698,cannot import name esmformaskedlm from transformers on google colab,python bioinformatics huggingfacetransformers fasta,esm is now added to hugging face use this
72912929,huggingface finetuning in tensorflow with custom datasets,tensorflow huggingfacetransformers transferlearning huggingfacetokenizers finetuning,there seems to be an error when you are passing the loss parameter you dont need to pass the loss parameter if you want to use the models builtin loss function i was able to train the model with your provided source code by changing mentioned line to or by passing a loss function transformers version hope it helps
72845812,bertbaseuncased typeerror tuple indices must be integers or slices not tuple,python machinelearning pytorch huggingfacetransformers bertlanguagemodel,each bertlayer returns a tuple that contains at least one tensor depending on what output you requested the first element of the tuple is the tensor you want to feed to the next bertlayer a more huggingfacelike approach would be calling the model with outputhiddenstates o modelinputids outputhiddenstatestrue printlenohiddenstates output the first tensor of the hiddenstates tuple is the output of your extractembeddings object token embeddings the other tensors are the contextualized embeddings that are the output of each bertlayer you should by the way provide an attention mask because otherwise your padding tokens will affect your output the tokenizer is able to do that for you and you can replace your whole texttoinput method with tokenizera sentence returntensorspt paddingmaxlength maxlength
72777174,huggingface tfrobertamodel detailed summary,python tensorflow keras tensorflow huggingfacetransformers,not exactly a model summary but you can print the layers like this you could also use sweights to get the weights of each layer
72695297,difference between fromconfig and frompretrained in huggingface,huggingfacetransformers transformermodel distilbert,the two functions you described fromconfig and frompretrained do not behave the same for a model m with a reference r fromconfig allows you to instantiate a blank model which has the same configuration the same shape as your model of choice m is as r was before training frompretrained allows you to load a pretrained model which has already been trained on a specific dataset for a given number of epochs m is as r after training to cite the doc note loading a model from its configuration file does not load the model weights it only affects the models configuration use frompretrained to load the model weights
72566262,how to create rnn layer on top of bert multilingual in pytorch,pytorch multilingual recurrentneuralnetwork huggingfacetransformers,you have to take care of the output of an rnn layer hn is probably what you want to use for your network also by default torch rnn is not batchfirst
72425277,how do you install a library from huggingface eg gpt neo m,machinelearning artificialintelligence huggingfacetransformers huggingface,for locally downloading gptneom onto your own desktop i actually have a youtube video going through these steps for gptneob model if you are interested the steps are exactly the same for gptneom first move to the files and version tab from the respective models official page in hugging face so for gptneom it would be this then click on the top right corner use in transformers and you will get a window like this now just follow the git clone commands there for gptneom it will be this will download all the files and the models that you see in that page to your local machines directory and now you can run the below code exactly following the official doc only changing the model parameters value to the local directory where you just gitcloned above below is my implementations taking model from local machine from transformers import pipeline generator pipelinetextgeneration modelyourlocaldirwhereyoudownloaded generatorusa will be dosampletrue maxlength minlength also note that you can manually download the model files by clicking on the down arrow from huggingface above site if you dont want to use git lfs in that case you need to pass gitlfsskipsmudge as the doc says does this pricing structure apply if you host the model on your own computer no if you are using offline ie host the model on your own computer there are no costs also if you are using these models on your own cloud hardware like aws ec or your own server then there are no costs but if you use huggingface api endpoints for inference then you will be charged accordingly so the pricing given in huggingfacecopricing applies when you are directly hitting huggingfaces own api endpoints for inference
72280030,how to resolve transformer model distilbert error got an unexpected keyword argument specialtokensmask,python huggingfacetransformers transformermodel,i was not able to reproduce your errors on my environment ubuntu but from what i see id suggest to try adding the returnspecialtokensmaskfalse parameter tokenspt tokenizerencodeplus text addspecialtokenstrue truncationtrue paddingmaxlength returnattentionmasktrue returntensorspt returnspecialtokensmaskfalse if that fails try to remove it explicitly tokensptpopspecialtokensmask
72261504,hugginface transformers bert tokenizer find out which documents get truncated,python machinelearning huggingfacetransformers huggingfacetokenizers huggingface,your assumption is correct anything with a length larger than assuming you are using distilbertbasemultilingualcased is truncated by having truncationtrue a quick solution would be not truncating and counting examples larger than the max input length of the model
72147225,pytorch model object has no attribute predict bert,python pytorch huggingfacetransformers bertlanguagemodel sentencetransformers,generally people wrote the prediction function for you if not you need to handle the low level stuff after this line you loaded the trained parameters model optimizer startepoch validlossmin loadckprbestmodelbestmodelpt bertclassifier optimizer after that you need to do the modelforwardintputseqthisattentionmaskmaybenull you can see the forward method here is the def forwardself inputids attentionmask in the model
72108945,saving finetuned model locally,huggingfacetransformers,you can use the savemodel method trainersavemodelpathtomodel or alternatively the savepretrained method modelsavepretrainedpathtomodel then when reloading your model specify the path you saved to automodelforsequenceclassificationfrompretrainedpathtomodel
71923159,how to get outputattentions of a pretrained distilbert model,python tensorflow tfkeras huggingfacetransformers distilbert,i am posting the answer as cronoik suggested i modified the code as dbertmodel tfdistilbertmodelfrompretraineddistilbertbaseuncasedconfig outputattentionstrue this gave both hidden states and attention in output
71768061,huggingface transformers classification using numlabels vs,python classification huggingfacetransformers,well it probably is kind of late but i want to point out one thing according to the hugging face code if you set numlabels it will actually trigger the regression modeling and the loss function will be set to mseloss you can find the code here also in their own tutorial for a binary classification problem imdb positive vs negative they set numlabels here is the link
71755917,how do i load a finetuned allennlp bertsrl model using bertpretrainedmodelfrompretrained,python pytorch huggingfacetransformers bertlanguagemodel allennlp,i believe i have figured this out basically i had to reload my model archive access the underlying model and tokenizer and then save those this last part is probably less interesting to most folks but also in the configini i mentioned the directory serpureclinicalbertlargethymeandontonotessavepretrained needed to be passed to the line pretrainedmodelnameorpath not to finetunedpath
71561761,how to load a fine tuned pytorch huggingface bert model from a checkpoint file,python machinelearning pytorch googlecolaboratory huggingfacetransformers,just save your model using modelsavepretrained here is an example you can download the model from colab save it on your gdrive or at any other location of your choice while doing inference you can just give path to this model you may have to upload it and start with inference to load the model
71557275,emotion detection in text using emoroberta,python deeplearning huggingfacetransformers,so the solution is to add a parameter in the pipeline for it to become
71434804,how to fed last concatenated hidden layers of bert to fc layers,python pytorch huggingfacetransformers bertlanguagemodel,there is no original bert approach for classification with concatenated hidden layers you have several options to proceed and i will just describe a comment on your approach and suggest an alternative in the following preliminary import torchnn as nn from transformers import berttokenizerfast bertmodel t berttokenizerfastfrompretrainedbertbasecased m bertmodelfrompretrainedbertbasecased fc nnlinear s this is a random sentence this is another random sentence with more words i ts paddingtruereturntensorspt with torchnograd o mi outputhiddenstatestrue printi at first you should look at your input printi inputids tensor tokentypeids tensor attentionmask tensor what you should notice here is that the shorter sentence gets padded that is relevant because simply pooling the mean with torchmean will result in different sentence embeddings for the same sentence depending on the number of padding tokens of course the model will learn to handle that to some extent after sufficient training but you should however use a more sophisticated mean function that removes the padding tokens right away def meanpoolingmodeloutput attentionmask inputmaskexpanded attentionmaskunsqueezeexpandmodeloutputsizefloat return torchsummodeloutput inputmaskexpanded torchclampinputmaskexpandedsum mine omean meanpoolingohiddenstatesxiattentionmask for x in range we want a tensor and not a list omean torchstackomean dim we want only one tensor per sequence omean torchmeanomeandim printomeanshape with torchnograd printfcomean output torchsize tensor these operations are pretty expensive and people often use an approach called cls pooling as a cheaper alternative with comparable performance we only use the cls token ie first token of the sequence id ocls ohiddenstatesx for x in range we want a tensor and not a list ocls torchstackocls dim we want only one tensor per sequence ocls torchmeanoclsdim printoclsshape with torchnograd printfcocls output torchsize tensor
71401458,hugginface dataloader bert valueerror too many values to unpack expected ax hyperparameters tuning with pytorch,pytorch huggingfacetransformers bertlanguagemodel hyperparameters dataloader,your dataloader returns a dictionary therefore the way you loop and access it is wrong should be done as such train network for in rangenumepochs your dataloader returns a dictionary so access it as such for batch in traindataloader move data to proper dtype and device labels batchtargetstodevicedevice attenmask batchattentionmasktodevicedevice inputids batchinputidstodevicedevice zero the parameter gradients optimizerzerograd forward backward optimize outputs netinputids attentionmaskattenmask
71398882,cuda runtimeerror cuda out of memory bert sagemaker,python gpu amazonsagemaker huggingfacetransformers,a cuda out of memory error indicates that your gpu ram random access memory is full this is different from the storage on your device which is the info you get following the df h command this memory is occupied by the model that you load into gpu memory which is independent of your dataset size the gpu memory required by the model is at least twice the actual size of the model but most likely closer to times initial weights checkpoint gradients optimizer states etc things you can try provision an instance with more gpu memory decrease batch size use a different smaller model
71336067,how to freeze some layers of bert in fine tuning in tfkeras,pythonx keras tensorflow huggingfacetransformers bertlanguagemodel,i found the answer and i share it here hope it can help others by the help of this article which is about fine tuning bert using pytorch the equivalent in tensorflowkeras is as below where i is the index of the proper layer
71318599,bert classifier valueerror target size torchsize must be the same as input size torchsize,python machinelearning pytorch huggingfacetransformers bertlanguagemodel,in case anyone stumbles on this like i did ill write out an answer since there arent a lot of google hits for this target sizeinput size error and the previous answer has some factual inaccuracies unlike the previous answer would suggest the real problem isnt with the loss function but with the output of the modelnnbcewithlogitsloss is completely fine for multilabel and multiclass applications chiara updated her post saying that in fact she has a binary classification problem but even that should not be a problem for this loss function so why the error the original code has this means run the model then create preds with the row indeces of the highest output of the model obviously there is only a index of highest if there are multiple predicted values multiple output values usually means multiple input classes so i can see why shai though this was multiclass but why would we get multiple outputs from a binary classifier as it turns out bert or huggingface anyway for binary problems expects that nclasses is set to setting classes to puts the model in regression mode this means that under the hood binary problems are treated like a twoclass problem outputting predictions with the size batch size one column predicting the chance of it being a and one for the chance of it being the loss fucntion throws an error because it is supplied with only one row of onehot encoded labels targets dtargetstodevice so the labels have dimensions batch size or after the unsqueeze batch size either way the dimensions dont match up some loss functions can deal with this fine but others require the exact same dimensions to make things more frustrating for version nnbcewithlogitsloss requires matching dimensions but later versions do not one solution may therefore be to update your pytorch version would work for example for me this was not an option so i ended up going with a different loss function nncrossentropyloss as suggested by shai indeed does the trick as it accepts any input with the same length in other words they had a working solution for the wrong reasons
71167641,in allennlp how can we set submodule argument in pretrainedtransformermismatchedembedder,python huggingfacetransformers allennlp,you are absolutely right we should have a submodule for the mismatched embedder if you make a pr to add it ill gladly review it and get it merged
71086923,transformers longformer classification problem with f precision and recall classification,python huggingfacetransformers,my guess is that the transformation of your dependend variable was somehow messed up this i think because all your metrics which depend on tp true posivites are both precision and sensitivityrecall depend on tp as numerator fscore depends on both metrics and therefore on tp as numerator if the numerator is because you have no tp the result will be as well a goodmoderate accuracy can also be achieved if you only got the tn right that is why you can have a valid looking accuracy and for the other metrics so peek into your testtraining sets and look whether the split was succesful and whether both possible outcomes of you binary variable are available in both sets if one of them is lacking in the training set this might explain a complete missclassification and lack of tp in the testset
71067376,when doing pretraining of a transformer model how can i add words to the vocabulary,pytorch huggingfacetransformers bertlanguagemodel,as per my comment im assuming that you go with a pretrained checkpoint if only to avoid learning a new tokenizer also the solution works with pytorch which might be more suitable for such changes i havent checked tensorflow which is mentioned in one of your quotes so no guarantees that this works across platforms to solve your problem let us divide this into two subproblems adding the new tokens to the tokenizer and resizing the token embedding matrix of the model accordingly the first can actually be achieved quite simply by using addtokens im referencing the slow tokenizers implementation of it because its in python but from what i can see this also exists for the faster rustbased tokenizers from transformers import autotokenizer tokenizer autotokenizerfrompretraineddistilbertbaseuncased will return an integer corresponding to the number of added tokens the input could also be a list of strings instead of a single string numnewtokens tokenizeraddtokensdennlinger you can quickly verify that this worked by looking at the encoded input ids printtokenizerthis is dennlinger inputids the index now corresponds to the new token with my username so we can check the first part however if we look at the function docstring of addtokens it also says note hen adding new tokens to the vocabulary you should make sure to also resize the token embedding matrix of the model so that its embedding matrix matches the tokenizer in order to do that please use the pretrainedmodelresizetokenembeddings method looking at this particular function the description is a bit confusing but we can get a correctly resized matrix with randomly initialized weights for new tokens by simply passing the previous model size plus the number of new tokens from transformers import automodel model automodelfrompretraineddistilbertbaseuncased modelresizetokenembeddingsmodelconfigvocabsize numnewtokens test that everything worked correctly modeltokenizerthis is dennlinger returntensorspt edit notably resizetokenembeddings also takes care of any associated weights this means if you are pretraining it will also adjust the size of the language modeling head which should have the same number of tokens or fix tied weights that would be affected by an increased number of tokens
71058732,how to load transformers pipeline from folder,python huggingfacetransformers,apparently the default initialization works with local folders as well so one can download a model like this and later load it like
71050697,transformers how to use cuda for inferencing,python pytorch huggingfacetransformers inference,you should transfer your input to cuda as well before performing the inference be aware nnmoduleto is inplace while torchtensorto is not it does a copy
71048521,how to freeze parts of t transformer model,huggingfacetransformers ttransformer,ive adapted a solution based on this discussion from the huggingface forums basically you have to specify the names of the modulespytorch layers that you want to freeze in your particular case of t i started by looking at the model summary from transformers import tmodelforconditionalgeneration model tmodelforconditionalgenerationfrompretrainedtsmall printmodel this gives the following abbreviated output with this we can then generate a list of modules that we want to freeze in particular i decided to freeze the entire tlayerselfattention block for the encoder and additionally the tlayercrossattention for the decoder all modules in the modulestofreeze modelencoderblockilayer for i in rangelenmodelencoderblock and the decoder modules which has both a selfattention layer modulestofreezeextendmodeldecoderblockilayer for i in rangelenmodeldecoderblock and crossattention layer block modulestofreezeextendmodeldecoderblockilayer for i in rangelenmodeldecoderblock and then simply freeze all the parameters in the respective modules for module in modulestofreeze for param in moduleparameters paramrequiresgrad false actual freezing operation you can verify that these are actually frozen in your model by running the following for param in modelparameters printparamrequiresgrad which should print quite a few false as well if you really only want to freeze k q and v you can adapt the above process to just subselect the modules you want
71024254,jupyter kernel dies when importing pipeline function from transformers class on mac os,python jupyternotebook jupyterlab huggingfacetransformers,it works fine for me you could try creating a fresh conda environment and reinstalling the app you could also try using jupyterlab instead of jupyternotebook are you on mac os i couldnt get it to run at first using conda install transformers my jupyterlab kept hanging as well then i did this conda install c huggingface transformers and here is the result it works fine for me on linux and mac now
71012012,modulenotfounderror no module named transformers,python artificialintelligence googlecolaboratory importerror huggingfacetransformers,probably it is because you have not installed in your new since youve upgraded to colabs pro session the library transformers try to run as first cell the following pip install transformers the at the beginning of the instruction is needed to go into terminal mode this will download the transformers package into the sessions environment
70960986,is there any way to change parameters in pretrained detrfeatureextractor,pytorch objectdetection featureextraction huggingfacetransformers pytorchlightning,your values to the constructor are not taken because you are calling frompretrained which loads all values from the respective config file of the pretrained model in your case the corresponding config file can be viewed here even if some values might not be specified in the config they will be primarily taken from the default values instead of whatever youre passing before if you want to change attributes you can do so after loading from transformers import detrfeatureextractor model detrfeatureextractorfrompretrainedfacebookdetrresnet modelsize printmodel will show the correct size i do want to point out that changing parameters of pretrained networks might lead to unexpected behavior after all the model wasnt originally trained with this particular setting so just be aware that this might lead to detrimental performance and experiment at your own risk
70957390,organize data for transformer finetuning,python huggingfacetransformers,you can use the tokenizer call method to join both sentences when encoding them in case youre using the pytorch implementation here is an example import torch from transformers import autotokenizer sentences list containing all sentences sentences list containing all sentences labels list containing all labels or tokenizername bertbasecased tokenizer autotokenizerfrompretrainedtokenizername encodings tokenizer sentences sentences returntensorspt labels torchtensorlabels then you can create your custom dataset to use it on training class customrealdatasettorchutilsdatadataset def initself encodings labels selfencodings encodings selflabels labels def getitemself idx item key valueidx for key value in selfencodingsitems itemlabels selflabelsidx return item def lenself return lenselflabels
70740565,optimize albert huggingface model,python huggingfacetransformers bertlanguagemodel onnx huggingfacetokenizers,optimise any pytorch model using torchoptimizer installation pip install torchoptimizer implementation import torchoptimizer as optim model optimizer optimdiffgradmodelparameters lr optimizerstep source torchsavemodelstatedict path source
70709572,typeerror not a string parameters in autotokenizerfrompretrained,python tensorflow huggingfacetransformers onnx huggingfacetokenizers,passing just the model name suffices tokenizer alberttokenizerfrompretrainedalbertbasev list of modeltypes can be found here
70619634,errors in loading statedict for robertaforsequenceclassification,pythonx pytorch huggingfacetransformers roberta,load with ignoremismatchedsizestrue then you can finetune the model
70609579,use quantization on huggingface transformers models,python deeplearning huggingfacetransformers bertlanguagemodel quantization,the pipeline approach wont work for quantisation as we need the models to be returned you can however use pipeline for testing the original models for timing etc quantisation code tokenlogits contains the tensors of the quantised model you could place a forloop around this code and replace modelname with string from a list modelname bertbaseuncased tokenizer autotokenizerfrompretrainedmodelname model automodelformaskedlmfrompretrainedmodelname sequence distilled models are smaller than the models they mimic using them instead of the large fversions would help tokenizermasktoken our carbon footprint inputs tokenizersequence returntensorspt masktokenindex torchwhereinputsinputids tokenizermasktokenid tokenlogits modelinputslogits can stop here source
70577285,valueerror you have to specify either inputids or inputsembeds when training automodelwithlmhead model gpt,python pytorch huggingfacetransformers gpt,i didnt find the concrete answer to this question but a workaround for anyone looking for examples on how to finetune the gpt models from huggingface you may have a look into this repo they listed a couple of examples on how to finetune different transformer models complemented by documented code examples i used the runclmpy script and it achieved what i wanted
70518826,onnx runtime bert inference runtimeerror input must be a list of dictionaries or a single numpy array for input attentionmask,python deeplearning huggingfacetransformers onnxruntime,according to the docs the returntensorsnp not returntensorspt
70449122,change last layer on pretrained huggingface model,python pytorch torch huggingfacetransformers,so there is a solution for this just add ignoremismatchedsizestrue when loading the model as
70371140,machine learning transformer multiclass classification number of classes is inconsistent in test data and training data,machinelearning huggingfacetransformers bertlanguagemodel,because your trained network has classes
70149699,transformers barttokenizeraddtokens doesnt work as id expect for suffixes,python huggingfacetransformers,the short answer is that theres behavior bug in the handling of added tokens for bart and roberta gpt etc that explicitly strips spaces from the tokens adjacent both left and right to the added tokens location i dont see a simple workaround to this added tokens are handled differently in the transformers tokenizer code the text is first split using a trie to identify any tokens in the added tokens list see tokenizationutilspytokenize after finding any added tokens in the text the remainder is then tokenized using the existing vocabbpe encoding scheme see tokenizationgptpytokenize the added tokens are added to the selfuniquenosplittokens list which prevents them from being broken down further into smaller chunks the code that handles this see tokenizationutilspytokenize explicitly strips the spaces from the tokens to the left and right you could manually remove them from the no split list but then they may be broken down into smaller subcomponents note that for special tokens if you add the token inside of the addedtoken class you can set the lstrip and rstrip behaviors but this isnt available for nonspecial tokens see for the else statement where the spaces are stripped
70055966,chatbot using huggingface transformers,tensorflow chatbot huggingfacetransformers blenderbot,here is an example of using the dialogpt model with tensorflow from transformers import tfautomodelforcausallm autotokenizer blenderbottokenizer tfblenderbotforconditionalgeneration import tensorflow as tf chatbots blenderbot blenderbottokenizerfrompretrainedfacebookblenderbotmdistill tftforconditionalgenerationfrompretrainedfacebookblenderbotmdistill dialogpt autotokenizerfrompretrainedmicrosoftdialogptsmall tfautomodelforcausallmfrompretrainedmicrosoftdialogptsmall key dialogpt tokenizer model chatbotskey for step in range newuserinputids tokenizerencodeinput user tokenizereostoken returntensorstf if step botinputids tfconcatchathistoryids newuserinputids axis else botinputids newuserinputids chathistoryids modelgeneratebotinputids maxlength padtokenidtokenizereostokenid printkey formattokenizerdecodechathistoryids botinputidsshape skipspecialtokenstrue if you want to compare different chatbots you might want to adapt their decoder parameters because they are not always identical for example using blenderbot and a maxlength of you get this kind of response with the current code in general you should ask yourself which special characters are important for a chatbot depending on your domain and which characters should can be omitted you should also experiment with different decoding methods such as greedy search beam search random sampling topk sampling and nucleus sampling and find out what works best for your use case for more information on this topic check out this post
69983684,list index out of range when saving a fine tuned bert model,tensorflow huggingfacetransformers,sorry i meant to write answer it resolved for me when i changed from sequenceoutput bertlayerbertinputids inputmasksidslasthiddenstate to sequenceoutput bertlayerbertinputids inputmasksidslasthiddenstate please check this below
69921629,transformers autotokenizertokenize introducing extra characters,python huggingfacetransformers huggingfacetokenizers,this is not an error but a feature bert and other transformers use wordpiece tokenization algorithm that tokenizes strings into either known words or word pieces for unknown words in the tokenizer vocabulary in your examle words cto tlr and pty are not in the tokenizer vocabulary and thus wordpiece splits them into subwords eg the first subword is ct and another part is o where denotes that the subword is connected to the predecessor this is a great feature that allows to represent any string
69914131,i want to analysis with classification algoritms using berts hidden state,python pytorch huggingfacetransformers,they did not mean that softmax layer because that one is inside bertattention they meant the pooler layer on top of bert i found their repository provided in the paper it seems when they train they use the plain bertforsequenceclassification which uses hiddenstates pooler activation linear classifier loss when they predict they only use the hiddenstates or in bertmodelingpy its called sequenceoutput then they pass it to a different classifier loaded in biaspredictorpyl so if you want to try a different classifier use it here
69874436,pyinstaller problem making exe files that using transformers and pyqt library,python pyqt pyside huggingfacetransformers,first pip install tqdm if you havent already second specify the path to your libsitepackages you can do this by either adding an argument to pathex in your spec file venv for a virtual environment at some folder venv in your local directory or the absolute path to your global python install libsitepackages if youre not using a virtual environment specifying the path to libsitepackages from the commandline pyinstaller paths venvlibsitepackages myprogrampy from the pyinstaller docs pathex a list of paths to search for imports like using pythonpath including paths given by the paths option some python scripts import modules in ways that pyinstaller cannot detect for example by using the import function with variable data using importlibimportmodule or manipulating the syspath value at run time
69781810,adding special tokens changes all embeddings tf bert hugging face,python tensorflow deeplearning huggingfacetransformers,when setting addspecialtokenstrue you are including the cls token in the front and the sep token at the end of your sentence which leads to a total of tokens instead of tokens tokenizerthis product is no good addspecialtokenstrue returntensorstf printtokenizerconvertidstotokenstfsqueezetokensinputids axis your sentence level embeddings are different because these two special tokens become a part of your embedding as they are propagated through the bert model they are not masked like padding tokens pad check out the docs for more information if you take a closer look at how berts transformerencoder architecture and attention mechanism works you will quickly understand why a single difference between two sentences will generate different hiddenstates new tokens are not simply concatenated to existing ones in a sense the tokens depend on each other according to the bert author jacob devlin im not sure what these vectors are since bert does not generate meaningful sentence vectors it seems that this is doing average pooling over the word tokens to get a sentence vector but we never suggested that this will generate meaningful sentence representations or another interesting discussion the value of cls is influenced by other tokens just like other tokens are influenced by their context attention
69733197,how to get the corresponding character or string that has been labelled as unk token in bert,python huggingfacetransformers bertlanguagemodel huggingfacetokenizers,the fast tokenizers return a batchencoding object that has a builtin wordids and tokentochars from transformers import berttokenizerfast t berttokenizerfastfrompretrainedbertbaseuncased tokens tword embeddings are vectors printtokensinputids printtdecodetokensinputids printtokenswordids printtokenstokentochars output
69628487,how to get shap values for huggingface transformer model prediction zeroshot classification,pytorch huggingfacetransformers transformermodel shap,the zeroshotclassificationpipeline is currently not supported by shap but you can use a workaround the workaround is required because the shap explainer forwards only one parameter to the model a pipeline in this case but the zeroshotclassificationpipeline requires two parameters namely text and labels the shap explainer will access the config of your model and use its labelid and idlabel properties they do not match the labels returned from the zeroshotclassificationpipeline and will result in an error below is a suggestion for one possible workaround i recommend opening an issue at shap and requesting official support for huggingfaces zeroshotclassificationpipeline import shap from transformers import automodelforsequenceclassification autotokenizer zeroshotclassificationpipeline from typing import union list weights valhalladistilbartmnli model automodelforsequenceclassificationfrompretrainedweights tokenizer autotokenizerfrompretrainedweights create your own pipeline that only requires the text parameter for the call method and provides a method to set the labels class myzeroshotclassificationpipelinezeroshotclassificationpipeline overwrite the call method def callself args o supercallargs selfworkaroundlabels return labelx score x for x in zipolabels oscores def setlabelsworkaroundself labels unionstrliststr selfworkaroundlabels labels exampletext this is an example text about snowflakes in the summer labels weathersports in the following we address issue modelconfiglabelidupdatevk for kv in enumeratelabels modelconfigidlabelupdatekv for kv in enumeratelabels pipe myzeroshotclassificationpipelinemodelmodel tokenizertokenizer returnallscorestrue pipesetlabelsworkaroundlabels def scoreandvisualizetext prediction pipetext printprediction explainer shapexplainerpipe shapvalues explainertext shapplotstextshapvalues scoreandvisualizeexampletext output
69616471,hugginface bert tokenizer build from source due to proxy issues,python tokenize huggingfacetransformers,you would need to download vocabulary and configuration files vocabtxt configjson put them into a folder and pass folders path to berttokenizerfrompretrained function here is the download location of vocabtxt for different tokenizer models location of configjson source transformers codebase steps
69480199,padtokenid not working in hugging face transformers,python pythonx tensorflow huggingfacetransformers,your code does not throw any error for me i would try reinstalling the most recent version of transformers if that is a viable solution for you
69433514,test intel extension for pytorchipex in multiplechoice from huggingface transformers,performance intel huggingfacetransformers intelpython intelpytorch,first you have to understand which factors actually increases the running time following are these factors the large input size the data structure shifted mean and unnormalized the large network depth andor width large number of epochs the batch size not compatible with physical available memory very small or high learning rate for fast running make sure to work on the above factors like reduce the input size to the appropriate dimensions that assures no loss in important features always preprocess the input to make it zero mean and normalized it by dividing it by std deviation or difference in max min values keep the network depth and width that is not to high or low or always use the standard architecture that are theoretically proven always make sure of the epochs if you are not able to make any further improvements in your error or accuracy beyond a defined threshold then there is no need to take more epochs the batch size should be decided based on the available memory and number of cpusgpus if the batch cannot be loaded fully in memory then this will lead to slow processing due to lots of paging between memory and the filesystem appropriate learning rate should be determine by trying multiple and using that which gives the best reduction in error wrt number of epochs
69249187,how to test a model before finetuning in pytorch lightning,huggingfacetransformers pytorchlightning,the trainer needs to call its fit in order to set up a lot of things and then only you can do test or other methods you are right about putting a fit just before test but the fit call needs to a valid one you have to feed a dataloaderdatamodule to it but since you dont want to do a trainingvalidation in this fit call just pass limittrainvalbatches while trainer construction the fit call here will just set things up for you and skip trainingvalidation and then the testing follows next time run the same code but without the limittrainvalbatches this will do the pretraining for you clarifying a bit about fit taking none for all but model its not quite true you must provide either a dataloader or a datamodule
69196995,using huggingface transformer with arguments in pipeline,pytorch huggingfacetransformers bertlanguagemodel transformermodel huggingfacetokenizers,the maxlength tokenization parameter is not supported per default ie no padding to maxlength is applied but you can create your own class and overwrite this behavior from transformers import autotokenizer automodel from transformers import featureextractionpipeline from transformerstokenizationutils import truncationstrategy tokenizer autotokenizerfrompretrainedemilyalsentzerbioclinicalbert model automodelfrompretrainedemilyalsentzerbioclinicalbert inputs hello world class myfeatureextractionpipelinefeatureextractionpipeline def parseandtokenize self inputs maxlength paddingtrue addspecialtokenstrue truncationtruncationstrategydonottruncate kwargs parse arguments and tokenize parse arguments if getattrselftokenizer padtoken none is none padding false inputs selftokenizer inputs addspecialtokensaddspecialtokens returntensorsselfframework paddingpadding truncationtruncation maxlengthmaxlength return inputs mynlp myfeatureextractionpipelinemodelmodel tokenizertokenizer o mynlphello world maxlength paddingmaxlength truncationtrue let us compare the size of the output printleno printleno printleno output please note that this will only work with transformers x and previous versions the team is currently refactoring the pipeline classes and future releases will require different adjustments ie that will not work as soon as the refactored pipelines are released
69195950,problem with inputs when building a model with tfbertmodel and autotokenizer from huggingfaces transformers,tensorflow keras huggingfacetransformers bertlanguagemodel huggingfacetokenizers,for now i solved by taking the tokenization step out of the model def tokenizesentences tokenizer inputids inputmasks inputsegments for sentence in sentences inputs tokenizerencodeplussentence addspecialtokenstrue maxlength padtomaxlengthtrue returnattentionmasktrue returntokentypeidstrue inputidsappendinputsinputids inputmasksappendinputsattentionmask inputsegmentsappendinputstokentypeids return npasarrayinputids dtypeint npasarrayinputmasks dtypeint npasarrayinputsegments dtypeint the model takes two inputs which are the first two values returned by the tokenize funciton def buildclassifiermodel inputidsin tfkeraslayersinputshape nameinputtoken dtypeint inputmasksin tfkeraslayersinputshape namemaskedtoken dtypeint embeddinglayer bertinputidsin attentionmaskinputmasksin model tfkerasmodelinputsinputidsin inputmasksin outputs x for layer in modellayers layertrainable false return model id still like to know if someone has a solution which integrates the tokenization step inside the modelbuilding context so that an user of the model can simply feed phrases to it to get a prediction or to train the model
69087044,early stopping in bert trainer instances,python deeplearning neuralnetwork huggingfacetransformers huggingface,there are a couple of modifications you need to perform prior to correctly using the earlystoppingcallback you need to use loadbestmodelatend true earlystoppingcallback requires this to be true evaluationstrategy steps or intervalstrategysteps instead of epoch evalsteps evaluate the metrics after n steps metricforbestmodel f in your trainer of course when you use computemetrics for example it can be a function like the return of the computemetrics should be a dictionary and you can access whatever metric you wantcompute inside the function and return note in newer transformers version the usage of enum intervalstrategysteps is recommended see trainingarguments instead of plain steps string the latter being soon subject to deprecation
69046964,can bert output be fixed in shape irrespective of string size,python pytorch huggingfacetransformers huggingfacetokenizers,when you call the tokenizer with only one sentence and paddingtrue truncationtrue maxlength it will pad the output sequence to the longest input sequence and truncate if required since you are providing only one sentence the tokenizer can not pad anything because it is already the longest sequence of the batch that means you can achieve what you want in two ways provide a batch from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedbertbaseuncased model automodelfrompretrainedbertbaseuncased inputs a a a abcede inputs tokenizerinputs paddingtrue truncationtrue maxlength returntensorspt printinputsinputids outputs modelinputs printoutputslasthiddenstateshape output set paddingmaxlength from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedbertbaseuncased model automodelfrompretrainedbertbaseuncased inputs a a a abcede for i in inputs inputs tokenizeri paddingmaxlength truncationtrue maxlength returntensorspt printinputsinputids outputs modelinputs printoutputslasthiddenstateshape i leni output tensor torchsize a tensor torchsize aaaaaaaaaaaaaaaaaaaa tensor torchsize aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa tensor torchsize abcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcede
69025750,how to finetune huggingface bert model for text classification,machinelearning huggingfacetransformers transferlearning,fine tuning approach there are multiple approaches to finetune bert for the target tasks further pretraining the base bert model custom classification layers on top of the base bert model being trainable custom classification layers on top of the base bert model being nontrainable frozen note that the bert base model has been pretrained only for two tasks as in the original paper bert pretraining of deep bidirectional transformers for language understanding pretraining bert we pretrain bert using two unsupervised tasks task masked lm task next sentence prediction nsp hence the base bert model is like halfbaked which can be fully baked for the target domain st way we can use it as part of our custom model training with the base trainable nd or nottrainable rd st approach how to finetune bert for text classification demonstrated the st approach of further pretraining and pointed out the learning rate is the key to avoid catastrophic forgetting where the pretrained knowledge is erased during learning of new knowledge we find that a lower learning rate such as e is necessary to make bert overcome the catastrophic forgetting problem with an aggressive learn rate of e the training set fails to converge probably this is the reason why the bert paper used e e e and e for finetuning we use a batch size of and finetune for epochs over the data for all glue tasks for each task we selected the best finetuning learning rate among e e e and e on the dev set note that the base model pretraining itself used higher learning rate bertbaseuncased pretraining the model was trained on cloud tpus in pod configuration tpu chips total for one million steps with a batch size of the sequence length was limited to tokens for of the steps and for the remaining the optimizer used is adam with a learning rate of e and a weight decay of learning rate warmup for steps and linear decay of the learning rate after will describe the st way as part of the rd approach below fyi tfdistilbertmodel is the bare base model with the name distilbert nd approach huggingface takes the nd approach as in finetuning with native pytorchtensorflow where tfdistilbertforsequenceclassification has added the custom classification layer classifier on top of the base distilbert model being trainable the small learning rate requirement will apply as well to avoid the catastrophic forgetting implementation of the nd approach rd approach basics please note that the images are taken from a visual guide to using bert for the first time and modified tokenizer tokenizer generates the instance of batchencoding which can be used like a python dictionary and the input to the bert model batchencoding holds the output of the encodeplus and batchencode methods tokens attentionmasks etc this class is derived from a python dictionary and can be used as a dictionary in addition this class exposes utility methods to map from wordcharacter space to token space parameters data dict dictionary of listsarraystensors returned by the encodebatchencode methods inputids attentionmask etc the data attribute of the class is the tokens generated which has inputids and attentionmask elements inputids inputids the input ids are often the only required parameters to be passed to the model as input they are token indices numerical representations of tokens building the sequences that will be used as input by the model attentionmask attention mask this argument indicates to the model which tokens should be attended to and which should not if the attentionmask is the token id is ignored for instance if a sequence is padded to adjust the sequence length the padded words should be ignored hence their attentionmask are special tokens berttokenizer addes special tokens enclosing a sequence with cls and sep cls represents classification and sep separates sequences for question answer or paraphrase tasks sep separates the two sentences to compare berttokenizer clstoken str optional defaults to clsthe classifier token which is used when doing sequence classification classification of the whole sequence instead of pertoken classification it is the first token of the sequence when built with special tokens septoken str optional defaults to septhe separator token which is used when building a sequence from multiple sequences eg two sequences for sequence classification or for a text and a question for question answering it is also used as the last token of a sequence built with special tokens a visual guide to using bert for the first time show the tokenization cls the embedding vector for cls in the output from the base model final layer represents the classification that has been learned by the base model hence feed the embedding vector of cls token into the classification layer added on top of the base model bert pretraining of deep bidirectional transformers for language understanding the first token of every sequence is always a special classification token cls the final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks sentence pairs are packed together into a single sequence we differentiate the sentences in two ways first we separate them with a special token sep second we add a learned embedding to every token indicating whether it belongs to sentence a or sentence b the model structure will be illustrated as below vector size in the model distilbertbaseuncased each token is embedded into a vector of size the shape of the output from the base model is batchsize maxsequencelength embeddingvectorsize this accords with the bert paper about the bertbase model as indicated in distilbertbaseuncased bert pretraining of deep bidirectional transformers for language understanding bertbase l h a total parametersm and bertlarge l h a total parametersm base model tfdistilbertmodel hugging face transformers finetuning distilbert for binary classification tasks tfdistilbertmodel class to instantiate the base distilbert model without any specific head on top as opposed to other classes such as tfdistilbertforsequenceclassification that do have an added classification head we do not want any taskspecific head attached because we simply want the pretrained weights of the base model to provide a general understanding of the english language and it will be our job to add our own classification head during the finetuning process in order to help the model distinguish between toxic comments tfdistilbertmodel generates an instance of tfbasemodeloutput whose lasthiddenstate parameter is the output from the model last layer tfbasemodeloutput parameters lasthiddenstate tftensor of shape batchsize sequencelength hiddensize sequence of hiddenstates at the output of the last layer of the model implementation python modules configuration tokenizer input layer the base model expects inputids and attentionmask whose shape is maxsequencelength generate keras tensors for them with input layer respectively base model layer generate the output from the base model the base model generates tfbasemodeloutput feed the embedding of cls to the next layer classification layers softmax layer final custom model data allocation train to implement the st approach change the configuration as below then freezebase is changed to false and learningrate is changed to e which will run further pretraining on the base bert model saving the model for the rd approach saving the model will cause issues the savepretrained method of the huggingface model cannot be used as the model is not a direct sub class from of huggingface pretrainedmodel keras savemodel causes an error with the default savetracestrue or causes a different error with savetracestrue when loading the model with keras loadmodel only keras model saveweights worked as far as i tested experiments as far as i tested with toxic comment classification challenge the st approach gave better recall identify true toxic comment true nontoxic comment code can be accessed as below please provide correctionsuggestion if anything code for st and rd approach related bert document classification tutorial with code fine tuning using tfdistilbertforsequenceclassification and pytorch hugging face transformers finetuning distilbert for binary classification tasks fine tuning using tfdistilbertmodel
68961546,get the index of subwords produced by berttokenizer in transformers library,pytorch huggingfacetransformers huggingfacetokenizers,the fast tokenizers return a batchencoding object that has a builtin wordids output
68945422,how to use a huggingface bert model from to feed a binary classifier cnn,python pytorch huggingfacetransformers,in pytorch you dont need to have a fixed input dim for a cnn the only requirement is that your kernelsize must not be smaller than the inputsize generally the best way of putting a classifier sequence classifier on top of a transformer model is to add a pooling layer fc layer you can use global pooling an average or max pooling or an adptative pooling and then a full connected layer note that you can also use automodelforsequenceclassification to get everything done for you
68918962,huggingfacetransformers ner single sentencesample prediction,pythonx deeplearning pytorch huggingfacetransformers huggingfacetokenizers,the answer is a bit trickier than expectedhuge credits to niels rogge firstly loading models in huggingfacetransformers can be done in at least two ways automodelfrompretrainedmymodelowncustomtrainingpth fromtffalse automodelfortokenclassificationfrompretrainedmymodelowncustomtrainingpth fromtffalse it seems that according to the task at hand different automodels subclasses need to be used in this scenario i posted it is the automodelfortokenclassification that has to be used after that a solution to obtain the predictions would be to do the following
68813979,bert transformer size error while machine traslation,pythonx translation huggingfacetransformers,in the problem described here credits to lysandrejik the problem appears to be the data type of a dict instead of tensor it might be the case that you need to change the tokenizer output from to
68759885,print input output grad loss at every stepepoch when training transformers huggingface model,python logging neuralnetwork pytorch huggingfacetransformers,while using hooks and custom callbacks is the right way to solve the problem i find better solution use builtin utility for finding naninf in losses weights inputs outputs since the transformers has such option you can use it manually in forward function or just use additional option for trainingarguments like this
68757944,is there a way to use a pretrained transformers model without the configuration file,python pytorch huggingfacetransformers,this is a warning message instead of a error it means that the pretrained model is pretrained in some task such as question answering mlm etc if your own fine tune task is the same as those pretrained task then this is not expected unless this is expected because some pooler of pretrained model will not be used in fine tune but this message doesnt mean that the bertconfigjson isnt the right one you can test it on huggingfaces official colab notebook you can find more information in this issue
68747152,how to get probability of an answer using bert model and is there a way to ask multiple questions for a context,python bertlanguagemodel huggingfacetransformers simpletransformers,you can use the huggingface question answering pipeline to achieve that from transformers import pipeline modelcheckpoint bertlargeuncasedwholewordmaskingfinetunedsquad qapipeline pipelinequestionanswering modelmodelcheckpoint tokenizermodelcheckpoint qapipelinequestionsky color weather contexttoday the sky is blue and it is cold out there output
68604289,attributeerror module transformers has no attribute tfgptneoforcausallm,python pytorch huggingfacetransformers gpt,try without using fromtftrue flag like below fromtf expects the pretrainedmodelnameorpath ie the first parameter to be a path to load saved tensorflow checkpoints from
68481189,huggingface autotokenizer cannot be referenced when importing transformers,huggingfacetransformers,for anyone who comes across a problem around circular import this could be due to the naming convention of your py file changing my file name solved the issue as there might be a file in my python lib folder with similar naming conventions
68421125,huggingface transformer with tensorflow saves two files as model weights,python tensorflow tensorflow huggingfacetransformers,you have possibilities to save a model either in keras h format or in tensorflow savedmodel format you can determine the format by passing the saveformat argument and set it to either h or tf if you dont specify this argument the format will be determined by the name you have passed if the name has the h suffix it will be saved in keras otherwise in the savedmodel format anyway since you have specified suffix hd instead of h it will be saved in savedmodel format you can simply load them in the same way you have saved them
68322542,problem connecting transformer output to cnn input in keras,tensorflow keras convneuralnetwork huggingfacetransformers transformermodel,i think you are right the problem seems to be the input to the convd layer according to the documentation the outputslasthiddenstate has a shape of batchsize sequencelength hiddensize convd is expecting an input of shape batchsize sequencelength maybe you could resolve the problem by either changing your convd to convd or adding a convd layer in between
68239361,cant install tensorflow for huggingface transformers library,python tensorflow pytorch package huggingfacetransformers,from comments you have multiple python interpreters installed that is why installing stuff does not show in your python interpreter use pip v and compare it to the python version that appears in the interpreter remove one and use only one then your issue will be resolved paraphrased from drsnoopy
68185061,strange results with huggingface transformermarianmt translation of larger text,python translation huggingfacetransformers huggingfacetokenizers,to translate long texts with transformers you can split your text by paragraphs paragraphs split by sentence and after that feed sentences to your model in batches in any case it is better to translate with marianmt in a sentencebysentence way because it can lose some parts if you feed a long text as a one piece to it
68072148,huggingface scibert automodelformaskedlm cannot be imported,python pythonx bertlanguagemodel huggingfacetransformers,try this
68058647,initialize huggingface bert with random weights,bertlanguagemodel huggingfacetransformers,you can initialize a random bert model using the hugginface capabilites from the documentation
68006956,attributeerror type object wavvecforctc has no attribute frompretrained,python visualstudiocode jupyternotebook huggingfacetransformers huggingfacetokenizers,issue solved when i uninstalled both the libraries and installed their respective versions needed using following command after this the issue got resolved
67979876,must the vocab size must math the vocabsize in bertconfigjson exactly,bertlanguagemodel huggingfacetransformers transformermodel,if it is really bert that uses wordpiece tokenizer then yes different lengths of vocabulary and vocabsize in the config would mean that there are either embeddings that can never be used or that there are vocabulary items without any embeddings in this case you will see no error message because the model and the tokenizer are loaded separately the embedding table of bert has embeddings that are no reachable note however that the model may use some very nonstandard tokenizer that saves the vocabulary in such a way it is items shorter although it is quite unlikely
67948945,force bert transformer to use cuda,python pytorch huggingfacetransformers transformermodel,you can make the entire class inherit torchnnmodule like so upon initializing your model you can then call todevice to cast it to the device of your choice like so the to recursively applies to all submodules of the class model being one of them hugging face model inherit torchnnmodule thus providing an implementation for to note that this makes choosing device in the init redundant its now an external context that you can switch tofrom easily alternatively you can hardcode the device by casting the contained bert model directly into cuda less elegant
67924216,why would a torchscript trace return different looking encodedinputs compared to the original transformer model,pytorch huggingfacetransformers transformermodel machinetranslation torchscript,found the answer here you cant directly convert a seqseq model encoderdecoder model using this method to convert a seqseq model encoderdecoder you have to split them and convert them separately an encoder to onnx and a decoder to onnx you can follow this guide it was done for t which is also a seqseq model you need to provide a dummy variable to both encoder and to the decoder separately by default when converting using this method it provides the encoder the dummy variable
67872803,huggingface scibert predict masked word not working,python bertlanguagemodel huggingfacetransformers,as the error message tells you you need to use automodelformaskedlm from transformers import pipeline autotokenizer automodelformaskedlm tokenizer autotokenizerfrompretrainedallenaiscibertscivocabuncased model automodelformaskedlmfrompretrainedallenaiscibertscivocabuncased unmasker pipelinefillmask modelmodel tokenizertokenizer unmaskerthe patient is a year old mask admitted with pneumonia output
67811619,this method is deprecated call should be used instead how to solve this problem in bert,python huggingfacetransformers huggingfacetokenizers,ive been working with this recently here is a good look at some documentation for the call function call inputsargskwargs a list or a list of list of dict parameters args str or liststr one or several texts or one list of prompts with masked tokens targets str or liststr optional when passed the model will limit the scores to the passed targets instead of looking up in the whole vocab if the provided targets are not in the model vocab they will be tokenized and the first resulting token will be used with a warning and that might be slower topk int optional when passed overrides the number of predictions to return returns a list or a list of list of dict heres an example of how i used it keep in mind that im using the fillmask mode of bert which is a one word prediction model and the python language the model will predict the word that is tokenized by mask the example output for this is
67699354,are these normal speed of bert pretrained model inference in pytorch,bertlanguagemodel huggingfacetransformers transformermodel huggingfacetokenizers,no you can speed it up first why are you testing it with batch size both tokenizer and model accept batched inputs basically you can pass a d arraylist that contains a single sample at each row see the documentation for tokenizer the same applies for the models also your for loop is sequential even if you use batch size larger than you can create a test data and then use trainer class with trainerpredict also see this discussion of mine at the hf forums
67689219,copy one layers weights from one huggingface bert model to another,python bertlanguagemodel huggingfacetransformers,weights and bias are just tensor and you can simply copy them with copy from transformers import bertforsequenceclassification bertconfig jetfire bertforsequenceclassificationfrompretrainedbertbasecased config bertconfigfrompretrainedbertbasecased optimus bertforsequenceclassificationconfig parts bertembeddingswordembeddingsweight bertembeddingspositionembeddingsweight bertembeddingstokentypeembeddingsweight bertembeddingslayernormweight bertembeddingslayernormbias def joltelectrify jetfire optimus parts target dictoptimusnamedparameters source dictjetfirenamedparameters for part in parts targetpartdatacopysourcepartdata joltelectrifyjetfire optimus parts
67639478,is there a significant speed improvement when using transformers tokenizer over batch compared to per item,pytorch huggingfacetransformers,i ended up just timing both in case its interesting for someone else
67633551,reading a pretrained huggingface transformer directly from s,amazons huggingfacetransformers,answering my own question apparently encouraged i achieved this using a transient file namedtemporaryfile which does the trick i was hoping to find an inmemory solution ie passing in the bytesio directly to frompretrained but that would require a patch to the transformers codebase import boto import json from contextlib import contextmanager from io import bytesio from tempfile import namedtemporaryfile from transformers import pretrainedconfig pretrainedmodel contextmanager def sfileobjbucket key yields a file object from the filename at bucketkey args bucket str name of the s bucket where you model is stored key str relative path from the base of your bucket including the filename and extension of the object to be retrieved s botoclients obj sgetobjectbucketbucket keykey yield bytesioobjbodyread def loadmodelbucket pathtomodel modelnamepytorchmodel load a model at the given s path it is assumed that your model is stored at the key pathtomodelmodelnamebin and that a config has also been generated at the same path named fpathtomodelconfigjson tempfile namedtemporaryfile with sfileobjbucket fpathtomodelmodelnamebin as f tempfilewritefread with sfileobjbucket fpathtomodelconfigjson as f dictdata jsonloadf config pretrainedconfigfromdictdictdata model pretrainedmodelfrompretrainedtempfilename configconfig return model model loadmodelmybucket pathtomodel
67511285,using transformers class bertforquestionanswering for extractive question answering,python bertlanguagemodel huggingfacetransformers,due to version update the model returns a dictionary and not a tuple of start end you can add the following parameter returndictfalse
67367757,how to build a dataset for language modeling with the datasets library as with the old textdataset from the transformers library,python bertlanguagemodel huggingfacetransformers,i received an answer for this question on the huggingface datasets forum by lhoestq hi if you want to tokenize line by line you can use this though the textdataset was doing a different processing by concatenating all the texts and building blocks of size if you need this behavior then you must apply an additional map function after the tokenization this code comes from the processing of the runmlmpy example script of transformers
67299510,understanding how gpt tokenizes the strings,python huggingfacetransformers transformermodel gpt,you can call tokenizerdecode on the output of the tokenizer to get the words from its vocabulary under given indices
67286034,tokenizing a dataframe using tensorflow and transformers,python dataframe tensorflow tokenize huggingfacetransformers,in short yes you also dont want to tokenize the entire but just a numpy array of the text column the steps missing are shown below
67190212,how does the bert model select the label ordering,pytorch bertlanguagemodel huggingfacetransformers logits,the first value corresponds to label and the second value corresponds to label what bertforsequenceclassification does is feeding the output of the pooler to a linear layer after a dropout which i will ignore in this answer lets look at the following example from torch import nn from transformers import bertmodel berttokenizer t berttokenizerfrompretrainedbertbaseuncased m bertmodelfrompretrainedbertbaseuncased i tencodeplusthis is an example returntensorspt o mi printopooleroutputshape output the pooledoutput is a tensor of shape batchsizehiddensize and represents the contextualized ie attention was applied cls token of your input sequences this tensor is feed to a linear layer to calculate the logits of your sequence classificationlayer nnlinear logits classificationlayeropooleroutput when we normalize these logits we can see that the linear layer predicts that our input should belong to label printnnfunctionalsoftmaxlogitsdim output will differ since the linear layer is initialed randomly the linear layer applies a linear transformation yxatb and you can already see that the linear layer is not aware of your labels it only has a weights matrix of size to produce logits of size ie first row corresponds to the first value and second row to the second import torch logitsowncalculation torchmatmulopooleroutput classificationlayerweighttransposeclassificationlayerbias printnnfunctionalsoftmaxlogitsowncalculationdim output the bertforsequenceclassification model learns by applying a crossentropyloss this loss function produces a small loss when the logits for a certain class label in your case deviate only slightly from the expectation that means the crossentropyloss is the one that lets your model learn that the first logit should be high when the input does not contain adverse effect or small when it contains adverse effect you can check this for our example with the following lossfct nncrossentropyloss label torchtensor does not contain adverse effect label torchtensor contains adverse effect printlossfctlogits label printlossfctlogits label output
67158554,finetuning models classifier layer with new label,pytorch huggingfacetransformers,you can just extend the weights and bias of your model with new values please have a look at the commented example below this is the section that loads your model i will just use an pretrained model for this example import torch from torch import nn from transformers import automodelforsequenceclassification autotokenizer tokenizer autotokenizerfrompretrainedjpcorbtoxicdetectordistilroberta model automodelforsequenceclassificationfrompretrainedjpcorbtoxicdetectordistilroberta we check the output of one sample to compare it later with the extended layer to verify that we kept the previous learnt knowledge f tokenizerencodeplusthis is an example returntensorspt printmodelflogits now we need to find out the name of the linear layer you want to extend the layers on top of distilroberta are wrapped inside a classifier section this name can differ for you because it can be chosen randomly use modelparameters instead find the classification layer printmodelclassifier the output shows us that the classification layer is called we can now extend the weights by creating a new tensor that consists of the old weights and a randomly initialized tensor for the new label modelclassifieroutprojweight nnparametertorchcatmodelclassifieroutprojweight torchrandn we do the same for the bias modelclassifieroutprojbias nnparametertorchcatmodelclassifieroutprojbias torchrandn and be happy when we compare the output with our expectation printmodelflogits output please note that you should finetune your model the new weights are randomly initialized and will therefore negatively impact the performance
67157185,what does outputdir mean in transformerstrainingarguments,python bertlanguagemodel huggingfacetransformers raytune,the trainer of the huggingface models can save many things most importantly vocabulary of the tokenizer that is used as a json file model configuration a json file saying how to instantiate the model object ie architecture and hyperparameters model checkpoints trainable parameters of the model saved during training further it can save the values of metrics used during training and the state of the training so the training can be restored from the same place all these are stored in files in the outputdir directory you do not have to create the directory in advance but the path to the directory at least should exist
67153058,cannot load bert from local disk,python tensorflow bertlanguagemodel huggingfacetransformers,as it was already pointed in the comments your frompretrained param should be either id of a model hosted on huggingfaceco or a local path a path to a directory containing model weights saved using savepretrained eg mymodeldirectory see documentation looking at your stacktrace it seems like your code is run inside contentdrivemy drivemscprojectcodemodelpy so unless your model is in contentdrivemy drivemscprojectcodeinputbertbasemultilingualcased it wont load i would also set the path to be similar to documentation example ie bert tfbertmodelfrompretrainedinputbertbasemultilingualcased
67089849,attributeerror gpttokenizerfast object has no attribute maxlen,tokenize huggingfacetransformers transformermodel huggingfacetokenizers gpt,the attributeerror berttokenizerfast object has no attribute maxlen github issue contains the fix the runlanguagemodelingpy script is deprecated in favor of languagemodelingrunclm plm mlmpy if not the fix is to change maxlen to modelmaxlength also pip install transformers might fix the issue since it has been reported to work for some people
66950157,getting predictproba from bert classififer,python pythonx pytorch bertlanguagemodel huggingfacetransformers,in your forward you hence the result of calling your model can be directly supplied to calculate the false positive and true positive rates eg from sklearn import metrics testprobs bertclftokenids masks fpr tpr thresholds metricsroccurvelabels testprobs rocauc metricsaucfpr tpr
66822496,no module named transformersmodels while trying to import berttokenizer,python importerror bertlanguagemodel huggingfacetransformers,you can change your code from transformersmodelingbert import bertmodel bertformaskedlm to
66820943,how to get word embeddings from the pretrained transformers,python tensorflow huggingfacetransformers transferlearning transformermodel,joining subword embeddings into words for word labeling is not how this problem is usually approached the usual approach is the opposite keep the subwords as they are but adjust the labels to respect the tokenization of the pretrained model one of the reasons is that the data is typically in batches when merging subwords into words every sentence in the batch would end up having a different length which would require processing each sentence independently and pad the batch again this would be slow also if you do not average the neighboring embeddings you get more finegrained information from the loss function which tells explicitly what subword is responsible for an error when tokenizing using sentencepiece you can get the indices in the original string from transformers import xlmrobertatokenizerfast tokenizer xlmrobertatokenizerfastfrompretrainedxlmrobertabase tokenizerdeception master returnoffsetsmappingtrue this returns the following dictionary with the offsets you can find out if the subword corresponds to a word that you want to label there are various strategies that could be used for encoding the labels the easiest one is just to copy the label to every subword a more fancy way would be using schemes used in named entity recognition such as iob tagging that explicitly says what is the begging of the labeled segment
66797173,issue while using transformers package inside the docker image,python docker pytorch huggingfacetransformers,i was having a similar issue it seems that starting the app somehow polutes the memory of transformers models probably something to do with how flask does threading but no idea why what fixed it for me was doing the things that are causing trouble loading the models in a different thread first reply here i would be really glad if this helps
66767832,berttokenizerfrompretrained errors out with connection error,python ssl sslcertificate huggingfacetransformers,i could eventually make everything work sharing the same here just in case it will be useful for anyone else in future the solution is quite simple something that i had tried initially but had made a minor mistake while trying anyways here goes the solution access the url huggingfaceco url in my case from browser and access the certificate that accompanies the site a in most browsers chrome firefox edge you would be able to access it by clicking on the lock icon in the address bar save all the certificates all the way up to the root certificate a i think technically you can just save the root certificate and it will still work but i have not tried that i may update this if i get around to try this out if you happen to try it before me please do comment follow the steps mentioned in this stack overflow answer to fetch the ca bundle and open it up in an editor to append the file with the certificates downloaded in the previous step a the original ca bundle file has heading lines before each certificate mentioning which ca root the certificate belongs to this is not needed for the certificates we want to add i had done this and i guess an extra space carriage return etc may have caused it to not work for me earlier in my python program i updated the environment variable to point to the updated ca root bundle osenvironrequestscabundle pathcacertcrt one may think that since most python packages use requests to make such get calls and requests uses the certificates pointed by the certifi package so why not find the location of the certificates pointed by certifi and update that the issue with that it whenever you update a package using conda certifi may get updated as well resulting in your changes to be washed away hence i found dynamically updating the environment variable to be a better option cheers
66656622,python importerror cannot import name version from packaging transformers,python bertlanguagemodel huggingfacetransformers transformermodel,i think this is one of those cases where you have a bad naming maybe your file or something inside your code has a name that is overlapping one of the references you are trying to get for example if you are importing a certain module named kivy and your file is named kivy then the code will go after your file instead of the actual package you are trying to import if thats the case try changing the name and the problem will be solved
66644432,use huggingface transformers without ipywidgets,python jupyternotebook huggingfacetransformers ipywidgets deepnote,you have to disable transformers logging even though it is possible to use transformersloggingsetverbosity to change the log level its not possible to set it to loggingnotset which is required to skip using iprogress and tqdm so we need to hack it like this import transformers import logging transformerslogginggetverbosity lambda loggingnotset transformerslogginggetverbosity after that you should be able to use from transformers import pipeline pipelinesentimentanalysiswe love you check out my deepnote project for details
66581492,how do i prevent a lack of vram halfway through training a huggingface transformers pegasus model,pytorch huggingfacetransformers huggingfacetokenizers,paddingtrue actually doesnt pad to maxlength but to the longest sample in the list you pass to the tokenizer to pad to maxlength you need to set paddingmaxlength
66524542,attributeerror str object has no attribute shape while encoding tensor using bertmodel with pytorch hugging face,python string pytorch attributeerror huggingfacetransformers,the issue is that the return type has changed since xx version of transformers so we have explicitly ask for a tuple of tensors so we can pass an additional kwarg returndict false when we call the bertmodel to get an actual tensor that corresponds to the lasthiddenstate in case you do not like the previous approach then you can resort to
66073395,bert convert spanannotation to answers using scores from hugging face models,python pytorch bertlanguagemodel huggingfacetransformers,so i did a little digging around and it looks like scores can be converted to tokens which can be used to build the answer here is a short example
65913053,how to set proxy in sentencetransformers,python proxy huggingfacetransformers,finally i figured it out how to download it behind the proxy download your favorite models from the link using wget set proxy to python using osenviron or unzip the files to the following location cachetorchsentencetransformers with a prefix sbertnetmodels now your models are good to use with embedder sentencetransformerparaphrasedistilrobertabasev
65881820,huggingface bert sentiment analysis,python bertlanguagemodel huggingfacetransformers huggingfacetokenizers,the pipeline already includes the encoder instead of do
65872566,using tensorflow and tfbertfornextsentenceprediction to further train bert on a specific corpus,python tensorflow keras huggingfacetransformers,the issue resides in your getkerasmodel function you defined here that you are only interested in the first of the element of the output ie logits with x transformermodelinputids inputids attentionmask inputmasksids tokentypeids tokentypeids just do the index selection as conditional like this to get the whole output of the model def getkerasmodeltransformermodel istrainingtrue your other code x transformermodelinputids inputids attentionmask inputmasksids tokentypeids tokentypeids if istraining x x your other code return model predict your other code model getkerasmodeltransformermodel istrainingfalse your other code printpredtestkeys output odictkeyslogits hiddenstates attentions ps the berttokenizer can truncate and add padding by themself documentation
65854722,huggingface albert tokenizer nonetype error with colab,googlecolaboratory huggingfacetransformers huggingfacetokenizers,i found the answer after install import the alberttokenizer and tokenizer i received an error asking me to install sentencepiece package however after i install this package and run tokenizer again i started receiving the error above so i open a brand new colab session and install everything including the sentencepiece before creating tokenizer and this time it worked the nonetype error simply means it doesnt know what is albertbasev however if you install the packages in right order colab will recognize better the relationship between alberttokenizer and sentencepiece in short for this to work in colab open a new colab session install transformers and sentencepiece import alberttokenizer create tokenizer
65683013,indexerror index out of range in self while try to fine tune roberta model after adding special tokens,bertlanguagemodel huggingfacetransformers robertalanguagemodel,you also need to tell your model that it needs to learn the vector representations of two new tokens from transformers import robertatokenizer robertaforquestionanswering t robertatokenizerfrompretrainedrobertabase m robertaforquestionansweringfrompretrainedrobertabase robertabase knows tokens printmrobertaembeddingswordembeddings specialtokensdict additionalspecialtokens toktok taddspecialtokensspecialtokensdict we now tell the model that it needs to learn new tokens mresizetokenembeddingslent mrobertaembeddingswordembeddingspaddingidx printmrobertaembeddingswordembeddings output
65676389,huggingface tfbertforsequenceclassification always predicts the same label,python tensorflow bertlanguagemodel huggingfacetransformers,you trained for a couple of minutes it is not enough even for pretrained bert try to decrease learning rate to get your accuracy increasing after every epoch for the first epochs and train for more epochs until you see the validation accuracy decreasing for epochs
65627663,bert extracting cls embedding from multiple outputs vs single,python pandas tensorflow keras huggingfacetransformers,i think there was an issue with the shape of the sliced datax that you passed in since you did not specify the shape of datax i first attempted to replicate it below where the shape of datax is datax is not the correct way to slice for your first row of data you prepared the inputids and attentionmask by slicing it using datax and datax the shape of your inputids and attentionmask becomes while the tf model expects the input shape of batchsize for both inputidslayer and inputmasklayer note that the shape argument supplied to input does not include the batchsize as quoted from its documentation here shape a shape tuple integers not including the batch size in fact when i attempt to pass the input datax datax both with shape i received the following warning from tensorflow the correct way to slice the input data you should be slicing them without changing the tensor dimension so that your inputids and attentionmask remains in the form of batchsize and after passing the above data to your model you will get the clf embeddings using output as described in the link you shared you can confirm that the embeddings for the st and nd sentences are the same whether you pass in input input or input hope this clears thing up for you always remember to check the input and output shape of the model when you are in doubt
65541788,how to reduce the inference time of helsinkinlpopusmtesen translation model from transformer,pytorch huggingfacetransformers machinetranslation,the opus models are originally trained with marian which is a highly optimized toolkit for machine translation written fully in c unlike pytorch it does have the ambition to be a general deep learning toolkit so it can focus on mt efficiency the marian configurations and instructions on how to download the models are at the opusmt models for huggingfaces transformers are converted from the original marian models are meant more for prototyping and analyzing the models rather than for using them for translation in a productionlike setup running the models in marian will certainly much faster than in python and it is certainly much easier than hacking transformers to run with onxx runtime marian also offers further tricks to speed up the translation eg by model quantization which is however at the expense of the translation quality with both marian and tranformers you can speed things up if you use gpu or if you narrow the beam width during decoding attribute numbeams in the generate method in transformers
65499783,apply transformer model to each row in a pandas column,pythonx pandas loops huggingfacetransformers huggingfacetokenizers,all you need to do is to wrap the steps into a function and use the apply function of your dataframe import pandas as pd from transformers import fsmtforconditionalgeneration fsmttokenizer mname allenaiwmtdeenbig tokenizer fsmttokenizerfrompretrainedmname model fsmtforconditionalgenerationfrompretrainedmname df pddataframewie geht es dir heute mir geht es gut columnsgermantext def translationpipelinetext inputids tokenizerencodetext returntensorspt outputs modelgenerateinputids decoded tokenizerdecodeoutputs skipspecialtokenstrue return decoded dfenglishtextdfgermantextapplytranslationpipeline printdf output
65440010,unable to find the word that i added to the huggingface bert tokenizer vocabulary,bertlanguagemodel huggingfacetransformers nltokenizer,you are calling two different things with tokenizervocab and tokenizergetvocab the first one contains the base vocabulary without the added tokens while the other one contains the base vocabulary with the added tokens from transformers import berttokenizer t berttokenizerfrompretrainedbertbaseuncased printlentvocab printlentgetvocab printtgetaddedvocab taddtokenscovid printlentvocab printlentgetvocab printtgetaddedvocab output
65419656,parsing the hugging face transformer output,huggingfacetransformers huggingfacetokenizers,what you see there is the proprietary inference api from huggingface this api is not part of the transformers library but you can build something similar all you need is the tokenclassificationpipeline from transformers import autotokenizer automodelfortokenclassification tokenclassificationpipeline tokenizer autotokenizerfrompretrainedvblagojebertenglishuncasedfinetunedpos model automodelfortokenclassificationfrompretrainedvblagojebertenglishuncasedfinetunedpos p tokenclassificationpipelinemodelmodel tokenizertokenizer pmy name is clara and i live in berkeley california output you can find the other available pipelines which might be used by the inference api here
65396968,how do i interpret my bert output from huggingface transformers for sequence classification and tensorflow,python tensorflow bertlanguagemodel huggingfacetransformers,your output means that probability of the first class is you can feed your labels either as integers or as onehot vectors you have to use an appropriate loss function categoricalcrossentropy with onehot or sparsecategoricalcrossentropy with integers
65396650,bert dataloader difference between shuffletrue vs sampler,python pytorch bertlanguagemodel huggingfacetransformers,you are not calculating the loss correctly firstly i wonder why you use the mean of the logits as loss but that might be sth task specific that i am not familiar with but you are definitely not accumulating the losses correctly or not at all to be precise the loss you print out is just from the last batch this explains why the results differ when using shuffle this should definitely not be the case when implemented correctly the fact that you get a negative loss is because you just use the mean of the logits as loss which of course can be negative still other metrics like accuracy should not be affected by this but you didnt provide the code that calculates those metrics so theres no way of spotting the point of failure
65383059,unable to import hugging face transformers,python huggingfacetransformers,as it appears to be a cache file simply move the huggingface dir to huggingfacebak then you should be able to rerun the script
65285054,how to add a multiclass multilabel layer on top of pretrained bert model,deeplearning pytorch bertlanguagemodel huggingfacetransformers transferlearning,you should use bertmodel and not bertmodelforsequenceclassification as bertmodelforsequenceclassification adds a linear layer for classification on top of bert model and uses crossentropyloss which is meant for multiclass classification hence first use bertmodel instead of bertmodelforsequenceclassification next multilabel classification uses sigmoid activation instead of softmax here the sigmoid layer is added in the above code further for multilabel classification you need to use bceloss instead of crossentropyloss
65242786,metrics mismatch between bertforsequenceclassification class and my custom bert classification,pytorch huggingfacetransformers,each model tells you via a warning message which layers are randomly initialized when you use the method frompretrained from transformers import bertforsequenceclassification b bertforsequenceclassificationfrompretrainedbertbaseuncased output the difference between your implementation and the bertforsequenceclassification is that you do not use any pretrained weights at all the method fromconfig does not load the pretrained weights from a statedict import torch from transformers import automodelforsequenceclassification autoconfig b automodelforsequenceclassificationfromconfigautoconfigfrompretrainedbertbaseuncased b automodelforsequenceclassificationfrompretrainedbertbaseuncased printdoes fromconfig provides pretrained weights formattorchequalbbertembeddingswordembeddingsweight bbasemodelembeddingswordembeddingsweight printdoes frompretrained provides pretrained weights formattorchequalbbertembeddingswordembeddingsweight bbasemodelembeddingswordembeddingsweight output therefore you probably want to change your class to class mycustombertclassificationnnmodule def initself encoderbertbaseuncased numlabels hiddendropoutprob supermycustombertclassification selfinit selfconfig autoconfigfrompretrainedencoder selfencoder automodelfrompretrainedencoder selfdropout nndropouthiddendropoutprob selfclassifier nnlinearselfconfighiddensize numlabels def forwardself inputsent outputs selfencoderinputidsinputsentinputids attentionmaskinputsentattentionmask tokentypeidsinputsenttokentypeids returndicttrue pooledoutput selfdropoutoutputs for both tasks logits selfclassifierpooledoutput return logits myb mycustombertclassification printtorchequalbbertembeddingswordembeddingsweight mybencoderembeddingswordembeddingsweight output
65132144,bertmodel transformers outputs string instead of tensor,bertlanguagemodel huggingfacetransformers huggingfacetokenizers,while the answer from aakash provides a solution to the problem it does not explain the issue since one of the x releases of the transformers library the models do not return tuples anymore but specific output objects output you can return to the previous behavior by adding returndictfalse to get a tuple o bertmodel encodingsampleinputids encodingsampleattentionmask returndictfalse printtypeo output i do not recommend that because it is now unambiguous to select a specific part of the output without turning to the documentation as shown in the example below o bertmodelencodingsampleinputids encodingsampleattentionmask returndictfalse outputattentionstrue outputhiddenstatestrue printi am a tuple with elements you do not know what each element presents without checking the documentationformatleno o bertmodelencodingsampleinputids encodingsampleattentionmask outputattentionstrue outputhiddenstatestrue printi am a cool object and you can acces my elements with olasthiddenstate olasthiddenstate or even o my keys are formatokeys output
65083581,how to compute meanmax of huggingface transformers bert token embeddings with attention mask,machinelearning pytorch bertlanguagemodel huggingfacetransformers,for max you can multiply with attentionmask for mean you can sum along the axis and divide by attentionmask along that axis
65072694,make sure bert model does not load pretrained weights,pytorch bertlanguagemodel huggingfacetransformers,use autoconfig instead of automodel this should set up the model without loading the weights documentation here and here
64702885,how to use pipeline from transformers summarization python,python pipeline huggingfacetransformers transformermodel,you gave as an argument maxlength meaning that the maximum length of the generated text should be no longer than tokens by increasing this number the generated summaries will get longer
64675655,bert always predicts same class finetuning,python machinelearning pytorch huggingfacetransformers simpletransformers,i want to leave an answer here for people that are struggling with a similar issue try different learning rates the learning rate is most likely too high try to lower it worked for me reduce the number of epochs while training this problem is related to finetuning if the dataset is extensive epochs may be enough try out different batch sizes or introduce dropout layers for a lengthy discussion see
64631665,what is the difference in robertatokenizer and frompretrained way of initialising robertatokenizer,pytorch huggingfacetransformers huggingfacetokenizers,when you compare the property uniquenosplittokens you will see that this is initialized for the frompretrained tokenizer but not for the other frompretrained tuniquenosplittokens init tuniquenosplittokens this property is filled by addtokens that is called by frompretrained but not by init im actually not sure if this is a bug or a feature frompretrained is the recommended method to initialize a tokenizer from a pretrained tokenizer and should therefore be used
64610841,bertbased ner model giving inconsistent prediction when deserialized,python pytorch bertlanguagemodel huggingfacetransformers,i fixed it there were two problems the indexlabel mapping for tokens was wrong for some reason the list function worked differently on colab gpu than my cpu the snippet used to save the model was not correct for models based on the huggingfacetransformers library you cant use modelsavedict and load it later you need to use the savepretrained method of your model class and load it later using frompretrained
64564545,bert tokenize urls,python machinelearning bertlanguagemodel huggingfacetransformers huggingfacetokenizers,well it depends if the url contains information that is relevant for the classification then the best thing you can do is keeping it as it is there certainly were some urls in the pretraining data and bert learned how to handle them properly if you are sure the urls are irrelevant for the classificaion you can replace them by a special token which is a very common thing to do in nlp in general but in that case you need to finetune bert so it knows what the special token mean if you do not finetune bert and only train a classifier on top of it then again the best thing you can do is keeping the urls as they are
64383443,whats difference robertamodel robertasequenceclassification hugging face,huggingfacetransformers,i think its easiest to understand if we have a look at the actual implementation where i randomly chose robertamodel and robertaforsequenceclassification as an example however the conclusion is valid for all other models too you can find the implementation for robertaforsequenceclassification here which looks roughly like this class robertaforsequenceclassificationrobertapretrainedmodel authorizedmissingkeys rpositionids def initself config superinitconfig selfnumlabels confignumlabels selfroberta robertamodelconfig addpoolinglayerfalse selfclassifier robertaclassificationheadconfig selfinitweights def forward as we can see there is no indication about the pretraining here and it simply adds another linear layer on top the implementation of the robertaclassificationhead can be found a bit further down namely here class robertaclassificationheadnnmodule head for sentencelevel classification tasks def initself config superinit selfdense nnlinearconfighiddensize confighiddensize selfdropout nndropoutconfighiddendropoutprob selfoutproj nnlinearconfighiddensize confignumlabels def forwardself features kwargs x features take token equiv to cls x selfdropoutx x selfdensex x torchtanhx x selfdropoutx x selfoutprojx return x so to answer your question these models come without any pretrained additional layers on top and you could easily implement them yourself now for the asterisk while it could be easy to wrap this yourself also note that it is an inherited class robertapretrainedmodel this has several advantages the most important one being a consistent design between different implementations sequence classification model sequence tagging model etc further there are some neat functionalities that they are providing like the forward call including extensive parameters padding masking attention output which would cost quite some time to implement last but not least there are existing trained models based on these specific implementations which you can search for on the huggingface model hub there you might find models that are finetuned on a sequence classification task eg this one and then directly load its weights in a robertaforsequenceclassification model if you had your own implementation of a sequence classification model loading and aligning these pretrained weights would be incredibly more complicated i hope this answers your main concern but feel free to elaborate either as comment or new question on any points that have not been addressed
64156202,add dense layer on top of huggingface bert model,python pythonx neuralnetwork pytorch huggingfacetransformers,there are two ways to do it since you are looking to finetune the model for a downstream task similar to classification you can directly use bertforsequenceclassification class performs finetuning of logistic regression layer on the output dimension of alternatively you can define a custom module that created a bert model based on the pretrained weights and adds layers on top of it
64044200,pytorch bert misshaped inputs,python pytorch huggingfacetransformers,i just discovered that pretrained bert models from huggingface have a maximum input length of
63939072,loading saved ner transformers model causes attributeerror,torch huggingfacetransformers,try to save your model with modelsavepretrainedoutputdir then you can load your model with model frompretrainedoutputdir where is the model class eg bertfortokenclassification
63904821,using transformer for textsummarization,tensorflow pytorch huggingfacetransformers transformermodel summarization,why not transfer learning train them on your specific texts and summaries i trained t on specific limited text over epoch and got very good results i adopted the code from here to my needs let me know if you have a specific training questions
63876450,reduce the output layer size from xltransformers,huggingfacetransformers,thats because you used lmheadmodel which predicts the next token you can use transfoxlmodelfrompretrainedtransfoxlwt instead then output is the last hidden state which has the shape batchsize sequencelength hiddensize
63774619,enhance a marianmt pretrained model from huggingface with more training data,python huggingfacetransformers,have you tried the finetunesh script shown here in addition to the short list of cli flags listed there you could try adding where the pathtopretrained could be either a local path on your machine or marianmt model opusende or equivalent the datadir has a trainsource and traintarget for the source target languages such that line number x of the target is a translation of line x in the source and same with valsource and valtarget i have changed the finetunepy script here to and then ran the finetunesh script note the gradients blew up when i used the fp flag with pytorch so i had removed it also you might want to check on the valcheckinterval checkvaleverynepoch and probably check this issue on how to save multiple checkpoints
63626014,how to get rid of gpt warning message,python huggingfacetransformers gpt,yes you need to change the loglevel before you import anything from the transformers library import logging loggingbasicconfiglevelerror from transformers import gptlmheadmodel model gptlmheadmodelfrompretrainedgpt
63478947,correct way to finetunetrain huggingfaces model from scratch pytorch,python pytorch bertlanguagemodel huggingfacetransformers,in this way you would unnecessarily download and load the pretrained model weights you can avoid that by downloading the bert config config transformersautoconfigfrompretrainedbertbasecased model transformersautomodelfromconfigconfig both yours and this solution assume you want to tokenize the input in the same as the original bert and use the same vocabulary if you want to use a different vocabulary you can change in the config before instantiating the model configvocabsize similarly you can change any hyperparameter that you want to have different from the original bert
63461821,what features are used in the default transformers pipeline,huggingfacetransformers,unfortunately as you have rightly stated the pipelines documentation is rather sparse however the source code specifies which models are used by default see here specifically the model is distilbertbasecased for a way to use models see a related answer by me here you can simply specifiy the model and tokenizer parameters like this from transformers import pipeline question answering pipeline specifying the checkpoint identifier pipelinefeatureextraction modelbertbasecased tokenizerbertbasecased
63461262,bert sentence embeddings from transformers,bertlanguagemodel huggingfacetransformers,while the existing answer of jindrich is generally correct it does not address the question entirely the op asked which layer he should use to calculate the cosine similarity between sentence embeddings and the short answer to this question is none a metric like cosine similarity requires that the dimensions of the vector contribute equally and meaningfully but this is not the case for bert weights released by the original authors jacob devlin one of the authors of the bert paper wrote im not sure what these vectors are since bert does not generate meaningful sentence vectors it seems that this is doing average pooling over the word tokens to get a sentence vector but we never suggested that this will generate meaningful sentence representations and even if they are decent representations when fed into a dnn trained for a downstream task it doesnt mean that they will be meaningful in terms of cosine distance since cosine distance is a linear space where all dimensions are weighted equally however that does not mean you can not use bert for such a task it just means that you can not use the pretrained weights outofthebox you can either train a classifier on top of bert which learns which sentences are similar using the cls token or you can use sentencetransformers which can be used in an unsupervised scenario because they were trained to produce meaningful sentence representations
63413414,is there a way to get the location of the substring from which a certain token has been produced in bert,tokenize bertlanguagemodel huggingfacetransformers huggingfacetokenizers,id like to make an update to the answer since huggingface introduced their much faster version of rustwritten fast tokenizers this task becomes much easier more than that if instead of the regular string you feed the tokenizer with the list of words and set issplitintowordstrue then one can easily differentiate between first and the consequence tokens of each word first value of the tuple would be zero which is very common need for token classification tasks
63358768,why is there no pooler layer in huggingfaces flaubert model,bertlanguagemodel huggingfacetransformers,pooler is necessary for the next sentence classification task this task has been removed from flaubert training making pooler an optional layer huggingface commented that poolers output is usually not a good summary of the semantic content of the input youre often better with averaging or pooling the sequence of hiddenstates for the whole input sequence thus i belive they decided to remove the layer
63312859,how to change huggingface transformers default cache directory,huggingfacetransformers,you can specify the cache directory whenever you load a model with frompretrained by setting the parameter cachedir you can also set a default location by exporting an environment variable hfhome each time before you use the library ie before importing it python example import os osenvironhfhome blablacache bash example export hfhomeblablacache windows example google colab example export via os works fine but not the bash variant an alternative are the magic commands env hfhomeblablacache transformers use the variable transformerscache instead of hfhome you can also use it in v futurewarning using transformerscache is deprecated and will be removed in v of transformers use hfhome instead
63280435,huggingface transformers truncation strategy in encodeplus,pytorch huggingfacetransformers,no longestfirst is not the same as cut from the right when you set the truncation strategy to longestfirst the tokenizer will compare the length of both text and textpair everytime a token needs to be removed and remove a token from the longest the could for example mean that it will cut at first tokens from textpair and will cut the rest of the tokens which need be cut alternately from text and textpair an example from transformers import berttokenizerfast tokenizer berttokenizerfastfrompretrainedbertbaseuncased seq this is a long uninteresting text seq what could be a second sequence to the uninteresting text printlentokenizertokenizeseq printlentokenizertokenizeseq printtokenizerseq seq printtokenizerseq seq truncation true maxlength printtokenizerdecodetokenizerseq seq truncation true maxlength inputids output as far as i can tell from your question you are actually looking for onlysecond because it cuts from the right which is textpair printtokenizerseq seq truncation onlysecond maxlength output it throw an exception when you try your text input is longer as the specified maxlength that is correct in my opinion because in this case it is not any longer a sequnece pair input just in case onlysecond doesnt meet your requirements you can simply create your own truncation strategy as an example onlysecond by hand output
63232732,how to use the past with huggingface transformers gpt,python pytorch huggingfacetransformers,i believe the problem is that context contains integer values exceeding vocabulary size my assumption is based on the last traceback line
63221913,named entity recognition with huggingface transformers mapping back to complete entities,huggingfacetransformers,the pipeline object can do that for you when you set the parameter transformers groupedentities to true transformers aggregationstrategy to simple from transformers import pipeline transformers ner pipelinener groupedentitiestrue ner pipelinener aggregationstrategysimple sequence hugging face inc is a company based in new york city its headquarters are in dumbo therefore very close to the manhattan bridge which is visible from the window output nersequence printoutput output
63141267,importerror cannot import name automodelwithlmhead from transformers,python pytorch huggingfacetransformers,i solved it apparently automodelwithlmhead is removed on my version now you need to use automodelforcausallm for causal language models automodelformaskedlm for masked language models and automodelforseqseqlm for encoderdecoder models so in my case code looks like this
63126386,where can i get the pretrained word embeddinngs for bert,embedding huggingfacetransformers bertlanguagemodel,the bertmodels have getinputembeddings import torch from transformers import bertmodel berttokenizer tokenizer berttokenizerfrompretrainedbertbaseuncased bert bertmodelfrompretrainedbertbaseuncased tokenembedding token bertgetinputembeddingstorchtensorid for token id in tokenizergetvocabitems printlentokenembedding printtokenembeddingcls output
63020991,cannot import name pipline from transformers unknown location,python jupyterlab huggingfacetransformers,in general when you face such an issue that an import is working in one environment script codetestpy but not in the other jupyterlab you need to compare the search path for modules with syspath and the location of the module with modulefile transformersfile in this case when you compare the outputs of syspath of both environments you will notice that usersmyusernamepathtomyprojectcodeenvlibpythonsitepackages is only listed in one and this is exactly the location where the transformers module is loaded from output of transformersfile that means jupyterlab is not using your virtual environment all you need to do is to register your environment for jupyterlab with and jupyterlab will now allow you to select the environment as kernels
62961194,how does bertforsequenceclassification classify on the cls vector,python transformermodel huggingfacetransformers bertlanguagemodel,is the cls token a regular token which has its own embedding vector that learns the sentence level representation yes from transformers import berttokenizer bertmodel tokenizer berttokenizerfrompretrainedbertbaseuncased model bertmodelfrompretrainedbertbaseuncased clstoken tokenizerconverttokenstoidscls printclstoken or printtokenizerclstoken tokenizerclstokenid printmodelgetinputembeddingstorchtensorclstoken output you can get a list of all other special tokens for your model with printtokenizerallspecialtokens output what i dont understand is how do they encode the information from the entire sentence into this token and because we use the cls tokens hidden state to predict is the cls tokens embedding being trained on the task of classification as this is the token being used to classify thus being the major contributor to the error which gets propagated to its weights also yes as you have already stated in your question bertforsequenceclassification utilizes the bertpooler to train the linear layer on top of bert outputs contains the output of bertmodel and the second element is the pooler output pooledoutput outputs pooledoutput selfdropoutpooledoutput logits selfclassifierpooledoutput loss calculation based on logits and the given labels why cant we just use the average of the hidden states the output of the encoder and use this to classify i cant really answer this in general but why do you think this would be easier or better as a linear layer you also need to train the hidden layers to produce an output where the average maps to your class therefore you also need an average layer to be the major contributor to your loss in general when you can show that it leads to better results instead of the current approach nobody will reject it
62863859,running transformers on docker,huggingfacetransformers,i believe the issue is that docker doesnt store a cache it isnt persistent between runs and hugging face doesnt automatically download the model files without that cache you canshould mount a storage folder like hfcache or something similar in your project directory then set your environment variable in the dockerfile like so then mount the cache directory like this or if youre using pycharm just use the docker run configuration this tells docker to use your local cache file and lets you persist information between docker runs and huggingface can accesscachereuse the downloaded models between runs ive run this locally on docker and it works for me its also good practice to include a requirementstxt file and direct docker to download the contents after resolving this issue i then had to download a couple of dependencies
62852940,how to get immediate next word probability using gpt model,transformermodel huggingfacetransformers,you can have a look at how the generation script works with the probabilities gptlmheadmodel as well as other mlheadmodels returns a tensor that contains for each input the unnormalized probability of what the next token might be ie the last output of the model is the normalized probability of the next token assuming inputids is a tensor with token indices from the tokenizer outputs modelinputids nexttokenlogits outputs you get the distribution by normalizing the logits using softmax the indices in the first dimension of the nexttokenlogits correspond to indices in the vocabulary that you get from the tokenizer object selecting the last logits becomes tricky when you use a batch size bigger than and sequences of different lengths in that case you would need to specify attentionmask in the model call to mask out padding tokens and then select the last logits using torchindexselect it is much easier either to use batch size or batch of equally long sequences you can use any autoregressive model in transformers there is distilgpt a distilled version of gpt ctrl which is basically gpt trained with some additional commands the original gpt under the name openaigpt xlnet designed for contextual embeddings but can be used for generation in arbitrary order there are probably more you can hugging face model hub
62746180,importerror cannot import name hfbucketurl in huggingface transformers,tensorflow pytorch huggingfacetransformers,it turns out to be a bug this pr solves the issue by importing the function hfbucketurl properly
62671668,how to freeze tfbertforsequenceclassification pre trained model,tensorflow huggingfacetransformers,found a way to do it freeze the base model before compiling it
62538079,hugginface transformers module not recognized by anaconda,python pythonx anaconda pytorch huggingfacetransformers,the problem is that conda only offers the transformers library in version repository information and this version didnt have a padtomaxlength argument im dont want to look it up if there was a different parameter but you can simply pad the result which is just a list of integers from transformers import berttokenizer berttokenizer berttokenizerfrompretrainedbertbaseuncased dolowercasetrue sentences this is just a test this is another test maxlength for sent in sentences encodedsent berttokenizerencodesent addspecialtokenstrue maxlength maxlength encodedsentextend maxlength lenencodedsent your other stuff the better option in my opinion is to create a new conda environment and install everything via pip and not via conda this will allow you to work with the most recent transformers version
62472438,with the huggingface transformer how can i return multiple samples when generating text,python pytorch huggingfacetransformers,as far as i can see this code doesnt provide multiple samples but you can adjust it with a some adjustments this line uses already multinomial but returns only nexttoken torchmultinomialfsoftmaxfilteredlogits dim numsamples change it to nexttoken torchmultinomialfsoftmaxfilteredlogits dim numsamplesnumsamples now you also need to change the result construction this concatenates line the nexttoken with the sentence you get now numsamples of nexttokens and you need unsqueeze all of them change it to generated torchcatgenerated nexttokenunsqueeze dim the whole function should look like this now def samplesequence model length context numsamples temperature topk topp repetitionpenalty devicecpu context torchtensorcontext dtypetorchlong devicedevice context contextunsqueezerepeatnumsamples generated context with torchnograd for in trangelength inputs inputids generated outputs model inputs note we could also use past with gpttransfoxlxlnetctrl cached hiddenstates nexttokenlogits outputs temperature if temperature else reptition penalty from ctrl for in setgeneratedviewtolist nexttokenlogits repetitionpenalty filteredlogits topktoppfilteringnexttokenlogits topktopk topptopp if temperature greedy sampling nexttoken torchargmaxfilteredlogitsunsqueeze else nexttoken torchmultinomialfsoftmaxfilteredlogits dim numsamplesnumsamples generated torchcatgenerated nexttokenunsqueeze dim return generated last but not least you have to change your tokenizerdecode call to tokenizerbatchdecode as the return value contains now multiple samples tokenizerbatchdecodeoutputtolist cleanuptokenizationspacestrue skipspecialtokenstrue something you have to think of byt yourself is what you want to do when there is no valide nexttoken currently you will receive an error message like runtimeerror invalid multinomial distribution with replacementfalse not enough nonnegative category to sample another thing you have to think of is if their code is even correct during the few test i have conducted it felt like that the quality of created sentences decreased with an increasing number of numsamples ie maybe the quality is better when you use a simple loop to call samplesequence multiple times i havent worked with gpt yet and cant help you here
62452271,understanding bert vocab unusedxxx tokens,huggingfacetransformers,a quick search reveals the use of this specifically in the discussion of the original bert implementation and this huggingface thread unused tokens are helpful if you want to introduce specific words to your finetuning or further pretraining procedure they allow you to treat words that are relevant only in your context just like you want and avoid subword splitting that would occur with the original vocabulary of bert to quote from the first discussion just replace the unusedx tokens with your vocabulary since these were not used they are effectively randomly initialized
62435022,where in the code of pytorch or huggingfacetransformer label gets renamed into labels,python pytorch huggingfacetransformers,the rename happens in the collator in the trainer init when datacollator is none a default one is used class trainer def init selfdatacollator datacollator if datacollator is not none else defaultdatacollator fyi the selfdatacollator is later used when you get the dataloader dataloader dataloader selftraindataset batchsizeselfargstrainbatchsize samplertrainsampler collatefnselfdatacollator here droplastselfargsdataloaderdroplast the default collator has a special handling for labels which does this renaming if needed special handling for labels ensure that tensor is created with the correct type it should be automatically the case but lets make sure of it if hasattrfirst label and firstlabel is not none if typefirstlabel is int labels torchtensorflabel for f in features dtypetorchlong else labels torchtensorflabel for f in features dtypetorchfloat batch labels labels here is where it happens elif hasattrfirst labelids and firstlabelids is not none if typefirstlabelids is int labels torchtensorflabelids for f in features dtypetorchlong else labels torchtensorflabelids for f in features dtypetorchfloat batch labels labels else batch
62422590,do i need to pretokenize the text first before using huggingfaces robertatokenizer different undersanding,huggingfacetransformers huggingfacetokenizers,hugingfaces transformers are designed such that you are not supposed to do any pretokenization roberta uses sentecepiece which has lossless pretokenization ie when you have a tokenized text you should always be able to say how the text looked like before tokenization the which is a weird unicode underscore in the original sentecepiece says that there should be a space when you detokenize as a consequence big and big end up as different tokens of course in this particular context it does not make much sense because it is obviously still the same word but this the price you pay for lossless tokenization and also how roberta was trained bert uses wordpiece which does not suffer from this problem on the other hand the mapping between the original string and the tokenized text is not as straightforward which might be inconvenient eg when you want to highlight something in a usergenerated text
62327803,having labels instead of in hugging face bertforsequenceclassification,python transformermodel huggingfacetransformers bertlanguagemodel,you can set the output shape of the classification layer with frompretrained via the numlabels parameter from transformers import bertforsequenceclassification model bertforsequenceclassificationfrompretrainedbertbaseuncased numlabels printmodelclassifierparameters output
62317931,how to predownload a transformers model,machinelearning flask amazonelasticbeanstalk transformermodel huggingfacetransformers,approach search for the model here download the model from this link pytorchmodel tensorflowmodel the config file source you can manually download the model in your case tensorflow model h and the configjson file put it in a folder lets say model in the repository you can try compressing the model and then decompressing once its in the ec instance if needed then you can directly load the model in your web server from the path instead of downloading model folder which contains the h and configjson approach instead of using links to download you can download the model in your local machine using the conventional method this downloads the model now you can save the weights in a folder using savepretrained function modelsavepretrainedcontent saving inside content folder now the content folder should contain a h file and a configjson just upload them to the repository and load from that
62255856,how to perform multi output regression using roberta,pytorch regression huggingfacetransformers bertlanguagemodel,bertforsequenceclassification is a small wrapper that wraps the bertmodel it calls the models takes the pooled output the second member of the output tuple and applies a classifier over it the code is here the simplest solution is writing your own simple wrapper class based on the bertforsequenceclassification class hat will do the regression that will do the regression with the loss you like
62235153,huggingface transformers bert model without classification layer,pytorch huggingfacetransformers bertlanguagemodel,output checkout the bertmodel definition here
62040309,why we need the initweight function in bert pretrained model in huggingface transformers,python huggingfacetransformers bertlanguagemodel,have a look at the code for frompretrained what actually happens is something like this find the correct base model class to initialise initialise that class with pseudorandom initialisation by using the initweights function that you mention find the file with the pretrained weights overwrite the weights of the model that we just created with the pretrained weights where applicable this ensure that the layers that were not pretrained eg in some cases the final classification layer do get initialised in initweights but dont get overridden
61969783,huggingface bert showing poor accuracy f score pytorch,pytorch huggingfacetransformers bertlanguagemodel,after some digging i found out the main culprit was the learning rate for finetuning bert is extremely high when i reduced my learning rate from to e both my training and test accuracy reached when bert is finetuned all layers are trained this is quite different from finetuning in a lot of other ml models but it matches what was described in the paper and works quite well as long as you only finetune for a few epochs its very easy to overfit if you finetune the whole model for a long time on a small amount of data src best result is found when all the layers are trained with a really small learning rate src
61916760,using huggingface transformers with a non english language,pythonx multilingual huggingfacetransformers,the problem is that pipelines by default load an english model in the case of sentiment analysis this is distilbertbaseuncasedfinetunedsstenglish see here fortunately you can just specify the exact model that you want to load as described in the docs for pipeline from transformers import pipeline pipe pipelinesentimentanalysis model tokenizer keep in mind that these need to be models compatible with the architecture of your respective task the only greek model i could find was nlpauebbertbasegreekuncasedv which seems like a base model to me in that case youd first need to finetune your own model for sentiment analysis and then could load from that checkpoint otherwise you might get questionable results as well
61913010,can not import pipeline from transformers,python pythonx pipeline huggingfacetransformers anaconda,check transformers version make sure you are on latest pipelines were introduced quite recently you may have older version
61832308,transformerscli error the following arguments are required modeltype,huggingfacetransformers,found the error it needs to be modeltype not modeltype
61825698,how to specify number of target classes for tfrobertasequenceclassification,python machinelearning deeplearning huggingfacetransformers huggingface,you can use numlabels parameter ref
61806929,huge memory usage when running huggingface transformers runlanguagemodelingpy with gpt,machinelearning torch huggingfacetransformers,their new runclmpy script does not have the problem and the old script seems now to be unsupported anyway
61798573,where does hugging faces transformers save models,huggingfacetransformers,update the cache location has changed again and is now cachehuggingfacehub as reported by victor yan notably the sub folders in the hub directory are also named similar to the cloned model path instead of having a sha hash as in previous versions update the cache location has now changed and is located in cachehuggingfacetransformers as it is also detailed in the answer by victorx this post should shed some light on it plus some investigation of my own since it is already a bit older as mentioned the default location in a linux system is cachetorchtransformers im using transformers v currently but it is unlikely to change anytime soon the cryptic folder names in this directory seemingly correspond to the amazon s hashes also note that the pipeline tasks are just a rerouting to other models to know which one you are currently loading see here for your specific model pipelinefillmask actually utilizes a distillrobertabase model
61776977,smote with multiple bert inputs,python keras scikitlearn huggingfacetransformers smote,i just want to clarify this that this is a wrong way to apply smote to input ids you need to take the corresponding embedding to the cls use bert to get cls token for each tweet then applies smote to it then pass it from classifier any classifier this should be done without finetuning
61774933,understanding the hugging face transformers,pretrainedmodel huggingfacetransformers bertlanguagemodel nlpquestionanswering squad,i would formulate it like this the second link basically describes communityaccepted models ie models that serve as the basis for the implemented huggingface classes like bert roberta etc and some related models that have a high aceptance or have been peerreviewed this list has bin around much longer whereas the list in the first link only recently got introduced directly on the huggingface website where the community can basically upload arbitrary checkpoints that are simply considered compatible with the library oftentimes these are additional models trained by practitioners or other volunteers and have a taskspecific finetuning note that al models from pretrainedmodelshtml are also included in the models interface as well if you have a very narrow usecase you might as well check and see if there was already some model that has been finetuned on your specific task in the worst case youll simply end up with the base model anyways
61717097,sequence labelling with bert,pytorch lstm huggingfacetransformers torchtext,yes bertmodel needed them since without those special symbols added the output representations would be different however my experience says if you finetune bertmodel on the labeling task without cls and sep token added then you may not see a significant difference if you use bertmodel to extract fixed word features then you better add those special symbols yes you can take out the embedding of those special symbols in fact this is a general idea for sequence labeling or tagging tasks i suggest taking a look at some sequence labeling or tagging examples using bert to become confident about your modeling decisions you can find ner tagging example using huggingface transformers here
61707371,about getspecialtokensmask in huggingfacetransformers,tokenize huggingfacetransformers,you are indeed correct i tested this for both transformers and the at the time of writing current release of and in both cases i do get the inverted results for regular characters and for the special characters for reference this is how i tested it import transformers tokenizer transformersautotokenizerfrompretrainedrobertabase sentence this is a special sentence encodedsentence tokenizerencodesentence specialmasks tokenizergetspecialtokensmaskencodedsentence i would suggest you report this issue in their repository or ideally provide a pull request yourself to fix the issue
61680408,huggingface transformers not getting imported in vs code,python pythonimport visualstudiocode huggingfacetransformers,the error clearly suggests modulenotfounderror no module named joblib try pip install joblib also make sure you have latest torch and transformers library installed
61667142,huggingfacetransformers train bert and evaluate it using different attentions,transformermodel huggingfacetransformers,yes confirmed by cronoik this is the correct operation
61580961,unable to load spanbert model with transformers package,python huggingfacetransformers bertlanguagemodel,download the pretrained weights from the github page spanbert base cased layer hidden heads m parameters spanbert large cased layer hidden heads m parameters extract them to a folder for example i extracted to spanberthfbase folder which contains a bin file and a configjson file you can use automodel to load the model and simple bert tokenizer from their repo these models have the same format as the huggingface bert models so you can easily replace them with our spanbet models out
61551797,how can i monitor both training and eval loss when finetuning bert on a glue task,python pytorch huggingfacetransformers,theres likely no change needed in the code if installing a more recent version i tried via pip just fire the finetuning with the additional flag evaluateduringtraining and output will be ok beware that the example scripts change quite frequently so flags to accomplish this may change names see also here
61465223,roberta tokenization of multiple sequences,huggingfacetransformers,as with many other questions this can probably be best answered by because it has been pretrained that way the main benefit of models in the transformer family is the insane amount of pretraining that goes into them unless you are willing to replicate the weeksmonths of that pretraining stage i think it is best to accept the feature as it comes related to this it also implies that your suggested approach of feeding in more than two sentences at a time probably wont work see this related issue since roberta is not trained to accept input of more than two sentences it might not work without having a very large pretraining dataset i think for more implementationspecific details you should probably also head over to the huggingface issue tracker itself this sounds like a promising feature that others might be interested to work onuse for themselves but keep in mind that the token limit stays the same and tokens is not much for three or more sentences
61465103,how to get intermediate layers output of pretrained bert model in huggingface transformers library,tensorflow keras tensorflow huggingfacetransformers bertlanguagemodel,the third element of the bert models output is a tuple which consists of output of embedding layer as well as the intermediate layers hidden states from documentation hiddenstates tupletftensor optional returned when configoutputhiddenstatestrue tuple of tftensor one for the output of the embeddings one for the output of each layer of shape batchsize sequencelength hiddensize hiddenstates of the model at the output of each layer plus the initial embedding outputs for the bertbaseuncased model the configoutputhiddenstates is by default true therefore to access hidden states of the intermediate layers you can do the following there are elements in hiddenstates tuple corresponding to all the layers from beginning to the last and each of them is an array of shape batchsize sequencelength hiddensize so for example to access the hidden state of third layer for the fifth token of all the samples in the batch you can do hiddenstates note that if the model you are loading does not return the hidden states by default then you can load the config using bertconfig class and pass outputhiddenstatetrue argument like this
61452697,how to use bertforsequenceclassification for token maxlength set at,huggingfacetransformers,unless you are training on a tpu your chances are extremely low of ever having enough gpu ram with any of the available gpus right now for some bert models the model alone takes well above gb in ram and a doubling in sequence length beyond tokens takes about that much more in memory for reference a titan rtx with gb gpu ram most of what is currently available for a single gpu can barely fit samples of tokens in length at the same time fortunately most of the networks still yield a very decent performance when truncating the samples but this is of course taskspecific also keep in mind that unless you are training from scratch all of the pretrained models are generally trained on token limits to my knowledge the only model currently supporting longer sequences is bart which allows up to tokens in length
61443480,huggingfaces bert tokenizer not adding pad token,tokenize huggingfacetransformers bertlanguagemodel,no it would not there is a different parameter to allow padding transformers padding accepts true maxlength and false as values transformers padtomaxlength accepts true or false as values addspecialtokens will add the cls and the sep token and respectively
61326892,gradient of the loss of distilbert for measuring token importance,pytorch transformermodel attentionmodel huggingfacetransformers bertlanguagemodel,by default the gradients are retained only for parameters basically just to save memory if you need gradients of inner nodes of the computation graph you need to have the respective tensor before calling backward and add a hook that will be executed at the backward pass a minimum solution from pytorch forum ygrad torchzeros def extractxvar global ygrad ygrad xvar xx variabletorchrandn requiresgradtrue yy xx zz yy yyregisterhookextract run the backprop print ygrad shows zzbackward print ygrad show the correct dzdy in this case the gradients are stored in a global variable where they persist after pytorch get rid of them in the graph itself
61306391,running squad script using albert huggingfacetransformers,python deeplearning huggingfacetransformers squad,you can use gradient accumulation steps to compensate for the small batch size essentially the gradient accumulation step parameter is this lets say you want a batchsize of but your gpu can only fit a batch of size so you make two passes of batches each accumulate your gradients and then do the backward pass after batches secondly hyperparameters play a humongous role in deep learning models you will have to try a few sets of parameters to get better accuracy i think reducing the learning rate to the order of e might help here though it is just speculation
61000500,tensorflowkerasbert multiclass text classification accuracy,python tensorflow machinelearning keras huggingfacetransformers,the main problem is in this line ids inputs actually the ids are the first element of inputs so it should be ids inputs but there is also another problem which might result in inconsistent validation accuracy you should fit the labelencoder only one time to construct the label mapping so you should use the transform method instead of fittransform on validation labels further dont use both softmax activation and fromlogitstrue in loss function simultaneously only use either of them see here for more info another point is that you might need to use a lower learning rate for the optimizer the default learning rate of adam optimizer is e which might be too high considering that you are finetuning a pretrained model try a lower learning rate say e or e eg tfkerasoptimizersadamlearningratee a high learning rate for finetuning a pretrained model might destroy the learned weights and disrupts finetuning process due to the large gradient values generated especially at the start of finetuning process
60914793,argument neversplit not working on bert tokenizer,python tensorflow huggingfacetransformers,i would call this a bug or at least not good documented the neversplit argument is only considered when you use the basictokenizer which is part of the berttokenizer you are calling the tokenize function from your specific model bertbaseuncased and this considers only his vocabulary as i would expect in order to prevent splitting of certain tokens they must be part of the vocabulary you can extend the vocabulary with the method addtokens i think the example below shows what i am trying to say from transformers import berttokenizer text lol thats funny lool tokenizer berttokenizerfrompretrainedbertbaseuncased neversplitlol what you are doing printtokenizertokenizetext how it is currently working printtokenizerbasictokenizertokenizetext how you should do it tokenizeraddtokenslol printtokenizertokenizetext output
60876394,does bertforsequenceclassification classify on the cls vector,python machinelearning pytorch bertlanguagemodel huggingfacetransformers,the short answer yes you are correct indeed they use the cls token and only that for bertforsequenceclassification looking at the implementation of the bertpooler reveals that it is using the first hidden state which corresponds to the cls token i briefly checked one other model roberta to see whether this is consistent across models here too classification only takes place based on the cls token albeit less obvious check lines here
60867353,using lime for bert transformer visualization results in memory error,python machinelearning huggingfacetransformers lime,ended up solving this by reimplementing along the lines of this github post my code is now very different from the above probably makes sense if you look to the github post for guidance if youre running into similar issues
60843698,how to define ration of summary with hugging face transformers pipeline,pytorch huggingfacetransformers,note that this answer is based on the documentation for version of transformers it seems that as of yet the documentation on the pipeline feature is still very shallow which is why we have to dig a bit deeper when calling a python object it internally references its own call property which we can find here for the summarization pipeline note that it allows us similar to the underlying bartforconditionalgeneration model to specifiy the minlength and maxlength which is why we can simply call with something like this would give you a summary of about length of the original data but of course you can change that to your liking note that the default value for bartforconditionalgeneration for maxlength is as of now minlength is undocumented but defaults to whereas the summarization pipeline has values minlength and maxlength
60832547,where is perplexity calculated in the huggingface gpt language model code,machinelearning huggingfacetransformers googlepublishertag perplexity,ah ok i found the answer the code is actually returning cross entropy in the github comment where they say it is perplexitythey are saying that because the op does which transforms entropy to perplexity
60710606,why should i call a bert module instance rather than the forward method,bertlanguagemodel huggingfacetransformers,i think this is just general advice concerning working with pytorch modules the transformers modules are nnmodules and they require a forward method however one should not call modelforward manually but instead call model the reason is that pytorch does some stuff under the hood when just calling the module you can find that in the source code def callself input kwargs for hook in selfforwardprehooksvalues result hookself input if result is not none if not isinstanceresult tuple result result input result if torchcgettracingstate result selfslowforwardinput kwargs else result selfforwardinput kwargs for hook in selfforwardhooksvalues hookresult hookself input result if hookresult is not none result hookresult if lenselfbackwardhooks var result while not isinstancevar torchtensor if isinstancevar dict var nextv for v in varvalues if isinstancev torchtensor else var var gradfn vargradfn if gradfn is not none for hook in selfbackwardhooksvalues wrapper functoolspartialhook self functoolsupdatewrapperwrapper hook gradfnregisterhookwrapper return result youll see that forward is called when necessary
60610280,bertforsequenceclassification vs bertformultiplechoice for sentence multiclass classification,python machinelearning pytorch bertlanguagemodel huggingfacetransformers,the answer to this lies in the admittedly very brief description of what the tasks are about bertformultiplechoice eg for rocstoriesswag tasks when looking at the paper for swag it seems that the task is actually learning to choose from varying options this is in contrast to your classical classification task in which the choices ie classes do not vary across your samples which is exactly what bertforsequenceclassification is for both variants can in fact be for an arbitrary number of classes in the case of bertforsequenceclassification respectively choices for bertformultiplechoice via changing the labels parameter in the config but since it seems like you are dealing with a case of classical classification i suggest using the bertforsequenceclassification model shortly addressing the missing softmax in bertforsequenceclassification since classification tasks can compute loss across classes indipendent of the sample unlike multiple choice where your distribution is changing this allows you to use crossentropy loss which factors in softmax in the backpropagation step for increased numerical stability
60459292,using past and attentionmask at the same time for gpt,python pytorch huggingfacetransformers,in order to make your current code snippet work you will have combine the previous and new attention mask as follows for the case that you want to test two possible suffixes for a sentence start you probably will have to clone your past variable as many times as you have suffixes that means that the batch size of your prefix inputids has to match the batch size of your suffix inputids in order to make it work also you have to change the positional encodings input of your suffix inputids gpt uses absolute positional encodings if one of your prefix inputids is padded this is not shown in the code above please take a look at to see how its done
60378466,if berts cls can be retrained for a variety of sentence classification objectives what about sep,transformermodel bertlanguagemodel huggingfacetransformers,in theory it can give some results so it would work its just a token but the question is why you would want to that these tokens have been pretrained for a specific purpose i suppose that by retrain you mean finetuning so if you would finetune the sep token suddenly as a classification token i think you wont get good results because you are only finetuning one token in the whole language model for a task that it wasnt even pretrained for
60297908,how to feed the output of a finetuned bert model as inpunt to another finetuned bert model,pytorch pretrainedmodel bertlanguagemodel huggingfacetransformers,to formulate this as an answer too and keep it properly visible for future visitors the forward call of transformers does not support these arguments in version or any earlier version for that matter note that the link in my comment is in fact pointing to a different forward function but otherwise the point still holds passing encoderhiddenstates to forward was first possible in version
60243099,what is the meaning of the second output of huggingfaces bert,python deeplearning pytorch huggingfacetransformers,the output in this case is a tuple of lasthiddenstate pooleroutput you can find documentation about what the returns could be here
60170037,how to use a batch size bigger than zero in bert sequence classification,python huggingfacetransformers,in that example unsqueeze is used to add a dimension to the inputlabels so that it is an array of size batchsize sequencelength if you want to use a batch size you can build an array of sequences instead like in the following example from transformers import berttokenizer bertforsequenceclassification import torch tokenizer berttokenizerfrompretrainedbertbaseuncased model bertforsequenceclassificationfrompretrainedbertbaseuncased sequences hello my dog is cute my dog is cute as well inputids torchtensortokenizerencodesequence addspecialtokenstrue for sequence in sequences labels torchtensor labels depend on the task outputs modelinputids labelslabels loss logits outputs in that example both sequences get encoded in the same number of tokens so its easy to build a tensor containing both sequences but if they have a differing amount of elements you would need to pad the sequences and tell the model which tokens it should attend to so that it ignores the padded values using an attention mask there is an entry in the glossary concerning attention masks which explains their purpose and usage you pass this attention mask to the model when calling its forward method
60157959,transformers summarization with python pytorch how to get longer output,pytorch huggingfacetransformers pytorchignite,the short answer is yes probably to explain this in a bit more detail we have to look at the paper behind the implementation in table you can clearly see that most of their generated headlines are much shorter than what you are trying to initialize while that alone might not be an indicator that you couldnt generate anything longer we can go even deeper and look at the meaning of the unusedx tokens as described by bert dev jacob devlin since the unusedx tokens were not used they are effectively randomly initialized further the summariazation paper describes position embeddings in the original bert model have a maximum length of we overcome this limitation by adding more position embeddings that are initialized randomly and finetuned with other parameters in the encoder this is a strong indicator that past a certain length they are likely falling back to the default initialization which is unfortunately random the question is whether you can still salvage the previous pretraining and simply finetune to your objective or whether it is better to just start from scratch
60133236,what does berts special characters appearance in squads qa answers mean,nlpquestionanswering bertlanguagemodel huggingfacetransformers squad,you should simply treat them as invalid because you try to predict a proper answer span from the variable text everything else should be invalid this is also the way how huggingface treats this predictions we could hypothetically create invalid predictions eg predict that the start of the span is in the question we throw out all invalid predictions you should also note that they use a more sopisticated method to get the predictions for each question dont ask me why they show torchargmax in their example please have a look at the example below output
60120849,outputting attention for bertbaseuncased with huggingfacetransformers torch,python attentionmodel huggingfacetransformers bertlanguagemodel,i think its too late to make an answer here but with the update from the huggingfaces transformers i think we can use this
59701981,bert tokenizer model download,python github pytorch huggingfacetransformers bertlanguagemodel,as described here what you need to do are download pretrain and configs then putting them in the same folder every model has a pair of links you might want to take a look at lib code for instance with usersyournameworkplaceberts refer to your folder below are what i found at srctransformersconfigurationbertpy there are a list of models configs and at srctransformersmodelingbertpy there are links to pretrains
59656096,trouble saving tfkeras model with bert huggingface classifier,python tensorflow huggingfacetransformers,this is indeed a problem with tensorflow please use modelsavemodelnamesaveformattf alternatively you can also try upgrading or downgrading tensorflow
59589483,why should the huggingface transformers library be installed on a virtual environment,python pytorch huggingfacetransformers,summing up the comments in a community answer its not needed to install huggingface transformers in a virtual environment it can be installed just like any other package though there are advantages of using a virtual environment and is considered a good practice you want to work in virtual envs for all python work you do so that you dont interfere the system install of python and so that you dont have a big global list of hundreds of packages that have nothing to do with each other and that may have conflicting requirements apart from that using a virtual environment for your project also means that it is easily deployable to a different machine because all the dependencies are selfcontained and can be packaged up in one go more in this answer
58532911,why is the input size of the multiheadattention in pytorch transformer module,pytorch tensor transformermodel attentionmodel huggingfacetransformers,from the nntransformer definition with the default values encoderlayer is instantiated with dmodel nhead the multiheadattention is instantiated with dmodel nhead equal to those values and kdim vdim are left to the default value of none if they are none selfqkvsameembeddim at this line evaluates to true when that happens as you correctly pointed out selfinprojweight is defined as a tensor of shape x embeddim embeddim in short yes thats correct
58454157,pytorch bert typeerror forward got an unexpected keyword argument labels,python pytorch bertlanguagemodel huggingfacetransformers,as far as i know the bertmodel does not take labels in the forward function check out the forward function parameters i suspect you are trying to finetune the bertmodel for sequence classification task and the api provides a class for that which is bertforsequenceclassification as you can see its forward function definition please note the forward method returns the followings hope this helps
58417374,how to load the saved tokenizer from pretrained model,machinelearning pytorch huggingfacetransformers,if you look at the syntax it is the directory of the pretrained model that you are supposed to pass hence the correct way to load tokenizer must be in your case savedmodel here is the directory where youll be saving your pretrained model and tokenizer
75552310,how to use my pretrained lstm saved model to make new classifications,pythonx tensorflow keras googlecolaboratory lstm,so from your model code you have the following tfkeraslayersdense activationsoftmax presumably you have different sentiment classes the output you are seeing from your modelpredict are the probabilities that the input belongs to the corresponding class ie chance that the sentiment is class that the sentiment is class that the sentiment is class etc so what is typically done to postprocess these results is take the largest probability as the prediction using npargmaxpred which in the case you posted should give you which then can be interpreted as your model believes your tweet is likely to belong to class zero
70388939,tensorflow how to input data already embedded by pretrain model into a lstm model,python tensorflow keras lstm,you are probably mixing up the input shape try something like this import tensorflow as tf batchsize def lstmmodelinputshape units inputdata tfkeraslayersinputshapeinputshape dtypefloat nameinputlayer x tfkeraslayerslstmunits namelstmlayer activationtanh returnsequencesfalseinputdata x tfkeraslayersdenseunits namefullconnectionlayerx x tfkeraslayersactivationsigmoid nameactivationlayerx model tfkerasmodelinputsinputdata outputsx return model model lstmmodelinputshape units modelsummary samples words embeddingrepresentation traindata tfrandomnormalsamples words embeddingrepresentation trainlabels tfrandomuniform maxval dtypetfint classweight modelcompilelossbinarycrossentropy optimizertfkerasoptimizersadamlearningratee metricsbinaryaccuracy precision recall auc historyweightedlstmmodel modelfittraindata trainlabels epochs batchsize batchsize shuffletrue classweightclassweight
68612469,add lstm layers after tensorflowhub pretrained model,python keras lstm tensorflowhub,you can reshape the output of the hubkeraslayer
68103873,finetuning universal sentence encoder with lstm,python tensorflow keras lstm,the simplest solution is to pass each stringsentence separately into the universal sentence encoder this produces an embedding for each stringsentence of shape that can be concatenated to form a tensor of shape none nsentences this is the code of the model at inference time here the running notebook
63809805,build pretrained cnnlstm network with keras functional api,python tensorflow keras convneuralnetwork lstm,here the correct way to build a model to classify video sequences note that i wrap into timedistributed a model instance this model was previously build to extract features from each frame individually in the second part we deal the frame sequences if you want to use the vgg x emb representation you can simply do
61820728,keras pretrained a cnndense model how to freeze cnn weights and substitute dense with lstm,keras lstm convneuralnetwork pretrainedmodel,you cant directly take the output from flatten lstm needs d features time filters you have to reshape your tensors you can take the output from the layer before flatten maxpooling lets say this layer has index i in the model we can take the output from that layer and reshape it based on our needs and pass it to lstm
61136393,keras pretraining of a cnn model and after use it for a cnnlstm model,tensorflow keras lstm convneuralnetwork,you can do this however in order to successfully implement this new model you need to use the functional api in order to achieve this below i am giving you an example on how you can add new layers to a pretrained model
55669695,how to feed bert embeddings to lstm,keras lstm keraslayer mlp bertlanguagemodel,you can create model that uses first the embedding layer which is followed by lstm and then dense such as here
54420732,how to calculate accuracy in training rnn language model in tensorflow,python tensorflow lstm,since the model has both targets and prediction probabilities for each class you can reduce the probabilities tensor to keep the class index of the highest probability then you can compare to the targets to know if it successfully predicted or not finally the accuracy is the ratio between correct prediction over the size of input aka mean of this boolean tensor
51886142,lstm pretrained word embedding positivenegative review prediction,python tensorflow keras deeplearning lstm,you need to decide few hyperparameters for model so if your sentence length is fixed then use in placeholder otherwise use none so if your sentence batch is and length is sentence x now you can either use embedding from scratch or you can use pretrained embedding for using pretrained embedding you use can define variable like this after embedding lookup your sentence will become xx here is full detailed tutorial on embedding in tensorflow after you have to feed this to lstm model but since you are using padding so you have to provide sequencelength to lstm which is actual length of sentence now at lstm part you have to define numunits of lstm which are nodes in lstm unit simple dynamic rnn with lstm example is now your numunits are for example then each timestep output shape will be x and final output of rnn which contains all time step shape will be xx for projection you have to take last timestep output after projection is x x hiddenunit x noof categories suppose your categories are labels x x x then final output will be x from there take the argmax probability index which will be your prediction now take last output of rnn and project with linear projection without any activation function here is sentiment tutorial with bidirectional rnn
51428696,keras access layer parameter of pretrained model to freeze,python machinelearning keras lstm keraslayer,an easy solution would be to name each layer ie then upon loading the model you can iterate over the layers and freeze the ones matching a name condition then you need to recompile your model for the changes to take effect and afterwards you may resume training as usual
49535488,lstm on top of a pretrained cnn,tensorflow keras convneuralnetwork lstm,you can wrap your cnnmodel also in a timedistributed wrapper frames inputshape x timedistributedcnnmodelframes x timedistributedflattenx x lstmneurons dropoutdropout namelstmx out densenoutput kernelinitializerweightinit nameoutx model modelinputsframes outputsout
43658327,load pretrained wordvec embedding in tensorflow,tensorflow lstm embedding wordvec,yes the fit step tells the vocabprocessor the index of each word starting from in the vocab array transform just reversed this lookup and produces the index from the words and uses to pad the output to the maxdocumentsize you can see that in a short example here vocabprocessor learnpreprocessingvocabularyprocessor vocab a b c d e pretrain vocabprocessorfitvocab pretrain vocabprocessor true nparraylistpretraintransforma b c b c d a e a b c d e array
42944787,load pretrained word embedding into tensorflow model,tensorflow lstm wordvec,correct me if i am wrong trying to answer with my limited understanding of tensorflow this simply states you are trying to initialize element of different graph so i guess you need to be in same scope in which your graph is define just adjusting your embedding initialization code in same scope can solve the problem i guess this should be only problem as you can see in your first example initialization are under same scope
39211791,using pretrained word embeddings in tensorflows seqseq function,python machinelearning tensorflow recurrentneuralnetwork lstm,there is no parameter you just hand over read in your embeddings make sure vocabulary ids match then once you initialized all variables find the embedding tensor iterate through tfallvariables to find the name then use tfassign to overwrite the randomly initialized embeddings there with your embeddings
79486243,chatbot ui like chatgpt in ios swiftui,ios swiftui chatbot swiftuilist chatgpt,if i understand correctly the main requirement is that you want a new question from the user to appear at the top of the scroll view the reply from the assistant should then appear in the space below it this means you need to add blank space between the users question and the bottom of the scroll view suggested changes organizing the data one way to approach the problem is to combine a message from the user with the responses from the assistant as a single group entity assuming that there can be or messages from the user which are followed by n messages from the assistant the following struct can be used to group them struct messagegroup identifiable let id uuid let usermessage chatmessage var assistantmessages chatmessage the view can now be changed to show these message groups instead of individual messages this makes the positioning simpler as will be seen below positioning scrolled positioning can be implemented as follows wrap the scrollview in a geometryreader so that the height of the scrollview can be measured set the minimum height of the last message group to the full height of the scrollview this way blank space is shown below the latest message in the group filling the area to the bottom of the scroll view finally when adding a new message group to the array perform a programmatic scroll to this group doing it this way you no longer need any of the position tracking that you had in your previous solution you also dont need the flag for a new message because the latest message group will always be the last group in the array safearea inset normally if a scrollview is in contact with the safe area inset at the top of the screen then the content of the scroll view will pass through this safe area this means the end of the previous message group will be seen above a new message you said in the question that the previous messages should move out of view at the top so if you dont want the end of the last message to be visible add top padding of pixel to break the contact with the safe area inset the environment value pixellength gives you the size of pixel in points view simplifications messageview and messagecontentview can be simplified to the following mark messageview struct messageview view let message chatmessage var body some view messagecontentviewmessage message framemaxwidth infinity alignment messageisassistant leading trailing padding backgroundwhite in rectcornerradius mark messagecontentview struct messagecontentview view let message chatmessage var body some view if messageistyping typingindicatorview framemaxwidth infinity alignment leading paddingtop else vstackalignment leading spacing texttimestamp fontcaption foregroundstylegray textmessagetext fontbody padding background messageisassistant grayopacity purpleopacity colorappprimarycolor in rectcornerradius foregroundstylecolorapptextcolor fixedsizehorizontal false vertical true putting it all together the part still missing is the programmatic scrolling there would be different ways to do this depending on which ios version you need to support ios and above programmatic scrolling can use scrollposition in connection with scrolltargetlayout whenever a new message group is added to the array set the id of the new group as the scroll position target this can be done withanimation here is the updated example to show it working this way struct chatview view environmentpixellength private var pixellength state private var messagegroups messagegroup messagegroup usermessage nil assistantmessages chatmessagetext hello how can i assist you today isassistant true messagegroup usermessage chatmessagetext i need help with swiftui isassistant false assistantmessages chatmessagetext sure what do you need help with in swiftui isassistant true state private var scrollposition uuid var body some view vstack geometryreader geoproxy in let scrollviewheight geoproxysizeheight scrollview vstackspacing foreachmessagegroups group in vstackspacing if let message groupusermessage messageviewmessage message foreachgroupassistantmessages message in messageviewmessage message frame minheight groupid messagegroupslastid scrollviewheight nil alignment top scrolltargetlayout paddinghorizontal scrollpositionid scrollposition anchor top paddingtop pixellength break contact with the safe area inset input field hstack textfieldtype a message text constant textfieldstyleroundedborder buttonsend action sendmessage buttonstyleborderedprominent padding private func sendmessage let newmessage chatmessagetext how can i create a smooth scrolling chat ui like chatgpt isassistant false let group messagegroupusermessage newmessage messagegroupsappendgroup withanimation scrollposition groupid dispatchqueuemainasyncafterdeadline now let assistantmessage chatmessagetext hold on let me fetch the best answer for you isassistant true messagegroupsmessagegroupscount assistantmessagesappendassistantmessage pre ios if you want to support older versions of ios then programmatic scrolling can be performed using a scrollviewreader like you were doing before use an onchange handler to detect when a new message group has been added to the array scroll to the latest message group using withanimation btw i found it was important to unwrap the optional inside the onchange handler otherwise scrollto didnt work geometryreader geoproxy in let scrollviewheight geoproxysizeheight scrollviewreader scrollproxy in scrollview vstackspacing paddinghorizontal deprecated modifier used intentionally targeting ios onchangeof messagegroupslastid lastid in if let lastid withanimation scrollproxyscrolltolastid anchor top paddingtop pixellength break contact with the safe area inset private func sendmessage let newmessage chatmessagetext how can i create a smooth scrolling chat ui like chatgpt isassistant false let group messagegroupusermessage newmessage messagegroupsappendgroup dispatchqueuemainasyncafterdeadline now let assistantmessage chatmessagetext hold on let me fetch the best answer for you isassistant true messagegroupsmessagegroupscount assistantmessagesappendassistantmessage both techniques work the same
78712629,gpt langchain experimental agent allow dangerous code,python visualstudiocode artificialintelligence chatbot,same answer but to make it clear change this agent createcsvagent openai csvfile verbosetrue to this agent createcsvagent openai csvfile verbosetrue allowdangerouscodetrue
77506707,chatbot with gpt in angular,angular typescript chatbot openaiapi gpt,i found the answer to the last one gpt was giving credit to new members for the first months for api trials when i opened a new account and tried it the problem was solved
76502113,error with fewshot prompting using gpt,chatbot openaiapi gpt chatgptapi,you made a mistake between chat completion and completion see documentation completion chat completion
76040193,how can i update my chatbot with chatgpt from textdavinci to gptturbo in python,python chatbot whatsapp openaiapi chatgptapi,to update your code to gptturbo there are four areas you need to modify call openaichatcompletioncreate instead of openaicompletioncreate set modelgptturbo change messages to an array as shown below change the way you are assigning repsonse to your resposta variable so that you are reading from the messages key this tested example takes into account those changes responseopenaichatcompletioncreate modelgptturbo messagesrole user content questao temperature maxtokens topp frequencypenalty presencepenalty respostaresponsechoicesmessagecontent additionally since more than one choice can be returned from the model instead of only looking at you may be interested in iterating over them to see what youre getting something like for choice in responsechoices outputtext choicemessagecontent printoutputtext print printn note that you dont need to do that if you are calling openaichatcompletioncreate with n additionally your example is setting both temperature and topp however the docs suggest to only set one of those variables
75718913,openai gpt api why do i get different nonrelated random responses to the same question every time,python chatbot openaiapi gpt,as stated in the official openai documentation the temperature and topp settings control how deterministic the model is in generating a response if youre asking it for a response where theres only one right answer then youd want to set these lower if youre looking for more diverse responses then you might want to set them higher the number one mistake people use with these settings is assuming that theyre cleverness or creativity controls change this to this
75408152,how can i call the chatgpt api in google chrome extension,googlechromeextension chatbot openaiapi,use the fetch api to send a request to the openai api you will need to signup for an api key with openai first to send a question to gpt you would make the following fetch api call ref
71680285,how to deploy a question answering bert model as a chat bot on ms teams,microsoftteams chatbot bertlanguagemodel,as youve seen there are a bunch of toolsapproaches to creating bots in the microsoft world for teams or otherwise underneath these all use the bot framework but you can develop directly ie write code or use a higherlevel tool like bot framework composer the choice is yours depending on your own internal skills if you want to work with code directly here are a bunch of bot samples in multiple languages for isntance here is an example of integrating the microsoft qnamaker service into your bot basically if you go the development approach your bot is just a web service once it receives the message it can call out to any other service behind the scenes that means it can receive a message call out to an aws service receive the response and send a reply to the user for multiple questions as part of a set of chats bot framework provides an idea called dialogs that should work for you
46566006,unable to load a pretrained model,python machinelearning tensorflow neuralnetwork chatbot,the importer cant find a very specific function in your graph namely attnaddfunfff which is likely to be one of attention functions probably youve stepped into this issue however they say its bundled in tensorflow double check that installed tensorflow version contains attentiondecoderfnpy or if you are using another library check that its there if its there here are your options rename this operation if possible you might want to read this discussion for workarounds duplicate your graph definition so that you wont have to call importmetagraph but restore the model into the current graph
72493271,detecting and retrieving text from a column based on language model in r,r languagedetection,it may not be vectorized we can use rowwise or with lapply to loop over each of the elements in text column and apply the function
73726816,multiprocess error while using map function in python with ngram language model,python multiprocessing speechtotext ngram,i finally fixed this error the brokenpipeerror error broken pipe arises from the linux operating system and it will occur when you are doing io tasks specifically when the pipeline of read and write on linux is closed as the other side of communication is still trying to read or write the data this error will occur now the fun part is here i was using workers which was the number of cpu cores in my map function the length of data in the dataset iterable was so the pipeline doing the map function had rows with files in each and row with files i guess it was the cause of the error while the last row with files made some disturbance so i reduced the number of files in the dataset from to and the number of workers to and removed the batchsize this made the length of the data to be divisible by the number of processes and made the error go away here is the link for more information about brokenpipeline error
50569428,internal server error updating watsons stt language model,speechtotext ibmwatson,please check your model now i am guessing you were experiencing an intermittent error sending another train request usually helps
36861620,how to add own language model to java program using sphinx,java speechrecognition speechtotext sphinx,your list contains acoustic model files language model and dictionary these are used for different purposes in different components of cmu sphinx the simplest thing to do would be to get an existing example from sphinx web page and replace the existing files with yours
78469341,synthesizing audio with unseen speakers using pretrained vits model,texttospeech voicerecognition speechsynthesis,for model ttsmodelsenvctkvits you cannot specify a reference voicespeaker this model does not support voice cloning you can carryout tts with a default voice and then carry out voice conversion so that generated voice sounds like your reference voice knnvc is a good voice conversion model
76773386,infer document vectors for pretrained word vectors,python gensim wordvec docvec,the docvec algoithm called paragraph vector in the papers that introduced it is not initialized from external pretrained wordvectors nor is creating wordvectors a distinct st step of creating a docvec model from scratch that could somehow be done separately or cachedreused across runs so not even the internal inference routines can do anything with just some external wordvectors they depend on model weights separate from wordvectors learned from doctoword relations seen in training ive occasionally seen some variantsimprovisedchanges that move a bit in the direction of taking outside wordvectors but ive not seen evidence such variations outperform the usual approach and theyre not implemented in gensim in standard dovvec rather that taking wordvectors as an input if the chosen mode of docvec creates typical perword wordvectors at all they get cotrained simultaneously with the docvectors in the plain pvdbow mode dm no typical wordvectors are trained at all only docvectors the support for inferencing new docvectors this mode is thus pretty fast and often works quite well for broad topical similarity for short docs of dozens to hundreds of words because the only thing training is trying to do is predict indoc words from candidate docvectors in this mode the window parameter is meaningless every word in a doc affects its docvector you can optionally add to that pvdbow mode interleaved skipgram wordvector training by using the nondefault dbowwords parameter this cotraining using a shared output center word prediction layer forces the wordvectors docvectors into a shared coordinate system so that theyre directly comparable to each other the window parameter then affects the skipgram wordtoword training just like in wordvec skipgram training training takes longer by a factor of about the window value and in fact the model is spending more total computation making the words predict their neighbors than the docvector predicting the doc words so theres a margin at which improving the wordvectors may be crowding out improvement of the docvectors the pvdm mode the default dm parameter inherently uses a combo of condidate docvector neighbor words to predict each center word that makes window relevant and inherently puts the wordvectors docvectors into a shared comparable coordinate space without as much overhead for larger window values as the interleaved skipgram above there may still be some reduction in docvector expressiveness to accomodate all the wordtoword influences which is best for a particular set of docs subject domain and intended downstream use is really a matter for experimentation as youve mentioned comparing docvectors to wordvectors is an aim only the latter two modes above pvdbow with optional skipgram or pvdm would be appropriate but if you dont absolutely need that have time to run more comparisons id still recommend trying plain pvdbow for its speed strength in some needs lets assume your sentences are an average of tokens each so your k docs k tokens tokenssentence give you million sentences yes holding say dimensional docvectors bytes each intraining for million texts has prohibitive ram costs gb as youve noted you could use a model trained for only the k docs to then infer docvectors for other smaller texts liek the sentences you shouldnt worry about those wasted k docvectors they were necessary to create the inferencing capability you could throw them away after training inference will still work and maybe you will have some reason to compare words or sentences or new docs or other docfragments to those fulldoc vectors you could also consider training on chunks larger than sentences but smaller than your full docs like paragraphs or sections if you can segment docs that way you could conceivably even use arbitrary ntoken chunks and it might work well only way to know is to try this sort of algorithm isnt supersensitive to small changes in tokenizationtextsegmenting as its the bulk of the data and broad relationships its modeling you can also simultaneously train docvectors for different levels of text by supplying more than one tag key for looking up the docvector posttraining per example text that is if your full document with id d has distinct sections ds ds ds you could feed it the doc as texts the st section with tags d ds the nd with tags d ds the rd with tags d ds then all the texts contribute to the traintuning of the d docvector but the subsections only affect the respective subsectionvectors whether thatd be appropriate depends on your goals the effect of supplying multiple tags varies a bit between modes but it may also be worth some experiments
70745209,unable to load pretrained gensim docvec from publication data,python numpy gensim docvec pretrainedmodel,where did the file patentdocvemodel come from if trying to load that file it generates such an error about another file with the name patentdocvemodeltrainablessynnegnpy then that other file is a necessary part of the full model that should have been created alongside patentdocvemodel when that patentdocvemodel file was first savepersisted to disk youll need to go back to where patentdocvemodel was created find the extra missing patentdocvemodeltrainablessynnegnpy file possibly others also starting patentdocvemodel all such files created at the same save must be keptmoved together at the same filesystem path for any future load to succeed additionally if you are training these yourself from original data id suggest being sure to use a current version of gensim only older pre versions will create any save files with trainables in the name
70458726,cant load the pretrained wordvec of korean language,gensim wordvec,while the page at isnt clear about the formats this author has chosen by looking at their source code at shows it using the gensim model save method such saved models should be reloaded using the load class method of the same model class for example if a wordvec model was saved with then it could be reloaded with note through that models saved this way are often split over multiple files that should be kept together and all start with the same root name but i dont see those here this work appears to be years old based on a pre version of gensim so there might be issues loading the models directly into the latest gensim if you do run into such issues absolutely need to make these vectors work you might need to temporarily use a prior version of gensim to load the model then you could save the plain vectors out with savewordvecformat for later reloading across any version or using the latest interim version that can load the model resave the model as save then repeat the process with the latest version that can read that model until you reach the current gensim but you also might want to find a more recent betterdocumented set of pretrained wordvectors for example facebook makes fasttext pretrained vectors available in both a text format and a bin format for many languages at trained on wikipedia only or trained on wikipedia plus web crawl data the text format should in fact be loadable with keyedvectorsloadwordvecformatfilename binaryfalse but will only include fullword vectors it will also be relatively easy to view as text or write simply code to massage into other formats the bin format is facebooks own native fasttext model format and should be loadable with either the loadfacebookmodel or loadfacebookvectors utility methods then the loaded model or vectors will be able to create the fasttext algorithms substringbased guesstimate vectors even for many words that werent in the model or training data
69412142,process to intersect with pretrained word vectors with gensim,python gensim,the intersectwordvecformat method still exists but as an operation on a set of wordvectors has moved to keyedvectors so in some cases older code that had called the method on a wordvec model itself will need to call it on the models wv property holding a keyedvectors object instead eg wvmodel wordvecvectorsizewordvectordim mincount wvmodelbuildvocabcorpusiterable youll likely need another workaround here see below wvmodelwvintersectwordvecformatpretraineddir googlenewsvectorsnegativebingz binarytrue however youll still hit some problems its always been at best an experimental advanced feature and not a part of any welldocumented processes so its best used if youre able to review its source code understand what limits tradeoffs will come with using such partiallypreinitialized wordvectors maybefurthertrained or maybefrozen depending on the vectorslockf values chosen the equally experimental vectorslockf functionality will now in gensim require manual initialization by the knowledgeable because intersectwordvecformat assumes a particular preallocation that method will break in gensim without an explicit workaround see this open issue for more details most generally preinitializing with other wordvectors is at best a fussy advanced technique so be sure to study the code consider the potential tradeoffs carefully evaluate its effects on your endresults before embracing it its not an easy automatic or wellcharacterized shortcut
68298289,finetuning pretrained wordvec model with gensim,gensim wordvec transferlearning pretrainedmodel,i dont think that code wouldve ever have worked in gensim versions before a plain listofwordvectors like googlenewsvectorsnegativebin does not never has had enough info to continue training its missing the hiddentooutput layer weights wordfrequency info essential for training looking at past source code as of release february that code wouldnve already given a deprecationerror with a pointer to the method for loading a plain setofwordvectors to address people with the mistaken notion that could work and raised other errors on any attempts to train such a model pre docs also warned that this would not work would have failed with a lesshelpful error as one of those errors mentioned there has at times been experimental support for loading some of a prior setofwordvectors to clobber any words in an existing models alreadyinitialized vocabulary via intersectwordvecformat but by default that both locks the imported vectors against further change brings in no new words thats unlike what people most often want from finetuning so its not a readymade help for that goal i believe some people have cobbled together custom code to achieve various kinds of finetuning in their projects but i dont know of anyone whos published a reliable recipe or strong results and i suspect some of the people who think theyre doing this well just havent rigorously evaluated the steps they are taking if you have any recipe you know worked pregensim it should be adaptable changes to the wordvecrelated classes were mainly refactorings optimizations new options with littletonone removal of functionality but a reliable description of what usedtowork or which particular finetuning strategy is being pursued for what specific benefits to make more specific recommendations
68048018,dutch pretrained model not working in gensim,gensim fasttext,are you sure youre using the latest version of gensim with many improvements to the fasttext implementation and there you will definitely want to use loadfacebookmodel to load a full bin facebookformat model but also note the posttraining expansion of the vocabulary is best considered an advanced experimental function it may not offer any improvement on typical tasks indeed without careful consideration of tradeoffs balancing influence of later traiing against earlier it can make things worse a fasttext model trained on a large diverse corpus may already be able to synthesize betterthannothing guess vectors for outofvocabulary words via its subword vectors if theres some data with verydifferent words wordsenses you need to integrate it will often be better to retrain from scratch using an equal combination of all desired text influences then youll be doing things in a standard and balanced way without hardertotune and hardertoevaluate improvised changes to usual practice
68003709,how can you pass a pretrainend lda model to ldaseq in gensim for dtm,gensim lda topicmodeling,in case anyone ever wonders this initializeown you need to supply sstats of the previously trained model in the shape vocablen numtopics and initializeldamodel you need to supply the previously trained lda model i found the answer here
63637245,how to load pretrained fasttext model in gensim with npy extension,gensim pretrainedmodel fasttext,that set of multiple files looks like it was saved from gensims fasttext implementation using gensims save method and thus is not in facebooks original fasttextformat so try loading them with the following instead from gensimmodelsfasttext import fasttext model fasttextloadcontentsafasttextsadmfasttextmodel upon loading that mainroot file it will find the subsidiary related files in the same directory as long as theyre all present the source where you downloaded these files should have included clear instructions for loading them nearby
62200198,is there a way to infer topic distributions on unseen document from gensim lda pretrained model using matrix multiplication,gensim lda topicmodeling,you can review the full source code used by ldamodels getdocumenttopics method in your installation or online at it also makes use of the inference method in the same file its doing a lot more scalingnormalizationclipping than your code which is likely the cause of the discrepancy but you should be able to examine linebyline where your process its differ to get the steps to match up it also shouldnt be hard to use the gensim codes steps as guidance for creating parallel javascript code that given the right parts of the models state can reproduce its results
60785538,in gensim with pretrained model wmdistance is working well but nsimilarity is not,gensim,when you get an error that a word is not in the vocabulary it means the word is not in that model any attempt to look it up will generate a keyerror to let you know you are trying to get a wordvector that isnt there you should filter your listsoftokens before passing them to nsimilarity to only include valid words of course that means you cant get a meaningful result about the word selfie its unknown nonsense to the model as if you asked for the word asruhfglaiwurfliuawiufsdfsdfs
60524589,cannot reproduce pretrained word vectors from its vectorngrams,pythonx gensim fasttext oov,the calculation of a full words fasttext wordvector is not just the sum of its character ngram vectors but also a raw fullword vector thats also trained for invocabulary words the fullword vectors you get back from ftwvword for knownwords have already had this combination precalculated see the adjustvectors method for an example of this full calculation the raw fullword vectors are in a vectorsvocab array on the modelwv object if this isnt enough to reconcile matters ensure youre using the latest gensim as there have been many recent ft fixes and ensure your list of ngramhashes matches the output of the ftngramhashes method of the library if not your manual ngramlistcreation and subsequent hashing may be doing something different
57695150,how can i use a pretrained embedding to gensim skipgram model,python machinelearning gensim wordvec,im not sure why youd want to do this if you have the whole corpus and can train on the whole corpus youre likely to get the best results from wholecorpus training and to the extent theres anything missing from the ndcorpus the ndcorpus training will tend to pull vectors for words still training away from words that are no longer in the corpus causing comparability of vectors within the corpus to decay its only the interleaved tugofwar between examples including all words that nudges them into positions that are meaningfully related to each other but keeping that caveat in mind you can continue to train a model with new data that is note in such a case the models discovered vocabulary is only based on the original initialization if there are words only in sentences when those sentences are presented to the model that didnt see those words during its initialization they will be ignored and never get vectors if using your tiny example corpus in this way the word cat wont get a vector again you really want to train on the largest corpus or at least use the largest corpus with a superset of words st also a warning will be logged because the nd training will again start the internal alpha learningrate at its larger starting value then gradually decrease it to the final minalpha value to be yoyoing the value like this isnt standard sgd and usually indicates a user error but it might be tolerable depending on your goals you just need to be aware when youre doing unusual training sequences like this youre off in experimentaladvanced land and have to deal with possible sideeffects via your own understanding
57630389,how to load pretrained lda model to jupiter notebook,pycharm jupyternotebook gensim lda,solved the issue was a difference in numpy versions i trained the lda model on numpy then i installed anaconda and ran jupiter with numpy
57426745,how to cluster words and phrases with pretrained model on gensim,gensim wordvec,the googlenews set does include many multiword phrases as created via some statistical analysis but might not include something specific youre hoping it does like computersoftware on the other hand i see an online wordlist suggesting that a phrase like compositefillings is in the googlenews vocabulary so this will likely work for you with that vectorset youre limited to what they chose to model as phrases if you need similarlystrong vectors for other phrases youd likely need to train your own model on a corpus where the phrases important to you have been combined into single tokens if you just need somethingbetterthannothing averaging together the constituent words wordvectors would give you something to work with but thats a prettycrude standin for truly modeling the bigrammultigram against its unique contexts
57244699,how to turn a list of words into a list of vectors using a pretrained wordvec modelgoogle,pythonx gensim wordvec,you get the vector via idiomatic python keyedindexaccess brackets for example you can create a new list based on some operation on every item of an existing list via an idiomatic python list comprehension expressionx for x in somelist for example
55612440,fasttext error typeerror supervised got an unexpected keyword argument pretrainedvectors,python gensim fasttext,according to the documentation the named parameter to the function is called pretrainedvectors not pretrainedvectors this naming convention is in line with pep style and so is normal for a python api
54655604,expected input to torch embedding layer with pretrained vectors from gensim,vector pytorch gensim wordvec recurrentneuralnetwork,the documentation says the following this module is often used to store word embeddings and retrieve them using indices the input to the module is a list of indices and the output is the corresponding word embeddings so if you want to feed in a sentence you give a longtensor of indices each corresponding to a word in the vocabulary which the nnembedding layer will map into word vectors going forward heres an illustration testvoc ok great test the word vectors for ok great and test are at indices and respectively myembedding torchrand e nnembeddingfrompretrainedmyembedding longtensor of indicies corresponds to a sentence reshaped to because batch size is mysentence torchtensor view res emysentence printresshape torchsize is the batch dimension and theres three vectors of length each in terms of rnns next you can feed that tensor into your rnn module eg lstm nnlstminputsize hiddensize batchfirsttrue output h lstmres printoutputshape torchsize i also recommend you look into torchtext it can automatate some of the stuff you will have to do manually otherwise
53998446,wvtransformer only works with one word as input,scikitlearn gensim wordvec,this is technically not an answer but cannot be written in comments so here it is there are multiple issues here logisticregression class and most other scikitlearn models work with d data nsamples nfeatures that means that it needs a collection of d arrays one for each row sample in which the elements of array contains the feature values in your data a single word will be a d array which means that the single sentence sample will be a d array which means that the complete data collection of sentences here will be a collection of d arrays even in that since each sentence can have different number of words it cannot be combined into a single d array secondly the wvtransformer in gensim looks like a scikitlearn compatible class but its not it tries to follows scikitlearn api conventions for defining the methods fit fittransform and transform they are not compatible with scikitlearn pipeline you can see that the input param requirements of fit and fittransform are different fit x iterable of iterables of str the input corpus x can be simply a list of lists of tokens but for larger corpora consider an iterable that streams the sentences directly from disknetwork see browncorpus textcorpus or linesentence in wordvec module for such examples fittransform x numpy array of shape nsamples nfeatures training set if you want to use scikitlearn then you will need to have the d shape you will need to somehow merge wordvectors for a single sentence to form a d array for that sentence that means that you need to form a kind of sentencevector by doing sum of individual words average of individual words weighted averaging of individual words based on frequency tfidf etc using other techniques like sentvec paragraphvec docvec etc note i noticed now that you were doing this thing based on dvtransformer that should be the correct approach here if you want to use sklearn the issue in that question was this line since that question is now deleted here you overwrite your original xtrain list of list of words with already calculated word vectors and hence that error or else you can use other tools libraries keras tensorflow which allow sequential input of variable size for example lstms can be configured here to take a variable input and an ending token to mark the end of sentence a sample update in the above given solution you can replace the lines with no need to fit and transform separately since pipelinefit will automatically do that
50237247,gensim docvec object has no attribute intersectwordvecformat when i load the google pretrained wordvec model,wordvec gensim docvec,a recent refactor made docvec no longer share a superclass with this method you might be able to call the method on your modeldmwv object instead but im not sure otherwise you could look at the source and mimic the code to achieve the same effect if you really need that step but note that docvec doesnt need wordvectors as input it can learn everything it needs from your own training data whether wordvectors from elsewhere will help will depend on a lot of factors and the larger your own data is or the more unique the less preloaded vectors from elsewhere are likely to help or even have any residual effect when your own training is done other notes on your apparent setup dbowwords will have no effect in dm mode that mode already inherently trains wordvectors it only has effect in dm dbow mode where it adds extra interleaved wordtraining if you need wordvectors often plain dbow without wordvector training is a fast and effective option recent versions of gensim require more arguments to train and note that typical published work with this algorithm use or sometimes more passes over the data as can be specified to train via the epochs argument rather than the default in some versions of gensim of
48898325,gensim docvec train more documents from pretrained model,gensim docvec pretrainedmodel resumingtraining,the gensim docvec class can always be fed extra examples via train but it only discovers the working vocabulary of both wordtokens and documenttags during an initial buildvocab step so unless wordstags were available during the buildvocab theyll be ignored as unknown later the words get silently dropped from the text the tags arent trained or remembered inside the model the wordvec superclass from which docvec borrows a lot of functionality has a newer moreexperimental parameter on its buildvocab called update if set true that call to buildvocab will add to rather than replace any prior vocabulary however as of february this option doesnt yet work with docvec and indeed often causes memoryfault crashes but even ifwhen that can be made to work providing incremental training examples isnt necessarily a good idea by only updating parts of the model those exercised by the new examples the overall model can get worse or its vectors made less selfconsistent with each other the essence of these denseembedding models is that the optimization over all varied examples results in generallyuseful vectors training over just some subset causes the model to drift towards being good on just that subset at likely cost to earlier examples if you need new examples to also become part of the results for mostsimilar you might want to create your own separate setofvectors outside of docvec when you infer new vectors for new texts you could add those to that outside set and then implement your own mostsimilar using the gensim code as a model to search over this expanding set of vectors rather than just the fixed set that is created by initial bulk docvec training
45310409,using a wordvec model pretrained on wikipedia,wikipedia gensim wordvec,you can check webvectors to find wordvec models trained on various corpora models come with readme covering the training details youll have to be a bit careful using these models though im not sure about all of them but at least in wikipedias case the model is not a binary file that you can straightforwardly load using eg gensims functionality but a txt version ie file with words and corresponding vectors keep in mind though that the words are appended by their partofspeech pos tags so for example if youd like to use the model to find out similarities for word vacation youll get a keyerror if you type vacation as is since the model stores this word as vacationnoun an example snippet of how you could use the wiki model perhaps others as well if theyre in the same format and an output is below and the output update here are some useful links to binary models pretrained word embedding models fasttext models crawldmveczip million word vectors trained on common crawl b tokens wikinewsdmveczip million word vectors trained on wikipedia umbc webbase corpus and statmtorg news dataset b tokens wikinewsdmsubwordveczip million word vectors trained with subword infomation on wikipedia umbc webbase corpus and statmtorg news dataset b tokens wiki word vectors dim wikienzip bintext model google wordvec pretrained wordphrase vectors googlenewsvectorsnegativebingz googlenewsvectorsnegativeslimbingz slim version with app k words pretrained entity vectors freebasevectorsskipgrambingz entity vectors trained on b words from various news articles freebasevectorsskipgramenbingz entity vectors trained on b words from various news articles using the deprecated en naming more easily readable the vectors are sorted by frequency glove global vectors for word representation glovebzip wikipedia gigaword b tokens k vocab uncased d d d d vectors mb download heres an example in action glovebdzip common crawl b tokens m vocab cased d vectors gb download webvectors models trained on various corpora augmented by partofspeech pos tags
45069715,after loading a pretrained wordvec model how do i get wordvec representations of new sentences,clusteranalysis gensim wordvec,wordvec only offers vector representations for words not sentences one crude but somewhat effective for some purposes way to go from wordvectors to vectors for longer texts like sentences is to average all the wordvectors together this isnt a function of the gensim wordvec class you have to code this yourself for example with the wordvectors already loaded as wordmodel youd roughly do real code might add handling for when the tokens arent all known to the model or other ways of tokenizingfiltering the text and so forth there are other more sophisticated ways to get the vector for a lengthoftext such as the paragraph vectors algorithm implemented by gensims docvec class these dont necessarily start with pretrained wordvectors but can be trained on your own corpus of texts
44693241,how to extract a word vector from the google pretrained model for wordvec,python filehandling gensim wordvec,use the following code to extract the word vector from the google trained model for wordvec result vector your system is freezing because of the large size of model try using system with more memory or you can limit the size of model you are loading limit model size while loading
43146420,gensim error while loading pretrained docvec model,python gensim docvec,gensim will generally try to support loading of models saved from older versions into newer versions but the reverse is a much harder problem and will only work sometimes so upgrade the environment where you want to load the model to to match where it was trained or try the mostrecent version but dont try to move models backwards
39549248,how to load a pretrained wordvec model file and reuse it,python file model wordvec gensim,just for loading now you can train the model as usual also if you want to be able to save it and retrain it multiple times heres what you should do
36815038,how to load pretrained model with in gensim and train docvec with it,python gensim wordvec docvec,docvec does not need wordvectors as an input it will create any wordvectors that are needed during its own training and some modes like pure dbow dm dbowwords dont use or train wordvectors at all seeding a docvec model with wordvectors might help or hurt theres not much theory or published results to offer guidance theres an experimental method on wordvec intersectwordvecformat that can merge wordveccformat vectors into a model with an existing vocabulary but youd need to review the source to really understand its assumptions
29591581,gensim wordvec augment or merge pretrained vectors,python gensim keyerror wordvec,avoiding the key error is easy the more difficult problem is merging a new word to an existing model the problem is that wordvec calculates the likelihood of words being next to each other and if the word yogurt wasnt in the first body that the model was trained on its not next to any of those words so the second model would not correlate to the first you can look at the internals when a model is saved uses numpysave and i would be interested in working with you to come up with code to allow adding vocabulary
78373468,input shape error when updating pretrained cnn from binary classification to multiclassification,python tensorflow keras deeplearning convneuralnetwork,your label data is not categorical modify getfeaturesandlabels return output to
78253997,vision transformers runtimeerror mat and mat shapes cannot be multiplied x and x,python machinelearning deeplearning pytorch transformermodel,if you look into the source code of visiontransformer you will notice in this section that selfheads is a sequential layer not a linear layer by default it only contains a single layer head corresponding to the final classification layer to overwrite this layer you can do
78213696,after loading a pretrained pytorch pt model file modulenotfounderror no module named models,python deeplearning pytorch yolo yolov,in the repository you linked there is a zip file called source code which contains models and some other helper modules i was able to load the model in colab by downloading the zip expanding it to a directory in my google drive called yolov moving yolovtinypt to this directory and then running the following from googlecolab import drive drivemountcontentdrive import torch import onnx import sys syspathappendcontentdrivemy driveyolov model torchloadcontentdrivemy driveyolovyolovtinypt
78183172,how to train a model in sagemaker via transformers library,deeplearning pytorch boto amazonsagemaker,you can also run the training using pytorch estimator and can have full control on which library to use please refer the below example to use pytorch estimator and install any additional libraries using requirementstxt
77885918,why finetuning mlp model on a small dataset still keeps the test accuracy same as pretrained weights,python machinelearning deeplearning pytorch neuralnetwork,you have to reinitialize the optimizer with the new model the modelfinetune object currently as i see it in your code it seems to still use the optimizer which is initialized with your old model weights modelparameters
77781734,sagemaker batch transformer with my own pretrained model,python machinelearning deeplearning amazonsagemaker mlops,this error message indicates python command was invoked with wrong file path instead of python script file path rather than the error within the python script for example if you hit following command in local environment you will get exact same error message please investigate where python command is executed and check if you are feeding correct python script path
77762264,tf transformer model never overfits and just plateaus interpretation of this training curve and suggestions for improvement,python machinelearning keras deeplearning transformermodel,a few things come to my attention you work with images or d data cnns are superb with at least for local information extraction multiple kernels allow to extract many different aspects features in one go which is nice but the receptive field is small very deep layers would solve this but come with new problems you therefore use self attention mha with qkv which is a nice approach to allow the network gain a maximum receptive field information regardless of its spatial location is weighted according to the q k v multiplication inside of mha when reading cv papers resolutions of can be considered rather large and authors will always downsample the spatial dimensions before applying costly operations like selfattention before a dog can be classified a network will have to understand colors lines fur eys legs and finally a dog at the moment you waste the computation to perform something a cnn is very good at understanding high frequency features local features since selfattention is expensive one uses far less heads you chose than people would choose features in a cnn for doing the semantically the same understand the shallow basics of the image i therefore highly recommend to down sample the data first with cnn layers which are much more sparse in terms of parameters only then when the spatial dimension is reduced use selfattention to find connections to distant locations that are important for your task the parameters saved by down sampling can be used to increase the number of heads each head can only understand a single concept similar to the number of channels in cnns depending on the complexity of your task you will certainly need more than numbers between are better suited if you can afford these numbers in terms of computation coming back to your learning curves from my point of view your network might even have many parameters due to the mha but they are currently used in way the actual capacity is comparable low this is the reason why your network shows no sign of over fitting which is nicely explained in the answer by mrk he also explained both approaches to enforce overfitting reduce data or increase the capacity bot allows you to verify that your network can over fit currently that does not happen my recommendations use a feature extractor cnn for down sampling and local feature extraction later apply selfattention to allow the network to understand how higherlevel information is related across the spatial dimension provide more information on the topic youre working on so we can give better feedback on loss general approach etc
77090888,implement dropout to pretrained resnet model in pytorch,machinelearning deeplearning pytorch resnet finetuning,the issue in your code is that you are appending the nndropout layers to the featslist within the loop that iterates over the models modules however when you append a nndropout layer to featslist it immediately gets added to the end of the list causing the loop to continue iterating over it so you are appending lot of dropout layers at the end of your architecture the following code contains a loop that runs through all the layers of the pretrained network and if it encounters a convolutional layer it creates an exactly equal one and appends it to the list followed by a dropout layer otherwise it appends the layer as is without adding dropout layers
77035667,transformerencoderlayer has nondeterministic random output,deeplearning pytorch transformermodel,i forgot that weight initialization was random and needed to be fixed by setting a seed
76682993,how to use pretrained encoder for customized unet,python deeplearning pytorch pretrainedmodel unetneuralnetwork,yes it is possible to use only a pretrained block instead of using the entire network such as resnet from torchvision since you mentioned a custom encoder based on a vggtype block im answering based on that instead of defining the layers in the vggblock manually you can just call the pretrained vgg network within that class and then select up to the nd conv layer first you would need to get the pretrained vgg network from torchvision then you can modify your vggblock by the following i also modified your unet class a bit and this is the modified code you would notice that both in the vggblock and in the unet class i skipped the use of middlechannels as you did in your snippets that input argument is actually irrelevant since your middlechannels and outchannels are essentially the same the above code would build you the exact unet architecture that you posted in the question with pretrained weights
76577467,pytorch is not working with trained modelpretrained model intel open vino,python opencv deeplearning pytorch openvino,i tested and compared your model with openvino pretrained model pedestriandetectionadas since you mentioned that this is the reference to your dl model development i inferred both model with object detection python demo i get this from omz repo my finding is that your model does not have the correct wrapper as the ov modelthe pedestriandetectionadas uses network based on ssd framework with tuned mobilenet v as a feature extractor perhaps this is the part that you need to cater to your custom model
75964691,bertvocabbertvocabfromdataset returning wrong vocabulary,python tensorflow deeplearning tokenize bertlanguagemodel,solved it was just the npgenfromtxt non using t as delimiter by default
75737232,loss is nan for segformer vision transformer trained on bddk,python tensorflow machinelearning deeplearning segformer,update ive published a starter notebook on kaggle with bdk dataset by using huggingface and kerascv library bddk huggingface kerascv segmentation as there is no reproducible code it is hard to spot the main issue but ive successfully trained segformer model with bddk dataset on semantic segmentation task im sharing the solutions ive used data and tested on kaggle environment bddkdataset here is the complete code to make the test more complete ive used this dataset on huggingface model segformer and also with open source segmentation model unetefficientnetb one of the key difference perhaps in the above gist is how the data is being processed the functionality likes datasetdict defaultdatacollator autoimageprocessor are abset in the above gist only tfsegformerforsemanticsegmentation is present it is better to perform data processing by looking at the code rather than relying on some auto tools another difference for the huggingfacae model is we cant pass custom or builtin loss function also metrics that is real unfortunate in their doc they showed some approach to evaluate the model but the procedure is for torch model to realize on this issue it would be better to reach huggingface forum here is the full code and below are some highlights preprocess and dataloader bddk segformer huggingfacetransformer some resource stanfordbackgroundsceneunderstandingstarter
75684685,failure to install old versions of transformers in colab,python machinelearning deeplearning googlecolaboratory transformermodel,colab has recently upgraded to python there is a temporary mechanism for users to run python runtime linuxxwithglibc platform this is available from the command palette via the use fallback runtime version command when connected to a runtime the issue can be tracked here
75672816,how does gptlike transformers utilize only the decoder to do sequence generation,deeplearning pytorch gpt textgeneration,the input for a decoderonly model like gpt is typically a sequence of tokens just like in an encoderdecoder model however the difference lies in how the input is processed in an encoderdecoder model the input sequence is first processed by an encoder component that produces a fixedsize representation of the input often called the context vector the context vector is then used by the decoder component to generate the output sequence in contrast in a decoderonly model like gpt there is no separate encoder component instead the input sequence is directly fed into the decoder which generates the output sequence by attending to the input sequence through selfattention mechanisms in both cases the input sequence is typically a sequence of tokens that represent the text data being processed the tokens may be words subwords or characters depending on the specific modeling approach and the granularity of the text data being processed
75092062,deeplabv without pretrained backbone will it the reason for bad dsc,deeplearning pytorch imagesegmentation trainingdata deeplab,you can try to use a few x convolutional layers on d volumes of images keeping dimensions h and w of the features constant and then convert such tensor to channel tensor using x convolutional layer now you will have a tensor of same height and width of the channels and you can use the pretrained models for reference check here
74868664,resourceexhaustederror only when finetuning a efficientnetvl in tensorflow,python tensorflow machinelearning deeplearning,the vl is a large model mb and so i think its normal to face resourceexhaustederror it depends on your gpu whether it can take it or not so the simple answer would be to use better accelerator however here are some common approach you can try but its not guaranteed use smaller input unfreeze not all layers but few etc enable mixedprecision configure jit compilation set memory growth for physical device gpu if possible use tpu accelerator freely available on kaggle and colab to set up tpu check this codeexample devicesection it might be helpfulalso check this ticket see the feature request section here you can find a gist you can use it to find the optimal batch size for training
74667517,invalid shape error when trying to leverage kerass vgg pretrained model,python keras deeplearning vggnet,the categoricalcrossentropy loss for classes together with the batch size of dictate the shape of labels for each bach to be the labels are currently ordinal and one can use the sparsecategoricalcrossentropy loss for ordinal labels alternatively one can still use the categoricalcrossentropy loss but in conjunction with the onehot encoded labels for for and for the following code snippet can accomplish such an encoding the nature of data ordered or unordered helps determining whether onehot encoding is preferred or ordinal
74535731,extracting features from a pretrained model using hook function,python imageprocessing deeplearning pytorch,looking at the link you provided the function createrplusd returns the following your object selfrplusd is already a net instance so your line is basically like calling net twice you probably only have to call it like that and it should work let me know if this helps
73853604,how to include to a transformer model,python tensorflow deeplearning,to include the positional encoding we need to add it to the input sequence each timestep in the input sequence will get a twoelement vector as the additional input we will use a custom layer to create and add the positional encoding to the input sequence example
73121935,printing the size of the input and output of all the layers of a pretrained model,python deeplearning model pytorch pretrainedmodel,you can use hooks to print the shape of the input and the output of each layer you can use this code to do what you want example a snippet of the output includes a layer that is defined before temporalnorm layer in block module but called or executed later norm
73107859,accessing a module inside of a block in a pretrained model,python deeplearning pytorch pretrainedmodel,having access to those blocks you can easily proceed with accessing the submodules via the dot notation assuming those blocks are custom nnmodule ie they are not subscriptable and the bracket notation cant be used for instance with block n
73102541,accessing a specific layer in a pretrained model in pytorch,python deeplearning pytorch pretrainedmodel,to extract the intermediate output from specific layers you can register it as a hook the example is showed by the snipcode below import torch from timesformermodelsvit import timesformer model timesformerimgsize numclasses numframes attentiontypedividedspacetime pretrainedmodelpathtopretrainedmodelpyth activation def getactivationname def hookmodel input output activationname outputdetach return hook modelmodelblocksregisterforwardhookgetactivationblock modelmodelblocksregisterforwardhookgetactivationblock modelmodelblocksregisterforwardhookgetactivationblock x torchrandn output modelx blockoutput activationblock blockoutput activationblock blockoutput activationblock to remove the last two layers you can replace them with identity modelnorm torchnnidentity modelhead torchnnidentity
73102413,loading a modified pretrained model using strictfalse in pytorch,python deeplearning pytorch pretrainedmodel,strict false is to specify when you use loadstatedict method statedict are just python dictionaries that helps you save and load model weights for more details see if you use strictfalse in loadstatedict you inform pytorch that the target model and the original model are not identical so it just initialises the weights of layers which are present in both and ignores the rest see so you will need to specify the strict argument when you load the pretrained model weights loadstatedict can be called at this step if the model for which weights must be loaded is selfencoder and if statedict can be retrieved from the model you just loaded you can just do this for more details and a tutorial see
71641343,extracting nested layer features from a pretrained model with keras sequential api,python tensorflow keras deeplearning computervision,following minnats comment so far i could solve the issue with keras functional api note that the number of parameters remains unchanged and retraining and the following code to extract the intermediate features for class activation map worked
71396540,how to save and load custom siamese bert model,python tensorflow keras deeplearning bertlanguagemodel,try using tfsavedmodelsave to save your model the warning you get during saving can apparently be ignored after loading your model you can use it for inference ftestdata
71301220,unexpected layer count when loading pretrained keras model,python tensorflow machinelearning keras deeplearning,interestingly the error comes from the absence of an input layer this for example would work so maybe try something like this also using the parameters inputshape and inputtensor does not make much sense
70787151,onnxload albert throws decodeerror error parsing message,python deeplearning onnx quantization onnxruntime,the problem was with updating the config variables for my new model changes configsoutputdir albertbasevmrpc configsmodelnameorpath albertbasevmrpc i then came across this separate issue where i hadnt git cloned my model properly question and answer detailed here lastly huggingface does not have an equivalent to bertoptimizationoptions for albert i had tried general pytorch optimisers offered by torchoptimizer on the onnx model but it seems that they arent compatible for onnx models feel free to comment for further clarification
70661251,unable to load custom pretrained weight in pytorch lightning,deeplearning pytorch pytorchlightning,it can be that your pth file is already a statedict try to load pretrained weight in your lightning class
70290586,error in keras model for classification model with transformers,tensorflow keras deeplearning neuralnetwork classification,disclosure i came here for the bounty then i tried on colab and everything worked fine next i read the comments this question is a joke in its current state there is no way to reproduce it and at this point i agree but as i am a hans in luck and obviously have to much time procrastinating i started pycharm following the ops cue no when i paste it to my pycharm i get the above error but this also worked for me which makes me wonder whether you have touched something so i am happy to provide an untouched working version for you also to make sure that we are talking of the same package versions i used numpy and tensorflow try with these versions or let me know in case you used different versions
70279801,how to run pytorch bert with amd,python deeplearning pytorch bertlanguagemodel amdgpu,thank you to chrispresso amd rocm seems to be the way to go but it requires one to run under linux
70245957,evaluating pretrained tensorflow keras model using various loss functions,python tensorflow keras deeplearning,you can use multiple loss functions without recompiling all you have to do is assuming first loss method as loss second as loss sorry about the inconvenient writing of code im new here
70197274,pretrained lightningbolts vae not doing proper inference on training dataset,deeplearning pytorch autoencoder pytorchlightning,first the the docs you show is for the ae not the vae the results for the vae look much worse second the docs state both input and generated images are normalized versions as the training was done with such images so when you load the data you should specify normalizetrue when you plot your data you will need to unnormalize the data as well which gives something like this without normalization it looks like
70118623,valueerror after attempting to use onehotencoder and then normalize values with makecolumntransformer,python pandas tensorflow deeplearning onehotencoding,using onehotencoder is not the way to go here its better to extract the features from the column time as separate features like year month day hour minutes etc and give these columns as input to your model the issue here is coming from the onehotencoder which is getting returning a scipy sparse matrix and get rides of the column time so to correct this you must retransform the output to a pandas dataframe and add the time column one way to countournate the memory issue is generate two indexes with the same randomstate one for the pandas data frame and one for the scipy sparse matrix use the pandas data frame for the minmaxscaler ct makecolumntransformerminmaxscaler time ctfitxtrainpd resulttrain cttransformxtrainpd resulttest cttransformxtestpd use generators for load data in train and test phase this will get ride of the memory issue and include the scaled time in the generators def nnbatchgeneratorxdata ydata scaled batchsize samplesperepoch xdatashape numberofbatches samplesperepoch batchsize counter index nparangenpshapeydata while true indexbatch indexbatchsize counterbatchsize counter scaledarray scaledindexbatch xbatch xdataindexbatch todense ybatch ydatailocindexbatch counter yield nparraynphstacknparrayxbatch scaledarray nparrayybatch if counter numberofbatches counter def nnbatchgeneratortestxdata scaled batchsize samplesperepoch xdatashape numberofbatches samplesperepoch batchsize counter index nparangenpshapexdata while true indexbatch indexbatchsize counterbatchsize counter scaledarray scaledindexbatch xbatch xdataindexbatch todense counter yield nphstackxbatch scaledarray if counter numberofbatches counter finally fit the model history btcmodelfitnnbatchgeneratorxtrain ytrain scaledresulttrain batchsize stepsperepochtodetermine batchsize epochs callbackscallback btcmodelevaluatennbatchgeneratorxtest ytest scaledresulttest batchsize batchsize stepstodetermine ypred btcmodelpredictnnbatchgeneratortestxtest scaledresulttest batchsize stepstodetermine
69947553,how to manually load pretrained model if i cant download it using tensorflow,tensorflow keras deeplearning transferlearning vggnet,youre using loadmodel on weights instead of a model you need to have a defined model first then load the weights
69909781,how to understand the results of training a neural network type transformer bert,python tensorflow deeplearning neuralnetwork pytorch,the loss starts at which is arbitrary because the first epoch is a randomisation of the weights and so you would be extremely lucky to be accurate early on the learning rate you supply to trainingarguments is just the initial learning rate the training method adapts this automatically the learning rate changing indicates that the initial rate may be too high or too low and the method is adapting to prevent overfitting or underfitting the data based on the returned loss and accuracy of each epoch the accuracy and loss are good measures to track across the epochs less loss is better more accuracy is better if you also had an accuracy measure you could compare accuracy to evalaccuracy and if the evalaccuracy becomes higher than the accuracy then you are starting to overfit the data
69620683,explain x tfkeraslayersdense activationrelupretrainedmodeloutput,tensorflow deeplearning computervision artificialintelligence,in the first line you define inputs to be equal to the inputs of the pretrained model then you define x to be equal to the pretrained models outputs after applying an additional dense layer tensorflow now automatically recognizes how inputs and x are connected if we assume the the pretrained model consists of the five layers pretrainedin pretrainedh pretrainedh pretrainedh pretrainedout this means that tensorflow realizes that the information will take the following way inputs pretrainedin pretrainedh pretrainedh pretrainedh pretrainedout newdenselayer x if we now take the final layers into account we will have the following information flow inputs pretrainedin pretrainedh pretrainedh pretrainedh pretrainedout newdenselayer x denselayersoftmax outputs now the model tfkerasmodelinputsinputs outputsoutputs statement just tells tensorflow that it is supposed to treat this information flow as a new model so that you can easily pass new information through all of these layers by just using this new model edit you asked why dense is followed by two brackets the layersdense call is actually not the function that processes your data instead if you call tfkeraslayersdense tensorflow basically creates a new dense layer and returns it to you which you can then use to process your data you could actually write this in two lines to make it more clear
69424452,get a dim feature vector from a pretrained model without fine tuning in pytorch,deeplearning neuralnetwork pytorch convneuralnetwork featureextraction,you could apply a principal component analysis pca on your features retrieved from the pretrained model to reduce the dimensionality to components
69302666,how to calculate flops of transformer in tensorflow,python tensorflow deeplearning profiler,the graph should be the tfgraph of the model that you are profiling see here for more information about tensorflow graphs and here for tensorflow profiler tutorials and examples
69223955,building a neural network for binary classification on top of pretrained embeddings not working,python machinelearning deeplearning neuralnetwork pytorch,you are only printing from the second iteration the above will effectively print for every k steps but i starts at ie one gradient descent step has already occurred this might be enough to go from the initial loss value log to the one you observed
69084540,how to write a forward hook function for nntransformer in pytorch,python machinelearning deeplearning pytorch hook,your hook will call your callback function with tuples for x and y as described in the documentation page of torchnnmoduleregisterforwardhook it does quite explain the type of x and y though the input contains only the positional arguments given to the module keyword arguments wont be passed to the hooks and only to the forward define your callback hook to your nnmodule do an inference
69004513,changing a custom resnet architecture subtly and still use it in pretrained mode,python dictionary deeplearning pytorch computervision,if you really want to do this you should construct the model and then call loadstatedict with the argument strictfalse keep in mind that a you should initialize any new layers you added explicitly because they wont be initialized by the state dict and b the model will probably not work out of the box because of the uninitialized weights but it should train faster than a randomly initialized model
68980724,the number of classes in pytorch pretrained model,python deeplearning pytorch convneuralnetwork imageclassification,you have to change the final linear layer of the respective model for example in the case of resnet when we print the model we see that the last layer is a fully connected layer as shown below fc linearinfeatures outfeatures biastrue thus you must reinitialize modelfc to be a linear layer with input features and output features with modelfc nnlinear numclasses for other models you can check here to freeze the parameters of the network you have to use the following code to validate note that for this example fc was the name of the classification layer this is not the case for other models you have to inspect the model in order to find the name of the classification layer
68951828,transformers longformer indexerror index out of range in self,python deeplearning pytorch,i have managed to fix this by reindexing my positionids when pytorch was creating that tensor for some reason some value in positionids was bigger than i used to create positionids for the entire batch bear in mind that it might not be the best solution the problem might need some more debugging but for a quick fix it works
68893554,how to freezeunfreeze a pretrained model as part of a subclassed model in tensorflow,tensorflow keras deeplearning,this is happening because one of the fundamental differences between the subclassing api and the functional or sequential apis in tensorflow while the functional or sequential apis build a graph of layers think of it as a separate data structure the subclassing model builds a whole object and stores it as bytecode this means that with subclassing you lose access to the internal connectivity graph and the normal behaviour that allows you to freezeunfreeze layers or reuse them in other models starts to get weird seeing your implementation i would say that the subclassed model is correct and it should be working if we were dealing with a library other than tensorflow that is francois chollet explains it better than i will ever do in one of his tweettorials
68874306,why there is error while predicting using inceptionv pretrained model,python deeplearning neuralnetwork,try this the first parameter is the number of samples that you will give so the model took the first as the number of samples then for the first sample the model expects the inputshape but it found for that you should provide the number of samples in the first dimension then the inputdimension
68849347,batch size reduces accuracy of ensemble of pretrained cnns,python deeplearning pytorch computervision,youre forgetting to call modeleval modeltodevice modeleval total correct with torchnograd for images labels in tqdmtestloader images labels imagestodevice labelstodevice outputs modelimages as your model has batchnorm layers batchsize is particularly degrading the preprocessing should also follow the one used for training as you can see in the repository of the author of the model you should normalize using the following statistics testtransform transformscompose transformstotensor transformsnormalizemean std
68797901,training an transformer encoder layer directly and the proper way to pad sequences,deeplearning pytorch transformermodel,my sequences have lengths varying between as little as to as many as does this mean that i should pad all my sequences to have parts no need the main property of transformer is that the sequence lengths are changeable if you look at the dot product or multi head attention formula you can see that so no need for padding for the attention mask i believe that i want each part to attend to all other parts in the sequence in the docs i see that they have it set up such that each part is only allowed to attend to earlier parts in the sequence is this the most natural approach or is it just for the language modeling task attention mask is for learning sequential generation you can assume that the transformer is like a rnn and will generate a sequential data one token at a time thats why the mask is used in the transformer decoder if that does not apply to you problem you can skip it the mask being inf or depends on where you apply it in dot product attention
68785255,i would try change channel for keras pretrained model,python tensorflow machinelearning keras deeplearning,you simply have to embed xception in the correct way in your new model we create a new input layer than we operate upsampling and in the end we pass all to xception here is the running notebook if you are interested
68694221,how to train only rpn for torch vision faster rcnn with pretrained backbone,python deeplearning pytorch computervision torchvision,you can do the following
68537629,when modifying a pretrained model in pytorch does the old weight get reinitialized,machinelearning deeplearning neuralnetwork pytorch,if you are redefining some of the layers which you seem to be doing with modelconvstem and modelfc then yes those will be randomly initialized thus meaning the loaded weights will no longer be used on those layers the rest of the model will of course stay untouched and will use the loaded weights
68536988,transformer model in tensorflow learns to only predict the end of sequence token,python tensorflow machinelearning keras deeplearning,i found the answer and its pretty simple actually the model is underfitting the data i originally decided to only use one encoder and one decoder in my transformer because i want to run it on mobile but i switched to the full size which is encoders and decoders and attention heads which created a much bigger model now it still passes through the phase of only predicting the end token but then starts predicting the outofvocabulary token too and then suddenly it starts to experiment with more and more less frequent words here is the beginning of epoch you can see its much more sentencelike now the model will still near accuracy towards the end of the epoch which i cant comprehend so if anyone has answers for that let me know it just seems unbelievably high even though it starts to do pretty well
68486156,do i need gpu while working with pretrained model,python tensorflow deeplearning gpu cpu,it depends how many predictions you need to do usually in training you are making many calculations therefore parallelisation by gpu shortens overall training time usually when using a trained model you just need to do a sparse prediction per time unit in such situation cpu approach should be ok however if you need to do as many predictions as during training then gpu would be beneficial this can particularly be true with reinforcement training when your model must adopt to continuously changing environmental input
68477306,positional encoding for time series based data for transformer dnn models,python tensorflow deeplearning pytorch transformermodel,positional encoding is just a way to let the model differentiates two elements words thatre the same but which appear in different positions in a sequence after applying embeddings in a lm language model for example we add pe to add an information about position of each word are the positional values added directly to the actual values of the elements in the sequence or to the word representation values or are they concatinated is the positional embedding part of the data preprocessing stage yes pe values are just added directly to actual values embeddings in a lm this will results that the embedding vector of the word a that appears in the beginning of the sequence will be different of the embedding vector of the same word that appears in the middle of the sequence and no pe is not a part of data preprocessing stage heres an example of code class positionalencodinglayernnmodule def initself dmodel maxlen superpositionalencodinglayer selfinit selfdmodel dmodel selfmaxlen maxlen def getanglesself positions indexes dmodeltensor torchfloattensorselfdmodeltopositionsdevice anglerates torchpow indexes dmodeltensor return positions anglerates def forwardself inputsequences param tensorbatchsize seqlen inputsequences return tensorbatchsize seqlen dmodel positionencoding positions torcharangeinputsequencessizeunsqueezetoinputsequencesdevice seqlen indexes torcharangeselfdmodelunsqueezetoinputsequencesdevice dmodel angles selfgetanglespositions indexes seqlen dmodel angles torchsinangles apply sin to even indices in the tensor i angles torchcosangles apply cos to odd indices in the tensor i positionencoding anglesunsqueezerepeatinputsequencessize batchsize seqlen dmodel return positionencoding class inputembeddingandpositionalencodinglayernnmodule def initself vocabsize maxlen dmodel dropout superinputembeddingandpositionalencodinglayer selfinit selfvocabsize vocabsize selfmaxlen maxlen selfdmodel dmodel selfdropout nndropoutpdropout selftokenembedding nnembeddingvocabsize dmodel selfpositionencoding positionalencodinglayerdmodeldmodel maxlenmaxlen def forwardself sequences param tensorbatchsize seqlen sequences return tensorbatchsize seqlen dmodel tokenembedded selftokenembeddingsequences batchsize seqlen dmodel positionencoded selfpositionencodingsequences batchsize seqlen dmodel return selfdropouttokenembedded positionencoded batchsize seqlen dmodel does the tensorflowkeras multiheadattention layer actually already contain an embeeding layer that takes care of the positional encoding or not simply no you have to build pe yourself what about the normalization of data are only the actual element values normalized and then the positional encoding is added to that normalized value or is the positional encoding value added to the raw value of the element and the resulting values are normalized the normalization part is at your discretion you do what you want but you should apply the normalization also pe is added to normalized values not actual one
68259714,get the output of the last convolutional layer of a pretrained architecture in subclassed keras model for gradcam,python tensorflow machinelearning keras deeplearning,to achieve what you need we can do something like as follows trainable model gradcam model compute gradient
68240362,combining a pretrained model with a custom model in tf,tensorflow machinelearning keras deeplearning computervision,a simple google search for transfer learning will have transfer learning and finetuning as the first result i suggest that you read it first as it has exactly what you are trying to do basically you will use inceptionv as you would do a normal layer inside your buildsiamesemodel function that returns your entire model something like this will do again you should read the documentation to understand how to properly instantiate a pretrained model and handle batch normalization layers
68152276,using roberta model cannot define the model compile or summary,python keras deeplearning bertlanguagemodel robertalanguagemodel,roberta is based on pytorch check out the helper function tfrobertamodel to convert it to a tensorflow model
68088523,evaluation with pretrained model results in type error,tensorflow keras deeplearning convneuralnetwork multilabelclassification,what you want is for your input have shapeif this is what the was for the training images you used to train your model next question is was your model trained on rgb or bgr images cv reads in images as bgr if your model was trained on rgb images then you need to convert the bgr to rgb with next question were the images your model was trained on have the pixel values scaled usually they are scale with if the training images were scaled you need to scale the input image finally to get the the right shape use this adds the extra dimension needed by modelpredict
68087780,pytorch transformer argument dimfeedforward,deeplearning pytorch,positionwise or pointwise means the feed forward network ffn takes each position of a sequence say each word of a sentence as its input so pointwise ffn is a shared ffn that inputs each word one by one and thats right it is neither input features determined by the self attention sublayer nor output features the same value as input features it is actually the hidden features the thing is this particular ffn in transformer encoder has two linear layers according to the implementation of transformerencoderlayer implementation of feedforward model selflinear lineardmodel dimfeedforward factorykwargs selfdropout dropoutdropout selflinear lineardimfeedforward dmodel factorykwargs so dimfeedforward is the feature no of hidden layer of the ffn usually its value is set to be several times larger than dmodel as default
68047331,how to add the last classification layer in efficienet pretrained model in pytorch,deeplearning pytorch convneuralnetwork transferlearning imageclassification,torchvision includes efficientnet and it does have a classifier attribute to get the infeatures
67662241,upgraded to tensorflow now get a lambda layer error when using pretrained keras applications models,tensorflow machinelearning keras deeplearning keraslayer,im not sure whats the main reason for your issue as its not reproducible generally but here are some notes about that warning message the traceback shown in your question is not from resnet but from efficientnet now we know that the lambda layer exists so that arbitrary expressions can be used as a layer when constructing sequential and functional api models lambda layers are best suited for simple operations or quick experimentation while it is possible to use variables with lambda layers this practice is discouraged as it can easily lead to bugs for example its because the mylayer layer doesnt trace the tfvariables directly and so that those parameter wont appear in mylayertrainableweights in general lambda layers can be convenient for simple stateless computation but anything more complex should use a subclass layer instead from your traceback it seems like there can be such a possible scenario with the stepconv layer quick surveying on source code of tfcompatvnnconvd lead to a lambda expression that might be the cause
67572091,how can i load pretrained model by pytorch mmfashion,python deeplearning pytorch pretrainedmodel,lets say if you downloaded weights for wideresnet and you performing same task that the weights you downloaded trained then and then the parameters you downloaded can be load as load model state dict
67514943,how to add layers to a pretrained model in pytorch,python deeplearning pytorch,when printing a model pytorch will print every layer up until it encounters an error in the forward irrespective of whether the model will run on appropriately formatted input data you have a number of issues in your code after loading the backbone googlenet model and hence all layers you have added after this fail to display when printed the immediately obvious issues are you must remove the after selffc nnlinear biasfalse or it will be interpreted as a tuple googlenet has no attribute avgpooling xsize is not subscriptable either use a function call xsize or use xshape flayernorm has no argument elementwiseaffine you need to correct these in order to get the model to run like so
67378194,loss is nan when using keras bert for classification,tensorflow keras deeplearning bertlanguagemodel,i noticed one issue in your code but im not sure if this the main cause better if you can possibly provide some reproducible code in your above code snippet you set sigmoid in your last layer activation with unit which indicate the problem dataset is probably multilabel and thats why the loss function should be binarycrossentropy but you set sparsecategoricalcrossentropy which is typical uses multiclass problem and with integer labels so if your problem data set is a multilabel with the last layer unit then the setup should be more like but if the problem set is a multiclass problem and your target labels are integer unit then the setup should more like as follows
67140627,bert text classification,tensorflow keras deeplearning bertlanguagemodel,maybe try adding precision and recall to a custom callback function so you can inspect whats going on ive added a debug point in pdbsettrace so the process will pause once the first epoch has ended and you can step through each point to investigate the data to pass the validation data to the callback youll need to add something like the below to your fit function
66966981,error when using a pretrained mobilenet after loading datas dimension error with pythonx tensorflow,python keras deeplearning tensorflow transferlearning,try changing the batchinputshape in line by doing this for more information check this link
66633854,differentiable custom integer sampling kernel spatial transformer network,math imageprocessing deeplearning pytorch,for future reference and for those who might have had similar questions to the one i posted ive emailed dr jaderberg one of the authors of the spatial transformer networks about this question and he has confirmed that the gradient wrt the coordinates for integer sampling is so i wasnt doing anything wrong and it was right all along he was very kind in his response and expressed that integer sampling was mentioned in the paper to introduce the bilinear sampling scheme and have given insights into how to possibly implement integer sampling if i really wanted to you could think about using some numerical differentiation techniques eg look at difference of x to its neighbours this would assume smoothness in the coordinates so with great thanks to dr jaderberg im happy to close this question i guess thinking about how id use numerical methods to implement the integer kernel for the sampling function is another challenge for myself but until then i guess the bilinear sampler is my friend
65982245,pretrained model or training from scratch for object detection,deeplearning pytorch convneuralnetwork objectdetection transferlearning,from my experience here are some important points your train set is not big enough to train the detector from scratch though depends on network configuration fasterrcnnresnet can work better to use a pretrained network on the imagenet the domain the network was pretrained on is not really that important the network especially the big one need to learn all those arches circles and other primitive figures in order to use the knowledge for detecting more complex objects the brightness of your train images can be important but is not something to stop you from using a pretrained network training from scratch requires much more epochs and much more data the longer the training is the more complex should be your lr control algorithm at a minimum it should not be constant and change the lr based on the cumulative loss and the initial settings depend on multiple factors such as network size augmentations and the number of epochs i played a lot with fasterrcnnresnet various number of layers and the other networks i recommend you to use maskcnn instead of fasterrcnn just command it not to use the masks and not to do the segmentation i dont know why but it gives much better results dont spend your time on mobilenet with your train set size you will not be able to train it with some reasonable ap and ar start with maskrcnnresnet backbone
65815072,concatenate predictions from two pretrained tensorflow models,python tensorflow keras deeplearning concatenation,based on hkyi s comment the answer is the models are not fully independent as they share vggmodel which is trainable therefore you need to clone the vggmodel before adding it to modelb otherwise the weights loaded to modelb will overwrite the vggmodel weights used in modela use tfkerasmodelsclonemodelvggmodel instead
65627620,custom small cnn has better accuracy than the pretrained classifiers,python deeplearning pytorch convneuralnetwork transferlearning,here is my theory pretraining is useful when you want to leverage already existing data to help the model train on similar data for which you have few instances at least this was the reasoning behind the unet architecture in medical now to me the key is in the notion of similar if your network have been pretrained on cats dogs and you want to extrapolate to weld seam theres a chance your pretraining is not helping or even getting in the way of the model training properly why when training your cnn you get randomly initialized weights whereas using a pretrained network you get pretrainned weights if the features your are extracting are similar across dataset then you get a head start by having the network already attuned to this features for example cats and dogs share similar spatial features visually eye position nose ears so theres chance that you converge to a local minima faster during training since your are already starting from a good base that just need to adapt to the new specific of your data conclusions if the similarity assumptions does not hold it means your model would have to unlearn what he already learned to adapt to the new specifics of your dataset and i guess that would be the reason why training is more difficult and does not give as good result as a blank slate cnn especially if you dont have that much data ps id be curious to see if your pre trained model end up catching up with your cnn if you give it more epochs to train
