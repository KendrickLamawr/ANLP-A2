id,title,tags,answer
79247672,error in getting captum text explanations for text classification,machinelearning pytorch nlp huggingfacetransformers textclassification,you need to slightly change the gradients calculation class also you didnt include forwardfunc into the gradients class constructor so the attribute method was not able to launch the stuff properly i think that using layerintegratedgradients is better for debugging bert in line with this tutorial below please find snippet that works
79005985,seqseq trainertrain keeps giving indexing error,python nlp huggingfacetransformers huggingfacetrainer,size indicates that the dataset your trainer gets when the finetuning starts is empty looking at this and this thread suggests adding removeunusedcolumns false to your trainingargs might resolve the issue so you could give that a try
78949607,trainer huggingface runtimeerror cannot pin torchcudafloattensor only dense cpu tensors can be pinned,nlp huggingfacetransformers,since pinning memory is only available on cpu and not gpu when running on gpu on colab you can just disable it by setting dataloaderpinmemory to false for trainingarguments trainingargs trainingarguments outputdirloracroissantllm dataloaderpinmemoryfalse perdevicetrainbatchsize numtrainepochs savesteps savetotallimit loggingdirlogs loggingsteps
78943401,finetuning a pretrained model with quantization and amp scaler error attempting to unscale fp gradients,python pytorch nlp huggingfacetransformers finetuning,you cant finetune a fpuint model with amp amp uses fp parameters the params are autocast to fp for the forward pass but amp expects the master set of parameters to be fp you also shouldnt finetune a quantized model in the first place the quantization causes all sorts of numerical issues and instability during training what you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model you can find more details here
78865486,spacy matcher with optional suffix in pattern reports multiple matches on same text,nlp spacy matcher,i will say that the behavior youre observing with the spacy matcher is expected and it is not a bug when you use the text op pattern the op operator means that the colon is optional so the matcher will generate both the shorter and the longer match as youve seen explanation pattern text mylabel text op text mylabel some value so for this pattern spacy will try to match mylabel alone because the colon is optional mylabel because the colon can be included therefore you will get two matches mylabel and mylabel now to answer your questions is this the intended behavior or is it a bug this is intended behavior the op operator allows the colon to be optionally matched leading to multiple matches how should i determine that the second match really is just a subset of the first match to determine if one match is a subset of another you can compare the start and end indices of the matches the longer match will have the same start index but a different end index now i wrote a code below even using spacy version see details below now example in code import spacy from spacymatcher import matcher nlp spacyloadencorewebsm doc nlpmylabel some value matcher matchernlpvocab pattern text mylabel text op matcheraddr pattern matches matcherdoc for matchid start end in matches span docstartend printfmatch spantext start start end end now we determine if one match is a subset of another matchessortkeylambda x x x sort by start index then by end index descending filteredmatches lastend for matchid start end in matches if start lastend this is for avoiding adding subsets filteredmatchesappendmatchid start end lastend end for matchid start end in filteredmatches span docstartend printffiltered match spantext now this code will filter out the shorter match and your output will be now will the shorter match always be reported before the longer match i dont think the matches are not guaranteed to be reported in a specific order so to handle this you can sort the matches by their start and end indices as shown in the code example abovenow after sorting you can now filter out matches that are subsets of longer matches another alternative solution if you want to ensure that only the longest match is returned you can change the way you define the pattern pattern text mylabel text op greedy longest note that the greedy flag doesnt change the behavior of matching itself but rather can influence how overlaps are handled in certain custom settings now back to the summary of what i explained the behavior youre seeing is by design due to the optional op operator in addition you can filter out the shorter match by comparing start and end indices of the matches furthermore sorting the matches by start and end indices allows you to keep only the longest nonoverlapping matches
78853409,nllb finetuning error missing dataprefix configuration englishgerman translation,python nlp machinetranslation finetuning fairseq,while i cant help you with the concrete error message you are getting my guess would be issues with structure of the provided json files my personal recommendation would be to finetune nllb in the transformers library specifically using the seqseqtrainer i did this before for multiple models including nllb check out this repository this way the finetuning and inference process for the nllb model is the same as any bilingual model you can find guides for those more easiely with the only exception that you load the tokenizer like so and generate translations like this
78823069,huggingface llm evaluate runtimeerror isintensortensorout only works on floating types on mps for pre macos received dtype long,pytorch nlp applem huggingface huggingfaceevaluate,i ran into a similar issue trying to run facebooks nougat ocr tool the error message mentions macos sonoma and i was on macos ventura upgrading to macos fixed the issue for me
78809281,importerror cannot import name preinit from langchaincoreutils,python pip nlp langchain largelanguagemodel,your import error suggests that the langchaincore module has not been installed you can confirm whether this is the case by checking the output from the following command if it hasnt been installed ensure it is installed via the following command as for using a requirementstxt file simply creating it doesnt automatically install the packages listed within you still have to run the following command in your terminal to install the listed packages finally you dont have to manually create the requirementstxt file by coping and pasting the output from pip freeze you can simply run you can read more about requirement files under the requirements files section of the pip user guide
78773758,indexerror list index out of range when trying to predict from the fine tuned model using hugginface,nlp huggingfacetransformers huggingface finetuning,the error you are encountering is because the trainerpredict method expects a dataset as input but you are passing a single example that has been tokenized into tensors to perform predictions on a single input you need to prepare it similarly to how the dataset was prepared before training and then use the model directly for prediction heres how you can modify your code to make predictions on a single input prepare the input correctly use the model directly for prediction heres the revised code from transformers import automodelforsequenceclassification autotokenizer trainingarguments trainer from datasets import loaddataset import numpy as np import torch define a simple accuracy metric def computemetricsp predictions labels p preds npargmaxpredictions axis return accuracy preds labelsmean load the dataset dataset loaddatasetimdb splittrain smalltraindataset datasettraintestsplittestsizetrain smallevaldataset datasettraintestsplittestsizetest load the tokenizer and model modelname bertbaseuncased tokenizer autotokenizerfrompretrainedmodelname model automodelforsequenceclassificationfrompretrainedmodelname numlabels tokenize the dataset def tokenizefunctionexamples return tokenizerexamplestext paddingmaxlength truncationtrue smalltraindataset smalltraindatasetmaptokenizefunction batchedtrue smallevaldataset smallevaldatasetmaptokenizefunction batchedtrue smalltraindataset smalltraindatasetrenamecolumnlabel labels smallevaldataset smallevaldatasetrenamecolumnlabel labels smalltraindatasetsetformattorch columnsinputids attentionmask labels smallevaldatasetsetformattorch columnsinputids attentionmask labels define training arguments trainingargs trainingarguments outputdirtesttrainer evaluationstrategyepoch perdevicetrainbatchsize perdeviceevalbatchsize numtrainepochs weightdecay initialize the trainer trainer trainer modelmodel argstrainingargs traindatasetsmalltraindataset evaldatasetsmallevaldataset computemetricscomputemetrics train the model trainertrain evaluate the model validationresults trainerevaluate printvalidationresults make a prediction on a single input inputs tokenizerdatasettext paddingmaxlength truncationtrue returntensorspt modeleval set the model to evaluation mode with torchnograd disable gradient calculation outputs modelinputs predictions torchargmaxoutputslogits dim printfpredicted label predictionsitem
78631769,problems with named entity recognition in spacy using german dedepnewstrf pipeline,python nlp spacy,problem is because this model doesnt have function to recognize entities see documentation for dedepnewstrf it has components transformer tagger morphologizer parser lemmatizer attributeruler but no ner for entityrecognizer so it may need to use one of other models decorenewssm decorenewsmd decorenewslg
78586621,huggingface pipeline debug prompt,python nlp huggingfacetransformers,you may use preprocess method and check generated tokenids generally would suggest to more closely look on the code of the method it will explain what is happening with the prompt before model forward pass internally preprocess is calling either for chats tokenizerchattemplate for simple text prompts tokenizerprompttext for example for gpt model default tokenizer outputs tokenids and masks another thing to consider during prompts debug is to look what will happen if youll invert token ids
78575305,attributeerror trainingarguments object has no attribute modelinitkwargs,python nlp huggingfacetransformers largelanguagemodel peft,just replace your trainingarguments constructor with sftconfig constructor and pass this to sfttrainer
78552651,how to fix error when loading custom finetuned model,pytorch nlp huggingfacetransformers largelanguagemodel peft,your directory contains only the files of the peftadapter and the files required to load the tokenizer but the base model weights are missing i assume you have used the savepretrained method from peft this method only saves the adapter weights and config i use a smaller model for my answer and a different task type from peft import loraconfig tasktype getpeftmodel peftmodel from transformers import automodelfortokenclassification from pathlib import path fergusollamabpclv in your case adapterpath bla metallamametallamab in your case basemodelid distilbertdistilbertbaseuncased peftconfig loraconfigtasktypetasktypetokencls targetmodulesalllinear automodelforcausallm in your case model automodelfortokenclassificationfrompretrainedbasemodelid model getpeftmodelmodel peftconfig modelsavepretrainedadapterpath printlistpathadapterpathiterdir sepn output to load your pretrained model successfully you need to load this basemodel weights as well and use the peft model class to load the adapter model automodelfortokenclassificationfrompretrainedbasemodelid model peftmodelfrompretrainedmodel adapterpath you can also merge the adapter weights back with mergeandunload and save it output this way you will be able to load the model without peft and only transformers as you tried in the example code of your question
78375631,valueerror cannot use a compiled regex as replacement pattern with regexfalse,python text nlp,this is a texthero bug triggering a pandas error pandas strreplace now uses regexfalse by default textheros replacedigits function hasnt been updated in two years and doesnt explicitly pass regextrue you should fill a bug report to texthero there are probably several other occurrences of strreplace to fix in there meantime you can patch the library by changing the code to or use a pandas version prior to eg
78344999,stanford nlp annotation pipelineannotate resulting into outofmemoryerror in java,java nlp outofmemory,so found this in stanfordnlp javadoc of void edustanfordnlppipelinestanfordcorenlpclearannotatorpool call this if you are no longer using stanfordcorenlp and want torelease the memory associated with the annotators calling this method itself almost has no impact but the real game changer is calling systemgc below is the fix for this just call clearannotatorpool and gc after annotate this is how used memory is with only stanfordcorenlpclearannotatorpool call note that this is without systemgc call notice that for calls to annotate in a loop the used memory crosses mb and then falls down below mb this is the case when we are letting jvm call gc however when i call both stanfordcorenlpclearannotatorpool and systemgc the results are drastically different note that used memory is within the range of to mb irrespective of whether the annotation pool has been cleared one starts seeing the real of stanfordcorenlpclearannotatorpool around after pipelineannotate hits below is memory utilization observed over hits to pipelineannotate observe that with stanfordcorenlpclearannotatorpool and systemgc the memory utilization hovers just above mb if you rightly understand this only signifies that somehow the jvm default gc execution is not releasing as much memory as when an explicit call is being made i understand there would be timing difference and all which only means that one needs to further research on the default gc and the jdk on being used am on jdk with ide enforcing jdk compliance level the graphs are all almost the same when run on a jdk directly too and how this changes with various gc strategies but then im happy with this and conclude hope this helps someone
78324647,sklearm featurehasher not working on a single column in a dataframe,pandas machinelearning scikitlearn nlp,you were almost there which gives
78307073,langchain agent parsing error with structuredchatagent and wikipedia tool handleparsingerrors hits limit,python nlp openaiapi langchain largelanguagemodel,author of the book developing apps with gpt and chatgpt here i already answered by mail but just in case someone else stumbles upon this question you can find updated code at the updated code looks like this hope this helps
78294720,error when calling hugging face loaddatasetglue mrpc,python nlp huggingface huggingfacedatasets,i tried on my pc and on google colab the strange thing is that on colab it works on my pc it does not anyway a possible workaround is the following if you print it you will see that the dataset is the same it just has a different name
78184077,solution to solve problem different results when run docvec gensim,nlp docvec,these are potentially two different issues with regard to the warning youre seeing the warning is complete and truthful it already describes what youre doing wrong and how to solve it you must set either hs or negative to be positive or else no training will happen in your model negative is an illegal setting and not positive if you want to use docvec you need to either have the negative parameter as a positive integer as with its default value negative or if you want to set negative then you need to enable the alternative hierarchical softmax mode with hs the algorithm will do nothing but error or given nonsense untrained results if you give it illegal configurations as is explained in the q of the gensim project faq other stackoverflow answers the operation of the docvec algorithm naturally allows for variance in the vectors returned by infervector from run to run and if that jitter between inferences is s making a big difference in results there are probably other serious problems in your use of docvec such as insufficient data or bad parameters that you should fix rather than trying to force a false determinism onto your calculations in particular if the model whose changing infervector results was trained not really with the shown parameters negative without enabled hs ignoring the warning that wont work that is the first big problem to solve it will make all inferred vetor random and meaninglfess as opposed to just a little noisy but if after fixing the total failure of training you then insistently want to do the incorrect thing you can force inference determinism as is described in another answer at removing randomization of vector initialization for docvec
78129126,typeerror exception encountered when calling layer embeddings type tfbertembeddings,tensorflow deeplearning nlp bertlanguagemodel transformermodel,it works with transformers
78031519,how to resolve valueerror you should supply an encoding or a list of encodings to this method that includes inputids but you provided label,nlp huggingfacetransformers huggingfacetokenizers peft,turns out the lora model changes the name of the column expected from label to labels in order to fix it you need also needed was the tasktype in the config
77940890,google semantic retriever example error credentials object has no attribute universedomain,googlecloudplatform nlp artificialintelligence largelanguagemodel googleaiplatform,can you try upgrading your google auth it appears that it has some issues with the some of the versions reference issue
77839628,loading encorewebsm results in attributeerror module transformers has no attribute berttokenizerfast,python pip nlp anaconda spacy,i think that there is older version of the transformers in your global environment that cause the problem to avoid version conflict create a new virtual environment using conda activate myenv install scipy check the instalation page download encorewebsm now you can run your code
77830490,spacy import error cannot import name combiningdiacritics from spacylangcharclasses,python pip nlp spacy pydantic,create a new enviroments and install scipy you can use the following command activate myenv install scipy check the instalation page then you can use scipy
77684999,googlepalm notimplementederror need to determine which default deprecation schedule to use within minor releases,nlp langchain largelanguagemodel palmpre,i solved it by upgradown langchain version
77653666,import error in training arguments in colaboratory,python nlp huggingface,there might be an issue with your transformers library installation uninstall the current transformers library reinstall the transformers library install the accelerate library ensure that you have the correct version of pytorch check your pytorch version with the following command the message suggests that you need to install the accelerate library with version or higher you can install it using the following command pip install accelerate u you might want to try installing the transformers library with the torch extra this can be done with the following command pip install transformerstorch
77544604,importing deepspeech error module not found,python deeplearning nlp mozilladeepspeech,latest release of deepspeech was years back in december you are not able to install deepsearch because it only supports python which is outdated if you want to use you can download older version of python separately and install deepsearch
77533488,nlp preprocessing on two columns in data frame gives error,python pandas dataframe nlp nltk,as the error suggests using gmedatedftitle body attempts to find a column in the dataframe under the following key title body no column in your dataframe is called that therefore the code fails if you wish to select multiple columns at once you need to provide them in a list like so gmedatedftitle body for more information head to the documentation page on data selection from a dataframe given your specific example you will need to fix the data selection and then use some string vectorisation something like
77311999,attributeerror when initializing a custom decoder class in tensorflow with nondefault tokenizer,python pythonx tensorflow nlp huggingfacetransformers,solution it was solved by separating the tokenizer from model and removing the tfkeraslayersstringlookup layer as it only works on tensorflow vectorizertokenizer
77210103,why am i getting hidden size error with pytorch rnn,deeplearning pytorch nlp recurrentneuralnetwork onehotencoding,there were main differences between the example code i use and my code which couldnt compute batchfirsttrue passed to the rnn when initiating the model the target preprocessing had to differ from the input preprocessing i am using sparse onehot vectors to encode words and while sparse vectors work in input the target had to be encoded with only the index of the word in the onehot instead of the whole onehot vector thanks erip for help debugging this
77182311,question about datacollator throwing a key error in hugging face,python pytorch nlp huggingfacetransformers huggingfacetokenizers,the actual issue is in your seqseqtrainingarguments which is leading the error in your datacollator reason the trainer is by default removing any unknown columns not present in the models forward method from your data when you are providing a custom datacollator as a result even though each sample in your traindataset has all the keys when you send that to datacollator the trainer automatically removes the unknown columns solution you need to include an argument in your training arguments like the following the removeunusedcolumnsfalse would prevent the default behaviour and youd get the entire data in datacollator this issue would be useful for further reference
77085879,lime gives this error classifier models without probability scores in python,python nlp lime,the limetabularexplainer requires probabilities not predictions so instead of passing clfpredict you need to either pass clfpredictproba or a wrapper function that returns probabilities from features for example based on this tutorial
77082604,error inserting spacytokensspanspan into pandas dataframe,python dataframe nlp spacy spacy,this ended up working for me
77074676,importerror cannot import name deprecated from typingextensions,python pip nlp spacy pythontyping,you should use typingextensions try i also suggest you to upgrade your python version from to or see a relevant answer
77021885,attributeerror str object has no attribute iscontextset,python machinelearning nlp namedentityrecognition flair,turn your text into a sentence first so the underlying code has access to the necessary functions and modify the sentence object
76969521,nlp how to fix that pretrained model paraphrasemultilingualmpnetbasev isnt accurate on some examples,python machinelearning nlp huggingfacetransformers,what youre looking for is called finetuning and hugging face does provide a trainer api the general steps are from the above tutorial ill be using the bertbasecased pretrained model and the yelpreviewfull dataset to finetune both can be found on huggingface as well as pytorch trainer for the actual finetuning the map function requires a function which it uses on every item in this case we want to tokenize only the text key in each item the logging statement above will show keys label and text to keep things simple we dont want the label but we could use it and concat the text onto the end of it as per the tutorial the above line to load the model will display a warning you will see a warning about some of the pretrained weights not being used and some weights being randomly initialized dont worry this is completely normal the pretrained head of the bert model is discarded and replaced with a randomly initialized classification head you will finetune this new model head on your sequence classification task transferring the knowledge of the pretrained model to it here we create the class that contains all hyperparameters outputdir says where to save checkpoints from training and evaluationstrategy sets when to report evaluation metrics in this case we want to do this at the end of every epoch while this covers the basics of finetuning there are many ways to optimize and customize every step in the process for a tutorial with an indepth look into things like the tokenizer and preprocessing adding layers ontop of tasks batchdistributed training hyperparameter customization experimentation and much more you can find it over at
76942574,how to fix memory allocation for bow in python,python memory nlp,at which line it is throwing memoryerror is it cvdataframepddataframe here if yes try to optimize the data type of countdata first like note this is just one way to reduce the overall memory size some more enhancements
76877589,langchain custom output parser not working with conversationchain,python nlp langchain largelanguagemodel,im not sure exactly what youre trying to do and this area seems to be highly dependent on the version of langchain youre using but it seems that your output parser does not follow the method signatures nor does it inherit from basellmoutputparser as it should for langchain to fix your specific question about the output parser try
76869283,program raised stopiteration error when using pattern library on python,python nlp stopiteration,i am posting this as an answer to allow me to nicely format the code suggestions try editing your libraryframeworkspythonframeworkversionslibpythonsitepackagespatterntextinitpy file find the definition of the read method for me this is line then wrap the block with a tryexcept like
76802096,runtimeerror when trying to extract text features from a bert model then using knn for classification,python machinelearning nlp bertlanguagemodel knn,it seems that you are feeding all your data to the model at once and you dont have enough memory to do that instead of doing that you can invoke the model sentence by sentence or with small sentence batches so that you keep the needed memory within the available system resources
76783884,bertbaseuncased install with spacy is not working,python nlp spacy huggingfacetransformers spacytransformers,this is addressed in a discussion on the spacy github repo the explanation of the error is that entrfbertbaseuncasedlg is a spacy x model and you are using x instead of said model you can download and use encorewebtrf which contains transformer models for spacy x
76752935,error while peforming tfidfvectorizer on the training values,python machinelearning scikitlearn nlp,you need to add a specific step after the tfidfvectorizer because the output is a sparse matrix you can create a densetransformer from transformermixin and add it in the pipeline import numpy as np from sklearnbase import transformermixin class densetransformertransformermixin def fitself x ynone fitparams return self def transformself x ynone fitparams return nparrayxtodense you need to make two modifications in your code first you need to select only the v as feature x dfv y dfv and you need to modify the pipeline pipegnb pipeline vect tfidfvectorizer todense densetransformer gnb gaussiannb
76702377,indexerror index out of range in self while implementing transformer model for translation,python pytorch nlp huggingfacetransformers machinetranslation,i went through your code and found out that in the error trace of yours error in forward call of sentenceembedding encoder stage if you add printtorchmaxx before the line x selfembeddingx then you can see that the error is because x contains id that is if the value is greater than then pytorch will raise the error mentioned in the stack trace it means that while you are converting tokens to ids you are assigning a value greater than to prove my point when you are creating englishtoindex since there are three in your englishvocabulary starttoken paddingtoken endtoken are all you end up generating since this value is greater than the lenenglishtoindex length hence you are getting indexerror index out of range in self solution as a solution you can give unique tags to these tokens which is generally prescribed as this will make sure that the generated dictionaries will have the correct sizes please find the working google colaboratory file here with the solution section i added to the englishvocabulary since after a few iterations we get a keyerror hope it helps
76688344,berbaseuncase does not use newly added suffix token,python nlp tokenize bertlanguagemodel sentencetransformers,you need to update the tokenizers vocabulary as such which results in document oldert han
76686270,modulenotfounderror no module named pycaretnlp,python module nlp pycaret,the nlp module has been removed from pycaret x if you want to use nlp you can use pycaret the latest version of pycaret for more information about pycaret x you can find here
76684567,nameerror name simplepreprocess is not defined,python nlp nltk,the error occurs on the last line of your code where you are calling the removestopwords you need to import the simplepreprocess function from the gensimutils module to use it in your code in the removestopwords function you are trying to create a list comprehension to remove stop words however there is a mistake in the code you are using the same variable name tweet for both the input parameter and the list comprehension
76663390,how do i fix the error invalid config override name should start with when using spacy,python nlp spacy,just had to change directory name to remove a hyphen
76625768,importerror cannot import name customllm from llamaindexllms,python nlp llamaindex largelanguagemodel,you need to change your import library change to this
76625249,load accuracy metric with evaluate sometime mistakes happen typeerror nonetype object is not callable,python deeplearning nlp computervision huggingfaceevaluate,nowi find whats wrong with this problemthe evaluate version of my computer is evaluate we should update the version of the evaluate use the code as follow pip install upgrade evaluate
76490589,valueerror when using modelfit even with the vectors being aligned,python machinelearning nlp valueerror naivebayes,i think that the main problem that tfidfvectorizer is able to work with onedimensional text data only as i see it from here thats why when it tries to convert several columns with text data it tries to do it for column names for some reason in your case i see ways how to solve this problem if you want to apply tfidfvectorizer for each column individually it would be better to do it like this for example but if you want to apply one vocabulary for your columns then i would recomment to do it like this
76448287,how can i solve importerror using the with requires when using huggingfaces trainarguments,python nlp importerror huggingfacetransformers huggingface,if youre not particular about which transformers and accelerate version to tie to then do this to use the most uptodate version in google colab then the issue you are having with accelerate should autoresolve itself note underspecifying pip install u transformers instead of pip install transformerspytorch might be easier since thats what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like trainingarguments instead of specifying accelerate to the pip install accelerate if you have no particular need to fixed the version automatically upgrading to the latest version might get you more stability when using the library esp with hottrending libraries that are constantly changing almost daily if further debugging is necessary ie if the above didnt work to check your transformers and accelerate version do this most probably you might have an importerror at the first line if accelerate is not already installed when you installed transformers and then if the first line works and the nd line is not outputting a version then that is the cause of your issue the current versions todate july are out heres an example notebook with the model that you wish to use as per the comments in your question if the error persist after the pip install try restarting the runtime if you cant find the buttons to press to restart try this in the cell restart kernel in google colab then rerun the cells for import
76313592,import langchain error typeerror issubclass arg must be a class,python nlp datascience chatbot langchain,so i was trying it for hours and at last i found a solution hope it helps you first i did this then after this will throw an error but once again i did then it started working
76264711,entsenttext in spacy returns labels instead of the sentence for ner problem,python machinelearning nlp spacy namedentityrecognition,the reason is that calling the below code so remove it from the train function which will also reinitialize all models as a result the parser which performs the sentence splitting will predict the sentence boundaries using a zeroedout softmax layer and will start detecting a boundary after every token so should remove the line that calls begintraining then later when you update the pipe you can remove the sgd parameter and the pipe will create an optimizer internally
76222579,how to fix cuda showing error in google colab,python jupyternotebook nlp machinetranslation,you need to choose gpu from runtime change runtime type and select gpu as accelerator
76185813,how to resolve error in seqeval in ner bert finetuning,nlp huggingfacetransformers namedentityrecognition evaluation huggingfaceevaluate,as indicated by the error message the expected predictions references should be lists of strings not integers for seqeval this makes sense since the seqeval metric is concerned with matching entity spans exactly as indicated by the b i prefixes of the tags so your labellist should map label identifiers to label tags such as o bper iper borg iorg bloc iloc
76156920,error while loading wordvec model using linux but running well in windows,python deeplearning nlp pickle gensim,theres a fair chance that the root cause of the error is some mismatched interpreterlibrary versions if so then by ensuring that youre using the exact same versions of python numpy gensim on the ubuntu system where youre getting an error as youve seen succeed on the windows system it will work on ubuntu as well one other lesslikely possibility might be some corruptiontruncation of the wordvecmodel file its supporting files checking that the files are identical in both places by size and secure checksum could ruleout any problems there
76141118,not sure why my python code that uses spacy to add a phonenumber entity is not working,nlp spacy spacy,the problem was two seperate componentsone constructed with the class entityruler and one constructed with nlpaddpipe the component created with the addpipe method wasnt aware of your patterns using just one method and then adding the patterns to that component did the trick import spacy nlp spacyloadencorewebsm patterns label phonenumber pattern orth shape ddd orth isspace true op shape ddd orth shape dddd ruler nlpaddpipeentityruler beforener ruleraddpatternspatterns doc nlpyou can reach me at for ent in docents printenttext entlabel i read about the different ways to initialize the component here
76056193,tokenclassificationchunkpipeline is throwing error batchencoding object is not an iterator,pytorch nlp huggingfacetransformers torch namedentityrecognition,not sure why the pipeline was coded that way in the blogpost but heres a working version import torch from transformers import autotokenizer automodelfortokenclassification from transformerspipelinestokenclassification import tokenclassificationpipeline modelcheckpoint davlanbertbasemultilingualcasednerhrl tokenizer autotokenizerfrompretrainedmodelcheckpoint model automodelfortokenclassificationfrompretrainedmodelcheckpoint class tokenclassificationchunkpipelinetokenclassificationpipeline def initself args kwargs superinitargs kwargs def preprocessself sentence offsetmappingnone preprocessparams tokenizerparams preprocessparamspoptokenizerparams truncation true if selftokenizermodelmaxlength and selftokenizermodelmaxlength else false inputs selftokenizer sentence returntensorspt truncationtrue returnspecialtokensmasktrue returnoffsetsmappingtrue returnoverflowingtokenstrue return multiple chunks maxlengthselftokenizermodelmaxlength paddingtrue inputspopoverflowtosamplemapping none numchunks leninputsinputids for i in rangenumchunks if selfframework tf modelinputs k tfexpanddimsvi for k v in inputsitems else modelinputs k viunsqueeze for k v in inputsitems if offsetmapping is not none modelinputsoffsetmapping offsetmapping modelinputssentence sentence if i else none modelinputsislast i numchunks yield modelinputs def forwardself modelinputs forward specialtokensmask modelinputspopspecialtokensmask offsetmapping modelinputspopoffsetmapping none sentence modelinputspopsentence islast modelinputspopislast overflowtosamplemapping modelinputspopoverflowtosamplemapping output selfmodelmodelinputs logits outputlogits if isinstanceoutput dict else output modeloutputs logits logits specialtokensmask specialtokensmask offsetmapping offsetmapping sentence sentence overflowtosamplemapping overflowtosamplemapping islast islast modelinputs we reshape outputs to fit with the postprocess inputs modeloutputsinputids torchreshapemodeloutputsinputids modeloutputstokentypeids torchreshapemodeloutputstokentypeids modeloutputsattentionmask torchreshapemodeloutputsattentionmask modeloutputsspecialtokensmask torchreshapemodeloutputsspecialtokensmask modeloutputsoffsetmapping torchreshapemodeloutputsoffsetmapping return modeloutputs pipe tokenclassificationchunkpipelinemodelmodel tokenizertokenizer aggregationstrategysimple pipebernard works at bnp paribas in paris out for reference take a look at how the preproces and the forward functions are coded in the tokenclassificationpipeline class the preprocess should return a generator thats why the forward is expecting a generator and complains typeerror batchencoding object is not an iterator
75908079,valueerror input of layer convd is incompatible with the layer expected minndim found ndim full shape received none,keras deeplearning neuralnetwork nlp convneuralnetwork,just change this bilstm layersbidirectionallayerslstm returnsequencestruedropout because in a convd it expects d
75904923,i am getting error here torchembeddingweight input paddingidx scalegradbyfreq sparse when i call trainertrain function of gpt model,python nlp huggingfacetransformers torch gpt,the error you are experiencing is most likely due to the size of the vocabulary you have set in your gptconfig you have set the vocabsize to but the actual size of the vocabulary in the gpt model is therefore the model is expecting input token ids to be between and but some of the token ids in your training data are outside this range to fix this you should set the vocabsize in your gptconfig to also make sure that the tokenizer you are using is the same as the one used to tokenize your training data if the tokenizer is different the token ids in your training data may not match the expected token ids of the model
75904558,attributeerror list object has no attribute similarity when using dense passage retriever pinecone in haystack python,python database nlp artificialintelligence haystack,i have modified your question for security reasons in any case i think you are instantiating the retriever incorrectly as you can see in the documentation densepassageretrieverinit expects the documentstore parameter which consists of the document store to be queried instead you are incorrectly using the preprocessed documents you should try the following retriever initialization
75849546,problem tokenizing with huggingfaces library when fine tuning bloom,python nlp artificialintelligence huggingfacetransformers,in the original tokenizefunction you were directly tokenizing the dialog key from the examples however this didnt ensure that the dimensions of the input and label tensors were consistent this mismatch in dimensions was causing the error you encountered during training i converted each dialog entry into a single string by joining the text key values in each dialog then i tokenize the dialog strings with proper truncation padding and a specified maximum length this creates tokenized input tensors with consistent dimensions then i shift the inputids by one position this means that the model will learn to predict the next token in the sequence i also clone the shifted inputids to avoid modifying the original tensor in place def tokenizefunctionexamples dialogtexts joinentrytext for entry in dialog for dialog in examplesdialog tokenized tokenizerdialogtexts truncationtrue paddingmaxlength maxlength returntensorspt tokenizedlabels tokenizedinputids clone tokenizedinputids tokenizedinputids tokenizedlabels torchcattokenizedlabels torchfulltokenizedlabelssize tokenizerpadtokenid dtypetorchlong dim return tokenized
75780103,huggingface transformers trainermaybelogsaveevaluate indexerror invalid index to scalar variable,python pytorch nlp huggingfacetransformers huggingface,your issue comes from your computemetrics function as youre using a qa metric with a textgeneration model to fix it replace metric loadsquad with a textgeneration metric for example bleu metric loadbleu and adapt your computemetrics function in consequence def computemetricsevalpred predictions references evalpred predictions tokenizerbatchdecodepredictions references tokenizerbatchdecodereferences references ref for ref in references return metriccomputepredictionspredictions referencesreferences
75643277,how can i solve the error the stopwords parameter of tfidfvectorizer must be a str among english an instance of list or none,nlp topicmodeling tfidfvectorizer,the stopwords youve imported from spacy isnt a list out cast the stopwords into a list and it should work as expected
75595065,huggingface trainer throws an attributeerrornamespace object has no attribute getprocessloglevel,python deeplearning pytorch nlp huggingfacetransformers,first check that this works for you out if it doesnt then most probably the version of transformers you have on cusertransformerlibsitepackagestransformers doesnt match the trainer script you have then try to upgrade your transformers version pip install u transformers if you get the output but when you run your script youre getting the error then most probably the version of your transformers is from a previous version that doesnt have the getprocessloglevel in the trainingarguments as a property add this line at the top of your code and check that the youll get something like out and with that do this to upgrade the library to the right sitepackage location and python binary after upgrading the script should run
75594448,what could be the reason for the transform not found attributeerror in scikitlearns countvectorizer,python scikitlearn nlp,fittransform returns an array you should use fit instead vectorizer countvectorizer vectorizerfit joinintent for intent in intentsvalues
75539599,syntactical error when yacc file is called,perl nlp yacc penntreebank,the only answer is that resurrecting this ancient software is a lost cause
75530862,automodelforquestionanswering valueerror too many values to unpack expected,python nlp huggingfacetransformers huggingface nlpquestionanswering,it seems like they havent cleaned up the config before uploading the configjson contains a numlabels which initializes your model with an output layer output size of you can fix that as follows from transformers import automodelforquestionanswering autotokenizer autoconfig modelid indobenchmarkindobertbasep model with wrong output shape model automodelforquestionansweringfrompretrainedmodelid tokenizer autotokenizerfrompretrainedmodelid printmodelqaoutputs model with correct output shape config autoconfigfrompretrainedmodelid confignumlabels model automodelforquestionansweringfrompretrainedmodelid configconfig tokenizer autotokenizerfrompretrainedmodelid printmodelqaoutputs context sindrom bazex acrokeratosis paraneoplastica question nama sinonim dari acrokeratosis paraneoplastica inputs tokenizerquestion context returntensorspt outputs modelinputs printoutputskeys output
75253479,terminal error from downloadbiospy the following arguments are required wetpaths,python terminal nlp artificialintelligence,the wetpaths argument of the downloadbiospy script refers to the path of a wet file type used by commoncrawl the source code says that it expects a commoncrawl date like or a path to a wetpaths file so you should pass a valid date as an argument eg is the latest crawl for novdec to understand where the wet format comes from and why its used some background information is required web crawls eg those done by commoncrawl were originally stored in the internet archive arc format the web archive warc is a revision to this format that includes additional secondary data like metadata abbreviated duplicate detection events and laterdate transformations since commoncrawl has used the warc format which allows for more efficient storage and processing of the archives the full warc specification can be found here one can think of warc files as providing the raw data from the crawl process by commoncrawl two additional formats are offered namely wet and wat the wat file format contains the metadata about the records stored in the warc format the wet file format contains the extracted plain text from the records stored in the warc format
75251168,bceloss between logits and labels not working,pytorch nlp lossfunction textclassification gpt,binary crossentropy is used when the final classification layer is a sigmoid layer ie for each output dimension only a truefalse output is possible you can imagine it as assigning some tags to the input this also means that the labels need to have the same dimension as the logits having for each logit statistically speaking for output dimensions you predict bernoulli binary distributions the expected shape is when using the softmax layer you assume only one target class is possible you predict a single categorical distribution over possible output classes however in this case the correct loss function is not binary crossentropy but categorical crossentropy implemented by the crossentropyloss class in pytorch note that it takes the logits directly before the softmax normalization and does the normalization internally the expected shape is as in the code snippet
75042153,cant load from autotokenizerfrompretrained typeerror duplicate file name sentencepiecemodelproto,python nlp protocolbuffers huggingface,ilyakam i ran into the same problem with mrmtbasefinetunedwikisql also in a notebook in a virtual environment your solution did almost work i had to add the line protocolbufferspythonimplementationpython so in case your solution does not work try adding the line in the notebook andreas python on ubuntu lts
75029755,error while loading spacy model from the pickle file,pythonx nlp spacy namedentityrecognition,i found one workaround to this error while on spacy i have loaded the pickle model and saved it using then i have updated the spacy version to latest spacy and reloaded the mymodel this time in that case it worked for me without the error
75000381,spacy regex syntaxerror invalid syntax,python nlp spacy,a pattern added to the matcher consists of a list of dictionaries from docs your code written more legibly the first dictionary has three entries but the third entry is malformed each entry to a dictionary should consist of key value but you only have one item which does not fit dictionary syntax along those lines each dictionary describes one token and its attributes something that lowercased is in hello hi hallo cannot ever be punctuation you seem to want to match something like hi hi hello two tokens with the first of them allowing for repetition this would be matched by something like
74972344,snscrape error twitter scrape crashes after a long time giving error,python webscraping twitter nlp twitterapiv,feel like your lucky day because i created an account here just to reply to you in in this issue the module creator explains what happened and how to solve the problem but you just need add toptrue in commands that have twitterscraper or if you are using it from the command line use the flag top i just found out that i could have posted as a guest
74966513,extracting text data from files in different subdirectories raises valueerror substring not found,python machinelearning nlp datascience,the valueerror is raised by the function call index in the line currentkwindex cleantextindexcurrentkw because cleantext does not contain the currentkw that the code is attempting to find it is likely that in one of your files the data and therefore the text that your inputting to result extracttextusingkeywordstext keywords does not contain either indication technique comparison findings or impression so the easiest way to resolve this is to check which file is causing the issue and add the necessary keyword to make this debugging easier you can update the extracttextusingkeywords function to include a try except block to give you a more useful output for the valueerror you can also update other parts of the code to deal with the subsequent issues that will follow as a result of being unable to find the keyword a complete solution is as follows import glob import pandas as pd import re get print all txt file names with directory information out for filename in globiglobcontentsampledatatxt recursive true outappendfilename printfile names out define keywords keywords indication technique comparison findings impression create empty pandas df with keywords as column names df pddataframecolumnskeywords create function to extract text between each of the keywords def extracttextusingkeywordscleantext keywordlist extractedtexts for prevkw currentkw in zipkeywordlist keywordlist try prevkwindex cleantextindexprevkw except valueerror printkeyword was not found in the textformatprevkw try currentkwindex cleantextindexcurrentkw except valueerror printkeyword was not found in the textformatcurrentkw try extractedtextsappendcleantextprevkwindex lenprevkw currentkwindex if currentkw keywordlist extractedtextsappendcleantextcurrentkwindex lencurrentkw lencleantext except unboundlocalerror printan index was not assigned for a particular keyword return extractedtexts iterate over all txt files for file in out with openfile as f data fread text resubrn data text resubrfinal report text text resubrs text printtext extract text result extracttextusingkeywordstext keywords if all keywords and their results were found if lenresult lenkeywords append list of extracted text to the end of the pandas df dfloclendf result else printnfailed to extract text for one or more keywords nplease check that are all present in the following textnnnformatkeywords text display results printdf with pdoptioncontextdisplaymaxcolwidth none for diplaying full columns displaydf produces the following error output when a keyword is not included eg technique keyword technique was not found in the text an index was not assigned for a particular keyword keyword technique was not found in the text failed to extract text for one or more keywords please check that examination technique comparison findings impression are all present in the following text examination chest pa and lat indication f with new onset ascites eval for infection chest pa and lateral comparison none findings there is no focal consolidation pleural effusion or pneumothorax bilateral nodular opacities that most likely represent nipple shadows the cardiomediastinal silhouette is normal clips project over the left lung potentially within the breast the imaged upper abdomen is unremarkable chronic deformity of the posterior left sixth and seventh ribs are noted impression no acute cardiopulmonary process empty dataframe columns indication technique comparison findings impression index and produces the desired output when all keywords are included file names contentsampledatamydatatxt contentsampledatamydatatxt indication technique comparison findings impression f with new onset ascites eval for infection chest pa and lateral none there is no focal consolidation pleural effusi no acute cardiopulmonary process chronic pain noted in lower erector spinae palpate none upper iliocostalis thoracis triggers pain alon nil
74942963,spacy tokenizer is not recognizing period as suffix consistently,python nlp spacy,similar to the example with modified infixes you need to look at the current suffix patterns and edit the rules that lead to this suffix for this particular case its probably this rule from the general suffix rules rauauformataualphaupper
74905744,unsure how to resolve language error message from googles natural language api the language sq is not supported for documentsentiment analysis,googlecloudplatform nlp googlenaturallanguage,the issue can be resolved by explicitly specifying document language in the code ie specify language en define the type then declare it on document for example sample code
74876117,bert embeddings in lstm model error in fit function,python tensorflow keras nlp bertlanguagemodel,regenerating your error after running this code i am getting the same error remember if you are using tfdatadataset then encolse it then while making the dataset enclose the dataset within the set like this tfdatadatasetfromtensorsliceswordsid wordsmask second problem as you asked the warning you are getting because you should be aware that lstm doesnt run in cuda gpu it uses the cpu only therefore it is slow so tensorflow is just telling you that lstm will not run under gpu or parallel computing
74857932,attributeerror module dilldill has no attribute log,python nlp pyarrow dill huggingface,nlp hasnt been updated since since then dill has had several releases the log variable has been removed from dillpy in the latest version try installing the previous one pip install dill if it doesnt work you may have to install an older version from around ps it doesnt look like nlp is maintained shouldnt you be using datasets
74847953,memoryerror when extracting articles into list using gensim wikicorpus,nlp gensim,by default the wikicorpus class surveys the entire dump files vocabulary upon creation even though most users dont need that and its during that step youre hitting this memoryerror however if you supply an empty python dict at wikicorpus creation itll skip this timeconsuming memoryconsuming step a specifically change your line wiki wikicorpusinf to wiki wikicorpusinf dictionary after this change you may not have any further problems as it looks like your code is otherwise doing things in an incremental fashion that shouldnt use much memory even on a smallmemory machine
74836900,valueerror e when changing the sentence segmentaion rule of spacy model,python nlp spacy,the syntax of nlpaddpipe with a custom function is given here you must declare the component function with a decorator and pass the name of the componentfunction as a string so it should be something like this note your function is doing a strange sentence segmentation it wont work in general for example it wont work if a sentence ends with or etc
74742143,attributeerror throwing up which says list object has no attribute lower,python string nlp nltk,wild guess im not that familiar with extending the sklearn classes can the problem be here seems like you are overriding the parent method by not fitting anything and just returning the class object
74623127,problem with for loop break statement does not do what i thought it would,python pandas nlp spacy conll,you should preserve the original tokenization to do this manually create the doc in order to skip the tokenizer in the pipeline import spacy from spacytokens import doc nlp spacyloadmodel words here are the original tokens doc docnlpvocab wordswords apply the model to the doc it skips the tokenizer for an input doc nlpdoc
74595449,calculating embedding overload problems with bert,python pytorch nlp bertlanguagemodel embedding,i fixed the problem the reason for the memory overload was that i wasnt saving the tensor to the gpu so i made the following changes to the code
74563930,how can i fix this indentationerror expected an indented block,python pythonx ifstatement nlp topicmodeling,i guess you want to use the ternary operator the format for it is x if condition else y this is on the same line and without the after the if else so your last return statement should be return joinfilteredsentence if lenfilteredsentence else none
74558516,tensorflow keras typeerror not supported between instances of int and tuple,tensorflow nlp,the tfkeraspreprocessingtexttokenizer api does not take trainsentences as an argument you are passing trainsentences to it hence raising the error replace the following with the line below in the fittokenizer menthod for more information on the tokenizer please refer to this document thank you
74519464,attributeerror tuple object has no attribute rank when calling modelfit in nlp task,python keras neuralnetwork nlp tfidf,this happens when function expects a tensor but other type of data is passed instead for instance numpy array like the one below out out attributeerror tuple object has no attribute rank to correct this data should be converted to tensor out
74494620,spacy doccharspan raises error whenever there is any number in string,python json nlp spacy spacy,the error typeerror object of type nonetype has no len occurs in line docents ents when one of the entries in ents is none the reason for having a none in the list is that doccharspanstart end label returns none when the start and end provided dont align with token boundaries the tokenizer of the model spacyblanken doesnt behave as needed for this use case it seems that it doesnt produce an end of token after a comma that follows a number without space after the comma examples tokenizing a number with decimals import spacy nlp spacyblanken nlptokenizerexplain token one single token tokenizing a number comma letter nlptokenizerexplaina token a one single token tokenizing a letter comma letter nlptokenizerexplainaa token a infix token a three tokens tokenizing a number comma space letter nlptokenizerexplain a token suffix token a three tokens tokenizing a number comma space number nlptokenizerexplain token suffix token three tokens therefore with the default tokenizer a space is needed after a comma following a number so the comma is used to create the token boundaries workarounds preprocess your text to add a space after the commas you desire to split tokens by this would also require to update the start and end values of the annotations create your custom tokenizer as described in spacy documentation
74461417,i am doing nlp lstm next word prediction but i get error of tocategorical indexerror index is out of bounds for axis with size,tensorflow keras nlp lstm tokenize,the tokenizer doesnt use it starts counting with is a reserved index that wont be assigned to any word try this
74354282,exception in thread main javalangoutofmemoryerror gc overhead limit exceeded using stanford corenlp,java nlp stanfordnlp,fixed using the flag xmsm which increases the default memory to gb to be used by corenlp
74330055,access json file error string indices must be integers,python json nlp typeerror,youre doubleencoding the json drop the first newintents jsondumpsintents indent and it should work remove this line newintents jsondumpsintents indent with openjsondatajson w as outfile jsondumpintents outfile indent change newintents to intents with openjsondatajson as jsonfile data jsonloadjsonfile printdataintents
74290324,typeerror unsupported operand types for sequenceclassifieroutput and int,python pytorch nlp huggingfacetransformers pytorchlightning,calling selfmodel returns an object of type sequenceclassifieroutput to access the loss you need to call its loss attribute replace by
74246516,invalid resource id error when tesing an azure web app bot in web chat,azure nlp chatbot azurecognitiveservices,i was able to solve this issue by redeploying
74228567,error while using bertbasenlimeantokens bert model,python nlp bertlanguagemodel sentencetransformers sslerrorhandler,it was simply a proxy issue i just added and http and their relative proxy values into system environment in windows
74175424,is spacy lemmatization not working properly or does it not lemmatize all words ending with ing,python nlp spacy,the spacy lemmatizer is not failing its performing as expected lemmatization depends heavily on the part of speech pos tag assigned to the token and pos tagger models are trained on sentencesdocuments not single tokens words for example partsofspeechinfo which is based on the stanford pos tagger does not allow you to enter single words in your case the single word consulting is being tagged as a noun and the spacy model you are using deems consulting to be the appropriate lemma for this case youll see if you change your string instead to consulting tomorrow spacy will lemmatize consulting to consult as it is tagged as a verb see output from the code below in short i recommend not trying to perform lemmatization on single tokens instead use the model on sentencesdocuments as it was intended as a side note make sure you understand the difference between a lemma and a stem read this section provided on wikipedia lemma morphology page if you are unsure if you really need to lemmatize single words the second approach on this geeksforgeeks python lemmatization tutorial produces the lemma consult ive created a condensed version of it here for future reference in case the link becomes invalid i havent tested it on other single tokens words so it may not work for all cases
74166201,python ifelif statement not working correctly,python nlp,your code is correct but changing the to and because is a bitwise logical operator not a logical one for example z b a b b b c b d b now the result of the expression a c is b or but in logical expression a and c the result will be true which i suppose you are interested in
74160976,an error in implementing regex function on a list,python nlp nltk nlpquestionanswering,your call to the cpparse function expects each of the tokens in your sentence to be tagged however the tags list you created only contains the tags but not the tokens as well hence your valueerror the solution is to instead pass the output from the postag call ie tagged to your checkgrammar call see below solution output
74123446,tenserflow issue when tokenizing sentences,python tensorflow nlp tokenize,to specify an unlimited amount of tokens use output
73853551,complex regex not working in spacy entity ruler,python regex nlp spacy namedentityrecognition,the problem is that your pattern is supposed to match at least two tokens while the regex operator is applied to a single token a solution can look like the likenum entity is defined in spacy source code mostly as a string of digits with all dots and commas removed so the dd pattern looks good enough it matches a token that starts with one or more digits and then contains zero or more occurrences of a comma or dot and then one or more digits till the end of the token
73698110,keras model fit throws shape mismatch error,tensorflow keras deeplearning neuralnetwork nlp,mismatched shape in the input layer the input shape needs to match the shape of a single element passed as x or datasetshape so since your dataset size is that is samples of size so your input shape should be change to
73607406,valueerror x has features but linearsvc is expecting features as input,python machinelearning nlp svm textclassification,to obtain predictions from your model you need to follow the same transformation steps that were undertaken during the training phase the valueerror you are encountering indicates that you are passing raw data to the classifier without vectorization as the model has been trained on a sparse matrix consisting of features the outcome of tfidffittransformxtrain it expects a vectorized input with the same number of features here is how it can be done this can of course be modified to work with batches instead of single inputs moreover the use of pipelines is highly recommended to assemble all the different steps
73602754,spacy phrase matchertypeerror when trying to remove matched phrases,python nlp spacy,there are a couple of issues with your approach here one is that because of the way replace works if youre using it theres no reason to use the phrasematcher replace will already replace all instances of a string what i would do instead is use an onmatch callback to set a custom attribute say tokenignore to true for anything your matcher finds then to get the tokens youre interested in you can just iterate over the doc and take every token where that value isnt true heres a modified version of your code that does this
73588661,how to redefine model for pytorch max pool error,python arrays machinelearning nlp pytorch,the line that raises error should be line at this point xconvs is batchsize nfmapsarg seqlennew lenfszarg xmaxpools fmaxpooldxi xisizesqueeze for xi in xconvs batchsize nfmapsarg lenfszarg actually maxpoold reduces the size of the dimension n that represents the sequence after batch and channel dimension see the documentation plus according to the discuss you point out the kernelsize argument now requires a concrete value at execution and cannot be inferred dynamically from inputs fortunately your case is particularly simple because the kernel size is the entire sequence length in other words your pooling is actually a global max pooling that reduces the size of the sequence to only one scalar this scalar is simply the maximum of all the values check the shapes in comments so it is actually easy to implement a version without dynamic shapes just change line by xmaxpools torchmaxxi dim keepdimtrue for xi in xconvs batchsize nfmapsarg lenfszarg note there is no direct implementation of globalmaxpooling layer but as you can see it is just a torchmax actually torchmax return a tuple max maxindicies so you hae to take only the first element with see the doc gradients can propagate through torchmax like usual maxpooling layers one source see here for instance torchmaxx dimsqueeze is changed to torchmaxx dim keepdimtrue that is exactly the same thing but cleaner
73574217,resub expected string or bytelike object error,python regex nlp nltk pythonre,if text is a list or some other iterable type loop over it and perform the replacements and return a list of results
73418212,attributeerror module networkx has no attribute nxpydot,pythonx nlp virtualenv networkx attributeerror,use nxdrawingnxpydotwritedotselfgraph dotfile instead of nxnxpydotwritedotselfgraph dotfile in the takahepy file install this package pip install pydot dont forget to import these in your file import networkx as nx from networkxdrawingnxagraph import writedot from networkxdrawingnxpydot import writedot import matplotlibpyplot as plt for safety i would install the following packages as well
73405232,transformer summariser pipeline giving different results on same model with fixed seed,deeplearning nlp huggingfacetransformers transformermodel summarization,you might reseed the program after bartlargecnn pipeline otherwise the seed generator would be used by the first pipeline and generate different outputs for your lidiya model across two scripts from transformers import autotokenizer automodelforseqseqlm pipeline import torch random text taken from uk news website text the veteran retailer stuart rose has urged the government to do more to shield the poorest from doubledigit inflation describing the lack of action as horrifying with a prime minister on shore leave leaving a situation where nobody is in charge responding to julys headline rate the conservative peer and asda chair said we have been very very slow in recognising this train coming down the tunnel and its run quite a lot of people over and we now have to deal with the aftermath attacking a lack of leadership while boris johnson is away on holiday he said weve got to have some action the captain of the ship is on shore leave right nobodys in charge at the moment lord rose who is a former boss of marks spencer said action was needed to kill pernicious inflation which he said erodes wealth over time he dismissed claims by the tory leadership candidate liz trusss camp that it would be possible for the uk to grow its way out of the crisis seed torchcudamanualseedallseed torchusedeterministicalgorithmstrue tokenizer autotokenizerfrompretrainedfacebookbartlargecnn model automodelforseqseqlmfrompretrainedfacebookbartlargecnn modeleval summarizer pipeline summarization modelmodel tokenizertokenizer numbeams dosampletrue norepeatngramsize device output summarizertext truncationtrue seed torchcudamanualseedallseed torchusedeterministicalgorithmstrue tokenizer autotokenizerfrompretrainedlidiyabartlargexsumsamsum model automodelforseqseqlmfrompretrainedlidiyabartlargexsumsamsum modeleval summarizer pipeline summarization modelmodel tokenizertokenizer numbeams dosampletrue norepeatngramsize device output summarizertext truncationtrue printoutput
73290224,python typeerror init got an unexpected keyword argument checkpointcallback,python errorhandling nlp pytorch pytorchlightning,as i am looking into pytorchlightning github i do not see checkpointcallback variable in init are you sure thats how its called what do you want to achieve by passing this checkpointcallback edit i think you just have to append checkpointcallback to callbacks list
73172306,how to resolve typeerror cannot use a string pattern on a byteslike object wordtokenize counter and spacy,python nlp counter spacy tokenize,taken your data and created dummy dataframe for the same you will get the desired ouptut
73107703,issue when importing bloomtokenizer from transformers in python,python nlp huggingfacetransformers huggingfacetokenizers huggingface,bloom has no slow tokenizer class it only has a fast tokenizer the official documentation is wrong at this point use the following instead from transformers import bloomtokenizerfast tokenizer bloomtokenizerfastfrompretrained
73078231,how to get all stop words from spacy and dont get any errors typeerror argument of type module is not iterable,python nlp datascience spacy,make sure stopwords and punctuations be a list or set and for getting a set of all stopwords from from spacylangen import stopwords you can use stopwordsstopwords or as an alternative solution you can use nlpdefaultsstopwords
73046919,why is modelfit working without clear attribute and label separation and the same method is not working for modelevaluate,python tensorflow machinelearning keras nlp,the input data could be a numpy array or arraylike or a list of arrays in case the model has multiple inputs a tensorflow tensor or a list of tensors in case the model has multiple inputs a dict mapping input names to the corresponding arraytensors if the model has named inputs a tfdata dataset should return a tuple of either inputs targets or inputs targets sampleweights a generator or kerasutilssequence returning inputs targets or inputs targets sampleweights the input data or the parameter x in the fitevaluate method passed to the model is of type tfdata dataset that returns a tuple of either inputs targets or inputs targets sampleweights if x is a tfdata dataset instance y should not be specified since targets will be obtained from x kindly refer this for more information your code is working fine in colab please find the gist here thank you
73017872,i get an nameerror although i defined my variable,python nlp tfidf nameerror,just like that
73015102,error in fittransform while finding tfidf in python,python nlp tfidf tfidfvectorizer,you can do it like this define analyzerchar so that tfidfvectorizer works with the letters find the index of d in the vocabulary and use it import pandas as pd from sklearnfeatureextractiontext import tfidfvectorizer mylist a a b c a c c c d e f a c d d d a d f df pddataframetexts mylist tfidfvectorizer tfidfvectorizerngramrange analyzerchar tfidfseparate tfidfvectorizerfittransformdftexts ind tfidfvectorizervocabularyd tfidfseparatetodense ind
72933472,modulenotfounderror in spacy version tried previous mentioned solution not working,python nlp chatbot lemmatization spacy,it looks like theyve changed the way the lemmatizer is instantiated but the following should work its unfortunate that you have to call the lemmatizer with a token but looking at the code i dont see a way to call it with word pos i think youre stuck with calling the empty pipeline with a single word to get a token then manually setting the pos before calling lemmatizet note that the pos tagger will not work correctly on a single word it only works in sentences and will probably always assign noun for pos if you only have one word this is why ive disabled the pipeline and set tpos manually btw if you only need to lemmatize you might look at lemminflect which is simpler for single word and also more accurate
72920750,error getting prediction explanation using shapvalues when using scikitlearn pipeline,pythonx scikitlearn nlp pipeline shap,i have figured out how to fix it posting to help others
72834436,typeerror tuple indices must be integers or slices not str facing this error in keras model,tensorflow keras deeplearning nlp computervision,the dataset traindataloader seems to return a tuple of items link in particular model input is a tuple images xbatchinput however your code in dualencoder seems to assume that its a dict with keys like caption image etc i think thats the source of the mismatch
72801555,typeerror expected argument to be a boolean but got bert,machinelearning deeplearning nlp datascience bertlanguagemodel,i saw this question again after facing this issue in same code now i am writing an answer as i have solved it there is a keyword name missing in the above code i changed it to by just putting name bert and now it works
72782449,nltk stopwords attributeerror function object has no attribute words,python errorhandling nlp nltk stopwords,hello the problem is that youve named your function like the nltkcorpus module you should find an other name for your function and itll work i think
72690203,getting keyerrors when training hugging face transformer,python pandas nlp huggingfacetransformers huggingface,converting pandasseries into a simple python list and getting rid of some extra materials would fix the issue
72652399,typeerror with dataloader,nlp pytorch dataloader pytorchlightning pytorchdataloader,the error is as it stated you should change your maxlen from string to int
72601057,attributeerror maskedlmoutput object has no attribute view,python nlp model pytorch bertlanguagemodel,you can refer to the documentation of maskedlmoutput basically it is an object holding the loss logits hiddenstates and attentions it is not a tensor so you are getting this error i think you are interested in logits ie score for each token before applying softmax then in the forward function you can just access to the logits tensor like this x selfbertinputlogits
72574801,keyerror on a certain word,python nlp textclassification naivebayes nonenglish,my guess is that this line vocabularyappendtupleword should be changed to vocabularyappendword since your version might put letters instead of words into vocabulary and therefore wordcountsperemail in case this doesnt work i suggest looking into contents of vocabulary wordcountsperemail so you can determine what went wrong
72480729,error when taking fftd in tensorflow on gpu,tensorflow machinelearning nlp tensorflow fft,i downgraded to tensorflow and the problem is gone
72480289,how to handle keyerrorfkey key not present worvec with gensim,python nlp gensim wordvec keyerror,if the token is not present in the model it cant give you a vector for it your model doesnt have a vector for the pseudoword bla bla bla all it can do is report that you could avoid the exception by prechecking whether the token is present and only requesting it if present if token in modelwv tokenvector modelwvtoken else whatever your nextbest step is when a vector not available or you could catch the exception try tokenvector modelwvtoken except keyerror whatever your nextbest step is when a vector not available but theres no magic way to create a good vector for an unknown token youll have to ignore such words or makeup some plug standin value or figure some other projectappropriate workaround if you have sufficient training data with varied examples of the tokens real usage you could train a model that includes the token you could also consider finding or training a wordvec variant model like fasttext which can synthesize guessvectors for unknown tokens based on which substrings they might share with words learned in training but such vectors may be quite poor in quality
72428445,error node binarycrossentropycast cast string to float is not supported while train model,python machinelearning nlp lstm tfkeras,i think the problem is that you are having strings as classes positif negativ as shown in your data excerpt just transform them to negativ and positiv and it should work if not we would have to look a little bit deeper into your code edit so as suspected the problem where the strings as classes you can change them to float with after this you also should change the dtype of the ynparry to float this should do the trick for reference see also my colab code
72397740,issues with spacy model encoreweblg how to prevent the package from downloading every time the code is run,python module nlp operatingsystem spacy,spacy doesnt automatically download models at all so this must be a bug with your code that checks if the model is already installed looking at this code the issue is that if the model is not installed this is an oserror not a modulenotefounderror first you need to fix that this approach seems like it should work except loading models in the same process you installed them in doesnt work very reliably the list of installed packages is not updated while python is running so even after fixing the above issue it may not work as intended i would recommend either download the model to a known directory extract it there and load it from a path instead of just the model name check the output of pip list to see if the model is installed and install it if not
72376112,valueerror if the argument is set then none of the individual field arguments should be set,python googlecloudplatform nlp sentimentanalysis,i have successfully replicated your use case with the same error message the error comes from the line of code result clientanalyzesentimentdocument encodingtype utf to resolve this you must explicitly assign the document to the individual field argument document for the sentiment analysis request as shown below result clientanalyzesentimentdocument document encodingtype utf below is the result of my sample sentiment analysis request result using the corrected line of code you may refer to this analyzesentimentrequest reference documentation for more information regarding individual field arguments for google cloud natural language api
72358012,notimplementederror the lemmatize parameter is no longer supported,python machinelearning text nlp artificialintelligence,i couldnt find any actual documentation for this function just some example page what i did was just calling if you still want to lemmatize call some lemmatization function in tokenizerfunc as described in error message and now wait around h to process d
72340801,huggingface loaddataset function throws valueerror couldnt cast,machinelearning nlp sentimentanalysis huggingfacetokenizers huggingface,the reason is since delimiter is used in first column multiple times the code fails to automatically determine number of columns some time segment a sentence into multiple columns as it cannot automatically determine is a delimiter or a part of sentence but the solution is simple just add column names dataset loaddatasetcsv datafilestrain traintesttestcolumnnamessentencelabel output datasetdict train dataset features sentence label numrows test dataset features sentence label numrows
72298933,runtimeerror expected dimensional tensor but got dimensional tensor for argument,python pytorch nlp matrixmultiplication allennlp,bmm stands for batch matrixmatrix product so it expects both tensors with a batch dimension ie d as the error says for single tensors you want to use mm instead note that tensormm also exists with the same behaviour or better for two d tensor you can use dot for dot product
72189892,python syntax error in list comprehension on string for lemmatization,python syntax nlp nltk lemmatization,you could try with this list comprehension then if you want a single string considering your input sample you can use the python join operation on strings
72118367,tensorflow invalidargumenterror graph execution error,python tensorflow keras nlp,the inputdim of the embedding layer has to correspond to the size of your datas vocabulary also your labels should begin from zero and not from one when using the sparsecategoricalcrossentropy loss function here is a working example based on your code and data
72091006,tokenization of compound words not working in quanteda,r nlp token quanteda,you need to apply phrasestack overflow and set concatenator in tokenscompound requirequanteda package version unicode version icu version speech cthis is the first speech many words are in this speech but only few are relevant for my research question one relevant word for example is the word stack overflow however there are so many more words that i am not interested in assessing the sentiment of this is a second speech much shorter than the first one it still includes the word of interest but at the very end stack overflow this is the third speech and this speech does not include the word of interest so im not interested in assessing this speech data dataframeid speechcontent speech testcorpus corpusdata docidfield id textfield speechcontent testtokens tokenstestcorpus removepunct true removenumbers true tokenscompoundpattern phrasestack overflow concatenator testkwic kwictesttokens pattern stack overflow window testkwic keywordincontext with matches for example is the word stack overflow however there are so many but at the very end stack overflow created on by the reprex package v
71972018,cannot see debug logs for number of documents converged info when running gensims lda suggested for choosing iterations and passes,nlp gensim lda,the line debug documents converged within iterations can be obtained in the logging file changing the logging configuration to debug in you case would be something like this now the line will appear inside the logging file
71916613,modulenotfounderror no module named haystackdocumentstoreelasticsearch haystackdocumentstore is not a package,python elasticsearch nlp documentstore haystack,according to the haystack documentation since version elasticsearchdocumentstore can be directly accessed from haystackdocumentstores also note that the plural of documentstores is necessary for versions after as well that is the s is needed at the end of documentstores
71892648,electra sequence classification with pytorch lightning issues with pooleroutput,python nlp huggingfacetransformers pytorchlightning,electra has no pooler layer like bert compare the return section for further information in case you only want to use the cls token for your sequence classification you can simply take the first element of the lasthiddenstate initialize electra without returndictfalse output selfclassifieroutputlasthiddenstate
71661321,valueerror dimensions must be equal but are and for node binarycrossentropymul with input shapes,tensorflow keras nlp tensorflow,the output dimension of the prediction layer of the binary classification should be add the prediction layer tfkeraslayersdense activationtfkerasactivationssigmoid flatten model creation modeltfkerassequential add an embedding layer tfkeraslayersembeddingwordcount inputlengthmaxlen tfkeraslayersdropout add another bilstm layer tfkeraslayersbidirectionaltfkeraslayerslstmreturnsequencestrue add flatten tfkeraslayersflatten add a dense layer tfkeraslayersdense activationtfkerasactivationsrelu tfkeraslayersdense activationtfkerasactivationsrelu tfkeraslayersdense activationtfkerasactivationsrelu tfkeraslayersdense activationtfkerasactivationssoftmax add the prediction layer tfkeraslayersdense activationtfkerasactivationssigmoid
71555062,word not in vocabulary error in gensim model,webscraping nlp nltk gensim wordvec,there are multiple problems if you want a turkish model you can try to find a pretrained wordvec model for turkish eg check out this repository or train a model for turkish yourself the way you use it now you seem to train a model but only from a single website which will barely do anything because the model needs a large amount of sentences to learn anything like at least better much more also you set mincount anyway so any word appearing less than times is ignored generally try something like training it on the turkish wikipedia see the linked repository wordvec by default is a unigram model so the input is a single word if you hand it a bigram consisting of two words like jaccard mesafesi it will not find anything also you should catch the case that the word is not in vocabulary otherwise each unknown word will cause an error and your program to cancel search for the unigram representation of each token then combine the two eg by using the statistical mean of the vectors the wordvec class for training takes as argument a list of tokenized sentences so a list word lists you handed it entire untokenized sentences instead
71530946,how can i fix this error when converting csv to json,python elasticsearch machinelearning nlp datascience,todict in pandas means that usually nanvalues in the original dataframe result in the corresponding dicts keyvaluepair not being created at all i could imagine that parts of the dataframe contain nans or empty strings that were autoconverted to nans so some of the dicts might not have a keyvaluepair for bodytext you can catch that case eg by filling in an empty string for those dicts like this
71512301,error could not build wheels for spacy which is required to install pyprojecttomlbased projects,python pythonx pip nlp spacy,try using python instead where there are binary wheels for pip install to use instead of having to compile from source this is a conflict with python and some generated cpp files in the source package python wasnt released yet when this version was published
71512064,error while loading vector from glove in spacy,python pythonx nlp spacy stanfordnlp,use spacy init vectors to load vectors from wordvecglove text format into a new pipeline
71504226,error running my spacy summarization function on a text column in pandas dataframe,python pandas nlp spacy,the logic of your text summarization assumes that there are valid sentences which spacy will recognize but your example text doesnt provide that spacy will likely just put it all in one long sentence i dont think the text you fed into it would be split into multiple sentences the sentence segmentation needs valid text input with punctuation marks etc try it with a text consisting of multiple sentences recognizable for spacy that is combined with the fact that you use intlensentencetokensper int conversion rounds down to the next smaller full number so int int aka it returns sentences this happens for every text with less than segmented sentences so change this ratio or use something like max intlensentencetokensper i think other than that the code should generally work i didnt look at every detail though but i am not sure if you know exactly what it does it summarizes by keeping only the per share of most representative full sentences it doesnt change anything on word level
71496028,valueerror class encoding field is specified without a type,python nlp altair,this error will generally arise when the encoding names here principal component andor principal component do not match the names of any columns in the dataframe passed to the chart check the names of the columns in the dataframe and make sure youre reproducing them correctly
71486242,attributeerror tconfig object has no attribute adapters,nlp pytorch huggingfacetransformers summarization,well i was thinking right i did experiments install only transformers library when i load the model each layer of the model was without an adapter attribute install only adaptertransformers library when i load the model each layer of the model was with an adapter attribute conclusion install adaptertransformers
71467995,value error when trying to train a spacy model,python pythonx nlp spacy spacy,base on documentation they made some changes in version x and now it uses directly batch without spliting texts labels zipbatch thats all
71437696,valueerror classification metrics cant handle a mix of multilabelindicator and multiclass targets,python keras nlp evaluation,both arguments of fscore must be in the same format either onehot encoding or label encoding you cannot pass two differently encoded arguments use one of the following options option you could convert ynew to onehot encoding option you could convert ynew to onehot encoding using labelbinarizer option you could convert ytest from onehot encoding to label encoding
71432983,runtimeerror error loading state dict for srlbert missing keys bertmodelembeddingspositionids unexpected keys,jupyternotebook nlp anaconda allennlp,if you are on the later versions of allennlpmodels you can use this archivefile instead the latest versions of the model archive files can be found on the demo page in the model card tab
71390078,pytorch datasetsudpossplits throwing error,deeplearning nlp pytorch,i solved the same problem by changing the code from torchtext import datasets to from torchtextlegacy import datasets
71368113,email classifier using spacy throwing the below error due to version issue when tried to implement bow,pythonx nlp spacy spacy,just from the way i would understand that error message it tells you that the spacy version you want to install is incompatible with the python version you have it needs python or so either create an environment with python or its quite easy to specify python version when creating a new environment in conda or use a higher version of spacy did you already try if the code works if you just use the newest version of spacy is there a specific reason for why you are using this spacy version if you are using some methods that are not supported anymore it might make more sense to update your code to the newer spacy methods especially if you are doing this to learn about spacy it is counterproductive to learn methods that are not supported anymore sadly a lot of tutorials fail to either update their code or at least specify what versions they are using and then leave their code online for years
71100013,keras textvectorization adapt throws attributeerror,python pandas tensorflow keras nlp,since you are using a internal dictionary you can try something like this import tensorflow as tf d title malaysia testing people for bird flu says outbreak isolate krogers profit climbs misses forecast reuters blasts shake najaf as us planes attack rebels description kerry camp makes video to defuse attacks ap malaysian officials on saturday were testing three people who fell ill in a village hit by the deadly hn bird flu strain after international health officials warned that the virus appeared to be entrenched in parts of najaf iraq reuters strong blasts were heard in the besieged city of najaf early sunday as us military planes unleashed cannon and howitzer fire and a heavy firefight erupted traintext tfdatadatasetfromtensorslicesdbatch maxfeatures sequencelength vectorizelayer tfkeraslayerstextvectorization maxtokensmaxfeatures outputmodeint outputsequencelengthsequencelength this example assumes that you have already excluded the labels traintext rawtraindsmaplambda x y x traintext traintextmaplambda x tfconcatxtitle xdescription axis vectorizelayeradapttraintext this example assumes that you have already excluded the labels
71099545,failedpreconditionerror table not initialized,python tensorflow keras deeplearning nlp,the textvectorization layer is a preprocessing layer that needs to be instantiated before being called also as the docs explain the vocabulary for the layer must be either supplied on construction or learned via adapt another important information can be found here crucially these layers are nontrainable their state is not set during training it must be set before training either by initializing them from a precomputed constant or by adapting them on data furthermore it is important to note that the textvectorization layer uses an underlying stringlookup layer that also needs to be initialized beforehand otherwise you will get the failedpreconditionerror table not initialized as you posted
71078218,valueerror no gradients provided for any variable tfcamembert,python tensorflow nlp huggingfacetransformers namedentityrecognition,try transforming your data into the correct format before feeding it to modelfit def mapfuncx y return inputids xinputids attentionmask xattentionmask labelsy traindataset traindatasetmapmapfunc the model seems to run after this step
71009675,nonetype error when using pegasustokenizer,python nlp huggingfacetransformers,the solution was to install sentencepiece package and restart the kernel of the python notebook
71006031,how to solve this attribute error in python,python pandas keras nlp artificialintelligence,i believe you have mispelled the textstosequences attribute the error states that it cannot find the texttosequences method in the line from the documentation you can see that this method should be spelled this should resolve the error
70994667,semantic role labeling tensor issue,python nlp tensor allennlp srl,you dont show what kind of predictor youre loading but i suspect that the model can only handle word pieces maybe longformer would be a solution but then youd have to train the srl model with longformer first think about what you actually want to accomplish though your example sentence is actually multiple sentences with a bulleted list of more sentences at the end the allennlp srl model was never trained on that kind of input data and will not perform well anyways i suggest you split the input into sentences and feed in one sentence at a time thatll be closer to the kind of data the model has seen at training time so you will get better results from it
70954157,modulenotfounderror no module named milvus,elasticsearch nlp documentclassification milvus haystack,i was facing the same problem and i got around it by simply uninstalling pymilvus and reinstalling the older version pip list showed pymilvus v pip uninstall pymilvus removed the current version pip install pymilvus installed the older version and voil it works peacefully
70922447,valueerror the first argument to must always be passed,python tensorflow keras deeplearning nlp,the textvectorizer layer should be passed to your model without parentheses try something like this import tensorflow as tf maxvocablength maxlength textvectorizer tfkeraslayerstextvectorizationmaxtokensmaxvocablength outputmodeint outputsequencelengthmaxlength textdataset tfdatadatasetfromtensorslicesfoo bar baz textvectorizeradapttextdatasetbatch model tfkerassequential tfkeraslayersinputshape dtypestring textvectorizer tfkeraslayersembeddingmaxvocablength tfkeraslayersglobalaveragepoolingd tfkeraslayersdense activationsigmoid namemodeldense printmodeltfconstantfoo
70819255,problem with creating dictionary with gensim for lda,python machinelearning nlp gensim corpus,each item in the corpus should be a sequence of unicode tokens words not a string if you want the strings door cat mom to be the words in the dictionary you could do from gensim import corpora corpus door cat mom dictionary corporadictionarycorpus
70769151,google mtsmall configuration error because number attention heads is not divider of model dimension,nlp huggingfacetransformers allennlp,this is a very good question and shows a common misconception about transformers stemming from an unfortunate formulation in the original transformers paper in particular the authors write the following in section in this work we employ h parallel attention layers or heads for each of these we use dk dv dmodel h note that the equality of dkdv dmodel is not strictly necessary it is only important that you do match the final hidden representation dmodel after the feedforward portion of each layer specifically for mtsmall the authors actually use an internal dimension of which is simply the product of parameters dkv numheads now the problem is that many libraries make a similar assumption of the enforced relation between dkv and dmodel because it saves some implementation effort that most people wont use anyways i suspect not super familiar with allennlp that they have made similar assumptions here which is why you cannot load the model also to clarify this here is a peek at the modules of a loaded mtsmall tblock layer modulelist tlayerselfattention selfattention tattention q linearinfeatures outfeatures biasfalse k linearinfeatures outfeatures biasfalse v linearinfeatures outfeatures biasfalse o linearinfeatures outfeatures biasfalse layernorm tlayernorm dropout dropoutp inplacefalse tlayerff densereludense tdensegatedgeludense wi linearinfeatures outfeatures biasfalse wi linearinfeatures outfeatures biasfalse wo linearinfeatures outfeatures biasfalse dropout dropoutp inplacefalse layernorm tlayernorm dropout dropoutp inplacefalse you can get the full model layout by simply calling listmodelmodules
70725486,error cannot install encorewebtrf because these package versions have conflicting dependencies,python pip nlp spacy,i solved this issue by installing pytorch thats what that conflict means i would also warn that you should make sure that spacy pytorch and python are all compatible for me that means the following install python install cuda create a python venv install pytorch in venv install gpu spacy install the particular spacy core the commands i ran after installing cuda and python were
70724874,nlp text classification countvectorizer shape error,python scikitlearn nlp decisiontree textclassification,countvectorizer requires dimensional inputs and the error suggests that your xtrain is d if its a dataframe reduce to a series if its a numpy array use reshape or ravel
70697478,issue related with scorers when trying to load a spacy ner model,python nlp spacy,after several trials when restarting the kernel and doing pip install u spacy again it actually solved the problem
70549999,valueerror the last dimension of the inputs to a dense layer should be defined found none,python tensorflow keras nlp embedding,you need to define a max length for the sequences if you check modelsummary output shape of the textvectorization will be none none first none indicates that model can accept any batch size and the second one indicates that any sentence that is passed to textvectorization will not be truncated or padded so the output sentence can have variable length example redefining it defining outputsequencelength to a number will make sure the outputs lengths are a fixed number
70545508,token secondteam not found and default index is not set error in torchtext function,python nlp torchtext,the vocabulary acts as a lookup table for your data translating str to int when a given string in this case secondteam doesnt appear in the vocabulary there are two strategies to compensate throw an error because you dont know how to handle it imagine something like a keyerror when calling in python assign a default unknown token to the missing tokens imagine a default value like get i dont know in python your code is currently doing you seem to want which you can achieve using vocabsetdefaultindex when you build your vocab add the specials kwarg and then call vocabsetdefaultindexvocab
70533739,how to solve the problem of importing when trying to import sentencesegmenter from spacypipeline package,pythonx nlp spacy,there are several methods to perform sentence segmentation in spacy you can read about these in the docs here this example is copied asis from the docs showing how to segment sentences based on an english language model you can also use the rulebased one to perform punctuationsplit based on language only like so also from the docs this should work for spacy and above
70473212,google translate python package not working after x calls,python googlecloudplatform text nlp googletranslationapi,for each gcp api theres a limit to a number requests per minute in this case this is a rate quota that applies rate quotas are typically used for limiting the number of requests you can make to an api or service rate quotas reset after a time interval that is specific to the servicefor example the number of api requests per day in your case translation is handled by a translategoogleapiscom api go to your quotas page and there you can see the numbers you can also view the quotas for the api with the gcloud however since this feature is in alpha it may not work as expected you can also reqest higher quotas in your translate api quota page to see which cap youre hitting go to cloud monitoring and follow the instructions described in the documentation when you establish which quota you need to raise just edit the value and click submit request button
70404372,attributeerror field object has no attribute vocab preventing me to run the code,nlp torchtext,you have to build the vocabulary for the english field before you try to access it you will need a dataset to build the vocabulary which will be the dataset you are looking to build a model for you can use englishbuildvocab here are the docs for buildvocab also if you would like to learn how to migrate what you are doing to the new version of torchtext here is a good resource
70364824,pytorch multiclass valueerror expected input batchsize to match target batchsize,nlp pytorch multiclassclassification,you shouldnt be using the squeeze function after the forward pass that doesnt make sense after removing the squeeze function as you see the shape of your final output is whereas it is expecting one way to fix this is to average out the embeddings you obtain for each word after the selfembedding function like shown below
70306493,view train error metrics for hugging face sagemaker model,python nlp amazonsagemaker huggingfacetransformers,this can be solved by increasing the number of epochs in training to a more realistic value currently the model trains in fewer than seconds which is when the following timestamp would be recorded and presumably the loss function changes to make hyperparameters epochs increase the number of epochs to realistic value trainbatchsize batchsize modelname modelcheckpoint task task
70260427,valueerror tffunctiondecorated function tried to create variables on nonfirst call while using custom loss function,python tensorflow keras deeplearning nlp,maybe try the following with tfeye import tensorflow as tf from tensorflowkeras import backend as k def tripletlossmargin def tripletytrueypred batchsize tfcasttfshapeytrue dtypetffloat v v ypredypred scores kdotv ktransposev positive tflinalgdiagpartscores negativewithoutpositive scores tfeyebatchsize closestnegative tfreducemaxnegativewithoutpositive axis negativezeroonduplicate scores tfeyebatchsize meannegative ksumnegativezeroonduplicate axis batchsize tripletloss kmaximum margin positive closestnegative tripletloss kmaximum margin positive meannegative tripletloss kmeantripletloss tripletloss return tripletloss return triplet tripletloss tripletloss def calculatemeanx axis return kmeanx axisaxis def normalizex return x ksqrtksumx x axis keepdimstrue basemodel tfkerassequential basemodeladdtfkeraslayersembeddinginputdim outputdim basemodeladdtfkeraslayerslstm returnsequencestrue basemodeladdtfkeraslayerslambdacalculatemean namemean basemodeladdtfkeraslayerslambdanormalize namenormalize input tfkeraslayersinputshape input tfkeraslayersinputshape encoding basemodelinput encoding basemodelinput merged tfkeraslayersconcatenateencoding encoding model tfkerasmodelinputs input input outputs merged modelcompile optimizer tfkerasoptimizersadam loss tripletloss x tfrandomuniform maxval dtypetfint y tfrandomuniform modelfitx x y epochs batchsize
70130957,how to handle errors from ibm watson when iterating over rows,python pandas dataframe errorhandling nlp,i would recommend creating a separate function to encapsulate all that sentiment analysis logic in the end you would call it like this dfsentimentscore dfcontentapplysafecomplexfunction safecomplexfuntion would be your brand new safe function give it the name you want it would be probably something like this def sentimentscorescontent try response naturallanguageunderstandinganalyze textcontent featuresfeatures sentimentsentimentoptionstargetsirish getresult jsontbl pdjsonnormalize responsesentiment recordpathtargets metadocumentscore documentlabel return jsontblsetindexpdindexindex except please dont put exception it is too general return none here is an example code creating a test dataframe import pandas as pd data i am happy i am sad i am neutral exception generator df pddataframedatacolumnsuseridcontent userid content i am happy i am sad i am neutral exception generator creating a mocking sentiment analysis function this function is solely for mocking def fakesentimentanalysiscontent sentimentscores sad happy neutral for sentiment score in sentimentscoresitems if sentiment in content return score rasises keyerror error only for demonstration purposes return sentimentscoresbroken def complexfunctionelement sentimentscore fakesentimentanalysiselement return sentimentscore applying that nonsafe function on dataframe you would got keyerror calling that function dfcontentapplycomplexfunction adding exception handler you can make it safer adding exception handling def safecomplexfunctionelement try sentimentscore fakesentimentanalysiselement except keyerror sentimentscore none return sentimentscore userid content sentimentscore i am happy i am sad i am neutral exception generator nan
70124620,integrated gradients error attempt to convert a value none with an unsupported type to a tensor,nlp gradient bertlanguagemodel,in your notebook the error is triggered by therefore it seems that ig requires nonempty baselines
70123519,valueerror unknown url type languagetoolzip,pythonx url pip nlp urllib,i resolved it used git and it worked
70119607,nlp tokenize typeerror expected string or byteslike object,python pythonx nlp chatbot tokenize,welcome to so given the following dataframe data and the function wordtokenize you must do applying the function on col in the simplest way possible first changing type to str second apply the function on every single element the output would be pandascoreseriesseries
70096375,fasttext typeerror loadmodel incompatible function arguments,pythonx nlp fasttext,supply a string not a path object per the error the arg st positional argument should be a str and its seeing a windowspath object instead it will probably be enough to just use strfacebookmodel instead of just facebookmodel as your argument to fasttextloadmodel but if theres any further confusion about where youre actually pointing the fasttext code you could also look at and try strfacebookmodelresolve so that youre sure to see the absolute full path to your file
70051086,python sklearn tfidfvectorizer arguments error,python machinelearning scikitlearn nlp tfidfvectorizer,library and their implementation did change if we look at the version we get a warning which states that it needs to pass with keyword args so fast forward to the same call will be like ambrayers added an alternative way is create the object and then fittransform refer the example in official documentation
69941156,the problem of the installation of transformers,python deeplearning nlp glibc huggingfacetransformers,i had the same issues and i downgraded to the following version
69861444,typeerror hypothesis expects pretokenized hypothesis iterablestr,python nlp nltk metrics,actually i believe the right answer for the problem is to tokenize the sentence before calling the function for example where ref and hypo is a sentence string
69854276,networkx decoding to str typeerror,python graph nlp networkx,to make it easier for future readers ill post this partial answer here it doesnt really solve why your graphviz layout doesnt work but your code works by change the layout like for example pos nxspringlayoutg
69845992,date pattern for whatsapp chat text file that has hour format split error too many values to unpack,python pandas date nlp,try with your output have a look at this example
69821842,wordvec error typeerror unhashable type list,python nlp bioinformatics gensim wordvec,you need to pass a list of list of strings to gensims wordvec in your code you are passing kmersdatapos to wordvec which is list of list of list of strings for example is a valid parameter for the wordvec function whereas is invalid
69818148,attempting to combine numeric and text features in tensorflow valueerror layer model expects inputs but it received input tensors,python tensorflow keras deeplearning nlp,thank you for posting all your code these two lines are the problem datatrain npconcatenatenlptrainnumerictrain axis datatest npconcatenatenlptestnumerictest axis a numpy array is interpreted as one input regardless of its shape either use tfdatadataset and feed your dataset directly to your model or just feed your data directly to modelfit as a list of inputs r modelfit nlptrain numerictrain ytrain epochs validationdatanlptest numerictest ytest
69704467,concatenate layer shape error in sequencesequence model with keras attention,python keras nlp attentionmodel sequencetosequence,replace axis with axis in the concatenation layer the example in this documentation should clarify why your problem resides in the inputs passed to the concatenation you need to specify the right axis to concatenate two differently shaped matrices or tensors as they are called in tensorflow the shapes and differ in the second dimension referenced by passing because dimensions start from upwards this would result in a shape increasing the elements in the second dimension when you specify axis as default value your concatenation layer basically flattens the input before use which in your case does not work due to the difference in dimensions
69677322,pretrained roberta relation extraction attribute error,nlp huggingfacetransformers roberta,you have to specify the type of tensor that you want in return for tokenizer if you dont it will return a dictionary with two lists inputids and attentionmask
69640025,fasttext attributeerror type object fasttext has no attribute reducemodel,python nlp gensim fasttext,it does have a function to do so its just not being utilized correctly adapt the dimension the pretrained word vectors we distribute have dimension if you need a smaller size you can use our dimension reducer in order to use that feature you must have installed the python package as described here for example in order to get vectors of dimension python then you can use ft model object as usual or save it for later use source as to the response why do you want to reduce the dimension size probably because the full pretrain model is a huge strain on ram this helps reduce that
69518571,why does the isdigit function return an error when i use it,python nlp,sentence is a list of strings and you are iterating over those strings so on line you are actually trying to get a specific element from your list but instead of integer ie sentence you are passing a string ie sentenceapple which is wrong for obvious reasons i think you want to do
69270987,how to resolve the error related to frame used in zstandard which requires too much memory for decoding,nlp reddit nlpquestionanswering,zstddecompressormaxwindowsize in future if anyone faces this error then above is the way to correct it in the file downloadredditqalistpy on line one can change
69240815,i am trying to importfrom torchtextlegacydata import field bucketiteratoriteratordata but get error no module named torchtextlegacy,python nlp pytorch,before you import torchtextlegacy you need to pip install torchtext maybe legacy was removed in version
69217404,spacy model load error from local directory,nlp spacy languagemodel spacy,the targz is a python package not just a model directory so you probably need to look one level deeper and load encoreweblgencoreweblg you can tell by looking for the directory that contains the subdirectories vocab tagger ner etc
69199961,oom error when training the bert keras model,python keras nlp sentimentanalysis bertlanguagemodel,a quick google search led me to this discussion where he states the splitting of the data with validationsplit parameter can lead to this oom error and the resolution was to split the data before calling modelfit by using sklearnpreprocessingtraintestsplit or any other way you prefer
69150110,problem in tqdm function in a docvec model,python nlp docvec tqdm,by using the list comprehension you are having tqdm iterate once over your traintaggedvalues sequence into an actual inmemory python list this will show the tqdm progress rather quickly then completely finish any involvement with tqdm then youre passing that plain result list without any tqdm features into docvectrain where docvec does its epochs training passes tqdm is no longer involved so therell be no incremental progressbar output you might be tempted to try or have already tried something that skips the extra list creation passing the tqdmwrapped sequence directly in like but this has a different problem the tqdmwrapper is only designed to allow report the progress of one iteration over the wrapped sequence so this will show that one iterations incremental progress but when train tries its next necessary reiterations to complete its epochs trainingruns the singlepass tqdm object will be exhausted preventing full proper training note that there is an option for progresslogging within gensim by setting the python logging level globally or just for the class docvec to info docvec will then emit a logline showing progress within each epoch and between epochs about every second but you can also make such logging lessfrequent by supplying a different seconds value to the optional reportdelay argument of train for example reportdelay for a log line every minute instead of every second if you really want a progressbar it should possible to use tqdm but you will have to work around its assumption that the iterable youre wrapping with tqdm will only be iterated over once i believe thered be two possible approaches each with different tradeoffs instead of letting train repeat the corpus n times do it yourself adjusting the other train parameters accordingly roughly thatd mean changing a line like into something that turns your desired epochs into something that looks like just one iteration to both tqdm gensims train like note that you now have to give tqdm a hint as to the sequences length because the onetime chainediterator from itertoolschain doesnt report its own length then youll get one progressbar across the whole training corpus which the model is now seeing as one pass over a larger corpus but ultimately involves the same passes youll want to reinterpret any remaining log lines with this change in mind and youll lose a chance to install your own perepoch callbacks via the models endofepoch callback mechanism but thats a seldomused feature anyway instead of wrapping the corpus with a single tqdm which can only show a progressbar for oneiteration wrap the corpus as a new fullyreiterable object that itself will start a new tqdm each time for example something like from collectionsabc import iterable class tqdmeveryiterationiterable def initself inneriterable selfinneriterable inneriterable def iterself return itertqdmselfinneriterable then using this new extra tqdmadding wrapper you should be able to do corpus utilsshuffletraintaggedvalues modeldbowtraintqdmeveryiterationcorpus totalexampleslencorpus epochs in this case you should get one progress bar per epoch because a new tqdm wrapper will be started each training pass if you try either of these approaches they work well please let me know they should be roughly correct but i havent tested them yet separately if the article from the author at actsusanlimediumcom that youre modeling your work on is note that its using an overlycomplex fragile antipattern calling train multiple times in a loop with manual alpha management that has problems as described in this other answer but that approach would also have the sideeffect of rewrapping the corpus each time in a new tqdm like the tqdmeveryiteration class above so despite its other issues would achieve one actual progressbar each call to train i sent the author a private note via medium about a month ago about this problem
69129913,valueerror input has nfeatures while the model has been trained with nfeatures,python pythonx machinelearning scikitlearn nlp,we never fittransform the test set we use simply transform instead change to similarly you should not refit the tokenizer on the test data with tokenizerfitontextstest you should change this to see the documentation and the so thread what does keras tokenizer method exactly do for more on tokenizer
69041790,attribute error creating a column of ner labels,nlp spacy namedentityrecognition,you need to do something like this the output of nlpx is a doc object and there is no label attribute on the doc object as is explicitly stated in the error you get you need the labels of the entities on the doc object which is why you need to iterate over nlpxents and get the label of each entity
68956738,r sql server file does not exist error but it does,sql r nlp udpipe,ive done it finally yes it was a permissions issue but not like you would expect aside from sql having access to the folder to have r access a file folder outside of working directory you have to give permissions to all application packages object to that folder hope that saves anyone else the hours of piecing together google bits
68905349,how to fix the error valueerror could not convert string to float in a nlp project in python,python pandas nlp logisticregression sentimentanalysis,you should define your variable exl as the following exl vectorizertransformthis book was so interstening it made me not happy and then do the prediction first put the testing data in a list and then use vectorizer to use features extracted from your training data to do the prediction
68889843,valueerror index length mismatch vs,python nlp vectorization tfidfvectorizer datapreprocessing,the reason you are getting the error is because tfidfvectorizer only accepts lists as the input you can check this from the documentation itself here you are passing a dataframe as the input hence the weird output first convert your dataframe to lists using and then pass it to the vectorizer also there are many ways to convert dataframe to list but the output of of all of them might be different formats of list for example if you try to convert dataframe to list using it will convert the dataframe to list but then the format of this list wont work with tidfvectorizer and will give you the same output as you were getting before in your question i found the above way of converting to list to work with the vectorizer another thing to keep in mind is that you can only have one columnvariable in your listdataframe if you have more than one columns in your dataframe and you convert it to list and pass it to the vectorizer it will throw an error i dont know why this is but just throwing it out there in case someone faces this problem
68760136,attributeerror caught attributeerror in dataloader worker process fine tuning pretrained transformer model,python machinelearning nlp bertlanguagemodel,updated i skimmed several lines of documentation here about how to use the fit method and i realized there is a simpler solution to do what you desired the only changes you need to consider are to define proper inputexample for constructing a dataloader create a loss
68742863,error while trying to finetune the reformermodelwithlmhead googlereformerenwik for ner,python nlp pytorch huggingfacetransformers namedentityrecognition,first of all you should note that googlereformerenwik is not a properly trained language model and that you will probably not get decent results from finetuning it enwik is a compression challenge and the reformer authors used this dataset for exactly that purpose to verify that the reformer can indeed fit large models on a single core and train fast on long sequences we train up to layer big reformers on enwik and imagenet this is also the reason why they havent trained a subword tokenizer and operate on character level you should also note that the lmhead is usually used for predicting the next token of a sequence clm you probably want to use a token classification head ie use an encoder reformermodel and add a linear layer with classes on topmaybe a dropout layer anyway in case you want to try it still you can do the following to reduce the memory footprint of the googlereformerenwik reformer reduce the number of hashes during training from transformers import reformerconfig reformermodel conf reformerconfigfrompretrainedgooglereformerenwik confnumhashes or maybe even to model transformersreformermodelfrompretrainedgooglereformerenwik config conf after you have finetuned your model you can increase the number of hashes again to increase the performance compare table of the reformer paper replace axialposition embeddings from transformers import reformerconfig reformermodel conf reformerconfigfrompretrainedgooglereformerenwik confaxialposembds false model transformersreformermodelfrompretrainedgooglereformerenwik config conf this will replace the learned axial positional embeddings with learnable position embeddings like berts and do not require the full sequence length of they are untrained and randomly initialized ie consider a longer training
68709240,error module object is not callable in docvec,python nlp docvec,you are importing tqdm module and not the actual class replace import tqdm with from tqdm import tqdm
68678278,runtimeerror shape is invalid for input of size while while evaluating test data,python neuralnetwork nlp bertlanguagemodel,i think the problem is that the training datasets dinputids was of size so it could be divided into and but the testing datasets dinputids is of size which cannot be divided into and since you havent given the model description i cant say if you should change it to or using in reshape tells numpy to figure that dimension out automatically eg reshaping an array of elements into can be done by reshape and reshape and reshape as well
68676637,attributeerror wordvec object has no attribute mostsimilar wordvec,python nlp gensim wordvec docvec,you are probably looking for wvmostsimilar so please try
68627093,bert problem with contextsemantic search in italian language,machinelearning nlp bertlanguagemodel,the problem is not with your code it is just the insufficient model performance there are a few things you can do first you can try universal sentence encoder use from my experience their embeddings are a little bit better at least in english second you can try a different model for example sentencetransformersxlmrdistilrobertabaseparaphrasev it is based on roberta and might give a better performance now you can combine together embeddings from several models just by concatenating the representations in some cases it helps on expense of much heavier compute and finally you can create your own model it is well known that single language models perform significantly better than multilingual ones you can follow the guide and train your own italian model
68607340,problem to covert data from conll format to spacy format,python nlp dataset spacy,well your first problem is exactly what the error says there is no o option the output is just the argument after the input i see that in my other answer i put an o but that was a mistake your second problems is that conll and conllu format are not the same thing the twocolumn format you have is referred to as conll or just ner in spacy if you fix those issues the conversion should just work i am not sure what your spacyformat line is referring to
68601439,i got valueerror x has features per sample expecting when applying linear svc model to test set,python machinelearning text nlp svm,do not call fittransform on the test data as the transformers will learn a new vocabulary and not transform the test data the same way the training data was transformed to use the same vocabulary as for the training data use only transform on the test data instead initialize transformers countvect countvectorizer tfidftransformer tfidftransformer fit and transform train data xtraincounts countvectfittransformtraincleantext xtraintfidf tfidftransformerfittransformxtraincounts transform test data xtestcounts countvecttransformtestcleantext xtesttfidf tfidftransformertransformxtestcounts note if you dont need the output of countvectorizer you could use tfidfvectorizer to reduce the amount of code to write tfidfvect tfidfvectorizer xtraintfidf tfidfvectfittransformtraincleantext xtesttfidf tfidfvecttransformtestcleantext
68564817,implementing custom loss function in tensorflow leading to valueerror outputs must be defined before the loop,python tensorflow keras nlp lossfunction,as mentioned previously the error shown has nothing to do with the custom loss function the code that you showed had numerous other bugs like not importing tfkeraslayers properly after fixing those bugs see the code below and test out on the versions below works fine
68538365,issues converting text to lower case even after making sure it is a string,python string text nlp lowercase,use this
68533685,attributeerror type object language has no attribute factory,nlp spacy bertlanguagemodel,the issue seems to be with your environment attributeerror type object language has no attribute factory message is a spacy x error please try to run it with spacy x if you have already installed x then please verify if your virtual environment is pointing to correct python
68495699,an error to build a custom model using spacy,pythonx machinelearning nlp cpu spacy,it looks like you doublepasted the config or something from the errors youll note that it says you have two paths sections about halfway through your file theres a comment like this try deleting everything from there and down and then doing it again
68383768,problems using spacy tokenizer with special characters,python nlp spacy tokenize,i suggest using import re text resubrs r text patterntest text regex rdd orth here s regex is used to match any nonwhitespace capturing it into group and then matching a or capturing it into group and then resub inserts a space between these two groups the dd regex matches a full token text that contains a float value and the is the next token text because the number and are split into separate tokens by the model full python code snippet import spacy re from spacymatcher import matcher nlp spacyloadptcorenewssm nlp spacyloadencorewebtrf matcher matchernlpvocab text total comex deriv ativo text resubrs r text patterntest text regex dd orth text nlptext matcheraddpattern test patterntest result matchertext for id beg end in result printid printtextbegend output
68374589,valueerror io operation on closed file flush,python text nlp valueerror,with automatically close and flush the file so just delete the last lines the file is already closed you dont need to flushclose it close already flushes the file flush on a file is used usually when youve a lot of writes pending in the os cache and you want them on the filesystem
68255318,how to write the code to avoid the error of tensorflow has no attribute session and globalvariablesinitializer,pythonx tensorflow keras nlp googlecolaboratory,to execute your code in tensorflow x you can try as shown below
68235963,problem analyzing a doc column in a df with spacy nlp,python pandas nlp spacy,you are trying to get the matches from the dfdoc string with doc nlpdfdoc you need to extract matches from the dfdoc column instead an example solution is to remove doc nlpdfdoc and use the nlp spacyloadencorewebsm def findmatchesdoc spans docstartend for start end in matcherdoc for span in spacyutilfilterspansspans return spanstart spanend spantext dfdocapplyfindmatches none love these none none name doc dtype object full code snippet import numpy as np import pandas as pd import matplotlibpyplot as plt import spacy from spacymatcher import matcher nlp spacyloadencorewebsm df pdreadcsvrcusersadmindesktopstxt calling on nlp to return processed doc for each review dfdoc nlpbody for body in dfbody sum the number of tokens in each doc dfnumtokens lentoken for token in dfdoc calling matcher to create pattern matcher matchernlpvocab pattern lemma love op matcheraddqualitypattern pattern doc nlpdfdoc matches matcherdoc def findmatchesdoc spans docstartend for start end in matcherdoc for span in spacyutilfilterspansspans return spanstart spanend spantext printdfdocapplyfindmatches
68191225,xgbclassifier valueerror operands could not be broadcast together with shapes,python machinelearning scikitlearn nlp,turns out the solution was to remove the onevsrestclassifier usage from the pipeline
68155580,sparknlp java error while trying to display model results,python apachespark pyspark nlp johnsnowlabssparknlp,i figured out the problem i installed sparknlp using conda but i also installed it using pip for some reason juypter will not recognize sparknlp when i install it using conda however when both versions were installed it creates this error i uninstalled the conda version and left only the pip method installed this solved the problem
68113990,error unable to load vocabulary from file when loading spacy frdepnewstrf model,python nlp spacy,the model download or install is probably corrupted uninstall the model package pip uninstall frdepnewstrf and try downloading it again without using any local cached copies spacy download frdepnewstrf nocachedir
68080447,error calling adapt in textvectorization keras,python tensorflow keras nlp,i referred the document you shared earlier following was mentioned for custom standardize when using a custom callable for standardize the data received by the callable will be exactly as passed to this layer the callable should return a tensor of the same shape as the input so i changed replaced the return tfstringssplitregex with return regex as splitting is changing the shape here please try like this providing gist for reference
68052695,error typeerror int object is not subscriptable while using lops in python,python list loops nlp,it should be not the
68048737,training spacy nameerror,python nlp spacy,its hot here thanks for the hint i missed importing from spacytraining import example when moving the code from jupyter to visual studio code for the deployment
67997713,modulenotfounderror no java install detected please install java to use languagetoolpython,python nlp grammar,i think this is not an issue with the code itself when i run the code you provided i get as result a number in this case in the documentation of the languagetoolpython is written by default languagetoolpython will download a languagetool server jar and run that in the background to detect grammar errors locally however languagetool also offers a public http proofreading api that is supported as well follow the link for ratelimiting details running locally wont have the same restrictions so you will need java jre and skd also its written in the requirements of the library prerequisites python languagetool java or higher the installation process should take care of downloading languagetool it may take a few minutes otherwise you can manually download languagetoolstablezip and unzip it into where the languagetoolpython package resides source python javaerror when using grammarcheck library i hope i could help
67915131,spacy error in loading pretrained custom model with entity rulers and ner pipeline,python machinelearning nlp spacy namedentityrecognition,upgrading to spacy version resolved this error
67909030,how to prepare text for bert getting error,pythonx nlp bertlanguagemodel transferlearning,the error is because your x dfsentiments and y dfreviews lines where your x and y are still dataframe columns or dataframe series and not list a simplet way to change them is x dfsentimentsvalues and y dfreviewsvalues which returns numpy array and it works if notit can be further converted to python list using x dfsentimentsvaluestolist and y dfreviewsvaluestolist
67865773,attributeerror cant get attribute on module gensimmodelswordvec,python nlp gensim wordvec,the problem is that the referenced repository trained a model on an incredibly old version of gensim which makes it incompatible with current versions you can potentially check whether the lifecycle meta data gives you any indication on the actual version and then try to update your model from there the documentation also gives some tips for upgrading your older trained models but even those are relatively weak and point mostly to retraining similarly even migrating from gensim x to x is not referencing direct upgrade methods but could give you ideas on what parameters to look out for specifically my suggestion would be to try loading it with any of the previous x versions and see if you have more success loading it there
67862504,attributeerror list object has no attribute ents,python list nlp attributeerror namedentityrecognition,the error is self explanatory you made an ordinary python list of objects called doc a python list has no attribute called ents simply iterate through the elements in the list like this provided your list elements indeed have attributes text and label this should work cant verify from the code shown that they do have those attributes
67777505,memoryerror with fastapi and spacy,nlp spacy fastapi uvicorn,the spacy tokenizer seems to cache each token in a map internally consequently each new token increases the size of that map over time more and more new tokens inevitably occur although with decreasing speed following zipfs law at some point after having processed large numbers of texts the token map will thus outgrow the available memory with a large amount of available memory of course this can be delayed for a very long time the solution i have chosen is to store the spacy model in a ttlcache and to reload it every hour emptying the token map this adds some extra computational cost for reloading the spacy model but that is almost negligible
67768470,valueerror found input variables with inconsistent numbers of samples,python pandas machinelearning scikitlearn nlp,this is happening because you have a text transformer object in your pipeline the problem with this approach is that the pipeline will pass the whole dataframe to the tfidfvectorizer however the text transformers of scikitlearn expect a d input passing a d dataframe to tfidfvectorizer causes some weird processing where it mistakes the column names as documents you can check with this simple example x pddataframe f this is doc this is doc this is doc this is doc this is doc f vec tfidfvectorizer printvecfitxgetfeaturenames f f this explains why the error message states that there is an inconsistent number of samples the tfidfvectorizer thought of the columns in the dataframe to be the samples and their names to be the features if you want to use tfidfvectorizer in your pipeline you have to make sure that only the column with the text documents is passed to it you can achieve this by wrapping it in a columntransformer if only one column needs to be transformed transformer columntransformer vec tfidfvectorizer column column should be a string or int remainderpassthrough if more than one column needs to be transformed discouraged see note below transformer columntransformer vec tfidfvectorizer col col should be a string or int vec tfidfvectorizer coln coln should be a string or int remainderpassthrough replace column above with the index or the name of the column the tfidfvectorizer has to transform and it will only process this particular column the remainderpassthrough will make sure the other columns are left as is and are concatenated with the result you can then use it in your pipeline like this model pipeline vect transformer smote smotetomek chi selectkbestchi k classifier algorithm note if you have to transform several columns with text you should consider merging the column entries into a single combined document and only transform this combined document otherwise each column will be treated with a new vocabulary although these might overlap to some extent and you might end up with a very high dimensionality a lot of features
67664837,problems understanding ndcg format in pytreceval,pythonx machinelearning nlp informationretrieval,in order to compute the ndcg you need to know what is the relevance of each document in a ranked list of results for this query this information is contained in qrels ranking means that you first need to sort retrieved documents in descending order of their score so you basically sort documents and then go rank by rank starting from the lowest rank ie the topscored document which is for each rank i you get the documents groundtruth relevance reli from qrels and then you divide this relevance by logi to get a term for this rank i you sum all these terms across all ranks and you get the discounted cumulative gain dcg for this query therefore pytreceval internally needs to create a sorted list from the dictionary mapping from doc id to score in order to get the ranks this is why the order of the documentscore pairs in the dictionary you pass as an input doesnt matter now as an additional detail to get the ndcg ie normalized dcg you divide the dcg by the ideal dcg which is the maximum dcg achievable by any model to get the idcg you sort the groundtruth as opposed to retrieved documents in descending order of their relevance score and compute the dcg again to get the groundtruth relevance scores you need qrels
67635055,python bert model pooling error mean received an invalid combination of arguments got str int,python nlp pytorch kaggle huggingfacetransformers,since one of the x updates the models return now taskspecific output objects which are dictionaries instead of plain tuples you can either force the model to return a tuple by specifying returndictfalse o selfbert ids attentionmaskmask tokentypeidstokentypeids returndictfalse or by utilizing the basemodeloutputwithpoolingandcrossattentions object o selfbert ids attentionmaskmask tokentypeidstokentypeids you can view the other attributes with okeys o olasthiddenstate
67616158,modulenotfounderror no module named flair,python nlp flair,install via the following command make sure you use user option otherwise you will get a permission error in windows after install flair you have to restart kernel in jupyter notebook
67612600,how to fix this code and make my own postagger python,python nlp postagger,starting simply with nlp makes it easier to understand and also to appreciate the more advanced systems this gives what youre looking for
67577227,indexerror list index out of range nlp bert tensorflow,tensorflow machinelearning keras deeplearning nlp,as shown in the ktrain tutorials and example notebooks like this one you need to use the predictor instance to make predictions on raw text inputs create a predictor instance predictor ktraingetpredictorlearnermodel preproc make prediction output predictorpredicti loved this movie printoutput save predictor to disk predictorsavetmpmypredictor reload predictor from disk reloadedpredictor ktrainloadpredictortmpmypredictor make another prediction output reloadedpredictorpredicti loved this movie printoutput
67573416,unable to recreate gensim docs for training fasttext typeerror either one of corpusfile or corpusiterable value must be provided,python nlp gensim fasttext,so i found the answer to this they have a problem with the argument sentence in both all you have to do is to remove the argument name or simply pass the first argument which is corpusiterable or
67526697,bert tokenizer addtoken function not working properly,python nlp pytorch bertlanguagemodel,yes if a token already exists it is skipped by the way after changing the tokenizer you have to also update your model see the last line below
67496616,runtimeerror input output and indices must be on the current device fillmaskrandom text,python nlp pytorch bertlanguagemodel huggingfacetransformers,the trainer trains your model automatically at gpu default value nocudafalse you can verify this by running modeldevice after training the pipeline does not this and this leads to the error you see ie your model is on your gpu but your example sentence is on your cpu you can fix that by either run the pipeline with gpu support as well fillmask pipeline fillmask modelmodel tokenizerberttokenizer device or by transferring your model to cpu before initializing the pipeline
67477497,spacy returns attributeerror spacytokensdocdoc object has no attribute spans in simple spans assignment why,nlp spacy spacy,this code should work correctly from spacy v onwards if it doesnt can you verify that you are in fact running the code from the correct virtual environment within colab and not a different environment using spacy v we have previously seen issues where colab would still be accessing older installations of spacy on the system instead of sourcing the code from the correct venv to double check you can try running the code in a python console directly instead of through colab
67352227,i have a data type problem in the text classification problem,python numpy deeplearning nlp textclassification,you need to add an embedding layer at the top of your nn to kind of vectorize words something like this
67350459,i was working on a movie sentiment analysis but code but im facing issues in my code related to processing words,nlp nltk sentimentanalysis,the most timeconsuming part of the written code is the stopwords part it will call the library to get the list of stopwords each time the loop iterates therefore its better to get the stopwords set once and use the same set at each iteration i rewrote the code as following other differences are made just for the sake of the readability
67259823,problem to extract ner subject verb with spacy and matcher,python nlp nltk spacy,this is a perfect use case for the dependency matcher it also makes things easier if you merge entities to single tokens before running it this code should do what you need check out the docs for the dependencymatcher
67244484,key error while comparing the similarity between two statements using glove vectors,nlp stanfordnlp similarity,i have found my mistake and i am just keeping this question so that somebody may get help the mistake i did is i have typed a wrong spelling like vehcile instead of vehicle
67229373,gensim lda error cannot compute lda over an empty collection no terms,python nlp gensim lda,as shown in the gensim lda tutorial you need to load the dictionary before passing dictionaryidtoken to the ldamodel using your example the code should be this is because idtoken is initialized in a lazy manner to save memory not created until needed you can refer to the documentation here
67225803,keras nonetypeerror in deep autoencoderdecoder structure shape error,machinelearning keras nlp,fixed version of the problematic endcoder decoder
67055391,how to fix classifer and lambda to textblob,python pandas lambda nlp,with a quick search in the doc and by executing it blobsentiment is not your classifier it is the default textblob sentiment classifier in order to use your classifier you should use textblobtweetclassifierclclassify not textblobtweetclassifierclsentiment
67043468,unparsedflagaccesserror trying to access flag preserveunusedtokens before flags were parsed bert,python nlp bertlanguagemodel,based on this issue you have to downgrade berttensorflow to check this answer to find a solution if you are following this tutorial downgrade berttensorflow and use the wget quiet as suggested because inside the python code the author has made the change from tfgfilegfilevocabfile r to tfiogfilegfilevocabfile r after that code compiles successfully ping me if you want anything else
66979328,pytorch valueerror target size torchsize must be the same as input size torchsize,python nlp pytorch convneuralnetwork,your issue is here selfconvs nnmodulelist nnconvdinchannels outchannels nfilters kernelsize fs embeddingdim for fs in filtersizes you are inputting data of shape which the convolutions are interpreting as batches of size of channel images of hxw x what it appears you want is a batch of size so swap those dimensions first embedded embeddedswapdims conved freluconvembeddedsqueeze for conv in selfconvs
66979242,r error in textranksentencesdata articlesentences terminology articlewords nrowdata is not true,r text nlp tidyr,the link that you shared reads the data from a webpage divclasspadded is specific to the webpage that they were reading it will not work for any other webpage nor the pdf from which you are trying to read the data you can use pdftools package to read data from pdf
66911216,param poolinglayer does not exist error coming while loading bert embedding model in sparknlp,nlp johnsnowlabssparknlp,its likely you have mixed versions of models and library that parameter that the exception is complaining has been recently removed from the bert model so you should try a different pretrained bert model
66818030,error when trying to run a gridsearchcv on sklearn pipeline,python scikitlearn nlp xgboost gridsearchcv,the main problem is your scoring parameter for the search scorers for hyperparameter tuners in sklearn need to have the signature estimator x y you can use the makescorer convenience function or in this case just pass the name as a string scorerf see the docs the list of builtins and information on signatures you do not need to explicitly use the transform method thats handled internally by the pipeline
66767760,error raised in modelfit if validationdata valueerror the truth value of an array with more than one element is ambiguous use aany or aall,keras tensorflow nlp numpy pythonx,itd be excellent if you could copypaste the entire stack of error that your code produces something that everyone should follow for errorrelated questions because that makes debugging that much easier heres an attempt to reproduce the same error using a dummy dataset on googlecolab this gives the following error which is identical to your op the reason itd be better to post the error stack is because the answer is hidden in these lines specifically the format of validationdata is identical to x y sampleweight heres what fit method documentation has to say validationdata will override validationsplit validationdata could be tuple xval yval of numpy arrays or tensors tuple xval yval valsampleweights of numpy arrays dataset for the first two cases batchsize must be provided for the last case validationsteps could be provided i think you now understand why youre getting an error theres no y for the your autoencoder which shouldnt be of any concern since your x itself is your y heres a line from an encoder tutorial that would help us in this situation train the model using xtrain as both the input and the target the encoder will learn to compress the dataset from dimensions to the latent space and the decoder will learn to reconstruct the original images so what you were expected to do is to write the following which indeed starts the training
66725902,attributeerror spacytokensspanspan object has no attribute merge,python nlp spacy,spacy did away with the spanmerge method since that tutorial was made the way to do this now is by using docretokenize i implemented it for your scrub function below loop through all the entities in a document and check if they are names def scrubtext doc nlptext with docretokenize as retokenizer for ent in docents retokenizermergeent tokens mapreplacenamewithplaceholder doc return jointokens s in alan turing published his famous article computing machinery and intelligence in noam chomskys syntactic structures revolutionized linguistics with universal grammar a rule based system of syntactic structures printscrubs other notes your replacenamewithplaceholder function will throw an error use tokentext instead i fixed it below if you are extracting entities and in addition other spans like docnounchunks you may run into some issues such as this one for this reason you also may want to look into spacyutilfilterspans
66675261,how can i work with example for nlpupdate problem with spacy,nlp spacy namedentityrecognition,you didnt provide your traindata so i cannot reproduce it however you should try something like this from spacytrainingexample import example for batch in spacyutilminibatchtrainingdata size for text annotations in batch create example doc nlpmakedoctext example examplefromdictdoc annotations update the model nlpupdateexample losseslosses drop
66628569,nlu watson api apiexception error invalid request content is empty code,python nlp ibmwatson namedentityrecognition,the apiexception error invalid request content is empty code error occurs when an empty string or invalid characters are passed as input to watson nlu check your input data for missing or invalid text values before dispatching the api request
66625389,attributeerror list object has no attribute size huggingface transformers,pythonx nlp huggingfacetransformers,the model requires pytorch tensors and not a python list simply add returntensorspt to prepareseqseq from transformers import autotokenizer automodelforseqseqlm tokenizer autotokenizerfrompretrainedhelsinkinlpopusmtenhi model automodelforseqseqlmfrompretrainedhelsinkinlpopusmtenhi text hello my friends how are you doing today tokenizedtext tokenizerprepareseqseqbatchtext returntensorspt perform translation and decode the output translation modelgeneratetokenizedtext translatedtext tokenizerbatchdecodetranslation skipspecialtokenstrue print translated text printtranslatedtext output
66579324,error running runseqseqpy transformers training script,python tensorflow machinelearning nlp huggingfacetransformers,the problem is that you clone the master branch of the repository and try to run the runseqseqpy script with a transformers version that is behind that master branch runseqseqpy was updated to import isofflinemode on the th of march with this merge all you need to do is to clone the branch that was used for your used transformers version git clone branch vrelease ps i do not think you need to clone the dataset library
66547373,connecting jupyter widgets with qa pipeline for question selection problem,python jupyternotebook nlp googlecolaboratory,it was a scope issue the scope is within the widget
66512572,receiving key error while calculating the polarity in python,python nlp sentimentanalysis keyerror,add this line in your code you have probably filtered out result which have changed the index in your jordandf you can see in head of your jordandf that the index starts with and thats why you get keyerror on key ie when i in jordandftexti
66444082,the pattern order issue in nlp spacy matcher in python,python nlp matcher,perhaps you are looking for in attribute or issubset attribute instead of mapping to a single value you can use these attributes to match the dictionary of properties take a look at extended patterns maybe you can use issubset too depending on your use case code output
66436192,feature extraction for multiple text columns for classification problem,python machinelearning nlp featureextraction,the way to use multiple columns as input in scikitlearn is by using the columntransformer here is an example on how to use it with heterogeneous data
66432499,making a list from nlp object is not working while the spacy lecture goes with that approach,python nlp spacy,somehow your list constructor seems to be gone this could have been because of operations like list something anyways this should fix it
66398873,lookuperror resource stopwords not found please use the nltk downloader to obtain the resource,python text nlp nltk textmining,i solved this problem by download the corresponding zip file on nltkorg then manually setup the cnltkdatacorpora dir
66342359,nlpupdate issue with spacy typeerror e the languageupdate method takes a list of example objects but got,python nlp spacy,you need to convert traindata to example type probably the easiest way is using examplefromdict method
66311315,custom ners training with spacy throws valueerror,python nlp spacy namedentityrecognition spacy,you need to change the following line in the for loop to the code should work and produce the following results
66290815,lightgbm on numericalcategoricaltext features typeerror unknown type of parameterboostingtype gotdict,python machinelearning scikitlearn nlp lightgbm,you are setting up the classifier wrongly this is giving you the error and you can easily try this before going to the pipeline gives you the same error you can set up the classifier like this then using an example you can see it runs
66213829,valueerror shape of passed values is indices imply,pythonx pandas jupyternotebook nlp tfidf,the index youre passing as docnames is empty which is obtained from dataset as follows so this means that the dataset is empty too for a workaround you can create doc indices based on the size of ldaoutput as follows let me know if this works
66197779,gensim wordvec attributeerror wordvec object has no attribute mostcommon,python nlp gensim wordvec,gensims wordvec doesnt contain a mostcommon method if for whatever reason you must extract wordfrequency pairs from your model you can use and sort the resulting list this is a decidedly strange use case however
66156046,allennlp using with multitaskdatasetreader leads to runtimeerror,python nlp allennlp,there are two issues here one is a bug in allennlp that is fixed in version the other one is that sinaj was missing the defaultpredictor in his model head
65989256,error in shape dimention and type of keras model input,python tensorflow keras nlp layer,your models output is of shape none ie each samples ouptput is x but your ground truth for each sample is a scalar there are two way to deal with this problem flatten the outputs and continue using your data remove the unnecessary dimension in your data ie flatten each sample from x to just and change the model architecture to deal with d data which will result in output being d fixed code approach output approach output
65939855,spacy custom name entity recognition ner catastrophic forgetting issue,python nlp spacy namedentityrecognition doccano,i am not spacy expert but i had the same problem there are some points which are necessary annotation tool amount of train data mixing of correct predicted entities first make sure that your training data is correctly labeled by tool of your choice you dont get userwarnings for a good prediction your model needs a lot of data it means at least examples for each entity you want to train i personally label as much data as possible and spacys maker reccomend to mix the entities which your model corretly predicted
65924090,simpletransformers error versionconflict tokenizers how do i fix this,tensorflow nlp bertlanguagemodel simpletransformers sentencetransformers,i am putting this here incase someone faces the same problem i was helped by the creator himself
65855882,keras problem attributeerror tensor object has no attribute kerashistory by fitting bilstmcrf,tensorflow keras deeplearning nlp crf,i solved this issue by installing tensorflow version and keras version its happening because keras now doesnt support kerascontrib in newer version
65698304,spacy language model load issues from encodewebsm to encodeweblg,python nlp spacy,are you sure you downloaded the model encoreweblg to disk you can do this by running this in the command line or in the script
65685310,colab oserror errno file name too long when reading a docxtext file,pythonx nlp nltk pythonre,it seems that you are using the contents of the file mytext as the filename parameter to loaddoc and hence the error i would think that you rather want to use one of the actual file names as a parameter possibly docx and not the contents of this file
65636002,notfittederror countvectorizer vocabulary wasnt fitted while performing sentiment analysis,python scikitlearn nlp sentimentanalysis countvectorizer,loadedvectorizer is not defined anywhere in this code so its not surprising that its not initialized also why do you initialize veczr twice apparently you dont use it the second time
65514944,tensorflow embeddings invalidargumenterror indices is not in node sequentialembeddingembeddinglookup,tensorflow nlp wordvec embedding wordembedding,i solved this solution i was adding a new dimension to vocabsize by doing it vocabsize as suggested by others however since sizes of layer dimensions and embedding matrix dont match i got this issue in my hands i added a zero vector at the end of my embedding matrix which solved the issue
65488631,attributeerror wordlist object has no attribute split,python nlp token lemmatization wordlist,what you are trying to do wont work because you are applying a string function split to a word list i would try to use nltk instead and create a new pandas column with my tokenized data
65484081,translating using pretrained hugging face transformers not working,pythonx nlp translation huggingfacetransformers huggingfacetokenizers,this is not how an mt model is supposed to be used it is not a gptlike experiment to test if the model can understand instruction it is a translation model that only can translate there is no need to add the instruction translate english to dutch dont you want to translate the other way round also the translation models are trained to translate sentence by sentence if you concatenate all sentences from the column it will be treated as a single sentence you need to either iterate over the column and translate each sentence independently split the column into batches so you can parallelize the translation note that in that case you need to pad the sentences in the batches to have the same length the easiest way to do it is by using the batchencodeplus method of the tokenizer
65333831,gensim error with mostsimilar jupyter kernel restarting,pythonx machinelearning nlp gensim wordvec,if the jupyter kernel is dying without a clear error message you are likely running out of memory there may be more information logged to the console where you started the jupyter server if you expand you question to include any info there as well as details about the model youve loaded size on disk and system youre running on especially ram available it may be possible to make other suggestions also whereas gensim requires a big new increment of ram when the first mostsimilar call is made the gensimbeta prerelease only needs a muchsmaller increment at that time so it is far more likely that if a model succeeds in loading you should also be able to get mostsimilar results so it would also be useful to know how did you install the gensimbeta and did you confirm thats the version actually used by your notebook kernels environment are you certain that the prior steps such as loading have succeeded and that its only exactly the mostsimilar thats triggering the failure is it in a separate cell and before attempting the mostsimilar can you query other aspects of the model such as its length or whether it contains certain words successfully
65273410,issue in creating semgrex patterns with relation names containing colon,parsing nlp constants stanfordnlp dependencyparsing,found the answer just wrap the expression within and it works for eg
65131297,nlp user specific text corpora python getting runtime error in hackerrank though program giving correct output,python nlp runtimeerror,remove the print statements and add a return statement
65061831,iterate over lists to get and store vectors valueerror could not broadcast input array from shape into shape,python nlp spacy,your error is due to your loading vectorless model try a model with vectors instead and youre fine to go note commented use of nlppipe generator which should speed up execution and allow processing of bigger files
65040277,spacy save model to disk with custom sentencizer error,python oop nlp spacy,the following should do note supermysentencizer selfinit
64888019,why does my python code gives the type error as the dict object is not callable when loading a list of dictionaries into a tokenizer object,machinelearning keras nlp tensorflow tokenize,the problem is the following probably you want to store tokenizers wordindex into wordindex variable instead you are calling tokenizerwordindex as if it was a methodfunction but it is a dictionary so i think that you have to apply the following correction
64664283,importing any embedding layer from tensorflow hub gives url error kaggle kernel,python tensorflow machinelearning nlp tensorflowhub,try turning on internet access of your kaggle kernel be default your kernel has no internet access you have to turn it on to get resources from other site see
64620423,keyerror true error when i try to convert labels to and,python dataframe nlp pytorch label,these is a trailing space in true thats why there is no match in labelmap try edit if you are not sure what lies in datalabelcolumn i would suggest catching unknown values with a default output value using labelmapgetxstrip
64472953,error when trying to add new words into wordsegment dictionary module wordsegment has no attribute bigramcounts,python nlp,try wsbigramsnew zealand e
64460399,readlines causing error after many lines,python nlp namedentityrecognition readlines,i used the wnuttrainconll file from and i ran a similar code to generate your required output i found that in some lines instead of tn as the blank line we have only n due to this lsplit will give an indexerror list index out of range to handle this we can check if length is and in that case also we add our tmp to train hope your question is resolved
64416381,typeerror unhashable type list for text summarization,python nlp nltk summarization,use a defaultdict
64385830,getting n gram suffix using sklearn count vectorizer,python machinelearning scikitlearn nlp ngram,yo can define a custom analyzer to define how the features are obtained from the input for your case a simple lambda function to obtain the suffixes from a word will suffice now if we construct a dataframe from the resulting vectorized matrix
64273610,how do i tokenize a text data into words and sentences without getting a type error,python nlp tokenize,moving comment to answer you are trying to process the file object instead of the text in the file after you create the text file reopen it and read the entire file before tokenizing try this code
64231130,tensorflow error concatenating char and word embedding,python tensorflow keras nlp embedding,finally i was able to resolve the problem but flattening the char embedding then it can be easily concatenated with word embeddings by adding this line it worked
64200872,how may i resolve tree not correctly binarized error in stanfordcorenlp,machinelearning nlp stanfordnlp,this is a really obscure bug in the sr parser caused by a small handful of training trees in one of the recently added training sets breaking the assumption that all trees end with s frag etc and then go to root as a unary transition the error produced here is only part of the sentence causing the problem and the sr parser was trying to split that sentence into two pieces with two separate roots in fact the problem is even weirder than that since this only caused a bug downstream when one of those roots then had or more pieces anyway the bug can be fixed by enforcing that root nodes only occur at the very top of the tree that fix is in this branch of the source tree if you dont see a branch there that means its already been merged into the plan is to post a new release in the next few weeks at which point you can simply use the new release rather than build from source in order to get this bugfix edit you specifically want this change anything past that will not be able to load old models since theres a hack in readobject required to make it load the old models and were going to redo the models with the updated transition rules
64136814,glove import error corpus unable to import,nlp stanfordnlp,you can use glove from mittens as well mittens use the same algorithm as glove and vectorizes the objective function install import for details
64135736,how to split prefix and suffix in python,python unix nlp,here is a rough approximation of what you seem to be asking using regular expressions by way of the python re library import re m rematchr word if m prefix root suffix mgroups your examples also seem to have and as separators but extending this to allow for those as well should be relatively straightforward once you understand how this is working in brief rematch returns false if the regex doesnt match and otherwise it returns a match object whose groups method contains the text which matched the grouping parentheses in the regular expression parentheses are nongrouping parentheses without immediately after the opening parenthesis capture into a group the expression can be divided into the following nongrouping expression to skip anything in braces nongrouping wrapper around a grouping expression for anything before a dash main root grouping expression to capture text which doesnt match one of the delimiters nongrouping expression to ignore anything after a slash similar to the dash capture any suffix after with a nongrouping wrapper around the whole group and then a grouping capture for the text after the delimiter this will seem intimidating at first but once you have deciphered the first couple you should understand how they all work in some more detail lets examine the dash expression we need a noncapturing group to mark all of this as optional down at the end capture any matching text between the grouping parentheses into a group match a single character which is not or or or or actually one or more of the previous as many as possible end of capture as long as all of this ends in a literal dash all of this is optional if skipping this will allow the overall expression to match the regex engine will but it will still prefer to match if possible this is called greedy matching notice how keeps appearing in all of these groups we dont want to allow the text we capture to match one of the delimiters you dont specify what to do with spaces so this simply regards them as any other character perhaps you want to keep them out of groups when they are adjacent to a delimiter too demo
64041738,how to use holavpn to resolve googletranss json decode error line column char,python json nlp googletranslate languagetranslation,the library makes a request and without checking the status code assumes that the requests was successful now google might not like your request for any reason it and return an error message with a matching status code xx xx the library still tries to parse the body as json which does not work since there is no body and raises a jsondecodeerror that is unrelated to the original problem you cant see the real cause conclusion the googletrans library is missing crucial error handling you may have to edit it and add error handling yourself
63924567,gpt on hugging facepytorch transformers runtimeerror grad can be implicitly created only for scalar outputs,python nlp pytorch huggingfacetransformers huggingfacetokenizers,i finally figured it out the problem was that the data samples did not contain a target output even tough gpt is selfsupervised this has to be explicitly told to the model you have to add the line to the getitem function of the dataset class and then it runs okay
63764186,this error comes indexerror list index out of range,python nlp nltk similarity wordnet,the problem is that there are empty lists contained in uploadedsentencesynset im not sure what youre trying to do but modify the last block of code to without the ifelse block youre essentially trying to index the first element of a list giving you an indexerror
63741078,error while extracting url from newspaper website,python webscraping beautifulsoup nlp pythonrequests,it seems that some tags dont contain tags so litagfindahref fails you need to check for that possibility prints
63726387,runtimeerror cost function returns nan values in its th output,python deeplearning nlp pytorch,the problem is the values in lengths variable in your costfunctionpredictionloss you divide the cross entropy loss by the length of each sequece torchsumtmptensordimfloat lengthsfloat however if you look at the values of your lengths tensor you will notice that some of the entries are the corresponding values in the loss function are also zero no loss for zerolength sequence when you divide zero by zero you get nan some good practices for coding if possible use library functions instead of reimplementing things these functions are usually tested and optimized and are more numerically stable for instance you can use torchnncrossentropyloss that combines both the cross entropy loss and the softmax in a numerically robust manner the variable lengths used for loss computation is not explicitly an argument of the loss function or a class member you should make it an explicit argument
63663679,bag of words gives keyerror,python pythonx nlp key,i think there might be a small bug def getbagofwordstitleslines bagofwords for line in titleslines courseid coursebagofwords getcoursebagofwordsline for word in coursebagofwords should check in bagofwords if word not in bagofwords bagofwordsword coursebagofwordsword else bagofwordsword coursebagofwordsword return bagofwords this should be the reason causing your keyerror didnt check your other functions
63585138,attributeerror running lowercasetranslate stringpunctuation on pandas df,python pandas nlp,pandas dataframes dont have a translate methodbut python strings do for example import string mystr hello world mystrtranslatestrmaketrans stringpunctuation if you want to apply that translation to each column value in the row of the dataframe you can use map on the column the map method takes a function that accepts the column value as an argument and you can return the transformed value def removepunctuationvalue return valuetranslatestrmaketrans stringpunctuation dfmycleanedcolumn dfmydirtycolumnmapremovepunctuation you can also use a lambda function rather than defining a new function dfmycleanedcolumn dfmydirtycolumnmap lambda x xtranslatestrmaketrans stringpunctuation if you have many columns you need to apply this to you can do this for columnname in dfcolumns dfcolumnname dfcolumnnamemap lambda x xtranslatestrmaketrans stringpunctuation
63539034,how can i fix the bug to realize reference resolution using a library,python pythonx nlp spacy,neuralcoref has a specially dedicated doccorefresolved method for tasks like this
63454262,unexpected clustering errors partitioning around mediods,r nlp clusteranalysis kmeans unsupervisedlearning,bar has n columns so the maxkrange has to be krange try pamkbest fpcpamkbar krangecdimbar
63440902,error message valueerror too many values to unpack in frequecy distribution of nltk,python machinelearning nlp nltk,the following code will give you the frequency of nouns of the mystery genre in brown corpus
63372648,error with input shape in keras while training cbow model,numpy keras nlp wordembedding,flatten layer get at least dimensional numpy array but you give it dimensional
63211181,error while using categoricalcrossentropy,python tensorflow deeplearning nlp,categorical cross entropy is used for multiclass classification problems when you use softmax as an activation there will be one node for each class in the output layer for each sample the node corresponding to the class of the sample should be close to one and the remaining nodes should be close to zero thus true class labels y needs to be an onehot encoding vector suppose your class labels in y are integers like please try the code below
63195714,label tokenizer not working loss and accuracy cannot be calculated,python tensorflow keras nlp tokenize,the problem seems to be twofold first binary targets should always be and not so i subtracted one from your targets tokenizer isnt made to encode labels you should use tfdsfeaturesclasslabel for that for now i just subtracted in the fit call second your input layer returned only nan for some reason on the page of the pretrained model they say that googletfpreviewgnewsswiveldimwithoov same as googletfpreviewgnewsswiveldim but with vocabulary converted to oov buckets this can help if vocabulary of the task and vocabulary of the model dont fully overlap and so you should use the second one since your dataset doesnt fully overlap with the data it was trained on then your model will start learning full running code look what happens if you have categories and use instead of but it works with
63173417,typeerror nlp object is not callable,python nlp nlg allennlp simplenlg,try that i think the example author forgot to instantiate the nlp object very common in spacy library consider reporting a issue in the nlg project
63146005,typeerror an integer is required in python spacystopword nlp,python nlp spacy,simply converted token into string and it works
63055632,issue with tokenizing words with nltk in python returning lists of single letters instead of words,python nlp nltk tokenize sentimentanalysis,your tokens are from the file name positivetweetscsv not the data inside the file add a print statement like below you will see the issue output from full script concerning the second error replace this with this
63011672,imputerror cannot import name function from modulepy,python pandas dataframe nlp,ensure your notebook always loads the latest version after each change and not a cached version at the top of your notebook before your imports enter loadext autoreload and in a new cell below autoreload then restart your kernel and try again
62938465,nlp problems to handle sentence with conjunctions,python pythonx nlp stanfordnlp spacy,theres no reason to think that the same code should be able to handle all of these situations as the function of the word and is very different in each case in pattern it is connecting two independent clauses in pattern it is creating a compound subject in pattern it is coordinating verb phrases i would caution you that if your ultimate aim is to split all sentences that contain the word and or any other coordinating conjunction in this way you have a very challenging job ahead of you coordinating conjunctions function in many different ways in english there are many common patterns different from those you list here such as nonconstituent coordination bill went to chicago on wednesday and new york on thursday which youd presumably want to turn into bill went to chicago on wednesday bill went to new york on thursday note the subtle but critical difference from bill went to chicago and new york on thursday which would need to become bill went to chicago on thursday bill went to new york on thursday coordinated verbs mary saw and heard him walk up the steps among others and of course more than two constituents can be coordinated sarah john and marcia and many patterns can all be combined in the same sentence english is complicated and handling this would be a huge job even for a linguist with a strong command of what is going on syntactically in all the cases to be covered even just characterizing how english coordinations behave is tough as this paper that considers just a handful of patterns illustrates if you consider that your code would have to handle realworld sentences with multiple ands doing different things eg autonomous cars shift insurance liability and moral responsibility toward manufacturers and it doesnt look like this will change anytime soon the complexity of the task becomes clearer that said if you are only interested in handling the most common and simple cases you might be able to make at least some headway by processing the results of a constituency parser like the one built into nltk or a spacy plugin like benepar that at least would clearly show you what elements of the sentence are being coordinated by the conjunction i dont know what your ultimate task is so i cant say this with confidence but im skeptical that the gains you get by preprocessing in this way will be worth the effort you might consider stepping back and thinking about the ultimate task you are trying to achieve and researching andor asking stackoverflow whether there are any preprocessing steps that are known to generally improve performance
62926022,modulenotfounderror no module named pegasus,python tensorflow machinelearning nlp googlecolaboratory,you cannot use this use this instead heres an example notebook that can run correctly i dont use contentpegasuspegasus i use contentpegasus directly by installing it with npx degit
62846950,another error to import spacy neuralcoref module even following the sample code,python pythonx nlp jupyternotebook spacy,downgrade to python neuralcoref works only for python and spacy the best way to fix this in opinion would be alter the requirementstxt of neuralcoref and change spacy to spacy hope that helps
62826516,error in the conversion of bidirectional lstm text classification model to tflite model,keras nlp lstm tensorflow tensorflowlite,current stable versions of tensorflow dont support dynamic input shapes however using the nightly build could solve your problem i found this issue in tensorflow github where this method is discussed however im not sure if this works on android
62801889,allennlp configurationerror key matrixattention is required at location model,nlp allennlp,problem solved it seemed like my code was outdated so i updated it first i needed to install the following dependencies and this is my code also i used this code on google colab if anyones interested although this should work elsewhere just fine as well
62794549,attributeerror while loading texthero library,python text nlp,can you please post the whole error message anyway you should be able to solve the problem by uninstalling and installing again nltk
62730174,i receive this error str object has no attribute text when i try to run the following code,python nlp lexical,where you are doing the text that you are using is a method of the response object that is returned by requestsget this method returns a string containing the contents of the page all this is fine however you are then doing this time what you are trying to do is access the text method of a str object but a str does not have any method called text if you do printdirpeterpan you will see the attributes methodsproperties that are available for you to access you should see something similar to you will see that text is not one of them which is why you are getting the error but you will also see that split is there and in fact all you need to do is to invoke split directly
62627157,failed to run the tflite model on interpreter due to internal error,tensorflow nlp tensorflow tensorflowlite,you can load the generated tflite file inside python notebook and pass the same inputs as at keras model you have to see the exact outputs because during conversion of model there is no loss of accuracy if there is a problem therethere will be problem during android operations if noteverything will work fine use below code from tensorflow guide to run inference in python happy coding
62611786,textblob tweets typeerror the argument passed to must be a string not rows are lists,python pandas dataframe nlp textblob,so your problem was be that function gettweetsentiment required string as input and you tried to pass pdseries as input so the solution is using dfapply with lambda which simply run gettweetsentiment at each row but again each cell in dftweetcontent column is a list which contains each word as a separate string element to make thing works you can use joinx which create from list hi i really like you string hi i really like you and this you could pass to function code output
62592377,typeerror numpylonglong object is not iterable,python pandas numpy nlp,as discussed in the comments the problem was freq is a number and you cant perform a sum function over a number so the way to fix it is to do the following instead of
62582099,keyerror true when matching pandas dataframe,python pandas dataframe nlp,i think it might be helpful to break the code into chunks this should work if i understood the code correctly
62487778,module error in code cannot figure out what is wrong,python nlp,you seem to be using predefined module as a variable name the problem here is this will import tqdm module you have to import it as follows
62453633,valueerror operands could not be broadcast together with shapes,python nlp,npmultiply do elementwise production in your case you may need dot production or make sure the size match the result is a by matrix
62413784,calculating cosine similarity valueerror input must be or d,python numpy nlp linearalgebra cosinesimilarity,npdiag starts with similarity npdota at works with a sparse because it delegates the action to the sparse matrix multiplication the result will be a sparse matrix you can check that yourself but then try to pass that to npasanyarray
62405867,error running config robertaconfigfrompretrained absolutepathtobertweetbasetransformersconfigjson,nlp googlecolaboratory bertlanguagemodel huggingfacetransformers robertalanguagemodel,first of all you have to download the proper package as described in the github readme after that you can click on the directory icon left side of your screen and list the downloaded data right click on bertweetbasetransformers choose copy path and insert the content from your clipboard to your code config robertaconfigfrompretrained contentbertweetbasetransformersconfigjson bertweet robertamodelfrompretrained contentbertweetbasetransformersmodelbin configconfig
62402942,error gcloudcomputeinstancescreate could not fetch resource quota gpusallregions exceeded limit globally,googlecloudplatform nlp googlecomputeengine googlecloudsdk,lets start with the first issue have a look again at the error message error gcloudcomputeinstancescreate could not fetch resource the zone projectscovidagentzonesuscentrala does not have enough resources available to fulfill the request try a different zone or try again later when you start an instance it requests resources like vcpu memory gpu and if theres not enough resources available in the zone youll get such message more information available in the documentation if you receive a resource error such as zoneresourcepoolexhausted or zoneresourcepoolexhaustedwithdetails when requesting new resources it means that the zone cannot currently accommodate your request this error is due to compute engine resource obtainability and is not due to your compute engine quota resource availability are depending from users requests and therefore are dynamic there are a few ways to solve this issue wait for a while and try to start your vm instance again as you tried but fruitless this time move your instance to another zone as you did reserve resources for your vm by following documentation to avoid such issue in future create reservations for virtual machine vm instances in a specific zone using custom or predefined machine types with or without additional gpus or local ssds to ensure resources are available for your workloads when you need them after you create a reservation you begin paying for the reserved resources immediately and they remain available for your project to use indefinitely until the reservation is deleted now lets have a look at the second issue have a look again at this error message error gcloudcomputeinstancescreate could not fetch resource quota gpusallregions exceeded limit globally more information about quotas you can find in the documentation to solve this issue you should follow steps below ensure that billing is enabled for your project request an increase in quota go to the quotas page in the quotas page select the quotas you want to change click the edit quotas button on the top of the page check the box of the service you want to edit fill out your name email and phone number and click next enter your request to increase your quota and click next submit your request a request to decrease quota is rejected by default if you must reduce your quota reply to the support email with an explanation of your requirements a support representative from the compute engine team will respond to your request within to hours youre not able to request an increase in quota if you use month free trial because of the limitations your free trial credit applies to all google cloud resources with the following exceptions you cant have more than cores or virtual cpus running at the same time you cant add gpus to your vm instances you cant request a quota increase for an overview of compute engine quotas see resource quotas you cant create vm instances that are based on windows server images you must upgrade your account to perform any of the actions in the preceding list you can estimate cost of usage with google cloud pricing calculator
62385670,spacy stemming on pandas df column not working,python pandas nlp spacy,you need to actually return the value you got inside the maketobase method use then use
62302499,huggingface bert which bert flavor is the fastest to train for debugging,machinelearning nlp huggingfacetransformers bertlanguagemodel,i think generally using a specific model for debugging can be critical and depends entirely on the kind of debugging you want to perform specifically consider the aspect of tokenization since each model also carries their own derivation of the basetokenizer class therefore any specifics of the respective model will only show up if you also use this specific tokenizer say eg you want to debug a later roberta implementation by using distilbert for debugging anything specific to robertas tokenization will not be the same in distilbert which uses berts tokenizer similarly any specifics to the training process might completely screw up the training from anecdotal evidence i had models train to completion and convergence with roberta but not on bert which makes the proposed solution of using different models for debugging a potentially dangerous substitution albert again has properties different from any of the above mentioned models but analogously the mentioned aspects still hold if you want to prototype services and simply require a model for in between i think both of the models suggested by you would do just fine and there should be only a minor difference in loadingsaving depending on the exact number of model parameters but keep in mind that inference time for applications is also something that is worth considering unless you are absolutely sure that there will not be any noticeable difference in the execution time at least make sure that you are testing with the full model as well
62141647,remove emojis and users from a list in python and punctuation nlp problem and my emoji function does not work,python nlp,im drawing on some other so answers here removing textual emojis removing graphical emojis this will also remove any twitter username wherever it appears in the string
62125582,how to fix a classifier from the confusion matrix,python machinelearning scikitlearn nlp,you may try different classifiers based on your usecase i think it may because of fewer samples for class manufacturing if u want to fix classifier than u will have to balance the sample distribution first if u want to try a new classifier than classifiers is available u can evaluate one by one else it depends on the nature of data as well eg how many features per documents which features u select etc i hope this explanation will help you
62064270,implement wordvec but i got error that word carnoun is in the vocabulary,python pandas nlp spacy,you had a mistake in your convert function you are supposed to pass a list of lists to wordvec as in a list that contains the sentences in lists i have changed that for you basically you want to go from sometihng like this to something like this i have also altered the code around training the model a bit for you to make it work for me you might want to experiment with that returns
62035756,how to find the prefix of a word for nlp,python nlp,short of a full morphological analyser you can work around this with exception lists and longest matching for example you assume un expresses negation first find longer prefixes such as uni and match for that first before looking at un there will be a handful of exceptions such as uninteresting which you can check for separately this will be a fairly smallish list then once all the uni words have been dealt with anything starting with un is a candidate though there will also be exceptions such as under a slightly better solution is possible if you have a basic word list cut of un from the beginning of the string and check whether the remainder is in your word list university will become iversity which is not in your list and thus its not the un prefix however uninteresting will become interesting which is so here you have found a valid prefix all you need for this is a list of nonnegated words you can of course also use this for other prefixes such as the alpha privative as in atypical the remainder typical will be in your list if you dont have such a list simply split your text into tokens sort and unique them and then scan down the line of words beginning with your candidate prefixes its a bit tedious but the numbers of relevant words are not that big its what we all did in nlp years ago
62032239,create tuples of lemma ner type in python nlp problem,python nlp nltk,i hope the following code snippets solve your problem
62030872,how can i convert entitieslist to dictionary my tried code is commented and not working nlp problem,python nlp nltk spacy,the list stored in variable entities is has type listlisttuplestr str where the first entry in the tuple is the string for the entity and the second is the type of the entity eg then you can create a reverse dict in the following way the dict stored in variable typeentities is what you want to get the most frequent peoples names in the first lines and their corresponding number of mentions
62019695,patterns with enttype from manually labelled span not working,python nlp spacy,you have the right idea but the problem here is an intrinsic design choice in spacy that any token can only be part of one named entity so you cant have warm welcome being both a greeting as well as part of a supergreeting one way you could work around this is by using custom extensions for instance one solution would be to store the greeting bit on the token level and then we adjust the phraserulercall so that it doesnt write to docents but instead does this now we can rewrite the supergreeting pattern to which will match super followed by one or more mygreeting tokens it will match greedily and output super warm welcome as hit heres the resulting code snippet starting from your code and making the adjustements as described which outputs this may not be exactly what you needwant but i hope it helps you move forward with an alternative solution for your specific usecase if you do want the normal greeting spans in the final docents maybe you can reassemble them in postprocessing after the entityruler has run eg by moving the custom attributes to docents if they dont overlap or by keeping a cache of the spans somewhere
61982023,using wordnetlemmatizerlemmatize with postags throws keyerror,python text nlp nltk lemmatization,you get a keyerror because wordnet is not using the same pos labels the accepted pos labels for wordnet based on source code are these adj adv adv and verb edit based on bivouac s comment so to bypass this issue you have to make a mapper mapping function is heavily based on this answer nonsupported pos will not be lemmatized import nltk import pandas as pd from nltkcorpus import wordnet from nltkstem import wordnetlemmatizer lemmatizer wordnetlemmatizer def getwordnetpostreebanktag if treebanktagstartswithj return wordnetadj elif treebanktagstartswithv return wordnetverb elif treebanktagstartswithn return wordnetnoun elif treebanktagstartswithr return wordnetadv else return none x pddataframedatathis is a sample of text one more text columnsphrase xphrase xphraseapplylambda v nltkpostagnltkwordtokenizev xphraselemma xphrasetransformlambda value joinlemmatizerlemmatizeaposgetwordnetposa if getwordnetposa else a for a in value
61852870,how to fix lda model coherence score runtime error,python nlp runtimeerror lda topicmodeling,i have faced the same issue adding coherence model inside ifnamemain resolved the issue for me
61771479,python loop typeerror string indices must be integers,python pandas dataframe nlp,filtereddates will return an iterator on the column names which are strings if you want to iterate over the rows you should use iterrows something like that should work
61768742,python typeerror unhashable type list indices must be integers,python pandas dataframe nlp,since d is a dictionary you cannot do this i have made the required changes to your for loop
61709530,typeerror only size arrays can be converted to python scalars when computing fscore,python nlp,did you mean to format your print string with the score variable instead the error is with your print call not the fscore call as seen from the stack trace youre receiving this error because you used a format specifier for a single float and youre trying to insert an entire array devypred rather than a single scalar value maybe you meant to do this printf score f score
61668111,attributeerror list object has no attribute lower in term frequency inverse document frequency,python nlp,fittransform method accepts an iterable which yields either str unicode or file objects as an argument there may overlooked items in your input data be sure all items are str check via below snippet
61660376,pyinstaller python exe when run shows error failed to execute script pyirthnltk,python nlp nltk pyinstaller,i have already solved it but by using another way of converting py to exe which is the cxfreeze
61631446,attributeerror list object has no attribute lower with countvectorizer,python pandas machinelearning nlp,countvectorizer cannot directly handle a series of lists which is why youre getting that error lower is a string method i looks like you want a multilabelbinarizer instead which can handle this input structure however the above approach wont account for duplicate elements in the lists the output elements can either be or if that is the behavior youre expecting instead you could join the lists into strings and then use a countvectorizer since it is expecting strings note that this is not the same as a tfidf of the input strings here you just have the actual counts for that you have tfidfvectorizer which for the same example would produce
61574364,reaching the main word from the suffix list,c nlp morphologicalanalysis,using a dictionary with assigned suffixes to words is a way of achieving this this code does however needs to be tweaked to discern suffixes not in the list and suffixes that are similar like a and ak in your example update fixed the search pattern for the suffixes the result of this code run szlatrmak
61569042,problems using snowballstemmer for a list of turkish words in python,python list nlp turkish,did you mean to have a list of strings instead of a list of lists containing strings i was able to get the stems for each word when i reformatted your code this way if you have a list of lists of strings lets say its what youve defined as df and you want to flatten it down to a single list of words you can do something like this credit for the above goes to this stackoverflow post alternatively you could just correct the looping to address the problem with your original layout
61568626,typeerror trying to compare texts using for loop,python scikitlearn nlp similarity,well youre concatenating string and integer thats why its through an error here the title is string and is an integer you can use below the code for avoiding the error
61538031,error while running a cnnlstm model valueerror input of layer lstm is incompatible with the layer expected ndim,tensorflow keras nlp lstm convneuralnetwork,change tfkeraslayersglobalaveragepoolingd with tfkeraslayersaveragepoolingd
61486629,problem adding custom entities to spacys ner,nlp spacy namedentityrecognition,in principle the way youre trying to solve the catastrophic forgetting problem by retraining it on its old predictions seems like a good approach to me however if you are having duplicate versions of the same sentence but annotated differently and feeding that to the ner classifier you may confuse the model the reason is that it doesnt just look at the positive examples but also explicitely sees nonannotated words as negative cases so if you have bob lives in london and you only annotate london then it will think bob is surely not an ne if then you have a second sentence where you annotate only bob it will unlearn that london is an ne because now its not annotated as such so consistency really is important i would suggest to implement a more advanced algorithm to resolve the conflicts one option is to always just take the annotated entity with the longest span but if the spans are often exactly the same you may need to reconsider your label scheme which entities collide most often i would assume org and orgname do you really need org perhaps the two can be merged as the same entity
61464726,how to keep model fixed during training,python deeplearning nlp pytorch huggingfacetransformers,a simple solution to this is to just exclude the parameters related to the bert model while passing to the optimizer paramoptimizer x for x in paramoptimizer if bert not in x optimizer adamwparamoptimizer lr
61402071,attributeerror nonetype object has no attribute lower in python how to preprocess before tokenizing the text content,python tensorflow nlp,this happens if you have some incorrect data in the text being fed to the tokenizer as the error message suggests that it found some element to be none so a cleanup in the data should be done to remove such cases you can see in the following snippet that an entry has invalid text for caption
61301118,load pickle notfittederror countvectorizer vocabulary wasnt fitted,python machinelearning scikitlearn nlp classification,in your apppy you are pickling the documentterm matrix instead of the vectorizer where bowtransformer is and in your temppy when you unpickle it you just have the documentterm matrixthe right way to pickle it would be now you can pickle your bowtransformer using which will be a transformer instead of the document term matrix and in your temppy you could unpickle it and use it as illustrated below
61182101,re enabling parser component of spacy give error,python pythonx nlp spacy,you are trying to add a blankuntrained parser back to the pipeline rather the one that was provided with it instead try disablepipes which makes it easier to save the component and add it back later disabled nlpdisablepipesparser do stuff disabledrestore see
61158024,typeerror lemmatize missing required positional argument word,machinelearning nlp nltk,there are some things wrong in your code wordnetlemmatizer is a class you need to instanciate it first tokenedtext is a nested list hence you need a nested listcomprehension to preserve the structure also lemmatize is expecting a string heres how you could do this
61157314,runtimeerror unknown device when trying to run albertformaskedlm on colab tpu,nlp pytorch tpu huggingfacetransformers tensorflowxla,solution is here before calling modeltodev you need to call xmsendcpudatatodevicemodel xmxladevice there are also some issues with getting the gelu activation function albert uses to work on the tpu so you need to use the following branch of transformers when working on tpu see the following colab notebook by for full solution
61117367,getting indexerror list index out of range when calculating euclidean distance,python pythonx nlp euclideandistance,the error in the example you provide is in the fact that transformedresults is a list with one element holding the tokenized sentence onlyevent though has sentences and you are using that to provide i so i will be and when i is transformedresultsi raises the error if you tokenize both sentences in onlyevent for example with which gives perhaps this code is incomplete or mistyped in some way use one of the following methods use a querypreparation api to safely construct the sql query containing usersupplied values only concatenate a usersupplied value into a query if it has been checked against a whitelist of safe string values or if it must be a boolean or numeric type then transformedresults will have length as well and you will compare the euclidean distance of both sentences including the reference sentence with itself
61098102,why is tokenpos not working while others tokenlemma etc are working,pythonx nlp spacy,you should remove the following line from your code snippet nlp english because it overwrites the line nlp spacyloadencorewebsm the latter encorewebsm has a pretrained pos tagger but english is just a blank model that doesnt have such a pos tagger builtin the model encorewebsm also can split sentences using the dependency parse so theres no need to add the sentencizer to it
60991253,invalidargumenterror input must be a vector got shape,pythonx pandas numpy tensorflow nlp,the code to save the embeddings of text data using universal sentence encoder in pandas dataframe new column along with output is shown below output is shown below
60973552,typeerror init got an unexpected keyword argument name convokit,python pandas nlp jupyternotebook,because init function of the class user takes as parameter so since name is not a parameter of the function user you get this kind of error
60901735,importerror cannot import name contractionmap from contractions,python machinelearning nlp datascience contractions,i believe you have mistaken the contractions package available on pypi with the contractions module from a textbook called text analytics with python source code the contractionsmap variable is defined in the latter and is not part of the contractions package api documented in the github readmemd from the documentation the package can be used to fix contractions like if you want access to the map of contraction to expanded version this can be imported using this contractionsdict contains entries like
60894547,r unnesttokens not working with particular file,r nlp textmining tidytext,we need to only specify the unquoted column name
60848440,nltk corpora indexerror list index out of range,python nlp nltk corpus,i run this code without error if there is problem with data then you should get this error also with two lines as i remember nltk at start needs to download data from server and this can be the problem see doc installing nltk data
60842493,why does my training function throw up the name error name decaying is not defined,python nlp spacytransformers,im not at all familiar with spacy but after a few google searches it looks like this function may be dependent on spacys utildecaying function without the decaying function loaded into memory this trainclassifier function will throw a nameerror the code for the decaying function is as follows and can be found here
60842476,valueerror error when checking target expected dense to have shape but got array with shape,python machinelearning keras nlp tfidf,that is because your input is twodimensional so that your model summary is as follows try to reshape the tensor shape after dense or dense by taking sumaverage over the second axis to get the same shape as your y here is my example for your reference i wrote a customized sum function to sum up over the second axis to make the tensor of none be none then add another dense layer import kerasbackend as k from keraslayers import dense lambda from kerasmodels import sequential def mysumx return ksumx axis keepdimstrue def mysumoutputshapeinputshape shape listinputshape printshape shape return tupleshape randomly generate data import numpy as np xtrain nprandomnormal ytrain npones model sequential modeladddense activationsigmoid inputshape modeladdlambdamysum outputshapemysumoutputshape modeladddense modelcompileoptimizerrmsprop lossmse modelfitxtrain ytrain epochs it is important to know what your input is and how tensors are transformed through every layer hope this helps
60833301,train huggingfaces gpt from scratch assert nstate confignhead error,python nlp huggingfacetransformers transformermodel gpt,i think the error message is pretty clear assert nstate confignhead tracing it back through the code we can see nstate nx in attention nstate which indicates that nstate represents the embedding dimension which is generally by default in bertlike models when we then look at the gpt documentation it seems the parameter specifying this is nembd which you are setting to as the error indicates the embedding dimension has to be evenly divisible through the number of attention heads which were specified as so choosing a different embedding dimension as a multiple of should solve the problem of course you can also change the number of heads to begin with but it seems that odd embedding dimensions are not supported
60759441,datatable error and warnings for finding trigram probability,r nlp datatable quanteda trigram,the problem is in your attempt to multiply the quantities in the last line this expression is length like triwords but the next expression is length then the last part is length and contains a lot of nas the error messages are saying that the shorter item cannot be recycled into the longer item because the longer items length is not a multiple of the length of the shorter item to fix this you need to implement this algorithm more carefully
60676502,detecting mistakes in words and fix them when classifying text nlp,python tensorflow neuralnetwork nlp,you can correct spelling errors by maintaining a vocabulary and finding the closest valid word using a string metric like the levenshtein distance there are also some more advanced python tools like spacy hunspell that being said if you plan to use pretrained word embeddings i wouldnt worry too much about text normalisation as the embeddings will likely contain most common spelling variants you can check how many outofvocabulary words you have in your data to see if its worth investing time in extra cleaning except for basic tokenisation and converting everything to lowercase
60667007,i am getting a lookuperror when using hunspellspellmyword,python nlp libraries linguistics hunspell,the problem is that you are calling spell directly you should initialize hunspell with which dictionary you want to use dictionaries can be found in
60511708,attributeerror module sst has no attribute trainreader,python nlp stanfordnlp sentimentanalysis sst,i guess youre importing the sst package seleniumsimpletest which is not what youre looking for try sstdiscover if you get the error you are using the seleniumsimpletest package
60491035,unable to load model trained in gensim picklerelated error,pythonx nlp gensim,python picklingunpickling can run into problems when saving code blocks or classesinstancesofclasses that you defined before saving but may not be available at loadtime especially anonymous or globalscope types not imported from explicit paths its a known hiccup with gensim modelsaving and future versions will likely avoid storing such callback code inside models at all instead youll have to specify callbacks each time you execute a method using them and theyll only remain effective for that one call see gensim project issue for more details including a workaround that seems to have helped others reload their models ensuring the same epochprogress class is definedimported where the load is attempted
60345476,apply nlp wordnetlemmatizer on whole sentence show error with unknown pos,python nlp nltk,the sentence should be split before sending it to postag function also the pos argument differs in what kind of strings it accepts it only accepts nv and so on i have updated your code from this
60292744,typeerror during extracting bigrams with gensimpython,python machinelearning nlp gensim,convert this into string and it will solve problem in your code just while splitting do
60283673,r convert dfm to lsa then compute cosine similarity error inheritsx matrix is not true,r nlp quanteda lsa,the error you receive basically means that the function sim does not work with the lsa object however im not really sure if i understand the question why do you want to convert the dfm to lsa textmatrix format in the first place if you want to calculate cosine similarity between texts you can do this directly in quenteda if you want to use sim from textvec you can do so using the same object without converting it first as you can see the results are the same update as by the comments i now understand that you want to apply a transformation of your data via latent semantic analysis you can follow the tutorial linked below and plug in the dfm instead of the dtm that is used in the tutorial note that these results differ from run to run unless you use setseed or if you want to do everything in quanteda
60261759,i am getting typeerror unhashable type list while trying to find the word frequency,python pandas nlp nltk,although you didnt specify exactly what data is datatweetsplit is likely returning a list of lists and freqdist is a probably a dictionarylike object so when you do fdi you are indexing fd with a list which with a dictionary or something that uses dictionaries in their implementation is not possible because lists are not hashable its possible you can convert i here to a tuple like tuplei but its not clear if this is exactly what you intended either this is because python dicts can only be indexed with immutable objects which by nature of being immutable have an immutable hash value tuples are like lists in that they are a collection of elements but one of the main differences is that they cannot be modified so as long as their contents are also immutable the tuples hash is constant lists could in principle be hashed but the problem is that elements of a list can be added removed or modified inplace so the hash of the list would not be constant and thus would not be useful as a dictionary key
60136834,typeerror while encoding data using label encoder in scikit learn,python pandas machinelearning scikitlearn nlp,
59881411,spacy valueerror cant read file modelsmodelbestaccuracyjson,python nlp spacy,heed the warning shown by the script and start with an empty output directory
59872949,having error while loading tensorflow x model in tensorflow x,machinelearning nlp datascience tensorflow,could you replace mydrive with my drive including the space in modelfilename ospathjoinmydriveaivaluesmodel modelckpt and try
59678617,how can you solve a model installation problem with spacy,python pythonx model nlp spacy,try
59646868,nameerror name cleantext is not defined,nlp datascience countvectorizer,here is the function that the badreesh put into github but is not in the blog
59489625,modelfit keras classification multiple inputssingle output gives error attributeerror nonetype object has no attribute fit,python tensorflow text keras nlp,your function keramultyclassificationmodel doesnt return anything so after model keramultyclassificationmodel you get model none as your function returns nothing and nones type is nonetype and it really doesnt have a method called fit just add return model in the end of keramultyclassificationmodel
59451822,dialogflow issue in extracting entities with similar items,nlp entity dialogflowes,you can enable fuzzy matching check docs for more info or you can use regexp entities check docs for more info some entities need to match patterns rather than specific terms for example national identification numbers ids license plates and so on with regexp entities you can provide regular expressions for matching update in reply to your comment you can get the original what the user actually said entity detected using entitynameoriginal check below example image but for your use case i advise against doing that using synonyms if there is a common pattern you should definitely use regex
59333165,attributeerror wordvec object has no attribute endswith,python machinelearning nlp artificialintelligence wordembedding,i think some functions might be deprecated try or
59281409,gensim lemmatize error generator raised stopiteration,python nlp gensim lemmatization,the error message is misleading it occurs when theres nothing to properly lemmatize by default lemmatize only accepts word tags nnvbjjrb pass in a regexp that matches any string to change this
59228806,i am splitting the data into testing and training set the error is found input variables with inconsistent number of samples,python pandas nlp python dataanalysis,ok the problem is that x and y must have the same dimensions if you want to use just reviews you can use the same for cycle and then when selecting y you just do otherwise if you want to use the whole dataset you must edit the first part of the cycle
59155770,nltk valueerror unable to parse line s npsbj vp expected a nonterminal found,python nlp nltk,this is just a guess but the normal form of phrase structure grammars does not allow terminal symbols as a category on the derivation side that is why it says expected a nonterminal the only terminal symbol to be found in your rule s npsubj vp is the dot in case this is not a copy paste error that the belongs to the rule remove the dot and try again then it should work
59115414,when i try to download stanfordnlp en model it gives an error,python nlp anaconda,if there is problem to download with then you can download it directly from web page stanfordnlp models and unpack to the folder which you see when you use stanfordnlpdownloaden on linux i got it in folder stanfordnlpresourcesenewtmodels and there are files btw i had no problem to download it with stanfordnlpdownload with version but i had also old version and it was trying to download from different url which doesnt exist any more they moved files to different urls on server
59050554,error running spacy entity linking example,pythonx nlp spacy entitylinking,this was asked and answered in the following issue on spacys github it looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rulebased ner component in the pipeline the new script adds such an entityruler to the pipeline as an example ie add a custom component to recognize russ cochran as an entity for the example training data note that in a realistic application an actual ner algorithm should be used instead ruler entityrulernlp patterns label person pattern lower russ lower cochran ruleraddpatternspatterns nlpaddpiperuler however this can be replaced with your own statistical ner model
59031625,invalidargumenterror indices is not in node embeddingembeddinglookup,python keras nlp lstm,the dimension of the vocabulary is the first parameter to the embedding class you are setting them as the second you just have to switch the parameters you are giving to the embedding instances
59030907,nlp transformers best way to get a fixed sentence embeddingvector shape,machinelearning deeplearning nlp pytorch wordembedding,this is quite a general question as there is no one specific right answer as you found out of course the shapes differ because you get one output per token depending on the tokenizer those can be subword units in other words you have encoded all tokens into their own vector what you want is a sentence embedding and there are a number of ways to get those with not one specifically right answer particularly for sentence classification wed often use the output of the special classification token when the language model has been trained on it camembert uses note that depending on the model this can be the first mostly bert and children also camembert or the last token ctrl gpt openai xlnet i would suggest to use this option when available because that token is trained exactly for this purpose if a cls or or similar token is not available there are some other options that fall under the term pooling max and mean pooling are often used what this means is that you take the max value token or the mean over all tokens as you say the danger is that you then reduce the vector value of the whole sentence to some average or some max that might not be very representative of the sentence however literature shows that this works quite well as well as another answer suggests the layer whose output you use can play a difference as well iirc the google paper on bert suggests that they got the best score when concatenating the last four layers this is more advanced and i will not go into it here unless requested i have no experience with fairseq but using the transformers library id write something like this camembert is available in the library from v import torch from transformers import camembertmodel camemberttokenizer text salut comment vastu tokenizer camemberttokenizerfrompretrainedcamembertbase encode automatically adds the classification token tokenids tokenizerencodetext tokens tokenizerconvertidtotokenidx for idx in tokenids printtokens unsqueeze tokenids because batchsize tokenids torchtensortokenidsunsqueeze printtokenids load model model camembertmodelfrompretrainedcamembertbase forward method returns a tuple we only want the logits squeeze because batchsize output modeltokenidssqueeze only grab output of cls token which is the first token clsout output printclsoutsize printed output is in order the tokens after tokenisation the token ids and the final size
59024220,error in computing the coherence score attributeerror dict object has no attribute idtoken,python scipy nlp gensim topicmodeling,i dont have your data so i cant reproduce the error so i will take a guess the problem is within your idword it should be a corporadictionarydictionary not just dict so you need to do the following and i think it should work just fine now
59020670,speech recognition duration setting issue in python,python nlp speechrecognition pyaudio,you can use timeout instead of duration like so this means that the model will wait two seconds at most for a phrase to start before giving up and throwing anspeechrecognitionwaittimeouterror exception if timeoutnone there will be no wait which is your case edit all the function recognizegoogle does is to call the google speech api and get back the result when i used the provided audio file i got back the transcription of the first seconds thats due to the limitation of the free version of the google speech api and has nothing to do with the code
58812949,tf transformer model valueerror shapes and are incompatible,python nlp tensorflow,the problem was with loading the tokenizers there were punctuations which were getting saved while using the save function but on load some of these failed to load dont know the root cause and that caused the mismatch in shapes once i cleaned the text to remove punctuation and extra spaces it worked fine
58794349,how to install spacy and avoid bit error,python nlp spacy,you currently have two different sitepackage folders from where packages are imported and which looks to me like there is currently a mix of different python installations on your systems or residues of previously installed and then incompletely removed versions i would therefore suggest to remove all python installations completely remove both cpython and appdataroamingpython reinstall only one python distribution choose the bit version if you dont care which version to use
58781161,streamlit valueerror the truth value of a series is ambiguous use aempty abool aitem aany or aall,python model nlp countvectorizer streamlit,one big clue is that it works in jupyter notebook but not in streamlit which suggests there are differences in your working environments the error youre seeing emits from pandas when a series is not compared correctly there is a very good explanation of this error on this stackoverflow answer but since your error is buried in sklearn not your own code chances are the problem youre having can be solved by matching the sklearn version thats being used in jupyter to the version you have installed when you use streamlit if you update your post with what versions of pandas sklearn and python you are using in each case jupyter and streamlit it will be easier to help you figure this out it may also help to post the entire traceback not just the top half as plain text rather than a screenshot thanks for trying out streamlit
58742990,no found errorlibtensorflow frameworkso,python tensorflow nlp,you should be using the tensorflowtext version specified in the pip package rc which matches tensorflow are you manually installing a different version
58735715,getting a spacy error no module named spacypipelinepipes spacypipeline is not a package,nlp spacy namedentityrecognition,close and reopen the terminal console activate the venv from the current folder youre working on
58688056,pytorch bilstm pos tagging issue runtimeerror inputsize must be equal to inputsize expected got,python pythonx nlp pytorch,the error is here selfwordembeddings nnembeddingvocabsize tagsetsize instead of using the embedding dimension you use the number of tags which is and not which is what the lstm layer expects
58647270,sklearn nlp text classifier newbie issue with shape and vectorizer x and y not matching up,python scikitlearn nlp textclassification sklearnpandas,you are using your whole dataframe to encode your predictor remember to use only the abstract in the transformation you could also fit the corpus word dictionary before and then transform it afterwards heres a solution the rest looks ok
58543163,how to solve logits and labels must have the same first dimension error,pythonx tensorflow machinelearning keras nlp,you should have returnsequencestrue and returnstatefalse in calling the lstm constructor in your snippet the lstm only return its last state instead of the sequence of states for every input embedding in theory you could have spotted it from the error message logits and labels must have the same first dimension got logits shape and labels shape the logits should be threedimensional batch size sequence length number of classes the length of the sequences is and indeed number of your labels this could have told you the length of the sequences disappeared
58518980,extracting fixed vectors from biobert without using terminal command,pythonx nlp pytorch,you can get the contextual embeddings on the fly but the total time spend on getting the embeddings will always be the same there are two options how to do it import biobert into the transformers package and treat use it in pytorch which i would do or use the original codebase import biobert into the transformers package the most convenient way of using pretrained bert models is the transformers package it was primarily written for pytorch but works also with tensorflow it does not have biobert out of the box so you need to convert it from tensorflow format yourself there is converttfcheckpointtopytorchpy script that does that people had some issues with this script and biobert seems to be resolved after you convert the model you can load it like this import torch from transformers import load dataset tokenizer model from pretrained modelvocabulary tokenizer berttokenizerfrompretraineddirectorywithconvertedmodel model bertmodelfrompretraineddirectorywithconvertedmodel call the model in a standard pytorch way embeddings modeltokenizerencodecool biomedical tetrahydrosentence addspecialtokenstrue use directly biobert codebase you can get the embeddings on the go basically using the code that is exctractfeautrespy on lines they initialize the model you get the embeddings by calling estimatorpredict for that you need to format your format the input first you need to format the string using code on line and then apply and call convertexamplestofeatures on it
58513452,spacy cli debug shows traindev docs in cliformatted json converted by spacygolddocstojson,python nlp spacy namedentityrecognition doccano,this is a legitimately confusing aspect of the api for internalhistorical reasons spacygolddocstojson produces a dict that still needs to be wrapped in list to get to the final training format try spacy debugdata doesnt have proper schema checks yet so this is more frustratingconfusing than it should be
58492616,colab error command errored out with exit status python setuppy egginfo check the logs for full command output,python pythonx nlp googlecolaboratory enchant,you need to install with apt first then with pip
58381909,spacy issue in finding root word in sentence using dependency parsing,nlp spacy,it should first be noted that the root is not necessarily a verb see here and here the root is the one node that is not dominated by one of the other nodes nonfinite clauses contain a verb which does not show tense for example the person to win the competition in this example it is pretty clear that the root is person and win is a dependent of person in the same way if we have the company to acquire jagged peak energy in an allstock deal it would also be clear that the root is company your first example parsley energy to acquire jagged peak energy in an allstock deal is less obvious i think it is an ellipsis the omitted word in the elliptical sentence being is the sentence would normally be parsley energy is to acquire see here if the main predicate is not present due to ellipsis and there are multiple orphaned dependents one of these is promoted to the head root position and the other orphans are attached to it to conclude it does not appear that spacy is making an error here
58309837,how does ulmfits language model work when applied on a text classification problem,nlp lstm textclassification languagemodel fastai,ulmfits model is a regular lstm which is a special case of a recurrent neural network rnn rnns eat the input text word by word sometimes character by character and after every bite they produce an output update an internal hidden state in text classification the output is discarded until the very end the updated hidden state is instead added to the next word to bite after the rnn ate the last word you can check the output layer typically a softmax layer with as many neurons as your labels compute the loss against the true label then update the weights accordingly after the training phase suppose you want to classify a new document the rnn eats the input again and updates its hidden state after each word you disregard the output layer until you see the last word at that point the max element of the output softmax layer will be your predicted label i found particularly helpful this pytorch tutorial
58256824,scikit learn typeerror float argument must be a string or a number not bunch,python pandas scikitlearn nlp,you are trying to apply a numerical scaling operation over text data that is logically incorrect if you see the official documentation of maxabsscalar its function is to scale each feature by its maximum absolute value if you want to find the vectors of the text data then you need to use something like countvectorizer see this example from official documentation here alternatively you can try tfidftransformer as well here is an example of using it with newsgroup data
58057021,oserror e cant find model en,python nlp spacy pytextrank,when using spacy we have to download the model using if you have already done that make sure you have shortcut link assigned properly meaning simlink between en and encorewebsm easy hack that worked when i am working directly with spacy more help at
58001318,no attribute contrib for problems,python tensorflow nlp tensortensor,tensortensor library is not yet compatible with tensorflow because it still uses a lot of deprecated apis your only option currently is to downgrade to an older version such as tensorflow
57958995,valueerror found input variables with inconsistent numbers of samples on binary svm,python pandas scikitlearn nlp valueerror,that error just tells you that you have a discrepancy in the number of samples for which you are trying to predict a label and the number of output labels it happens because you use the same data as training and test set but then you try to match the label of the test set which has a different size just fix this line and the result for your script is
57886076,python keyerror when using pandas,pandas dataframe nlp,youre getting this error because you did not understand how to structure your data when you do dfreviewsdfpositivereviewsdfnegativereviews youre actually summing the values of positive reviews to negative reviewswhich does not exist currently into the reviews column chich also does not exist your csv is nothing more than a plaintext file with one text in each row also since youre working with text remember to enclose every string in quotation marks otherwise your commas will create fakecolumns with your approach it seems that youll still tag all your reviews manually usually if youre working with machine learning youll do this outside code and load it to your machine learning file in order for your code to work you want to do the following result however i would recommend you to have a single column isreviewpositive and have it to true or false you can easily encode it later on
57757356,does docvec work with multiclass problem with only sample per class,nlp gensim docvec,note that your dataset of one descriptive paragraph per diplomatype wouldnt normally be described as a multiclass problem the term multiclass more usually describes situations where each item itself has morethanone classlabel applied but your approaches as described might be productive and the only way to know which is better for your data goals would be to evaluate them and other variants against each other specifically as you describe you could train a docvec model on your short documents that each describe a different diploma this is on the smallish side for docvec training data but perhaps enough using a smaller docvector size say dimensions or fewer andor more training epochs can sometimes help squeeze better results out of smaller datasets youll now have docvectors one for each documentdiploma then for any new test document your skill sentence youd calculate another docvector you might do this by using the gensim docvec infervector method note that especially for short texts you may want to use far more than default number of epochs for this calculation then youd search the model for which of the st docvectors are closest to your new vector and you might find that the closest match or one of the topn closest is a good fit for what you need but there are some other variants to consider if in fact your diplomadescriptions are all multiple sentences say dozens or even hundreds of words while your skillsentences are shorter say a handful to dozen or two words then perhaps the best matches to your testsentence will be to just a subset of the longerparagraphs in such a case it might be helpful to initially train the model on smaller fragments of text for example if the diplomadocs are each sentences maybe you train the model to know separate docvectors testing a skillsentence would then be both finding the topn nearest matches then looking up which diploma the nearby descriptionsentence was about if your skillsentences are numerous and also available at training time you could plausibly include them in the model training as well for example perhaps in addition to your diplomadescriptions you have another skillsentences you could let the initial model learn docvectors then you can lookup the diplomadocvectors closest to a skilldocvector without using infervector at all just using the skilldocvector learned during modeltraining but youd have to filter out other skilldocvector results from the mostsimilar list you still have the option of using infervector on any new texts that come along but including more text in the initial training may make the overall model more expressive rather than simply mapping any probe skillsentence to the single diplomaparagraph thats closest in docveccoordinate space if you already had many humanvetted examples of whichskillsentences should be associates with whichdiplomaparagraphs you could use a secondstep supervised classifier algorithm that is youd only use docvec to create featurevectors for texts but learn the labels diplomatypes for texts via another training process there are many potential classifier algorithms and they are datahungry but with enough data some might have a better chance of learning irregular shapes in the underlying data for example when a certain diplomalabel should based on historic examples cover more of the skillsspace than a simple textvectordistance would indicate
57643717,error while exporting fastai text classifier model,python nlp kaggle fastai,you have to replace pathfile with the name of the file encased in strings in your code python is treating path and file as variables instead of strings literals and trying to divide them so you need some like this or you can try
57509156,how can i get my bigrams to actually print i get an error typeerror sequence item expected str instance list found,python nlp nltk ngram,try this here is a simple example
57496400,importerror no module named languagevgapic when running dataflow job,python python nlp googleclouddataflow apachebeam,it seems to be a mismatch with the googlecloudlanguage version installed in the dataflow workers to solve it create a requirementstxt file and add googlecloudlanguage for example then add requirementsfilerequirementstxt to the options arguments of your pipeline i tested it with this code and it worked for me
57443096,bugs when fitting multi label text classification models,pythonx scikitlearn nlp classification logisticregression,onevsrestclassifier fits one classifier per class you need to tell it which type of classifier you want for example losgistic regression the following code works for me output array
57231616,valueerror e text of length exceeds maximum of spacy,python pythonx nlp spacy,i differ from the answer above and i think nlpmaxlength did execute correctly but the value set is too low it looks like you have set it to exactly the value in the error messageincrease the nlpmaxlength to a little over the number in the error message it should ideally work after this so your code could be changed to this
57223482,kept getting error while installing spacy inside virtualenv,python pythonx nlp virtualenv spacy,the errors that i kept getting because i was using bit python executable how i handle the problem by using bit version of python and installing the virtualenv spacy packages again additionally you will probably need administrator permission to using spacy properly if your os is windows just run the prompt as administrator
57124182,how to fix stopwords preprocessing inconsistency,python scikitlearn nlp,there seems to be an issue with preprocessing from my personal experience the stemming step in preprocessing leads to certain stems such as separating ing from the word financing to keep the stem financ eventually these carry forward and cause inconsistencies with the tfidfvectorizer stopwords list you can see this post to get some more info on this python stemmer issue wrong stem you can also try to avoid the stemming process and only tokenize this will at least solve the inconsistencies error
56969572,how to solve sklearn memory error when fitting large data,python pandas machinelearning scikitlearn nlp,according to tfidfvectorizer contains a parameter called maxfeatures that takes an int this parameter would help us pick how many features we want from our matrix thus giving us a bit of control over the memory issue its also worth to mention that both parameters maxdf and mindf also help with reducing our matrix size
56914166,how to fix valueerror found input variables with inconsistent numbers of samples,machinelearning text scikitlearn nlp traintestsplit,if can you comment out the fscore and try it should not give you that error let me know if it works thanks
56870701,how to fix tokenizing phrases such as tc from being split into t c,pythonx nlp nltk,the tokenizers that come with nlp systems are sometimes pretty basic and even advanced ones may handle some edge cases in ways you might not prefer for a particular project bottom line you have several options find an offtheshelf solution that does exactly what you want find a setting or configuration that adjusts one to do what you want stanford nltk has several variations such as casual mwetokenizer nist and punkt and some options like adding your own regexes to some of them see write code to change an existing solution if its open source you can change the code itself many systems also have an api that lets you override certain parts without digging too far into the guts write you own tokenizer from scratch this is considerably harder than it looks pre or postprocess the data to fix specific problems but ampersand may not be the only case youll run into i suggest going through each punctuation mark in turn and spending a minute thinking about what you want to happen when it shows up then youll have a clearer set of goals in mind when evaluating your options for example also shows up in urls and be careful of lt if youre parsing html and if youre parsing code you probably dont want to tokenize urls at every slash and certainly dont want to try parsing the resulting tokens as if they were a sentence theres also and many more cases hypens are highly ambiguous the double hyphen for clauselevel dash and the decrement operator in some code endofline hyphenation which might or might not want to be closed up long strings of hyphens as separator lines quotes curly vs straight singlequote vs apostrophe for contractions or possessives or incorrectly for plurals and so on unicode introduces cases like different types of whitespace quotes and dashes many editors like to autocorrect to unicode characters like those and even fractions may end up as a single character do you want the tokenizer to break that into tokens its fairly easy and imho an extremely useful exercise to write up a small set of test cases and try them out some of the existing tokenizers can be tried out online for example stanford corenlp python nltk spacy morphadorner this is just a small sample there are many others and some of these have a variety of options if you want a really quickanddirty solution for just this one case you could postprocess the token list to recombine the problem cases or preprocess it to turn rww into some magic string that the tokenizer wont break up than turn it back afterward those are pretty much hacks but in limited circumstances they might be ok
56850711,importerror please install apex from to use distributed and fp training,pythonx deeplearning nlp pytorch,this worked for me
56778591,valueerror must be iterable in keras,python tensorflow keras nlp,the padsequences function expects that the given sequences object has a len attribute ie which basically gives the number of sequences the reviewtokenidx which is a map object does not have a len attribute so you need to convert it to an object eg a list which has such an attribute
56721753,attributeerror list object has no attribute lower from tfidfvectfit,python vector nlp svm tfidf,the error says that tfidfvectorizer expects a string as its input not a list of string it does all the tokenization by itself but you can plug in a custom tokenizer within tfidfvectorizer if you want so i would try a simpler pipeline without the first line with nltktokenize but i cannot be sure because you did not provide any examples of actual input data that causes the error
56651465,training spacy model not working running the trainner script has no effect,nlp spacy trainingdata namedentityrecognition,looking in more detail on the github issues it turns out that even though the example script only gives it a couple of sentences to train it when you run the script you are expected to actually uses hundreds of examples this is not clear for someone with no nlp experience from reading the documentation hopefully now this question and answer will come up when people search for it so other people dont have to spend weeks wondering what they were doing wrong basically i just need more sentences
56642816,valueerror e could not find an optimal move to supervise the parser,python pythonx nlp spacy namedentityrecognition,passing the training data through this function below works fine without any error
56566824,error when exporting predictions of machine learning models,scikitlearn nlp classification svm prediction,when you set k in kfold the split method splits your dataset into portions for each iteration testindex will be indices of the ith portion while trainindex will be the rest of the portions in your original code the df shows the test set xval yval instead of the predictions for each iteration i am not sure that you intend to do but if you would like to see the prediction for each model the following code will do
56550268,sklearn nltk problems predicting,pythonx machinelearning nlp nltk,in the tutorial the method similarityscore tries to find the highest similarity for each synset in s and average them however it doesnt count the words in s that couldnt find any synset in s into account it makes more sense to me if we add zeros into slargestscores for those occasions take two sentences will it be uncomfortably hot and will it rain for example the method in the tutorial will give you for similarity while the method that i purposed will give you for similarity the sentences are in different categories so wed like the similarity to be low here is my code and here is the result which i consider more reasonable
56345812,spacy named entity recognition issue,python nlp spacy,xlabel holds the name of the entity so all you need is add a condition to only return those tuples where xlabel equals org
56307733,python nlp spacy oserror e cant find model de,python nlp spacy,i had to install spacy as the following
56286510,how to handle unseen words for pretrained glove wordembedding to avoid keyerror,python nlp wordembedding,i would suggest below all missing words assigned to some unique vector say all zeros find words similar to it and use their embedding try ngrams prefix or suffux of the words and check if it is in vocab stem the word and check if it is in vocab simplest solution use fasttext it assembles word vectors from subword ngrams which allows it to handle out of vocabulary words
56265563,while removing html text from column object of type float has no len error is occuring,pandas datascience nlp,check for nan with dfdfreviewsisnull if you find any try to dropna first
56205728,i get isnan error when i merge two countvectorizers,python scikitlearn nlp textclassification countvectorizer,you should union vectorizern and vectorizermx not mx and xn change the line to
56155584,sigmoid function prediction generates continuous number and error when exported to df,python tensorflow machinelearning nlp sigmoid,you can see the definition of the sigmoid function this will always have a continuous output if you want to discretize your output you need to determine some threshold above which you will set your solution to and below will be zero pred tfmathgreaterymodel tfconstant however you must be careful choosing an appropriate threshold as it is not guaranteed that your model will be well calibrated with probability you can choose a suitable threshold based on the best discrimination on some heldout validation set it is important that this step is for evaluation only as you will not be able to backpropagate your loss signal through this op
56128347,lookup table not working in training data of rasa nlu,nlp markdown rasanlu,according to rasanlu documentation in order for lookups to work you need to include a few examples from the lookup table also you need to understand that phone type and region are different patterns because phone type has two words and region is a single word keeping this in mind i have extended your dataset as now when i tried all the examples you mentioned they worked even though the price was not included in the dataset but the patters were all covered i recommend using for generating simple dataset it would make things easier for you and generate synonyms etc automatically also just in case you dont know you can also use files to point to large lookups such as
56109714,valueerror while finding best hyperparameter in scikit learn logisticregression using gridsearchcv,scikitlearn logisticregression hyperparameters nlp,refer scikit logistic regression hparam is not listed as a hyper parameter for logistic regression
55874253,python gensim wordvec gives typeerror typeerror object of type generator has no len on custom dataclass,python machinelearning nlp gensim trainingdata,update after discussion your trainingdata class iter function isnt providing a generator which returns each text in turn but rather a generator which returns a single other generator theres one too many levels of yield thats not what wordvec is expecting changing the body of your iter method to simply so that iter is a synonym for your getdata and just returns the same textbytext generator that getdata does should help original answer youre not showing the trainingdatapreproccesstext sic method referenced inside getdata which is what is actually creating the data wordvec is processing and its that data thats generating the error wordvec requires its sentences corpus be an iterable sequence for which a generator would be appropriate where each individual item is a listofstringtokens from that error it looks like the individual items in your trainingdata sequence may themselves be generators rather than lists with a readable len separately if perchance youre choosing to using generators there because the individual texts may be very very long be aware that gensim wordvec and related classes only train on individual texts with a length up to wordtokens any words past the th will be silently ignored if thats a concern your source texts should be prebroken into individual texts of tokens or fewer
55871850,mosestokenizer issue winerror the system cannot find the file specified,python nlp anaconda nltk tokenize,use sacremoses instead of moses and for complete details sacremoses
55839374,how to fix the npcumsum function when mapping a series with regex,regex pythonx pandas numpy nlp,thanks guys for helping me fix my code i did see an alternative solution using ffill which was great i used a liner solution below
55693826,manage keyerror with gensim and pretrained wordvec model,python nlp gensim,adding a synthetic token would just let you look up that token like modelthe model would still give key errors for absent keys like kjklk theres no builtin support for adding any such catchall mapping often ignoring unknown tokens is better than using some plug value such as a zerovector or randomvector its fairly idiomatic in python to explicitly check if a key is present via the in keyword if you want to do something different for absent keys for example notably the expr if expr else expr defers evaluation of the initial expr avoiding keyerror python also has the defaultdict variant dictionary which can have a default value returned for any unknown key see itd be possible to try replacing the keyedvectors vocab dictionary with one of those if the behavior is really important but there could be side effects on other code
55693318,encoding problem while training my own glove model,python encoding nlp wordembedding glove,i just found myself a way to save the data with an utf format im sharing it here in case someone faces the same problem instead of using the glove saving method glovesaveglovemodeltxt try to simulate by yourself a glove record then you will be able to read it
55640826,permission denied error while reading the googlenewsvectorsnegativebin file,python nlp jupyternotebook wordvec,you would likely get the same iorelated error no matter how you try or for what purpose you try to open the file so this isnt really a question about nlp or wordvec or even jupyternotebook note that sometimes errors that wed consider other things get reported as permission problems because at some level you cant do that to that kind of path or file youve specified the file path as cusersabhishekdocumentssarcasmgooglenewsvectorsnegativebin with a trailing that usually indicates something is a directory that could be a problem also i believe this particular file is usually gb in size and some dosdescended filesystems or a python interpreter which is only bit might have problems handling files over certain sizes like gb or gb
55622630,how to fix attributeerror nonetype object has no attribute inboundnodes that comes while creating lstm model using manhattan distance,keras nlp lstm quora,leftoutput and rightoutput are obtained from the lstm layer the inputs are fed to the input layer and through a series of dense layers however note that there is no connection anywhere between the set of dense layers and the lstm the model expects the output from the lstm layer which is not possible this line keraslayersconcatenate should use the outputs from the sharedlstm rather than using the outputs of input layers directly like this only then this can be a siamese network
55470063,pyspark error valueerror not enough values to unpack expected got when trying to group with groupbykey,pythonx pyspark nlp,the problem is in your data and the map you used the principle of groupbykey is to use a key and a value to group by key and perform some aggregation on your value data but in your rdd you do not have that key value data just a list of lists that is the reason why you have this error the list is the argument causing the error message i do not know exactly your data and what you want to achieve but i think you could do something like that for example
55436859,how to fix nameerror name phrasedocs is not defined,python nlp,simply you defined phrasedocs inside a function which is not seen from outside and the function return should be captured in a variable edit your code
55382596,how is wordpiece tokenization helpful to effectively deal with rare words problem in nlp,nlp wordembedding,wordpiece and bpe are two similar and commonly used techniques to segment words into subwordlevel in nlp tasks in both cases the vocabulary is initialized with all the individual characters in the language and then the most frequentlikely combinations of the symbols in the vocabulary are iteratively added to the vocabulary consider the wordpiece algorithm from the original paper wording slightly modified by me initialize the word unit inventory with all the characters in the text build a language model on the training data using the inventory from generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model goto until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold the bpe algorithm only differs in step where it simply chooses the new word unit as the combination of the next most frequently occurring pair among the current set of subword units example input text she walked he is a dog walker i walk first bpe merges w a wa l k lk wa lk walk so at this stage your vocabulary includes all the initial characters along with wa lk and walk you usually do this for a fixed number of merge operations how does it handle rareoov words quite simply oov words are impossible if you use such a segmentation method any word which does not occur in the vocabulary will be broken down into subword units similarly for rare words given that the number of subword merges we used is limited the word will not occur in the vocabulary so it will be split into more frequent subwords how does this help imagine that the model sees the word walking unless this word occurs at least a few times in the training corpus the model cant learn to deal with this word very well however it may have the words walked walker walks each occurring only a few times without subword segmentation all these words are treated as completely different words by the model however if these get segmented as walk ing walk ed etc notice that all of them will now have walk in common which will occur much frequently while training and the model might be able to learn more about it
55331297,how to resolve typeerror languagemodellearner missing required positional argument arch in python,python pythonx nlp,you need to define architecture like this
55263332,stanford nlp sentiment prediction bug differs from live demo,machinelearning nlp stanfordnlp,lemmatizer is an normally optional in a typical nlp pipeline one has to do extrinsicintrinsic evaluation with and without various components in the pipeline try dropping lemmatizer and adding ner
55210989,python machine learning string matching problem,python machinelearning nlp stringmatching,best situation if you have access to all possible usage names of the product it will be the best situation all you have to do is check if the name entered by the user falls in the synonyms products with say synonyms each with a well desired schema should be easily handled by a powerful database system search engine based solution lets say if you dont have access to synonyms but say you have access to detailed english description of the product then you can search for the user entered name in the description one can use search engine like apache solr which uses inverted index based on tfidf the document which solr returns as top result will be the corresponding product then in short index you document desciptions into solr and search for the user entered product name in solr mind that it is lexicon based not semantic based but lexion based will suffice for you as long as your user will not call a banana as yellow color cylinder shaped fruit ml based the are good distributed vector representations wordvec glove called embeddings the important properly of embeddings is that the distance between related words will be small however these vectors are not good for you because what you have are phrases not words red is a word but red chilly is a phrase there are no good pretrained phrase to vector embeddings available in open source if you want to use a model based on vector similarity then you will have to build your own phrasevec model so assuming you are able to build a phrasevec model you have to find the vectorcorresponding to the product which is close to the vector of the product name typed by your customer
55201291,how to resolve package module import error,python nlp,for mac validate the gcc path in your bash file specify gcc path something like below export ccusrlocalcellargccbingcc and execute pip install glovepython
55185021,error with nltk package and other dependencies,python nlp stanfordnlp namedentityrecognition,i found the answer for this issue i am using nltk from nltk and above stanford nlp pos ner tokenizer are not loaded as part of nltktag but from nltkparsecorenlpcorenlpparser the stackoverflow answer is available in stackoverflowcomquestionsstanfordparserandnltk and the github link for official documentation is githubcomnltknltkwikistanfordcorenlpapiinnltk additional information if you are facing timeout issue from the ner tagger or any other parser of corenlp api please increase the timeout limit as stated in by dimazest
55062077,illegal hardware instruction error when using glove,nlp stanfordnlp wordembedding glove illegalinstruction,ive hit the same issue in recent days the docker built on a server using jenkins it has been running fine until the underlying cluster host orchestration software and physical hardware was upgraded my solution has been to remove the build of glove from the dockerfile and instead put the buildmake inside a script which runs when the container starts the actual cause of the error may be caused by the cflags marchnative set in the glove makefile this will cause the glove build to rely on the underlying cpu instruction set on which the docker built theres a discussion of this further here mtune and march when compiling in a docker image
54969887,wordnetlemmatizer on daskdataframe errors with wordnetcorpusreader object has no attribute lazycorpusloaderargs,python nlp nltk python dask,this is because the wordnet module was lazily read and not evaluated yet one hack to make it work is to first use the wordnetlemmatizer once before using it in the dask dataframe eg alternatively you can try pywsd then in code
54967302,python treetaggerwrapper returns a binary invalid error treetaggerexe,python pythonx pip nlp anaconda,i have the same problem of treetaggerwrappertreetaggererror treetagger binary invalid treetaggercmdbintreetagger and i downloaded treetagger from i solved it by copying the whole bin directory to cmd
54915855,typeerror generator object is not callable when trying to iterate over string data,python string loops nlp iteration,the error is in your map function i think you didnt understood how it works properly it has arguments functiontoapply receives each element of the iterable and returns a value listofinputs list of your data your text in the example your first argument is not a function is just a list so change it by joinmaplambda t tlowerstrip text the parameter t of the annonymous lambda function corresponds to each piece of text like you would have in for t in text hope this example clarified how it works
54890488,textlmdatabunch memory issue language model fastai,nlp outofmemory pytorch languagemodel fastai,when you use this function your dataframe is loaded in memory since you have a very big dataframe this causes your memory error fastai handles tokenization with a chunksize so you should still be able to tokenize your text here are two things you should try add a chunksize argument the default value is k to your textlmdatabunchfromdf so that the tokenization process needs less memory if this is not enough i would suggest not to load your whole dataframe into memory unfortunately even if you use textlmdatabunchfromfolder it just loads the full dataframe and pass it to textlmdatabunchfromdf you might have to create your own databunch constructor feel free to comment if you need help on that
54660886,tfidf vectorizer for multilabel classification problem,python nlp tfidf multilabelclassification tfidfvectorizer,the problem is you are using fittransform here which make the tfidftransform fit on the test data and then transform it rather use transform method on it also you should use tfidfvectorizer in my opinion the code should be also why are you using countvect i think it has no usability here and in traintestsplit you are using xtfidfr which is not mentioned anywhere
54635962,issue in creating keras model input tensors to a model must come from,python tensorflow keras nlp,the model only accepts inputs you cant pass embeddings to the inputs of a model sounds like this
54238222,how to fix encoding issue in python using vadersentiment package,python encoding nlp sentimentanalysis vader,the vadersentiment package doesnt support python you should use python or little bit edit the packages source code
54203702,when we run coreference resolution program it will throw an error how can i solve,java nlp stanfordnlp opennlp,you need to run your java process with more memory on the command line you typically do this with java xmxg i am not sure specifically how much memory you should use for your code but i think something around g should be fine its possible less will work as well
54097067,tfidf multiple regression prediction problem,python scikitlearn nlp regression prediction,as you mentioned you could only so much with the body of text which signifies the amount of influence of text on selling the cars even though the model gives very poor prediction accuracy you could ahead to see the feature importance to understand what are the words that drive the sales include phrases in your tfidf vectorizer by setting ngramrange parameter as this might gives you a small indication of what phrases influence the sales of a car if would also suggest you to set norm parameter of tfidf as none to check if has influence by default it applies l norm the difference would come based the classification model which you are using try changing the model also as a last option
54054299,how to tell in advance if countvectorizer will throw valueerror empty vocabulary,python pythonx scikitlearn nlp,you could identify those documents using buildanalyzer try this ps am too confused to see all the words in third document is in stop words
54044579,metamap javalangoutofmemoryerror java heap space,java nlp heapmemory uima,your java app is very probably using to the limit the heap space allocated and it reaches the point where the garbage collector cannot efficiently obtain the necessary heap space considering that your app is using the allocated heap space efficiently the only option i think you have is to increase the allocated heap space you can do this busing however maybe it is worth analysing how the heap space is used there are tools that allow you to do this if using intellij you could try to use visualvm plugin
53975449,why do i get this import error when i have the required dlls,python scikitlearn nlp anaconda,according to this github issue the solution is to install mkl general advice in case like this is to google last two lines of the stack trace usually you will find a github or similar thread about it
53951661,how can i solve a classification problem with a dependent variable with more than two values,python machinelearning nlp classification,it is just multiclass classification problem here is a sample code from where you can get an idea what you are calling dependent variable is called class class that the input example belongs to
53929134,how to handle text classification problems when multiple features are involved,python nlp featureextraction textclassification,try these things apply text preprocessing on job description job designation and key skills remove all stop words separate each words removing punctuations lowercase all words then apply tfidf or count vectorizer dont forget to scale these features before training model convert experience to minimum experience and maximum experience features and treat is as a discrete numeric feature company and location can be treated as a categorical feature and create dummy variableone hot encoding before training the model try combining job type and key skills and then do vectorization see how if it works better use random forest regressor tune hyperparameters nestimators maxdepth maxfeatures using gridcv hopefully these will increase the performance of the model let me know how is it performing with these
53846331,typeerror can only concatenate str not numpyint to str,python nlp,if you only want to print then do this there is no need to convert the int to str here you get an error in your statement because you cant concatenate a string and an integer try doing a in python console and see what error it shows
53755893,facing attributeerror for tag using spacy in python,python pythonx nlp spacy postagger,this is because things like ents or chunks are spans ie collections of tokens hence you need to iterate over their individual tokens to get their attributes like tag or tag
53746225,possible error with stanford pos tagger and classifying intent and the replies,nlp speechrecognition stanfordnlp postagger,you can use the truecaseannotator to fix case issues in general you probably just want to use tokensregex and write rules patterns to handle these templates more info here
53718267,module import issue with a japanese tokenizer,python nlp cjk,im a developer of the package i kindly thank you for using my package i fixed bugs related to the issues here and released newer package version you could installupgrade the package with pip also install pip install japanesetokenizer upgrade pip install u japanesetokenizer
53711253,applying literaleval on string of lists of pos tags gives valueerror,python pandas nlp,parse only nonnull rows you can drop the lambda if you have rows or fewer you can also use pdeval
53541327,error while embedding could not convert string to float ng,python machinelearning nlp glove,this seems to work fine
53443475,keras valueerror error when checking target expected dense to have dimensions but got array with shape,python machinelearning keras deeplearning nlp,look at the output shape of the last layer in model summary it is none this is not what you are looking for your labels for each sample have a shape of and not why this happened thats because the dense layer is applied on the last axis of its input and therefore since the output of embedding layer is d the output of all the following dense layers would also have dimension how to resolve this one approach is to use flatten layer somewhere in your model possibly right after the embedding layer this way you would have a d ouput of shape none which is what you want and is consistent with the shape of labels however note that you may end up with a very big model ie too many parameters which would be highly prone to overfitting either reduce the number of units in the dense layers or alternatively use convd and maxpoolingd layers or even rnn layers to process the embeddings and reduce the dimensionality of resulting tensors which is also probable that using them would also increase the accuracy of the model as well
53438598,nltk tokenizer encoding issue,python nlp nltk,output reference unicodedatanormalize
53168652,test data giving prediction error in keras in the model with embedding layer,python tensorflow keras nlp wordembedding,you can use keras tokenizer class and its methods to easily tokenize and preprocess the input data specify the vocab size when instantiating it and then use its fitontexts method on the training data to construct a vocabulary based on the given texts after that you can use its texttosequences method to convert each text string to a list of word indices the good thing is that only the words in the vocabulary is considered and all the other words are ignored you can set those words to one by passing oovtoken to tokenizer class you can optionally use padsequences function to pad them with zeros or truncate them to make them all have the same length now the vocab size would be equal to nwords if you have not used oov token or nwords if you have used it and then you can pass the correct number to embedding layer as its inputdim argument first positional argument
53130738,gensim example typeerrorbetween str and int error,python nlp gensim,the main problem is that machine learning is not a known tag in your model maybe your model knows machine learning or machinelearning or some other such its harder to recognize thats the real issue because of a poor error message from the code in this case its a known issue at the gensim project
52777183,attributeerror str object has no attribute beforerequest,python machinelearning googlecloudplatform nlp,the credentials argument does not accept a str but a credentials object create one from your json file and pass it in from this doc page the hint here is this line selfcredentialsbeforerequest you pass in credentials the languageserviceclient object puts it in private variable credentials and tries to call a method on it since your string doesnt have that method it blows up
52716222,indexerror only integers slices ellipsis numpynewaxis and integer or boolean arrays are valid indices,pythonx nlp kaggle,counter counter should be counter counter note the dot or counter the dot makes counter a float since is equivalent to and floats can not be used as indexes
52573331,list index out of range error with textblob to csv,python csv nlp textblob,after playing around a bit i figured out a more elegant solution for this using pandas
52469001,why is this basic spacy example not working,python nlp conda spacy,i solved this by creating a new conda environment that uses python rather than im now seeing the same results that the demo in the spacy docs produces
52455774,googletrans stopped working with error nonetype object has no attribute group,python nlp googletranslate googletrans,update a new official alpha version of googletrans with a fix was released install the alpha version like this translation example in case it does not work try to specify the service url like this see the discussion here for details and updates update another fix was released as pointed out by desikeki and ahmed breem there is another fix which seems to work for several people github discussion here in case the fixes above dont work for you if the above doesnt work for you googletransnew seems to be a good alternative that works for some people its unclear why the fix above works for some and doesnt for others see details on installation and usage here
52190700,combining structured and text data in classification problem using keras,python pythonx keras nlp,yes this is possible with the functional api from keras all you need is an additional input for the hoursofrevision that is concatenated with the embeddings from the text data before going to the final classifier scale the additional data first build the model using the functional api for more examples on how to use the functional api for multiinputmultioutput models have a look at the keras docs
52140526,python nltk stemmers never remove prefixes,python nlp nltk stemming porterstemmer,youre right most stemmers only stem suffixes in fact the original paper from martin porter is titled porter m an algorithm for suffix stripping program and possibly the only stemmers that has prefix stemming in nltk are the arabic stemmers but if we take a look at this prefixreplace function it simply removes the old prefix and substitute it with the new prefix but we can do better first do you have a fixed list of prefix and substitutions for the language you need to process lets go with the unfortunately de facto language english and do some linguistics work to find out prefixes in english without much work you can write a prefix stemming function before the suffix stemming from nltk eg now that we have a simplistic prefix stemmer could we do better what if we check if the prefix stemmed words appears in certain list before stemming it we resolve the issue of not stemming away the prefix causing senseless root eg but the porter stem would have still make remove the suffix ed which maymay not be the desired output that one would require esp when the goal is to retain linguistically sound units in the data so depending on the task its sometimes more beneficial to use a lemma more than a stemmer see also nltk words corpus does not contain okay stemmers vs lemmatizers how does spacy lemmatizer works
52139386,gensim raise keyerrorword s not in vocabulary word,python nlp gensim wordvec topicmodeling,it could help answerers if you included more of the information around the error message such as the multiplelines of callframes that will clearly indicate which line of your code triggered the error however if you receive the error keyerror uword business not in vocabulary you can trust that your wordvec instance wvmodel never learned the word business this might be because it didnt appear in the training data the model was presented or perhaps appeared but fewer than mincount times as you dont show the typecontents of your rawdocuments variable or code for your tokengenerator class its not clear why this would have gone wrong but those are the places to look doublecheck that rawdocuments has the right contents and that individual items inside the docgen iterableobject look like the right sort of input for wordvec each item in the docgen iterable object should be a listofstringtokens not plain strings or anything else and the docgen iterable must be possible of being iteratedover multiple times for example if you execute the following two lines you should see the same two listsofstring tokens looking something like hello world if you see plain strings docgen isnt providing the right kind of items for wordvec if you only see one item printed docgen is likely a simple singlepass iterator rather than an iterable object you could also enable logging at the info level and watch the output during the wordvec step carefully and pay extra attention to any numberssteps that seem incongruous for example do any steps indicate nothing is happening or do the counts of wordstextexamples seem off
52123026,sklearn pipeline valueerror could not convert string to float,python scikitlearn nlp textclassification,first part of your pipeline is a featureunion featureunion will pass all the data it gets parallely to all internal parts the second part of your featureunion is a pipeline containing single standardscaler thats the source of error this is your data flow since text is passed to standardscaler the error is thrown standardscaler can only work with numerical features just as you are converting text to numbers using tfidfvectorizer before sending that to truncatedsvd you need to do the same before standardscaler or else only provide numerical features to it looking at the description in question did you intend to keep standardscaler after the results of truncatedsvd
52061739,spacy download en not working in virtualenv,python pythonx nlp virtualenv spacy,refer to add the following in your bash
52031337,stanfords corenlp name entity recogniser throwing error server error internal server error for url,pythonx nlp stanfordnlp,the issue has been resolved as there were some empty tokens getting passed to ner so now i have put a check for them
52028566,swift naturallanguage framework error token sequencetype length is,swift xcode nlp swift createml,your json file must be in this format to work with mlwordtaggers tokencolumn param tokens as a list of strings and labels as a list of string
51978633,stanford name entity recognizerner using pyner not working,pythonx nlp nltk stanfordnlp spacy,the tool is badly outdated if youre using nltk first update your nltk version then still in terminal then in python for windows you can use the above using powershell which you really do so but if you like to click on your mouse step download the zip file from step unzip it step open command prompt and go to the folder where file has been unzipped step run command pip install u nltk step now run command then in python
51937926,valueerror not enough values to unpack,pythonx dictionary nlp nltk sentimentanalysis,the easiest way to resolve this would be to put the unpacking statement into a tryexcept block something like my guess is that some of your lines have a label with nothing but whitespace afterwards
51838583,keras grulstm layer input dimension error,python keras nlp deeplearning lstm,the problem is that the shape of xtrain is so it means that considering the preprocessing stage there are sentences encoded as onehot vectors with vocab size of on the other hand a gru or lstm layer accepts a sequence as input and therefore its input shape should be batchsize numtimesteps or sequencelength featuresize currently the features you have are the presence or absence of a particular word in a sentence so to make it work with gru you need to add a third dimension to xtrain and xtest and then remove that returnsequencestrue and change the input shape of gru to inputshape this way you are telling the gru layer that you are processing sequences of length where each element consists of one single feature as a side note i think you should pass the vocabsize to numwords argument of tokenizer that indicates the number of words in vocabulary instead pass maxlength to maxlen argument of loaddata which limits the length of a sentence however i think you may get better results if you use an embedding layer as the first layer and before the gru layer thats because currently the way you encode sentences does not take into account the order of words in a sentence it just cares about their existence therefore feeding gru or lstm layers which relies on the order of elements in a sequence with this representation does not make sense
51772605,sklearn notfittederror for countvectorizer in pipeline,machinelearning scikitlearn nlp tfidf countvectorizer,there are several issues with your question for starters you dont actually fit the pipeline hence the error looking more closely in the linked tutorial youll see that there is a step textclffit where textclf is indeed the pipeline second you dont use the notion of the pipeline correctly which is exactly to fit endtoend the whole stuff instead you fit the individual components of it one by one if you check again the tutorial youll see that the code for the pipeline fit textclffittwentytraindata twentytraintarget uses the data in their initial form not their intermediate transformations as you do the point of the tutorial is to demonstrate how the individual transformations can be wrappedup in and replaced by a pipeline not to use the pipeline on top of these transformations third you should avoid naming variables as fit this is a reserved keyword and similarly we dont use cv to abbreviate count vectorizer in ml lingo cv stands for cross validation that said here is the correct way for using your pipeline from sklearnfeatureextractiontext import countvectorizer tfidftransformer from sklearnnaivebayes import multinomialnb from sklearnpipeline import pipeline traindf testdf traintestsplitnlpdf stratifynlpdfclass xtrain traindftext xtest traindftext ytrain traindfclass ytest testdfclass textclf pipelinevect countvectorizerstopwordsenglish tfidf tfidftransformer clf multinomialnb textclffitxtrain ytrain predicted textclfpredictxtest as you can see the purpose of the pipelines is to make things simpler compared to using the components one by one sequentially not to complicate them further
51549641,problems while tfidf vectorizing tokenized documents,python regex machinelearning scikitlearn nlp,the problem is that default tokenization used by tfidfvectorizer explicitly ignores all punctuation tokenpattern string regular expression denoting what constitutes a token only used if analyzer word the default regexp selects tokens of or more alphanumeric characters punctuation is completely ignored and always treated as a token separator your problem is related to this previous question but instead of treating punctuation as separate tokens you want prevent tokeninfo from splitting the token in both cases the solution is to write a custom tokenpattern although exact patterns are different assuming every token already has info attached i simply modified the default tokenpattern so it now matches any or more alphanumeric characters followed by or more alphanumeric or whitespace characters and ending with a if you want more information on how to write your own tokenpattern see the python doc for regular expressions
51473376,opennlp gives error when using thai model,java nlp opennlp thai,i think youre missing the manifestproperties file can you unzip the thaitokbin file and check that it contains these files tokenmodel binary tokenizer model manifestproperties configuration contents of manifestproperties should be like this taken from the question you link to
51359808,attributeerror tensor object has no attribute kerashistory during implementing coattention layer,python keras nlp deeplearning artificialintelligence,i cant reproduce your code but i presume the error happens here the backend operations and kabs are not wrapped within a lambda layer so the resulting tensors are not keras tensors and therefore they lack some attributes such as kerashistory you could wrap them as follows note not tested
51330549,using wordvec for polysemy solving problems,nlp wordvec,you pick the desired dimensionality as a metaparameter of the model rigorous projects with enough time may try different sizes to see what works best for their qualitative evaluations individual dimensionselements of each wordvector floatingpoint numbers in vanilla wordvec are not easily interpretable its only the arrangement of words as a whole that has usefulness placing similar words near each other and making relative directions eg towards queen from king match human intuitions about categoriescontinuousproperties and because the algorithms use explicit randomization and optimized multithreaded operation introduces threadscheduling randomness to the orderoftrainingexamples even the exact same data can result in different but equally good vectorcoordinates from runtorun basic wordvec doesnt have an easy fix but theres a bunch of hints of polysemy in the vectors and research work to do more to disambiguate contrasting senses for example generally morepolysemous wordtokens wind up with wordvectors that are some combination of their multiple senses and often of a smallermagnitude than lesspolysemous words this early paper used multiple representations per word to help discover polysemy similar later papers like this one use clusteringofcontexts to discover polysemous words then relabel them to give each sense its own vector this paper manages an impressive job of detecting alternate senses via postprocessing of normal wordvec vectors
51316438,german stemmer is not removing feminine suffixes in and innen,python nlp nltk stemming snowballstemmer,the german snowball stemmer follows a three step process remove ern em er en es e s suffixes remove est en er st suffixes remove isch lich heit keit end ung ig ik suffixes not knowing a lot about german grammar it seems that in would belong to the same class as the step suffixes these are referred to as derivational suffixes in the nltk source it would seem that adding in to this list of suffixes should force the snowball stemmer to remove it but there are two problems the first problem is that from your examples i see that in becomes inn when followed by en this could be worked around by adding both in and inn to the list of step suffixes but that doesnt solve the second problem looking at the germanstemmerstem source each step will only remove a single suffix thus if there is more than one derivational suffix ie in plus any of the suffixes listed above only the one will be removed in such cases and i dont know enough about german to know if this can actually happen youd need to manually edit germanstemmerstem to add a fourth in removal step this would also allow finer control in the case of plurals but honestly at that point its probably better to just ad hoc remove in by wrapping your germanstemmerstem call for example edit if you wanted to add in to one of the snowball stemmer steps you can do so using note the comma after in this code will not work without it you can also replace the with whichever step you wish to modify im not entirely sure why its germanstemmerstepsuffixes and not just stepsuffixes but ive verified that this code works on python and nltk i would not recommend this approach though as it will not properly deal with innen also since each step removes a maximum of one suffix it will not properly deal with words like lehrerinnen which have en in and er step doesnt check for er i think your best bet is to just copy and paste the entirety of germanstemmer found in the source code link above use ctrlf and add a step to stem that checks for and removes ininn
51245608,welcome intent creating error in dialogflow,nlp chatbot dialogflowes,the welcome event is a way to trigger an intent at the beginning of the interaction with the dialogflow agent although you can have multiple welcome intents one for each of the supported oneclick integrations you can only have one default welcome intent which is triggered if no messagingplatformspecific welcome intent is set therefore as per the answer in your comments you already have an intent with the welcome event the default welcome intent so the error message you are getting is correct given that you can only have one welcome intent per input context if you are interested in having different responses for the welcome intent try adding other training phrases and responses but you will not be able to create another intent with the same welcome event alternatively you may use a different input context and in that case you can have multiple welcome intents although you should take into account that the new ones will only be triggered if the given context is met
51012476,spacy custom tokenizer to include only hyphen words as tokens using infix regex,regex nlp tokenize spacy linguistics,using the default prefixre and suffixre gives me the expected output import re import spacy from spacytokenizer import tokenizer from spacyutil import compileprefixregex compileinfixregex compilesuffixregex def customtokenizernlp infixre recompiler prefixre compileprefixregexnlpdefaultsprefixes suffixre compilesuffixregexnlpdefaultssuffixes return tokenizernlpvocab prefixsearchprefixresearch suffixsearchsuffixresearch infixfinditerinfixrefinditer tokenmatchnone nlp spacyloaden nlptokenizer customtokenizernlp doc nlpunote since the fourteenth century the practice of medicine has become a profession and more importantly its a maledominated profession tokentext for token in doc note since the fourteenth century the practice of medicine has become a profession and more importantly it s a maledominated profession if you want to dig into to why your regexes werent working like spacys here are links to the relevant source code prefixes and suffixes defined here with reference to characters eg quotes hyphens etc defined here and the functions used to compile them eg compileprefixregex
50996659,dimension error for convolutiond in keras for text classification,keras nlp lstm recurrentneuralnetwork convneuralnetwork,you have two options a use convd with rows cols and channels by adding a dimension to x b use convd with steps and inputdim and use maxpoolingd
50910287,loading of fasttext pre trained german word embeddings vec file throwing out of memory error,nlp gensim wordembedding fasttext,other than working on a machine with more memory the gensim loadwordvecformat methods have a limit option which can be given a count n of vectors to read only the first n vectors of the file will be loaded for example to load just the st words since such files usually sort the morefrequent words first and the long tail of rarer words tend to be weaker vectors many applications dont lose too much power by discarding rarer words
50908667,tokenization not working the same for both case,nlp spacy,just like language itself tokenization is contextdependent and the languagespecific data defines rules that tell spacy how to split the text based on the surrounding characters spacys defaults are also optimised for generalpurpose text like news text web texts and other modern writing in your example youve come across an interesting case the abstract string xxxmessageid is split on punctuation while the isolated lowercase string id is split into i and d because in written text its most commonly an alternate spelling of id or id i could i would etc you can find the respective ruleshere if youre dealing with specific texts that are substantially different from regular natural language texts you usually want to customise the tokenization rules or possibly even add a language subclass for your own custom dialect if theres a fixed number of cases you want to tokenize differently that can be expressed by rules another option would be to add a component to your pipeline that merges the split tokens back together finally you could also try using the languageindependent xx multilanguage class instead it still includes very basic tokenization rules like splitting on punctuation but none of the rules specific to the english language
50906734,google dialogflow small talk issue,nodejs nlp googlecloudplatform actionsongoogle dialogflowes,queries defined in small talk section of your dialogflow agent will not have an associated intent if there was a matching intent then you shouldnt have really added that query into small talk therefore since there is not matching intent the dialogflow node library will return an unmatched intent
50851727,dialog api v unexpected error while acquiring application default credentials could not load the default credentials,nodejs nlp googlecloudplatform dialogflowes serviceaccounts,according to this link and github repo here you should be able to set the credentials in const sessionclient
50814296,syntax error when using list comprehensions in python to loop over and compare dependency triplets for two sentences,python performance numpy nlp listcomprehension,what you seem to want to achieve is a cumulative result but you cant do it in that way because the expression sim theta is not returning an independent object to be considered as an item of final list result what you can do instead is multiplying the theta variable with a counter or create a list of thetas and then create a cumulative version using npcumsum or itertoolsaccumulate which is not recommended unless you want to keep both original result and the cumulative one also instead of using two loops you can use itertoolsproduct in order to create all the combinations of triplets and as counter you can use itertoolscount and to do both conditions in one list comprehension you can do the following
50720589,how i can iterate through a bunch of documents and execute spacys nlp for each of them without getting a memory error,python numpy nlp spacy,i changed from python bit to bit version now its working i tried a lot but no other thing worked except this version change
50712349,command not found error for deepspeech if installed with user flag,linux nlp ubuntu mozilladeepspeech,i used pip install deepspeech on my wsl linux and it was installing deepspeech in my homeuserlocalbin directory instead try using sudo pip install deepspeech this fixes the problem
50657546,fixing error output from seqseq model,tensorflow machinelearning neuralnetwork nlp texttospeech,i found an answer to my own question in section of the paper deep voice so they trained both of phonemebased model and characterbased model using phoneme inputs mainly except that characterbased model is used if words cannot be converted to their phoneme representations
50326147,attributeerror ldamodel object has no attribute minimumphivalue,python tensorflow nlp gensim topicmodeling,the minimumphivalue is a property of ldamodel that is set when an instance is created and for some reason it hasnt been serialized which is pretty strange probably a bug to workaround this particular issue you can add after selflda loading or avoid savingrestoring the model if possible ie always train it but i encourage you to examine the fields of selflda before and after serialization to check they are identical
50019632,importerror cannot import name summarywriter,python pythonx nlp tensorboard,change from tensorboard import summarywriter to from tensorboardx import summarywriter
49964028,spacy oserror cant find model en,nlp spacy,finally cleared the error best way to install now always open anaconda prompt command prompt with admin rights to avoid linking errors tried multiple options including python m spacy download en conda install c condaforge spacy python m spacy download encorewebsm python m spacy link encorewebsm en none worked since im using my companys network finally this command worked like a charm pip install nodeps updated with latest link pip install nodeps thanks to the updated github links
49861842,attributeerror tokenizer object has no attribute oovtoken in keras,python nlp keras pickle tokenize,this is most probably this issue you can manually set tokenizeroovtoken none to fix this pickle is not a reliable way to serialize objects since it assumes that the underlying python codemodules youre importing have not changed in general do not use pickled objects with a different version of the library than what was used at pickling time thats not a keras issue its a generic pythonpickle issue in this case theres a simple fix set the attribute but in many cases there will not be
49770521,typeerror slice indices must be integers or none or have an index method nlp,python nlp nltk,i noticed something in the line the type of i here seems to be a tuple you might have to modify it
49642756,python regex spliting sentences not working properly,regex pythonx nlp,you are getting the last element as blank because your regex sn matches the last due to which split operation is performed on that which gives you strings one present to the left of that and one present at the right the string present at the right of the last is a blank string hence you get the last element of the array blank instead of splitting you can get the matches by using the following regex click for demo see the python code output here
49631758,gensim docvec mostsimilar method not working as expected,python nlp gensim docvec sentencesimilarity,docvec doesnt work well on toysized examples published work uses tensofthousands to millions of texts and even tiny unit tests inside gensim uses hundredsoftexts combined with a muchsmaller vector size and many more iter epochs to get justbarely reliable results so i would not expect your code to have consistent or meaningful results this is especially the case when maintaining a large vector size with tiny data which allows severe model overfitting using a mincount because words without many varied usage examples cant get good vectors changing the minalpha to remain the same as the larger starting alpha because the stochastic gradient descent learning algorithms usuallybeneficial behavior relies on a gradual decay of this updaterate using documents of just a few words as the docvectors are trained in proportion to the number of words they contain finally even if everything else was working infervector usually benefits from many more steps than the default to the tens or hundreds of and sometimes a starting alpha less like its inference default and more like the training value so dont change mincount or minalpha get much more data if its not tensofthousands of texts use a smaller vector size and more epochs but realize results may still be weak with small data sets if each text is tiny use more epochs but realize results may still be a weaker than with longer texts try other infervector parameters such as steps or more especially with small texts and alpha
49553836,reading a big language corpus without memory error in gb ram computer,pythonx tensorflow nlp bigdata machinetranslation,if the file size is very huge it is recommended to process it line by line below code will do the trick
49507004,solrcloud opennlp error cant find resource opennlpensentbin in classpath or configsdefault,solr nlp solrcloud opennlp,you need to put those files ensentbin and the other one into cores conf directory that is dutilssolrsolrservercorenameconf then use the files directly following should be the xml configuration
49438154,error with dimensions in keras,python neuralnetwork nlp keras,thanks for the edit youre running into trouble for two reasons one shallow one deep first shallow the dense layer requires a threedimensional input but the embedding is two dimensional you can fix this with a flatten the deep is because onehot encoding and embedding are two options that serve the same purpose so you dont need both see here and here the embedding layer wants a series of sentences made of integers representing words or tuples and a vocabulary size so something like would be represented as and fed into an embedding layer like this im certain you know the one hot encoding of our sentences would look like this because the sentences have already been transformed one hot has embedded our sentences as binary vectors in dimensional space we can feed this directly into a dense layer without the need for further embedding heres a functional toy example using both ways
49312591,tensorflow code typeerror unsupported operand types for int and flag,python tensorflow nlp seqseq,the api of these flags changed basically you have to write value after each instance eg selfhpsbatchsizevalue
49239738,memory error while creating large one hot encoding for lstm,numpy nlp keras,sure create batches only process say entries characters at a time computing and feeding them into your neural network just before theyre needed say by using a generator instead of a list keras has a fitgenerator training function to do this group chunks of data together say instead of a line being a matrix of the onehot encodings of its characters instead use the summax of all those columns to produce a single vector for the line now each line is only a single vector with dimensionality equal to the number of unique characters in your data set eg instead of use to represent the entire line
49109014,error while doing textclassification in keras,python nlp keras convneuralnetwork wordvec,you have encoded the labels as categorical but didnt actually used the result of this change to
49108041,issue in arabic preprocessing techniques,python nlp,the value that you are seeing is the last item in list text all preceding items are lost because they are not being stored anywhere furthermore the sequence of operations in the body of the for loop are assigning a value to newlist however newlist is not referenced in subsequent operations so any cumulative effect is lost to solve the first problem you can create a new empty list before the for loop to which items are appended as they are processed this would be the final result list the second problem would be solved by referencing index in each step and to assign the result back to index here is a solution output
48975683,udpipeaccuracy always gives the same error the conllu line does not contain columns,r nlp udpipe,udpipeaccuracy is used in combination with udpipetrain if you trained a custom udpipe model with udpipetrain based on data in conllu format you can see how good it is by using udpipeaccuracy on holdout conllu data which was not used to build the model
48465968,error while installing spacy,pythonx nlp anaconda spacy,my issue got resolved works only if the installation folder has admin rights if your installation folder doesnt have admin rights try this to know more about various spacy models refer
48199096,natural language calculator conversion issues,python pythonx nlp calculator,courtesy of banana this code is what works thanks
48165486,spacy permission error,python nlp spacy,i think that it can be that the path you use model is seen as an absolute path so either exits a model directory writeable by the user or you can try to use a path like model which is a relative path
48153854,valueerror operands could not be broadcast together with shapes in naive bayes classifier,python machinelearning nlp classification naivebayes,you got the the problem right say you have a corpus made of different words then your bag of words at training time will have columns now you are using another corpus which has only different words you end up with a matrix with columns and the model wont like that hence you need to fit the second corpus in the same bag of words matrix you had at the beginning with columns there are different ways to do this well explained here for example one way is to save the transform object you used at training time with fit and then apply it at test time only transform
48095667,google nlp authenticationcall issue,c aspnetmvc nlp googlecloudplatform googleauthentication,with the suggestion given by jonskeet i copied the code into a console application and executed the call unfortunately the issue persisted what i did next was to move the console application onto another server it worked there so it was indeed an issue with the server where there maybe some features missing the firewall is disabled network dept is checking it out whereas i have deployed my web application on another server update there was an issue on the server where some required framework features were not installed the issue has been resolved by moving the deployment to another server
48090426,attributeerror list object has no attribute words in python gensim module,python machinelearning nlp gensim docvec,docvec requires not just the list of sentences but the list of tagged sentences from this discussion on dsse in wordvec there is no need to label the words because every word has their own semantic meaning in the vocabulary but in case of docvec there is a need to specify that how many number of words or sentences convey a semantic meaning so that the algorithm could identify it as a single entity for this reason we are specifying labels or tags to sentence or paragraph depending on the level of semantic meaning conveyed consequently gensim expects the following input obviously you might want to set different tags depending on the sentences to get meaningful vectors by the way are you sure you want to read csv in binary mode
48055117,no member nametype error implementing nslinguistictagger in a in swift project,swift macos machinelearning nlp,first of all i need to apologize that i first wrote an answer without confirming my settings are really right the following descriptions updated based on the settings which i believe correct xcode target swift version set to changing the project swift version does not affect the target swift version oh my shame when you want to work with old swift versions some swiftfriendly wrapper types are not available and apis and constants are imported from objectivec world with simple rules in such cases you need to reinterpret objectivec versions of references with such rules in mind nslinguistictagscheme as you see in the linked article above nslinguistictagscheme as just a typealias of string and the constants are named as nslinguistictagschemenametype as in the article in your questions link they are imported asis in old swift versions so this compiles in swift still i strongly recommend you to move to swift as apples official references are based on the latest version of swift its swift
47836295,tensorflow valueerror the two structures dont have the same number of elements,python machinelearning tensorflow nlp lstm,resolved the problem by changing initial state and input initinput tfzerosbatchsize inputembeddingsize dtypetffloat initcellstate cellzerostatebatchsize tffloat
47768750,issues in lemmatization nltk,python nlp nltk,you can replace the second last line with this pos is part of speech tag
47634206,prolog operator errors in pereira,prolog nlp swiprolog,there are a couple of problems the operator values etc are compatible with older values for the standard operators and should be higher also the backquotebacktick ascii changed at some point from being a symbol to being a quote character i managed to get the files to load in swi prolog by changing the operators and giving the command line parameter traditional to make backquote a symbol instead of a quote character
47614742,python bag of words nameerror name unicode is not defined,python twitter nlp,unicode is a python method if you are not sure which version will run this code you can simply add this at the beginning of your code so it will replace the old unicode with new str
47485687,nameerror name stopwords is not defined,python nlp stopwords,you just have to add the following line before using stopwords in your code
47424075,python how to extract the nth element of a pandas dataframe and iterate without subscriptable problems,python pandas dataframe nlp,please try the question in python first before asking it your column is missing a quote lets fix it and name your column so now you are saying that this is actually a column in a dataframe lets put this into a dataframe to make the question reproducible you are saying that you have a number of these columns and the th item is the url that you are looking for then you can iterate over the columns and query the th element and collect it in a list named urls as follows after your comments i think the structure is a bit different so you have only column and inside that column you have multiple rows and the th element of each row is a url you want to collect lets see if this mimics what you have i made up urls which gives us if that is the case the solution is as simple as please comment if this does or doesnt answer your question
47400302,valueerror setting an array element with a sequence after making tfidf vectorization,pandas scikitlearn nlp tfidf,when you execute the following line pandas treats the result of vectorizerfittransform as a scalar object as a result you will have the same sparse matrix in every row in the vectmessage column basically the same is happening when we do dfnewcol we will have a column of zeros workaround ps imo it doesnt make much sense to save well to try to save d sparse matrix result of vectorizerfittransform call in pandas column series d structure
47004912,r error inheritsx cdocumenttermmatrix termdocumentmatrix is not true,r nlp tm textanalysis,removesparseterms and findfreqterms are expecting a documenttermmatrix or a termdocumentmatrix object not a matrix create the documenttermmatrix without converting to a matrix and you wont get the error
46985320,gensim wordvec online training attributeerror wordvec object has no attribute modeltrimmedposttraining,nlp wordvec gensim,its likely youre loading a model from a earlier version of gensim where the property modeltrimmedposttraining wasnt defined you can likely work around the issue by setting the property yourself after loading but before train
46643825,imdb review encoding error,python nlp recurrentneuralnetwork,it looks like your terminal is configured for ascii because the character xe is outside of the range of ascii characters xxf it can not be printed on an ascii terminal it also can not be encoded as ascii you could work around this by explicitly encoding the string at print time and handling encoding errors by replacing unsupported characters with the character looks like its the iso encoding for a small letter e with acute you can check the encoding used for stdout in my case its utf and i have no problem printing that character you might be able to coerce python into using a different default encoding there is some discussion here but the best way would be to use a terminal that supports utf
46544808,spacynightly spacy issue with thincextramaxviolation has wrong size,python nlp spacy,issue is probably with thinc package spacynightly needs thinc but version is causing some issues way how to solve it is run command bellow before you install spacynightly after this everything works perfectly fine for me i found later on that i am not the only one facing this issue
46519084,nltk postag module returns lookuperror,python nlp nltk postagger,tldr on the terminal or in python in long firstly please update your nltk version to version on the command line use sudo if necessary now you can try using the postag function again and you should see a more helpful error message note that the punkt resource is used for wordtokenize but the postag function requires the averagedperceptrontagger model so on the terminal do or in python
46464353,preparing data for stanford deepdive valueerror,postgresql nlp specialcharacters stanfordnlp,i found the problem it was caused by a singular tab t entry i replaced that by a singe space and in the end it would not be a valid antry anymore so if you use some text for deepdive you will want to treat etrys consisting of a single space as if they were empty
46382635,textacy with jupyter notebook how to suppress multiple error warnings,python nlp jupyternotebook spacy textacy,add the following two lines at the top of your notebook and execute them
46368720,tfidif model creation typeerror in gensim,python nlp gensim tfidf languagefeatures,instead of tfidfcorpus try tfidfcorpus
46147013,error getting trigrams using gensims phrases,python nlp datamining textmining gensim,according to the docs you can do bigram being a phrases object cannot be called again as you are doing so
45943832,gensim docvec finalizevocab memory error,python nlp gensim docvec,million doctags will require at least doctags dimensions bytesfloat gb just to store the raw doctagvectors during training if the doctag keys rowid are strings therell be extra overhead for remembering the stringtointindex mapping dict if the doctag keys are raw ints from to million that will avoid filling that dict if the doctag keys are raw ints but include any int higher than million the model will attempt to allocate an array large enough to include a row for the largest int even if many other lower ints are unused the raw wordvectors and model outputlayer modelsyn will require about another gb and the vocabulary dictionary another few gb so youd ideally want more addressable memory or a smaller set of doctags you mention a cluster but gensim docvec does not support multimachine distribution using swap space is generally a bad idea for these algorithms which can involve a fair amount of random access and thus become very slow during swapping but for the case of docvec you can set its doctags array to be served by a memorymapped file using the docvecinit optional parameter docvecsmapfile in the case of each document having a single tag and those tags appearing in the same ascending order on each repeated sweep through the training texts performance may be acceptable separately your management of training iterations and the alpha learningrate is buggy youre achieving passes over the data at alpha values of and even though each train call is attempting a default passes but then just getting a single iteration from your nonrestartable sentencestoarray object you should aim for more passes with the model managing alpha from its initialhigh to default finaltiny minalpha value in fewer lines of code you need only call train once unless youre absolutely certain you need to do extra steps between multiple calls nothing shown here requires that make your sentences object a true iterableobject that can be iterated over multiple times by changing toarray to iter then passing the sentences alone rather than sentencestoarray to the model then call train once with this multiplyiterable object and let it do the specified number of iterations with a smooth alpha update from hightolow the default inherited from wordvec is iterations but to are more commonly used in published docvec work the default minalpha of should hardly ever be changed
45929291,ldanew model constructor textvec r package error error in subsetpublicbindenv initialize unused argument,r nlp textmining lda textvec,please check documentation lda and signature of function was changed since textvec now there should not be vocabulary argument
45886128,unable to set up my own stanford corenlp server with error could not delete shutdown key file,nlp stanfordnlp,this error happens when the shutdown key file already exists on your filesystem youre starting a new corenlp server instance and it cant delete the old shutdown key file are you running the server as two different users more generally do you have permissions to the directory stored in java property javaiotmpdir this is traditionally tmp on linux machines the shutdown key is stored in so for a linux system the error says that this file exists and cannot be deleted by java you should check your permissions on this file and that should help you debug whats wrong an easy workaround in the worst case is to set the tmpdir yourself when starting the server for example
45847370,issue with gensimmodelsphrases,python nlp gensim,i realised that phrases phrases are both making a generator only and the following class was required
45676075,too late for c option error with perl and shell scripts,java html xml perl nlp,the error comes from having c on the shebang line of a perl script but not passing the c to perl this type of error happens when someone does instead of
45458402,peter norvigs word segmentation issue how can i segment words with misspellings inside,python nlp spellchecking spelling,it could be achieved by peter norvigs algorithm with minor changes the trick is to add a space character to the alphabet and treat all bigrams separated by space character as a unique word since bigtxt doesnt contain deep learning bigram we will have to add a little bit more text to our dictionary i will use wikipedia library pip install wikipedia to get more text i will create a new dictionary with all unigrams and bigrams now just add a space character to the letters variable in edits function change bigtxt to newdicttxt and change this function to this and now correctiondeeplerning returns deep learning this trick will perform well if you need a spelling corrector for specific domain if this domain is big you can try to add to your dictionary only most common unigrams bigrams this question also may help
45420466,gensim keyerror word not in vocabulary,python nlp gensim wordvec topicmodeling,the first parameter passed to gensimmodelswordvec is an iterable of sentences sentences themselves are a list of words from the docs initialize the model from an iterable of sentences each sentence is a list of words unicode strings that will be used for training right now it thinks that each word in your list b is a sentence and so it is doing wordvec for each character in each word as opposed to each word in your b right now you can do to get it to work for words simply wrap b in another list so that it is interpreted correctly
45164340,gensim wordvec error valueerror missing section header before line,python html nlp gensim wordvec,the argument to accuracy should be a set of analogies to test the model against in the format of the questionswordstxt file available from the original wordvecc distribution it should not be your own file
45132387,bad input shape sklearn error after hashingvectorizer,pythonx machinelearning scikitlearn nlp keras,i changed my code as follows and now its working i changed my model from perceptron to multi layer perceptron classifier though i am not completely sure how this is working explanations are welcome now i have to approach the same problem using ngram model and compare the results
45097276,attributeerror tensor object has no attribute attention,tensorflow nlp,can you write all your calls to use kwargs everywhere ie tfnndynamicrnncell inputs etc i think your args are misplaced somewhere and using kwargs should resolve it
45050805,attributeerror nonetype object has no attribute items for classifier nltknaivebayesclassifiertraintrainingset,machinelearning nlp nltk textclassification naivebayes,youre not returning the list from the function in your findfeature function use
44980966,spacy model download issue,model nlp spacy,there seems to be a problem with the download server this will be fixed asap im one of the spacy maintainers btw sorry about the inconvenience all models are also attached as archives to the v release so in the meantime you can always download them manually from there unzip the archive and place the contained folder in spacydata if you dont have to use v id also recommend checking out the newer versions and upgrading to spacy v models are now hosted on github which makes the downloading process more transparent and doesnt rely on a separate download server theyre also wrapped as native python packages which lets you install them via pip add them to your projects requirementstxt and even import them as a module at the top of your file you can read more about this in the models documentation this makes it easier to manage model dependencies especially as more models become available if youre using spacy youll be able to use models for english german french and spanish if you have trained your own models and decide to upgrade note that you will have to retrain your models with the input from the new version models trained on spacy
44972641,tfidf vectorizer not working,pythonx nlp tfidf,there is most likely an error in your cleandoc function the tokenizer argument should be a function that takes a string as input and returns a list of tokens
44944249,pip install pyemd error,python nlp pip gensim,the error you are receiving is error microsoft visual c is required get it with microsoft visual c build tools you need to read the error message carefully you just need to go to the link they have provided for you and follow the instructions
44855603,typeerror cant pickle threadlock objects in seqseq,pythonx tensorflow nlp lstm sequencetosequence,the problem is with latest changes in seqseqpy add this to your script and it will avoid deepcoping of the cells setattrtfcontribrnngrucell deepcopy lambda self self setattrtfcontribrnnbasiclstmcell deepcopy lambda self self setattrtfcontribrnnmultirnncell deepcopy lambda self self
44817396,attribute error issue while training pos tags in spacy nlp,python nlp spacy,replace stringstoredump with stringstoretodisk and stringstoretobytes this has been updated in the new documentation
44807639,convert averaged perceptron tagger pos to wordnet pos and avoid tuple error,python pythonx nlp nltk postagger,python interpreter clearly told you tokenspos is an array of tuples so you cant pass its elements directly to lemmatize method look at code of class wordnetlemmatizer here only string type object have method endswith so you need to pass first element of every tuple from tokenpos just like that method lemmatize uses wordnetnoun as a default pos unfortunately wordnet uses different tags than other nltk corpora so you have to manually translate them as in the link you provided and use proper tag as a second parameter to lemmatize full script with method getwordnetpos from this answer
44645497,value error when running sklearn classifier model,python scikitlearn nlp,the error is in how you are using the traintestsplit you are using it as but the output order is different actually as given in documentation it is also one recommendation is that if you are using scikit version then change the package from crossvalidation to modelselection because its deprecated and will be removed in new versions so instead of use the following
44643501,wikipedia entity annotator not working in stanford corenlp,nlp stanfordnlp,svjans answer will work fine but perhaps easier is downloading and including in your classpath the english kbp models jar from the download page the naming is somewhat cryptic kbp stands for knowledge base population which subsumed the entity linking models and the new relation extraction models direct link is here for the version warning mb download
44505697,add spellinggrammatical error to data,machinelearning nlp deeplearning,have you taken a look at edit distance edit distance measures the distance between words in terms of basic operations insertion adding a character x unit deletion deleting x unit transposition swapping two adjacent characters x and y unit substitution replacing x with y units insertion deletion for example algorithm and logarithm are separated by an edit distance of to introduce meaningful noise that models real world data you could consider the following approach in each string take words at random for each random word choose one of operations and apply it randomly on any part of the word you could also apply operations on the same word in real world you wont find errors more drastic than edit distance of the reason to consider edit distance is that you should make sure your incorrect spellings do not deviate from the correct spelling by more than
44365435,python text classification error expected string or byteslike object,python text twitter nlp classification,try thatll convert it to a str object from whatever type it was before
44216254,tensorflow valueerror cannot feed value of shape for tensor placeholder which has shape,python machinelearning tensorflow nlp,first of all your code is incomplete check neuralnetworkmodel function anyways the following code works for now i have just used one network layer you can add more layers in your neuralnetworkmodel function making sure that nclasses and the size of output in neuralnetworkmodel function is same for now run the below code and then later update neuralnetworkmodel function note the code has flaws at other levels but that is not the point of this question i took the missing functions from the place you pointed edit i guess i should not be encouraging you with your silly mistakes this is last time i am fixing things you have again just messed up in the same function you must first go through your code completely before posting it to stackoverflow so that you are sure you are asking the correct problem you encountering and not a side silly mistake
43959815,list index out of range error when tagsents method of nltk sennatagger is called,nlp nltk postagger indexerror senna,the input for sennatagsents is list of list of strings which can be achieved through wordtokenizesent for sent in sents or use map if you dont want to materialize tokenizedsents before tagging
43918886,error downloading spanish model in spacy,python nlp spacy,you can download the spanish models with you can see the two different spanish model on then you load the model with
43825162,error while installing nltk with python,python nlp nltk python,it seems you dont have permission to install packages try running the command line as administrator
43765066,issue with timedistributed lstms,neuralnetwork nlp keras keraslayer,i found the reason for the error you cannot apply a timedistributed layer at that position i had to replace it with a normal lstm which also would make more sense considering the paper then it worked
43548369,failed to write core dump a fatal error has been detected by the java runtime environment,java machinelearning nlp fasttext,the error occurred because you havent load the model yet you can find out how to load the trained model here there you can also take a look at how to train supervised models or unsupervised models the function loadmodel works with all of these models
43343820,how to resolve r error using textvec glove function unused argument grainsize,r nlp wordembedding textvec,apparently globalvectors constructor was changed once more and now takes vocabulary information directly from tcm
43010861,older versions of spacy throws keyerror package error when trying to install a model,python nlp keyerror spacy,tldr thats because the sputnik package has been deprecated since spacy best bet is to upgrade your spacy to the latest one or at least up till otherwise you could try but do note that this might mess up your python environment if have the new spacy models already installed remember to use virtual environment esp on backversioned libraries also this is dependent on the fact that spacy can be installed properly in short see and in long looking at the code from from sputnikpackagepy looking at we see that metapackage pointing to sputnikdefaultpy ie that is pointing to metafilename ie the metajson which is refering to the json from and if we follow the breadcrumbs to we see and the end of trail leads to
42953837,importerror no module named ntlk,python nlp,update your pythonpath variable as per your installed python x or x export pythonpathpythonpathusrlocallibpythonsitepackages
42673590,gensim memory error using googlenewsvector model,nlp gensim wordvec,gb is very tight for that vector set you should have gb or more to load the full set alternatively you could use the optional limit argument to loadwordvecformat to just load some of the vectors for example limit would load just the first instead of the full million as the file appears to put the morefrequentlyappearing tokens first that may be sufficient for many purposes
42621652,tensorflow valueerror shape must be rank but is rank,python tensorflow nlp lstm bidirectional,for anyone encountering this issue in the future the snippet above should not be used from tfcontribrnnstaticbidirectionalrnn v documentation returns a tuple outputs outputstatefw outputstatebw where outputs is a length t list of outputs one for each input which are depthconcatenated forward and backward outputs outputstatefw is the final state of the forward rnn outputstatebw is the final state of the backward rnn the list comprehension above is expecting lstm outputs and the correct way to get those is this outputs tfcontribrnnstaticbidirectionalrnnlstmcell lstmcell predmod tfmatmulitem weightsoutw biasesoutb for item in outputs this will work because each item in outputs has the shape batchsize numhidden and can be multiplied with the weights by tfmatmul addon from tensorflow v the recommended function to use is in another package tfnnstaticbidirectionalrnn the returned tensors are the same so the code doesnt change much outputs tfnnstaticbidirectionalrnnlstmcell lstmcell predmod tfmatmulitem weightsoutw biasesoutb for item in outputs
42441257,issues regarding training maltparser model,java nlp postagger dependencyparsing maltparser,ok i found the solution for first problem you dont need xpostag duplicating upostag will allow training my problem was that no word or punctuation mark in the question can be left blankit has to be pos tagged and must be made dependent on the root it solved my issues in case of the second question the answer is ambiguous there is no valid one to one relationship between upostag and xpostag as it is language dependent any table using the penn tree bank tags will work but will need postprocessing for accuracy
42363897,attributeerror type object wordvec has no attribute loadwordvecformat,python nlp gensim wordvec,gojomos answer is right gensimmodelskeyedvectorsloadwordvecformatgooglenewsvectorsnegativebingz binarytrue try to upgrade all dependencies of gensimeg smartopen if you still have errors as follows pip install upgrade gensim file homeliangnpythonprojectsdeeprecommendationalgorithmwordvecpy line in init selfmodel gensimmodelskeyedvectorsloadwordvecformatwvpath binarytrue file homeliangnpythonprojectsvenvlianglibpythonsitepackagesgensimmodelskeyedvectorspy line in loadwordvecformat with utilssmartopenfname as fin file homeliangnpythonprojectsvenvlianglibpythonsitepackagessmartopensmartopenlibpy line in smartopen return filesmartopenparseduriuripath mode file homeliangnpythonprojectsvenvlianglibpythonsitepackagessmartopensmartopenlibpy line in filesmartopen return compressionwrapperopenfname mode fname mode file homeliangnpythonprojectsvenvlianglibpythonsitepackagessmartopensmartopenlibpy line in compressionwrapper return makeclosinggzipfilefileobj mode file usrlibpythongzippy line in init fileobj selfmyfileobj builtinopenfilename mode or rb typeerror coercing to unicode need string or buffer file found
42347678,error in implementing aspectbased sentiment analysis deep learning model,nlp deeplearning keras sentimentanalysis,the problem lies in a wrong dimensionality of your data it should have shape recheck your data preparation as an error probably occured there
42179322,nltk concordance not working,python nlp nltk,you must create a text instance from a sequence of strings use a tokenizer from nltktokenize to tokenize your sentence
42120594,error creating edustanfordnlptimetimeexpressionextractorimpl,python nlp stanfordnlp,you need to make sure you use the proper dependencies with the proper version if you use stanford corenlp make sure you also have the latest lib and liblocal folders i believe this error is because you have an incompatible dependency jar somewhere update this is an error due to java add this flag and it should go away
41677636,ms luisai model performance issues how to increase prediction accuracy,nlp azurecognitiveservices azurelanguageunderstanding,if you expect some common misspellings that you expect to be repeated luis has something called phrase list features that will allow you to define exchangeable are not exchangeable words and ultimately to improve the performance of your model in this case i imagine fundz being an exchangeable word of funds here you will find the documentation around phrase list features
41618481,edustanfordnlpioruntimeioexception error using stanford nlp pos tagger,java nlp stanfordnlp buildpath,found the solution if anyone else ever needs it you need to make sure that your maxenttagger tagger has the correct and updated english file passing through it
41415483,stanford parser multithreading issue lexicalizedparser,java multithreading nlp stanfordnlp,here is an example command that will run the parser in multithreaded mode
41398680,error loading list when adding a list to arabic plugin gazetteer,nlp namedentityrecognition gate,the problem was due to two major issues which are the file was not saved correctly as utf encoding which was resolved by using online converter the file contains special characters which were resolved by using the following replaceall regular expression line linereplaceall
41351804,class index differ error in weka,nlp weka,you should use the same transformations on your testset before you use it to evaluate a trained model when using the gui you could use the preprocessor view from the explorer apply the same transformations by hand and than save the set to a new arff file when you want to conduct a series of experiment i suggest writing a routine that does your transformation for you that would look a little something like this also the reorder filter can help you place your target class at the end of the file it takes a new order of the old indices as arguments in this case you could apply reorder r last
41339649,ctf reader throwing error for big files in cntk,c nlp deeplearning cntk,you need to know the dimensionality of your input and also know that indices start from so if you created an input file mapping your vocabulary to the range to the dimensionality is
41020725,stanfordnlp openie error,java nlp stanfordnlp,ive managed to solve this error the issue lies with the compilation of the openie jar and openie jar files i downloaded from the official website how to solve compile the classes yourself go to choose a release i chose download the zip file and unzip the file use terminal and redirect to the folder directory run sbt package and it will start compiling your final jar file will be found in the target folderscalaopenieassemblyversionjar note there could be some changes you need to make to your code if you are running openie as a dependency the code for test is as follows i hope this will help someone in the future
40982653,stanfordcorenlp object creation error,machinelearning nlp textclassification stanfordnlp,whoever encounters this problem i would suggest them to visit and download the latest model files from there it will mostly solve the issue
40814969,reinit method of ontorootgazetteer is not working,nlp stanfordnlp gate,i have used flexiblegazetteer so it has the parameter gazetteerinst which is nothing but a processing resource ontorootgazetteer so first you need to get all the processing resources that you are using in your pipeline iterate over it and extract the ontorootgazetteer from it after that ontorootgazetteer has a property gazetterinst whose value is actual a ontology so you just need to update that ontology or give the path of the ontology to it then use reinit method for ontorootgazettterwhich you extracted from the flexiblegazettteer through coding here use this will solve your problem
40728751,instance method is not working from an object while it works perefctly from its class,python pythonx nlp,is this a bug in python in a word no instance members and instance methods share the same namespace thus your line in wordinit obliterates the reference to the method wordnext inside the newlyallocated word object
40466285,word vectors example issue in spacy,python nlp spacy,what version of python are you using this might be the result of a unicode error i got it to work in python by replacing with youll then get this error theres a similar issue on the spacy repo but these can both be fixed by replacing hasrepvec with hasvector and repvec with vector ill also comment on that github thread as well complete updated code i used hope this helps
40188226,nltks spell checker is not working correctly,python nlp nltk wordnet,it is giving wrong spellings because those are stopwords which are not contained in wordnet check faqs so you can instead use stopwords from nltk corpus to check for such words
39951340,nltk assertionerror when taking sentences from plaintextcorpusreader,python nlp nltk,that particular file has a utf byte order mark ef bb bf at the start which is confusing nltk removing those bytes manually or copypasting the entire text into a new file fixes the problem im not sure why nltk cant handle boms but at least theres a solution
39864634,automate solving of customer technical issue production l tickets,machinelearning automation nlp prediction googleprediction,if you want to use java libraries for your nlp pipeline have a look at opennlp youve a lot of basic support here and then youve deeplearningj where youve a lot of neural network implementations in java as you want a dynamic model which can learn from past experiences rather than a static one youve a number of neural netwrok implementations which you can play with in deeplearningj hope this helps
39790837,noclassdeffounderror stanfordcorenlp,java nlp stanfordnlp apachestorm sentimentanalysis,make sure the stanford nlp libraries are on classpath if you are running your project using eclipse this link might help you error in stanford nlp core
39318400,typeerror module object is not callable in spacy python,python nlp spacy,is it spacyloaden or spacyloadsen the official doc says spacyloaden it may be the problem
39144991,nltk nltktokenizeregexptokenizer regex not working as expected,python regex nlp nltk tokenize,the point is that your b was a backspace character you need to use a raw string literal also you have literal pipes in the character classes that would also mess your output this works as expected note that putting a single w into a character class is pointless also you do not need to escape every nonword char like a dot in the character class as they are mostly treated as literal chars there only and require special attention
38950643,train some embeddings keep others fixed,nlp neuralnetwork deeplearning keras recurrentneuralnetwork,i do not believe that this is achievable with the existing embedding layer to get around it i would just create a custom layer that builds two embedding layers internally and only puts the embedding matrix of one of them into the trainableparameters
38687056,nltk issue in deriving sql query using fcfg,python parsing nlp nltk informationextraction,the issue was that i was modifying the grammarsbookgrammarssqlfcfg when i saved it as separate file and loaded grammar from there the problem got solved dont know why it happened but it resolved the issue
38566041,ner crf exception in thread main javalangnoclassdeffounderror orgslfjloggerfactory,java nlp crf stanfordnlp,can you try unix windows
38446231,slfj issues in stanford core nlp and openccg,java linux nlp classpath stanfordnlp,problem solved by using an version of corenlp
38309909,override a function in nltk error in contextindex class,python nlp nltk similarity,you are getting the error because the contextindex constructor is trying to take the len of your token list the argument tokens but you actually pass it as a generator hence the error to avoid the problem just pass a true list eg
38105023,how to handle huge gb xml parse it into json in python using xmletree to iterate over it but memory error,python json xml memorymanagement nlp,if anyone still struggling on this problem i added elementclear after every instance i capture the element although it is mentioned at many places but i am still to find some resource where it explains how xmletree handle memory internally any leads will be appreciated thank you
38021980,classpath error when training a model with stanford nlp,nlp stanfordnlp,youre missing the ner models if you include the models jar in your classpath in addition to what you have already the command should work
37900973,stanford nlp outofmemoryerror,nlp stanfordnlp,ok that last sentence of the question made me go double check the answer is that i was keeping reference to coremap in one of my own classes in other words i was keeping in memory all the trees tokens and other analyses for every sentence in my corpus in short keep stanfordnlp coremaps for a given number of sentences and then dispose i expect a hard core computational linguist would say there is rarely any need to keep a coremap once it has been analyzed but i have to declare my neophyte status here
37525012,error while installating yamcha package,nlp svm postagger,there is a header file missing in your code files the link you provided above contains a source file srccommonh just add include in this file with this addition it should work
37446104,python sklearnlinearmodel linearregression valueerror occured when predict,python machinelearning nlp scikitlearn linearregression,you trained a linar model on features but want to predict some new sample with only features thats not how linear models work or most of the classifiers the number of features need to be the same how should your linear model which consists of a linear combination of variables work on only variables if some vars are unknown during prediction you could impute them eg setting to zero or mean but even this approach needs to know the exact mapping of your variables which variable in train corresponds to which in predict
37403428,svmlight error features must be in increasing order,machinelearning nlp svm svmlight,ok i found the issue its a duplicate feature id this happened due to an inconsistency between matching functions in the database and my code
37299003,nltk sentence boundary error,python pythonx nlp nltk,it looks like you need to loop over enumeratewords instead of enumeratewords as youve written it you are calling punctfeatureswords i on the last word in the list when the index of the last word in the list i is passed to punctfeatures you then try to access wordsi as tokensi since there are only i items in words you get an indexerror
36830269,possible mistakebug in stanford corenlp andor nlp parse visualization,nlp stanfordnlp,the stanford parser is generally significantly worse at imperatives than it is on other sentences this is likely just a simple parse error inherent in the fact that these are imperfect models the dependency parser actually seems to also mess up on this sentence i suspect its just a hard sentence
36815117,exception in thread twitterj async dispatcher javalangnoclassdeffounderror,java twitter nlp stanfordnlp twitterj,javalangnoclassdeffounderror orgslfjloggerfactory means that need the slfj library in your classpath if you use maven you can use this dependency
36745217,error extracting noun in r using konlp,r csv encoding nlp extract,the number of lines shouldnt be a problem i think that there might be a problem with the encoding see this post your csv file is encoded as euckr i changed the encoding to utf using but that results in the following error warning message in preprocessingsentence input must be legitimate character so this might be an error with your input i cant read korean so cant help you further
36230641,stemdocment in tm package not working on past tense word,r nlp tm stemming snowball,if there is a data set of irregular english verbs in a package this task would be easy i just do not know any packages with such data so i chose to create my own database by scraping i am not sure if this website covers all irregular words if necessary you want to search better websites to create your own database once you have your database you can engage in your task first i used stemdocument and clean up present forms with s then i collected past forms in words ie past infinitive forms of the past forms ie infidentified the order of the past forms in temp i further identified the positions of the past forms in temp i finally replaced the sat forms with their infinitive forms i repeated the same procedure for past participles
36109717,stanford nlp pos tagger has issues with very simple phrases,nlp stanfordnlp linguistics partofspeech,youre not doing anything wrong youre of course welcome to decide for yourself how much to trust any tool but i suspect youll see similar issues with any parser trained empiricallystatistically as to your issues periods are treated like any other token in model building so yes they can influence the parse chosen there are indeed a lot of ambiguities in english as there are in all other human languages and the question of whether to interpret forms ending in ing as verbs nouns verbal nouns or gerunds or adjectives is a common one the parser does not always get it right in terms of particular bad choices it made often they reflect usagedomain mismatches between the parser training data and the sentences you are trying the training data is predominantly news articles last millennium news articles for that matter although we do mix in some other data and occasionally add to it so the use of flagging as a verb common in modern internet developer use doesnt occur at all in the training data so it not surprisingly tends to choose jj for flagging since thats the analysis of the only cases in the training data in news articles drinking is just more commonly a noun with discussions of underage drinking coffee drinking drinking and driving etc
36039483,simple issue import net file wordoccurences into cytoscapewhich attributes are which,nlp textprocessing cytoscape,is this the net format you are using if so then you can see that the format like any network file format requires at least two columns of node identifiers ie source and target your shows the first columns id along with some node attributes the column headers for weight and cluster are not as clear my naive guess is that the weight occurrences column is actually the second list of target node ids and the weight cooccurrences is an edge attribute but this is just a guess if you can deduce the meaning of the columns in your file then its a simple matter to assign and import them into cytoscape for net or any tabluar file format hope this helps
35475677,custom pos tagging with nltk error,python nlp nltk,quick answer use nltk for now pip install nltk better answer they changed the treebank tagger last september and it has a lot of other ramifications we currently are fixed on as the new tagger is worse at least for our needs this appears to work but i am unsure of how correct the code is
35118596,python regular expression not working properly,python regex nlp nltk tokenize,possibly its something to do with how regexes were compiled previously using nltkinternalscompileregexptononcapturing that is abolished in v see here but it doesnt work in nltk v with slight modification of how you define your regex groups you could get the same pattern to work in nltk v using this regex in code without nltk using pythons re module we see that the old regex patterns are not supported natively note the change in how nltks regexptokenizer compiles the regexes would make the examples on nltks regular expression tokenizer obsolete too
34940417,train stanford ner with big gazette memory issue,java memory nlp stanfordnlp namedentityrecognition,regexner could help you with this some thoughts start with entries and see how big of a gazetteer you can handle or if is too large shrink it down more sort the entries by how frequent they are in a large corpus and eliminate the infrequent ones hopefully a lot of the rarer entries in your gazetteer arent ambiguous so you can just use regexner and have a rule based layer in your system that automatically tags them as person
34557078,why nltkalignbleuscorebleu gives an error,python nlp nltk machinetranslation bleu,it seems like youve caught a bug in nltk implementations this tryexcept is wrong at in long firstly lets go through what the pn in bleu score means note that the papineni formula is based on a corpuslevel bleu score and the native implementation is using a sentencelevel bleu score the bleeding edge version of nltk contains an implementation that follows the papineni paper to calculate corpus level bleu in multireference bleu the countmatchngram is based on the reference with a higher count see so the default bleu score uses n which includes unigrams to grams for each ngrams lets calculate the pn note the latest version of modifiedprecision in bleu score since this has been using fraction instead of float outputs so now we can clearly see the numerator and the denominator so lets now verify the outputs from the modifiedprecision for unigram in the hypothesis the bold words occurs in the references there are tokens overlapping with of the is a duplicate that occurs twice now lets check how many times these overlapping words occurs in the references taking the value of the combined counters from the different references as our numerator for the p formula and if the same word occurs in both references take the maximum count now for the denominator its simply the no of unigrams that appears in the hypothesis so the resulting fraction is and our modifiedprecision function checks out now lets get to the full bleu formula from the formula lets consider only the exponential of the summation for now ie exp it can be also simplified as the sum of the logarithm of the various pn as we calculated previously ie sumlogpn and that is how it is implemented in nltk see ignoring the bp for now lets consider summing the pn and taking their respective weights into consideration ah ha thats where the error appears and the sum of the logs would have returned a valueerror when putting them through mathfsum to correct the implementation the tryexcept should have been references the formulas comes from that describes some sensitivity issues with bleu
34427678,utf decode error when loading a wordvec module,python nlp gensim wordvec,the module was tons of chinese characters trained by java i cannot figure out the encoding format of the original corpus the error can be solved as the description in gensim faq using loadwordvecformat with a flag for ignoring the character decoding errors but ive no idea whether it matters when ignoring the encoding errors
34361725,nltk stanfordnertagger noclassdeffounderror orgslfjloggerfactory in windows,python windows nlp nltk stanfordnlp,edited note the following answer will only work on nltk version stanford tools compiled since as both tools changes rather quickly and the api might look very different months later please treat the following answer as temporal and not an eternal fix always refer to for the latest instruction on how to interface stanford nlp tools using nltk step first update your nltk to the version using or for windows download the latest nltk using then check that you have version using step then download the zip file from and unzip the file and save to csomepathtostanfordner in windows step then set the environment variable for classpath to csomepathtostanfordnerstanfordnerjar and the environment variable for stanfordmodels to csomepathtostanfordnerclassifiers or in command line only for windows see for clickclick gui instructions for setting environment variables in windows see stanford parser and nltk for details on setting environment variables in linux step then in python without setting the environment variables you can try see more detailed instructions on stanford parser and nltk
33773157,wordtokenize typeerror expected string or buffer,python pythonx nlp nltk tokenize,the input for wordtokenize is a document stream sentence ie a list of strings eg this is sentence thats sentence the file is a file object not a list of strings thats why its not working to get a list of sentence strings first you have to read the file as a string object finread then use senttokenize to split the sentence up im assuming that your input file is not sentence tokenized just a raw textfile also its better more idiomatic to tokenize a file this way with nltk
33603534,issue recognizing nes with stanfordner in python nltk,python nlp nltk stanfordnlp namedentityrecognition,since you are doing this through the nltk use its tokenizers to split your input edit youre probably better off with the stanford toolkits own tokenizer as recommended by the other answer so if youll be feeding the tokens to one of the stanford tools tokenize your text like this to get exactly the tokenization that the tools expect to use this method youll need to have the stanford tools installed and the nltk must be able to find them i assume you have already taken care of this since youre using the stanford ner tool
33015326,maltparser giving error in nltk,python linux parsing nlp nltk,the maltparser api in nltk just had a patch that fixes and stabilizes the problems that it used to have how to use malt parser in python nltk malt parser throwing class not found exception maltparser not working in python nltk heres an example of how to use maltparser api in nltk see here for more demo code or here for a more elaborated demo code note that you can also use the export features and you can escape the usage of full path when initializing the maltparser object but you have to still tell the object what is the name of the parser directory and model filename to look for eg
32652725,importerror cannot import name stanfordnertagger in nltk,python nlp nltk,i worked it out set the stanfordmodels as you did i learnt from you thx import nltktagstanford as st tagger ststanfordnertaggerpathtogz pathtojar here pathtogz and pathtojar are the full path to where i store the file allclassdistsimcrfsergz and the file stanfordnerjar now the tagger is usable try taggertagrami eid is studying at stony brook university in nysplit it has nothing to do with classpath hope it helps
32326065,stanford nn dependency parser unrecoverable error while loading a tagger model,java nlp stanfordnlp,it seems like your path to the file englishleftwordsdistsimtagger is not correct check if path you provide is correct you can also try it with absolute path belphegor replied i solved it with absolute path i first created the following folders in the src folders edustanfordnlpmodelspostaggerenglishleftwords and inside i pasted the file englishleftwordsdistsimtagger which is in the postagger file stanfordpostaggerfullzip after this it worked
32113346,eclipse maven cannot be resolved to a type error,java eclipse maven nlp opennlp,in your pom in section add and then you have to import classes in java file before line public class app add
32078901,genia tagger file not found error in anacondanltk,python package nlp nltk anaconda,tldr im not sure whether the packages for genia tagger works out of the box from conda so i think a native pythonpip fix is simpler firstly theres no support for genia tagger in nltk at least not yet so it isnt a problem with the nltk installationmodules the problem might lie in some outdated imports that the original geniatagger c code uses so to resolve the problem you have to add include to the original code but thankfully saffsd has already done so and put it nicely in his github repo then comes installing the python wrapper you can either install from the official pypi with pip install or use some other github repo to install eg that appears first from google search lastly the geniatagger initialization in python is rather weird because it doesnt really take the path to the directory of the tagger but the tagger itself and assumes that the model files are in the same directory as the tagger see and possibly it expects some use of in the first level of directory path so you would have to initialize the tagger as such geniataggergeniataggergeniatagger beyond the installation issues if you use the python wrapper for the geniatagger theres only one function in the geniatagger object ie parse when you use parse it will output a list of tuples for each sentence and the input is one sentence string the items in each tuple are token surface word lemma see stemmers vs lemmatizers pos tag looks like penn treebank tagset see what are all possible pos tags of nltk noun chunk see output results in conll format postagging stanford pos tagger named entity chunk
31628698,metamap run local raise error when querying prolog server connection refused,java nlp,after running skrmedpostctl and the optional word sense diambiguation server wsdserverctl you need to run the mmserver executable in order to use the java api for metamap this can be run by running the command also be sure to include the two jar files for metamap and prologbeans in your classpath in your ide this can be done by adding a dependency to these jars
31425421,creating termdocumentmatrix issue with number of documents,r statistics nlp tm termdocumentmatrix,youll need to use the paste function on your corpussample vector paste with a value set for collapse takes a vector with many text elements and converts it to a vector with one text elements where the elements are separated by the string you specify
31345593,error using nltkwordtokenize function,python twitter nlp nltk tokenize,the problem youre getting is not from the code you included its from the code that include open command the script is opening the file fine but when youre accessing your data its give you that traceback
31056762,stemming problems in python,python nlp,you dont have to put all of your code sara we are only concerned with the snippet that causes the problem my guess is that the problematic part is the check if i in verb that might fail most of the time because of trailing characters after splitting the characters normally when you split the tokens you also need to trim the ending characters with the strip method conditionals like will always fail and thats why the program doesnt check the exceptions at all
30822131,nltk package errors punkt and pickle,python commandline package nlp nltk,perform the following then when you receive a window popup select punkt under the identifier column which is locatedin the module tab
30794913,ruby rjb cant create java vm error,ruby path jvm nlp rjb,so after i finally came across a solution rjb will only work if java and ruby are both bit or both bit different versions will cause the vm error to solve find version of ruby you are running ruby v in terminal find version of java javac or java v in terminal look up if versions are bit or bit if different i thought it was easier to change my version of jdk uninstall jdk downloadinstall x bit version of jdk make sure javahome env variable is set to the location of the jdk may have to restart for any changes you make to take effect boom hope this works for everyone else as well
30219234,lingpipe sentiment analysis tutorial demo error,java eclipse nlp lingpipe,the specified path fileusersdylandesktoppolaritydir should contain the unpacked data see tutorial in the directory txtsentoken you can see this in the output data directoryfileusersdylandesktoppolaritydirtxtsentoken also the the tutorial is not setup to use an url so the command should be java cp sentimentdemojarlingpipejar polaritybasic usersdylandesktoppolaritydir
30017491,problems obtaining most informative features with scikit learn,python pandas machinelearning nlp scikitlearn,to solve this specifically for linear svm we first have to understand the formulation of the svm in sklearn and the differences that it has to multinomialnb the reason why the mostinformativefeatureforclass works for multinomialnb is because the output of the coef is essentially the log probability of features given a class and hence would be of size nclass nfeatures due to the formulation of the naive bayes problem but if we check the documentation for svm the coef is not that simple instead coef for linear svm is nclasses nclasses nfeatures because each of the binary models are fitted to every possible class if we do possess some knowledge on which particular coefficient were interested in we could alter the function to look like the following this would work as intended and print out the labels and the top n features according to the coefficient vector that youre after as for getting the correct output for a particular class that would depend on the assumptions and what you aim to output i suggest reading through the multiclass documentation within the svm documentation to get a feel for what youre after so using the traintxt file which was described in this question we can get some kind of output though in this situation it isnt particularly descriptive or helpful to interpret hopefully this helps you with output
29763296,cluto docmat specified stop word list not working,perl nlp document datamining stopwords,theres one important thing i should remember i should include all the unwanted words in my stop list this is somewhat difficult since theres always some variations available for example if i want to exclude method i add it to my list however the resulting vocabulary may also contain method since there are words like methodist methods etc then docmat by default stems these words and i will still get method in the output another thing is to make sure that nostop option must be provided for userspecified stop list
29733476,i am having problems doing word sense disambiguation in python using lesk algorithm,python nlp nltk postagger wordsensedisambiguation,nltkwsdlesk does not return score it returns the predicted synset lesk is not perfect it should only be used as a baseline system for wsd although this is nice theres a simpler to get the synset identifier then you can do to convert pos tag to wn pos you can simply try converting pos tags from textblob into wordnet compatible inputs
29311650,noclassdeffounderror opennlptoolschunkerchunkermodel,java nlp opennlp,i dont see any nlpspecific reasons here so just check tutorials about noclassdeffounderror for example verify that all required java classes are included in the applications classpath the most common mistake is not to include all the necessary classes before starting to execute a java application that has dependencies on some external libraries the classpath of the application is correct but the classpath environment variable is overridden before the applications execution or related question in particular check that you have appropriate and only one version of opennlp jar in your classpath it is not a good style to import all content of the package by using wildcard instead use ides support eg ctrlshifto in eclipse ctrlalto in idea automatically resolves all needed imports
29006811,nlp error while tokenization and tagging etc,java nlp stanfordnlp,youre trying to use stanford nlp tools version or later using a version of java or earlier either upgrade to java or downgrade to stanford tools version
28788845,error generating a model reading corpus from a big txt file,python machinelearning nlp taggedcorpus,you may be able to reduce your memory usage if you combine these two chunks of code you can check to see if an item exists in the count list already and by doing so not add duplicates in the first place this should reduce your memory usage see below
28642525,using wordnet to perform stemming io error too many open files,java io nlp,try this
28616317,corenlp maxenttagger data format error,java nlp stanfordnlp illegalargumentexception,answering my own question here the training file must have a perfect format of worddelimitertag or else it will throw fatal runtime error you can use whatever delimiter you want such as the hashtag symbol for example but if there are whitespaces missing tags between the worddelimitertag pattern it will fail
28545078,python countvectorizer error attributeerror file object has no attribute lower,python nlp scikitlearn,according to the documentation in your case the vectorizer should be initialized with the input parameter set to file therefore
28522106,fcfg error in nltk python grammar issue,python nlp nltk contextfreegrammar,the error comes from how nltk implements types lambda calculus it expects lowercase letters to have type and uppercase letters to have type that is to say that lowercase letters cannot represent predicates the following parses xxysomey as an aside one represents the concept of some in some x are y with a conjunction as follows in words some x are y is logically equivalent to there are some items have both x and y quality
28475620,wordnet lemmatizer in nltk is not working for adverbs,python nlp nltk wordnet,try see getting adjective from an adverb in nltk or other nlp library for more information the question is why do you have to go through the lemmas to get the pertainyms its because wordnet sees it as a lexical association between word categories see pertainyms are relational adjectives and do not follow the structure just described pertainyms do not have antonyms the synset for a pertainym most often contains only one word or collocation and a lexical pointer to the noun that the adjective is pertaining to participial adjectives have lexical pointers to the verbs that they are derived from then again if we look at the java interface getting a synsets pertainym is as easy as adjectivesynsetgetpertainyms so i guess it depends on who writes the interface what sort of perspective they take towards adjectiveadverb relationship for me i think pertainyms would have been directly related to the synset rather than the lemma
28469476,typeerror wordlistcorpusreader object has no attribute getitem while using nltkclassifyapplyfeatures,python machinelearning nlp classification nltk,firstly i think there is a typo in the tutorial on the wordlist corpus cannot be access like a list next see here on what applyfeatures does basically given a list of tuples of input label inputn labeln it returns featurefunctok label for tok label in toks eg the full code to get the naivebayes to work in nltk for the names corpus out
28399340,install issue with python spacy package in anaconda environment,pythonx installation nlp anaconda spacy,you have hit this bug which should be already fixed in the last version apparently spacy cant download the data because the destination already exists may be from a previous interrupted download a workaround would be to delete the tempdata folder and retry the download
28389564,stanford corenlp error creating edustanfordnlptimetimeexpressionextractorimpl,c nlp stanfordnlp,adding propssetpropertynerusesutime false fixed this for me
28314337,typeerror sparse matrix length is ambiguous use getnnz or shape while using rf classifier,python numpy machinelearning nlp scikitlearn,i dont know much about sklearn though i vaguely recall some earlier issue triggered by a switch to using sparse matricies internally some of the matrices had to replaced by mtoarray or mtodense but to give you an idea of what the error message was about consider len usually is used in python to count the number of st level terms of a list when applied to a d array it is the number of rows but ashape is a better way of counting the rows and mshape is the same in this case you arent interested in getnnz which is the number of nonzero terms of a sparse matrix a doesnt have this method though can be derived from anonzero
27761803,problems loading textual data with scikitlearn,python python machinelearning nlp scikitlearn,the first problem is youve got the wrong directory structure you need it to be like you need to have both the train and test set in this directory structure alternatively you can have all data in one directory and use traintestsplit to split it in two secondly needs to be here is a complete and working example the directory structure of sampledataweb is
27713944,problems classifiying labeled text wrong prediction,python machinelearning nlp scikitlearn nltk,i will leave it to you to get the training data into the expected format feature extraction with bigger corpus you should increase nfeatures to avoid collisions i used so that the resulting matrix can be visualized also note that i used stopwordsenglish i think with so few examples it is important to get rid of stopwords otherwise you could confuse the classifier model training prediction edit note that the correct classification of the first test example is just a fortunate coincidence as i dont see any word that could have been learned from the training set as negative in the second example the word good could have triggered the positive classification
27658151,error in calculating beta in second order point wise mutual information,machinelearning statistics nlp,the paper you posted gets the algorithm slightly wrong so you shouldnt rely upon it try following along with the example in this paper instead
27584047,error using scipyoptimizeminimize lbfgs,python scipy nlp mathematicaloptimization,the error message means that numpy is getting a vector someplace that it expects a scalar value your objective function is returning the parameter vector v do you mean instead for it to return the scalar vmodel
27507550,utf issues with python and nltk,python string utf nlp nltk,the thing is nltkwordpuncttokenize doesnt work with nonascii data it is better to use punktwordtokenizer from nltktokenizepunkt so import is as and replace with
27091571,get synonyms from synset returns error python,python nlp nltk wordnet,which version of nltk are you using try print nltkversion are you using python or python it seems that in the version you are using lemmanames is a method and not an attribute this is the case in nltk for python if this is the case then you can probably fix your code by using this instead
26928930,stopwords filtering not working entirely,c string nlp,you almost there stringtolower returns new string instance you need to assign it to another or same string reference stringbuilderreplace returns new stringbuilder instance you need to assign it to another or same stringbuilder reference and since you first use tolower than replace the the you shouldnt have the part in your instance because it matches in your stopwords array item result will be
26899235,python nltk syntaxerror nonascii character xc in file sentiment analysis nlp,python unicode nlp nltk,add the following to the top of your file codingutf if you go to the link in the error you can seen the reason why defining the encoding python will default to ascii as standard encoding if no other encoding hints are given to define a source code encoding a magic comment must be placed into the source files either as first or second line in the file such as coding
26868077,how to solve standfordopennlp error,nlp lemmatization,i also tried to install that but got the same error this is because library has compiled with jdk but u may try to run it with a lower version than to solve this download the latest jdk version from here go to cprogram filesnetbeans xetcnetbeansconf and change the following line to point it where your java installation is netbeansjdkhomecprogram filesjavajdkxxxxx you may need administrator privileges to edit netbeansconfthen try to run the program
26662618,python nltk interpret a fixed pattern of sentence and tokenize it,python nlp speechrecognition nltk,this problem is called named entity recognition or just ner googling those phrases should point you towards many libraries online apis clever rules of thumb for specific types of data etc checkout a demo ner system at detecting references to dates and times is probably the case which has the most heuristicbased solutions out there if you have a specific and pretty limited domain of text you are working with then setting up manually curated lists of entities might prove to be very helpful eg just make a list of all airport codesnames of all cities that have a commercial airport and try to do exact string matching of those names against any input text
26622370,attributeerror parentedtree object has no attribute label,python tree nlp nltk corpus,google suggests youre using nltk which im going to assume is the case a parentedtree does not have a method called label so when you write things like this python doesnt know what to do a tree on the other hand does have a label method did you perhaps use a parentedtree instead of a tree somewhere
26196695,problems with langutil in common lisp how to tokenize a file,lisp nlp commonlisp tokenize,ok lets see in no particular order yes langutils seems buggy and taking a look at its source quite a few things are still labelled not implemented yet what are you trying to do with it specifically what do you expect tag string and tokenizefile to do the docstring wasnt particularly clear if youre going to be trying to parse lisp expressions you can use the builtin read if youre going to be trying to parse arbitrary files with their own syntax rules and are using sbcl take a look at esrap its a peg parser implementation in common lisp the github has some examples if you want to omit empty sequences from splitsequence you can pass it the additional removeemptysubseqs keyword argument im not entirely sure why this isnt the default since ive never called the function without this option but its possible that it makes sense in whatever the primary usecase was your call should look like tokenizestream seems to return the text contents of a file along with some metadata about it if thats all you want its pretty easy to write your own without resorting to multiplevaluecall see the files and directories section of the cl cookbook i think by doing tokenizestream open hellotxt youre leaving a file handle dangling since youre not closing it afterwards the typical thing to do here is to call withopenfile
25614279,hunspell affix condition regex format any way to match the start,nlp stemming hunspell,why would it match asset thats not a verb and as such shouldnt have that suffix attached to it the problems that languages arent perfectly regular the solution that weve used in the asturian spell checker at softastur is to keep track a list of verbs that form certain suffixes one way or another and have a script construct the dic file based on the lists weve kept so for english youd define two separate affixes there are still other irregulars like singeing to contrast with singing that are uncommon enough they are probably best coded as separate so your dictionary file then would like the following more or less i prefer twoletter tags as they can be easier to read if you have a word with lots of tags such that gd gerund doubled and gs gerund single or similar probably not a problem for english but it definitely is for other languages if you dont have a lot of affixes you might just go with g no doubling and g doubling
25256195,python nlp typeerror not all arguments converted during string formatting,python nlp typeerror,suffix is a tuple because items returns keyvalue tuples when you use if the right hand side is a tuple the values will be unpacked and substituted for each format in order the error you get is complaining that the tuple has more entries than formats you probably want just the key the actual suffix in which case you should use suffix or keys to only retrieve the dictionary keys
25196640,near unexpected error in python,python matplotlib nlp,make sure you start the python interpreter
25175318,memoryerror in scikit even with sparse matrices,python memory nlp scikitlearn sparsematrix,i dont think it is the vectoriser as the traceback shows it fails on the following line this allocates a dense numpy array which uses a lot of memory its shape is nclasses nfeatures and nfeatures is the same nfeatures that you passed in as a parameter to the vectoriser m how many classes do you have in your dataset a quick and easy solution is to reduce the value of nfeatures alternatively you can try other classifiers that do not convert the input to a dense array i dont know of the top of my head which of sklearns classifiers do that though ps this question shows how to determine the actual inmemory size of a matrix you can verify it is not the vectoriser or the tfidf transformer that are failing
24803064,using ngram with r for error correction,r nlp ocr ngram,as for the package tm youll need to use an outside tokenizer according to this part of their documentation but this should be pretty straightforward and there are many ways to accomplish your goal gram analysis this is how you create a confusion matrix if you need tokens in a termdocument matrix of length you could use a gram tokenizer n see eg you can use the dictionary argument to store only your selection the corresponding example from the tm documentation was
23391151,are high values for c or gamma problematic when using an rbf kernel svm,machinelearning nlp svm,in general you have to perform cross validation to answer whether the parameters are all right or do they lead to the overfitting from the intuition perspective it seems like highly overfitted model high value of gamma means that your gaussians are very narrow condensed around each point which combined with high c value will result in memorizing most of the training set if you check out the number of support vectors i would not be surprised if it would be the of your whole data other possible explanation is that you did not scale your data most ml methods especially svm requires data to be properly preprocessed this means in particular that you should normalize standarize the input data so it is more or less contained in the unit sphere
22775938,nltk bag of bigrams words function raises dont know how to concatenate types error python,python nlp nltk corpus,firstly your variable words is not a list of tokens as you expect it to be it is of type nltkcorpusreaderutilstreambackedcorpusview to verify that try out note that when you specify a fileid when using the words function of a corpus object you will get different corpusview objects see out possibly what you need is a list of tokens instead of a corpusview object so you would need iterate through the generator to get a list of string out and back to your code you would need to iterate through a generator before return bagofwordswords bigrams
21909165,valueerror could not find stanfordpostaggerjar file for hazm library python nlp,java python nlp nltk postagger,you will first need the postaggerjar file from stanford and also train your own tagger but the hazm dev has kindly uploaded the resource directory that you will need here you will need to unzip and save the folder to the directory where youre running your script for example
21342811,javalangnoclassdeffounderror crfclassifier in a rails app,ruby nlp stanfordnlp namedentityrecognition,i think you need this rjbloadpathtojarstanfordpostaggerjarpathtojarstanfordnerjar xmxm i just tried this and it works create a dir in lib called nlp put the jars there and then create a class which loads the jars using the full path so you end up with little test class output
19814202,detecting shell error in python when using ossytem,python shell nlp ossystem,a call to ossystemyour command returns a unix status code edit ossystem is a bit old and out of date and its recommended that you use the subprocess module instead see
19510158,solr numerical trie vs traditional trie prefix tree,solr lucene nlp trie,solr support searching numeric fields for numeric ranges it is useful for things like price facets numeric trie generates the range boundaries at different levels of precision so when you are looking for the entries that fit within the range you could reject whole groups of them at once eg anything below is definitely not within range
18230269,where can i download the ispell dict and affix files,dictionary fulltextsearch nlp ispell,heres a good reference this is a good resource for those dictionaries of any language
17646721,randomly generated parse tree using a fix set of vocabulary,python nlp parsetree,try nltk
16369667,more issues regarding the installation of rstem package,r nlp,if make is not found it suggests you need to install an xcode appropriate to your unspecified version of osx if you did have xcode installed this should succeed at least it did just now with my macpro running osx r rc and xcode whether there might be other problems i cannot tell the package was removed from cran so there may be other alligators in the swamp
16251708,python openopt integration prob ip interalg typeerror module object is not callable,python optimization ip nlp,i found my error i didnt have the latest version of openopt funcdesigner etc i uninstalled openopt first from the software center and then using pip to uninstall openopt funcdesigner derapproximator and spacefuncs easyinstall pip and run the commandssudo pip uninstall spacefuncs etc pip installation and use i then reinstalled everything with easyinstall and now it works
15640874,suggestions for a small error in lex and yacc,linux nlp yacc lex,you have put azaz the catchall of jonathan as the first rule in case of ambiguities lex will take the longest pattern that matches if two patterns match a string of the same length then the first one is taken so by placing the catchall first it will also match the other keywords like whose that is why jonathan said to place this catchall rule last that way all keywords will be matched first and only if they dont match the catchall will be used
15035279,python composition causes attributeerror,python oop nlp,you said what did you mean that to mean if you want to have a start attribute you need to include it in your slots list or dont use slots at all unless you have a memory issue youre better off just omitting the slots attribute
14598250,perfomance issue while using stanford lexicalized parser in java,java nlp stanfordnlp,full constituency parsing of text is just kind of slow if you stick with it there may not be much that you can do but a couple of things to mention i if youre not using the englishpcfgsergz grammar then you should because its faster than using englishfactoredseergz and ii parsing very long sentences is especially slow so if you can get by omitting or breaking very long sentences say over words that can help a lot in particular if some of the text is from web scraping or whatever and has long lists of stuff that arent really sentences filtering or dividing them may help a lot the other direction you could go is that you appear to not really need a full parser but just an np chunker something that identifies minimal noun phrases in a text these can be much faster as they dont build recursive structure there isnt one at present among the stanford nlp tools but you can find some by searching for this term on the web
14014019,using wordnet gem wordnetlexiconnew gives nameerror,ruby nlp wordnet,had this similar issue using the wordnet gem the wordnetdefaultdb gem relies on two dependencies sqlite which should be standard and wordnet itself add gem wordnet to your gemfile and the wordnetdefaultdb gem should now be working fine
13472076,ngrammodel error need to calculate perplexity,python nlp nltk,you have forgotten to import ngrammodel you should do something like
12215372,java heap space error on stanford ner using netbeans,java nlp netbeans stanfordnlp,the stacktrace shows java running out of memory simply while loading the large models features and weights used for ner in corenlp these do use a considerable amount of memory but this is still very surprising you dont say what os what jdk version whether bit etc you are using but for your program above with a main method added and a couple of types filled in on java u on linux centos i can run it with mxm with either bit or bit java yay compressed oops so i would think m should be enough for any architectureversion so id try running with a bit more memory and seeing if that changes things like mxm if it doesnt make sure that the vm really is getting the amount of memory you state above even though what you write looks correct eg try printing runtimegetruntimemaxmemory
10606983,problems installing srilm with llvmgcc x,c build installation nlp,the problem is due to apple using llvmgccclang which does not support variable length arrays this problem can actually be addressed by modifying srilmdstructsrcarrayh and has been noted and addressed in the upcoming release of srilm for the time being on a mac build srilm using g instead using the following command this builds srilm without problem on all my macs
10156448,parsing words into prefix root suffix in python,python parsing nlp,pyparsing wraps the string indexing and token extracting into its own parsing framework and allows you to use simple arithmetic syntax to build up your parsing definitions the results are returned in a rich object called parseresults which can be accessed as a simple list as an object with named attributes or as a dict the output from this program is
10065710,nltk chunking error,python nlp nltk chunking,you evidently havent implemented your chunker yet chunkparseri is an abstract interface means that you need to derive a class from it and define your own parse method the nltk chapter you link to shows how to define an example class consecutivenpchunker the final step will be to create an instance of your new class and call its eval method which it inherits from chunkparseri so you dont to provide a replacement
9969029,finding synonyms for a certain word creates a wordneterror,python nlp nltk,the error says no lemma eat with part of speech n that means that eat isnt in wordnet as a noun try it as a verb
9259572,spell checker with fused spelling error correction algorithm,algorithm nlp spellchecking,i hacked up norvigs spell corrector to do this i had to cheat a bit and add the word checker to norvigs data file because it never appears without that cheating the problem is really hard basically you need to change the code so that you add space to the alphabet to automatically explore the word breaks you first check that all of the words that make up a phrase are in the dictionary to consider the phrase valid rather than just dictionary membership directly the dict contains no phrases you need a way to score a phrase against plain words the latter is the trickiest and i use a braindead independence assumption for phrase composition that the probability of two adjacent words is the product of their individual probabilities here done with sum in log prob space with a small penalty i am sure that in practice youll want to keep some bigram stats to do that splitting well
8952760,how can i vary the sentence prefix i am working on x such that it has correct sentence structure for all x,api nlp,in addition to natural language processing youre also asking about natural language generation you can try to use a parser like the stanford parser to figure out which kind of phrase you have on hand and to identify the main verb if there is one you might just fall back on a partofspeech tagger for this in english youll also want to identify helping verbs called auxiliaries in technical articles like will may can etc that often come right before the verb because these can change the tense as well if its just a noun phrase im working on x will likely sound okay if its a nominal if the stanford parser gives you only nns without any nps or nnps or dets inside the top np then it might sound better with an article attached eg pepper project im working on the pepper project you wouldnt do that for peppers project or if its already the pepper project or for most proper nouns there are always tricky cases though if its a verb phrase if its already progressive great else use a lemmatizer or fall back on a stemmer to get the root form of the main verb expand that root form into the present progressive for this probably a few heuristics will suffice based on whether or not the lemma ends in a vowel or a consonant that gets doubled eg walk walking run running double n fly flying y doesnt behave like a vowel in this case glide gliding drop the last e after a consonant but flee fleeing not after a vowel the most comprehensive place to look for regularities and exceptions is the comprehensive grammar of the english language or a similar online resource tools for this include morphg and morphadorner finally remove any helper verbs and substitute the present progressive form for the main verb while this wont be perfect itll probably look smarter than most if its an entire clause sentencelike thing with a subject too or a question or some other larger thing you might cop out and just use a generic prefix like right now has jenn gotten back to me right now i must head out im not an expert so i may have missed some tools already out there for this kind of thing and if so i hope to learn that from others its not an easy thing to do but it sounds pretty useful there will always be mistakes and they might be jarring to your users or perhaps theyll find the oddities endearing if you put something together will you post the api here
8379321,debugging the implementation of baum welch algorithm for pos tagging,nlp postagger unsupervisedlearning,unsupervised pos tagging is a very interesting emerging research topic if i understand correctly you are actually asking how to evaluate your tagging accuracy not how to debug the code evaluation is a known issue in unsupervised pos induction the short answer to your question is get this annotated corpus from nltk then map your states to the corpus tags by mapping a state to the tag it most often cooccurs with and find the percentage of correct ones this evaluation procedure is called manytoone mapping you should make yourself familiar with the literature as it will answer your questions and more here are some places to start an early paper mark johnson why doesnt em find good hmm postaggers in proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning emnlpconll pp a survey paper christos christodoulopoulos sharon goldwater and mark steedman two decades of unsupervised pos induction how far have we come in proceedings of emnlp when you say unsupervised you should ask yourself whether you want to use only raw text or also want to use a dictionary for example there are works on that too also there is code available out there for the task another place to ask about nlp is if you have other questions dont hesitate to ask
7290197,paraphrasing for math word problems changing sentence structure without changing meaning,nlp,when creating the word problem use some sort of syntax to denote various equivalent phrases as is done in article spinning sometimes then when displaying the word problem pick randomly between them example syntax example word problems that the syntax above could create for even more combinations you could make it recursive heres an article explaining a similar syntax to what i showed above
7143723,applying svd throws a memory error instantaneously,python memory numpy nlp scipy,yes the fullmatrices parameter to scipylinalgsvd is important your input is highly rankdeficient rank max so you dont want to allocate the entire x matrix for v more importantly matrices coming from text processing are likely very sparse the scipylinalgsvd is dense and doesnt offer truncated svd which results in a tragic performance and b lots of wasted memory have a look at the sparsesvd package from pypi which works over sparse input and you can ask for top k factors only or try scipysparselinalgsvd though thats not as efficient and only available in newer versions of scipy or to avoid the gritty details completely use a package that does efficient lsa for you transparently such as gensim
6123212,are there any good c suffix trie libraries,c algorithm tree nlp trie,being a bioinformatician my pick would be seqan check out the sequence index section it implements a lazy suffix tree and an enhanced suffix array an equivalent data structure both of which have good cache behaviour
5319358,natural language processing fix for combined words,regex nlp,this may be of interest to you you can probably use the medical nature of the text to your advantage by using two dictionaries one containing only medical terminology and one of general english if you can isolate out medical words then run the rest of the string against the general dictionary you should get some decent results
5270571,stanford parse bash script error linux bash,java bash nlp stanfordnlp,as well as the colon which should be a or a new line the dir d doesnt do what you think it does the loop will just have one iteration where file is a long string beginning with dir d and with all your files afterwards also you initially change to a path based on file but then reuse the variable file in your loop which is suspect im having to guess somewhat about your intent but it can be much simpler eg even if you used the more correct version with backticks it would still qualify for a useless use of ls award update originally i forgot to quote file as pointed out in another answer
5241079,cannot import edustanfordnlp stanford parser with jython problem,java python nlp jython stanfordnlp,you have a typo in your sysappend statement the filename says when it should be
4885114,utf encoding problem in java text output,java utf nlp,i noticed that it works correctly when system encoding is configured as utf perhaps input file is assumed to be in system encoding read baluscs post mentioned in the comments to see how to perform inputoutput independent from system encoding
4827365,problems with prologs dcg,prolog nlp grammar dcg,i cant reproduce the error but i suspect it may be in your label rule when i used the following definition of this rule i get correct results in general id recommend tokenizing strings into lists of atoms before doing dcg manipulation that way its much easier to debug because of prologs awkward string output
4806176,what are the most challenging issues in sentiment analysisopinion mining,nlp sentimentanalysis,the key challenges for sentiment analysis are named entity recognition what is the person actually talking about eg is spartans a group of greeks or a movie anaphora resolution the problem of resolving what a pronoun or a noun phrase refers to we watched the movie and went to dinner it was awful what does it refer to parsing what is the subject and object of the sentence which one does the verb andor adjective actually refer to sarcasm if you dont know the author you have no idea whether bad means bad or good twitter abbreviations lack of capitals poor spelling poor punctuation poor grammar
4721586,out of heap space memory error,java netbeans nlp stanfordnlp,which os are you running this on is it a bit system if not then you are pretty much restricted when it comes to how much heap you can allocate to a single java process try running with xmsm xmxm and see if it solves your issue
4355402,mysql natural lanquage search not working as id hoped,mysql nlp fulltextsearch,fulltext indices are nothing more than indices on the full text they only allow searches on the text you actually have mysql does have a soundex function and a shorthand x sounds like y operator which is the same as soundexx soundexy if soundex doesnt meet your needs you would indeed need to involve a programming language like php to accomplish what you want
3776357,parser generator or library that supports suffix agreement,java parsing nlp parsergenerator,i havent seen a parser than can do this directly though we did use a unification parser in a grad school class i had unfortunately the name escapes me and it was really old even then im sure it wasnt open source you could try the kimmo parser though i have never used it so cant attest to its applicability to your problem
2821575,java text classification problem,java machinelearning nlp textprocessing classification,this looks like a reasonably straightforward keywordbased classification task since youre using java good packages to consider for this would be classifierj weka or lucene mahout classifierj classifierj supports classification using naive bayes and a vector space model as seen in this source code snippet on training and scoring using its naive bayes classifier the package is reasonably easy to use its also distributed under the liberal apache software license weka weka is a very popular tool for data mining an advantage of using it is that youd be able to readily experiment with using numerous different machine learning models to categorize the books into topics including naive bayes decision trees support vector machines knearest neighbor logistic regression and even a rule set based learner youll find a tutorial on using weka for text categorization here weka is however distributed under the gpl you wont be able to use it for closed source software that you want to distribute but you could still use it to back a web service lucene mahout mahout is designed for doing machine learning on very large datasets its built on top of apache hadoop and supports supervised classification using naive bayes youll find a tutorial covering how to use mahout for text classification here like classifierj mahout is distributed under the liberal apache software license
2301214,python syntax error,python nlp,what is posted here does not have the error note that what is posted has two space characters between the and the u in output word ud counter what is probably happening is that you have a whitespace character other than a space in there a possibility is nbsp ua aka nobreak space what so does to format your code is likely to scrub away such things diagnosis at the python interactive prompt type what do you see between the and the u fix in your editor delete both characters between the and the u insert a single space by the way with a syntax error the problem is entirely within the named source file the code has not been run because it couldnt be compiled and so what is in your input file has no bearing on the problem
2236858,build a natural language model that fixes misspellings,java parsing nlp linguistics,peter norvig has written a terrific spell checker maybe that can help you
1288291,how can i correctly prefix a word with a and an,c nlp linguistics,download wikipedia unzip it and write a quick filter program that spits out only article text the download is generally in xml format along with nonarticle metadata too find all instances of an and make an index on the following word and all of its prefixes you can use a simple suffixtrie for this this should be case sensitive and youll need a maximum wordlength letters optional discard all those prefixes which occur less than times or where a vs an achieves less than majority or some other threshholds tweak here preferably keep the empty prefix to avoid cornercases you can optimize your prefix database by discarding all those prefixes whose parent shares the same a or an annotation when determining whether to use a or an find the longest matching prefix and follow its lead if you didnt discard the empty prefix in step then there will always be a matching prefix namely the empty prefix otherwise you may need a special case for a completelynon matching string such input should be very rare you probably cant get much better than this and itll certainly beat most rulebased systems edit ive implemented this in jsc you can try it in your browser or download the small reusable javascript implementation it uses the net implementation is package avsan on nuget the implementations are trivial so it should be easy to port to any other language if necessary turns out the rules are quite a bit more complex than i thought its an unanticipated result but its a unanimous vote its an honest decision but a honeysuckle shrub symbols its an number or an of oregano acronyms its a nasa scientist but an nsa analyst a fiat car but an faa policy which just goes to underline that a rule based system would be tricky to build
479825,problem trimming japanese string in java,java string nlp,as an alternative to the stringutils class mentioned by mike you can also use a unicodeaware regular expression using only javas own libraries or to really only trim and not remove whitespace inside the string
70043586,subprocess call error while calling generatelmpy of deepspeech,python speechtotext languagemodel mozilladeepspeech,able to find a solution for the above question successfully created language model after reducing the value of topk to my phrases file has about entries only we have to adjust topk value based on the number of phrases in our collection topk parameter says this much of less frequent phrases will be removed before processing
68732271,runtimeerror cuda error deviceside assert triggered bart model,pytorch huggingfacetransformers languagemodel,i suggest you change the batch size to and run the code in cpu temporarily to get a more descriptive traceback error this will tell you if you where the bug is sarthak
68363587,tensorflow hubnnlm word embedding using sentiment data gives input shape error,keras sentimentanalysis wordembedding tensorflowhub languagemodel,as described on the model expects a vector of strings as input youre basically calling the model twice since youre executing and then passing that embedding to model which actually takes strings as input since it starts with the nnlm keraslayer id propose to remove embed and xtrainembed and just call modelfit with xtrain
51170775,fastai valueerror len should return,deeplearning languagemodel,looks like your data is in txt file and languagemodeldatafromtextfiles expects to deal with folders containing many files upd solved there must be at least bs number files in each folder otherwise languagemodelloader for languagemodeldata has its data empty i face the same error during validation and the problem seems to be in the way languagemodeldata constructed dataset x should be a pytorch tensor or shape smth batchsize and y a dimensional tensor of size smthbatchsize same with mdvaldl in your case it is likely that there is no x y something is very wrong with data lenmdtrndl and lenmdvaldl must not equal to ill appreciate any solutions thank you for the question also newer version of language model drops torchtext and makes it easier to debug
76170604,huggingface pipeline with a finetuned pretrained model errors,python pipeline huggingfacetransformers textclassification huggingface,after the model training your model seems to be still placed on your gpu the error message you receive runtimeerror expected all tensors to be on the same device but found at least two devices cuda and cpu when checking argument for argument index in method wrapperindexselect is thrown because the input tensors that are generated from the pipeline are still on cpu that is also the reason why the pipeline works as expected when you move the model to cpu with modeltocpu per default the pipeline will perform its actions on cpu you change that behavior by specifying the device parameter cuda classifier pipelinezeroshotclassification modelmodel tokenizertokenizer device cpu classifier pipelinezeroshotclassification modelmodel tokenizertokenizer devicecpu
76099140,hugging face transformers bart cuda error cublasstatusnotinitialize,python pytorch huggingfacetransformers textclassification huggingface,i was able to reproduce your problem here is how i solved it on both of the clusters you provided in order to solve it i used at the frompretrained call ignoremismatchedsizestrue because the model you use has fewer labels than what you have numlabels insert the number of your labels i used just to be sure its based on both your errors but mainly the second one i suspect it was also the source of the second error on the gpu please test and confirm it i also used the following at the trainingarguments call for memory optimizations fptrue gradientcheckpointingtrue i tested it with up until numtrainepochs perdevicetrainbatchsize perdeviceevalbatchsize warmupsteps and it worked just fine hopefully it will help you get the desired results you can look at the final links i provided for more details about how to optimize the speed and memory while training on both gpu and cpu for reference huggingface models search for ignoremismatchedsizes huggingface configuration search for numlabels finetuning with custom datasets efficient training on a single gpu efficient training on cpu
75595450,xgboost only predcinting single class for the unseen data out of classes for multiclass text classification problem,pythonx scikitlearn xgboost textclassification,where am i going wrong tldr you are trainingmaking predictions using sparse data matrices but you should be using dense data matrices convert your fittedvectorizertransformx results to dense using todense method and see if the situation improves xgboost interprets an empty cell as a missing value rather than a count if you replace xgbclassifier with some scikitlearn classifier eg gradientboostingclassifier then your existing code would work as expected the reason being that scikitlearn interprets empty cells differently as counts
74006276,valueerror call arguments received inputstftensorshapenone dtypefloat trainingnone,python tensorflow keras textclassification,i tried to build model for text classification and it worked for me providing the shape as blank and mentioning data type as string in the input layer worked for me as we are dealing with text data example code snippet
73484411,can i plot roc curve for multiclass text classification problem without using onevsrestclassifier,python scikitlearn textclassification roc multiclassclassification,i assume your ytest is single column with class id and your yproba has as much columns as there are classes at least thats what youd usually get from predictproba how about this it should yield you ovrstyle curves update solution for nonmonotonic class labels
73027195,merging text and layers not working,python tensorflow concatenation textclassification imageclassification,as per the documentation on fit if youre passing a dictionary the keys need to point to an array or tensor youre using tensorflowpythondataopsdatasetopsprefetchdataset which wont work with dict
72112204,getting a valueerror shapes none and none are incompatible,python tensorflow keras textclassification multiclassclassification,you have to map your labels to integer values
71616761,valueerror multiclass format is not supported on roccurve for text classification,svm textclassification roc multiclassclassification tfidfvectorizer,a roc curve is based on soft predictions ie it uses the predicted probability of an instance to belong to the positive class rather than the predicted class for example with sklearn one can obtain the probabilities with predictproba instead of predict for the classifiers which provide it example note op used the tag multiclassclassification but its important to note that roc curves can only be applied to binary classification problems one can find a short explanation of roc curves here
71598259,typeerror len of unsized object,scikitlearn classification randomforest textclassification,youre providing the test instances features testvectors instead of the true test instances labels to classificationreport as per the documentation the first parameter should be ytrue d arraylike or label indicator array sparse matrix ground truth correct target values
71108243,valueerror requesting fold crossvalidation but provided less than examples for at least one class,python scikitlearn svm crossvalidation textclassification,you need to check the distribution of your target value datacharacter it seems that the number of values in one of the classes in the target column is too small to do it you can use datacharactervaluecounts
70680290,indexerror target is out of bounds,python pytorch textclassification bertlanguagemodel,youre creating a list of length in your getitem call which is one more than the length of the labels list hence the out of bounds error in fact you create the same list each time this method is called youre supposed to fetch the associated y with the x found at idx if you replace batchy nparrayrange with batchy nparrayselflabelsidx youll fix your error indeed this is already implemented in your getbatchlabels method
70255689,my function to investigate the impact of sample size on the text classifier performance is not working correctly,python scikitlearn svm textclassification naivebayes,you have an error in createmodel youre using the global full training data train every time instead of the argument traindocs it should be
69293878,calibrated classifier valueerror could not convert string to float,scikitlearn textclassification valueerror,try this then build the model
68513269,matmul error when trying to predict a new text using skmultilearnbinaryrelevance,python scikitlearn textclassification skmultilearn,you are using a tfidfvectorizer to transform your text features you should fit the transformer only once on the training data which is corpus in your case when preparing the data to testpredict you should however use the transform method and not fittransform again since that would refit the transformer change the following to make it work xpredict tfidftransformpredicttext
64293642,valueerror input is incompatible with layer lstm expected ndim found ndim,python keras lstm textclassification convneuralnetwork,this is because when you apply globalmaxpoolingd it returns the tensor with shape batchsize as mentioned in the docs this layer downsamples the input representation by taking the maximum value over the time dimension you have two options either not to use globalmaxpoold you can instead use local pooling layers such as maxpoold or you can use repeatvector to change the shape of output of globalmaxpoold from batchsize to batchsize n where n is the parameter to repeatvector defining how many times you want the sequence to be repeated
62812198,valueerror in while predict where test data is having different shape of word vector,python machinelearning scikitlearn textclassification,the error is with fittransform of test data you fittransform training data and only transform test data change this xtesttfidf ifidfvectorizerfittransformxtest xtesttfidfshape to xtesttfidf ifidfvectorizertransformxtest xtesttfidfshape reasons when you do fittransform you teach your model the vectors with fit the model learns the vectors to which they are used to transform data you use the train data to learn the vectors then you apply them to both train and test with transform if you do a fittransform on test data you replaced the vectors learned in training data and replaced them with test data given that your test data is smaller than your train data it is likely you would get two different vectorisation a better way the best way to do what you do is using pipelines which will make your flow easy to understand from sklearnfeatureextractiontext import tfidfvectorizer from sklearnsvm import linearsvc from sklearnpipeline import pipeline clf pipelinesteps vectorizer tfidfvectorizer model linearsvc train clffitxtrainytrain predict clfpredictxtest this is easier as the transformation are taking care for you you dont have to worry about fittransform when fitting the model or transform when predicting or scoring you can access the features independently if you with with clfnamedstepsvectorizer or model under the hood when you do clffit your data will pass throw your vectorizer using fittransform and then to the model when you predict or score your data will pass throw your vectorizer with transform before reaching your model
61356035,separate the words in the sentence for text classification problem,pythonx machinelearning deeplearning neuralnetwork textclassification,the problem that you are trying to solve is textword segmentation it is possible to approach this based on ml using a sequence model such as lstm and a word embedding such as bert this link details such an approach for chinese language chinese language does not adopt spaces so this sort of approach is necessary as a preprocessing component in chinese nlp processing tasks i would like to describe an automaton based approach using ahocorasick algorithm first do a pip install pyahocorasick im resorting to use only the words in your input string for the sake of demonstration in a real world scenario you could just use a dictionary of words from something like wordnet produces results
59637050,receiving an error was thrown and was not caught the validation data provided must contain when creating a text classifier model with createml,validation textclassification createml,try this it works for me
59032757,docvec infervector not working as expected,python textclassification docvec,first you wont get good results from docvecstyle models with toysized datasets just four documents and a vocabulary of about unique words cant create a meaningfullycontrasting dense embedding vector model full of dimensional vectors second if you set negative in your model initialization youre disabling the default modeltrainingcorrection mode negative and youre not enabling the nondefault lessrecommended alternative hs no training at all will be occurring there may also be an error shown in the code output but also if youre running with at least infolevel logging you might notice other issues in the output third infervector requires a listofwordtokens as its argument youre providing a plain string that will look like a list of onecharacter words to the code so its like youre asking it to infer on the word sentence the argument to infervector should be tokenized exactly the same as the training texts were tokenized if you used wordtokenize during training use it during inference too infervector will also use a number of repeated inferencepasses over the text thats equal to the epochs value inside the docvec model unless you specify another value since you didnt specify an epochs the model will still have its default value inherited from wordvec of epochs most docvec work uses epochs during training and using at least as many during inference seems a good practice but also dont try to call train more than once in a loop or manage alpha in your own code unless you are an expert whatever online example suggested a code block like your for epoch in rangemaxepochs printiteration formatepoch modeltraintaggeddata totalexamplesmodelcorpuscount epochsmodeliter decrease the learning rate modelalpha fix the learning rate no decay modelminalpha modelalpha is a bad example its sending the effective alpha rate downandup incorrectly its very fragile if you ever want to change the number of epochs it actually winds up running epochs modeliter its far more code than is necessary instead dont change default alpha options and specify your desired number of epochs when the model is created so the model will have a meaningful epochs value cached to be used by a later infervector then only call train once it will handle all epochs alphamanagement correctly for example model docvecsizevecsize mincount not good idea w real corpuses but ok dm not necessary to specify since its the default but ok epochsmaxepochs modelbuildvocabtaggeddata modeltraintaggeddata totalexamplesmodelcorpuscount epochsmodelepochs
58509143,how to solve fix list index out of range while accessing large amount of data from file,python file textclassification filewriter,you are assuming that you always have or more lines in each block perhaps your file ends in nn or you have some blocks that are corrupted simply test for the length and skip the block note that you really dont need to read the whole file into memory here you can also loop over the file object and read additional lines from a file object personally id create a separate generator object that picks out specific lines from the file
57269570,multinomial naive bayes classification problem normalization required,machinelearning scikitlearn textclassification naivebayes multinomial,your test set gives the same probability for both and heres an example of how naive bayes calculates probability of each output category in your example none of the attributes in the test data appears in the training data the words street shop and car has a probability of try running the code both the classes have an accuracy of so the model returns the first class which is
56060044,text classification issue,tensorflow machinelearning keras textclassification,your loss curves makes sense as we see the network overfit to training set while we see the usual bowlshaped validation curve to make your network perform better you can always deepen it more layers widen it more units per hidden layer andor add more nonlinear activation functions for your layers to be able to map to a wider range of values also i believe the reason why you originally got so many repeated values is due to the size of your network apparently each of the data points has roughly features pretty large feature space the size of your network is too small and the possible space of output values that can be mapped to is consequently smaller i did some testing with some larger hidden unit layers and bumped up the number of layers and was able to see that the prediction values did vary it is also understandable that your network performance varies so because the number of features that you have is about times the size of your training usually you would like a smaller proportion keep in mind that training for too many epochs like more than for so small training and test dataset to see improvements in loss is not great practice as you can seriously overfit and is probably a sign that your network needs to be widerdeeper all of these factors such as layer size hidden unit size and even number of epochs can be treated as hyperparameters in other words hold out some percentage of your training data as part of your validation split go one by one through the each category of factors and optimize to get the highest validation accuracy to be fair your training set is not too high but i believe you should hold out some of the training as a sort of validation set to tune these hyperparameters given that you have such a large number of features per data point at the end of this process you should be able to determine your true test accuracy this is how i would optimize to get the best performance of this network hope this helps more about training test val split
55561060,error multiclass text classification with pretrained bert model,python textclassification multiclassclassification transferlearning,solved just by replacing with strx for x in range in the getlabel function the two are actually equivalent but for some unknown reason this solved the issue
54354431,spacy text categorization getting the error massage float object is not iterable,python textclassification spacy,according to the documentation first argument of languageupdate accepts a batches of unicode or docs probalby texts contatin some nan values which has a type float related code spacy tries to iterate a nan float and it causes an so you can drop all nan values or replace them with empty string also this kind of error is very frequent for nlp but not only nlp tasks always check out text data for nans and replace them especially when you receive similar error message
54054455,feeding lstmcell with whole sentences using embeddings gives dimensionality error,python tensorflow machinelearning lstm textclassification,i could solve the problem myself as it seems the lstmcell implementation is more hands on and basic in relation to how a lstm actually works the keras lstm layers took care of stuff i need to consider when im using tensorflow the example im using is from the following official tensorflow example as we want to feed our lstm layers with a sequence we need to feed the cells each word after another as the call of the cell creates two outputs cell output and cell state we use a loop for all words in all sentences to feed the cell and reuse our cell states this way we create the output for our layers which we can then use for further operations the code for this looks like this numsteps represents the amount of words in our sentence that we are going to use
53028377,valueerror the shape of the input to flatten is not fully defined with variable length lstm,python keras lstm textclassification variablelength,you cant fix this particular problem because you can pass a variable size vector to a dense layer why because it has a fixed size weights matrix ie the kernel w you should instead look at layers that can handle variable length sequences such as rnns for example you can let the lstm learn a representation over the entire sequence if you want more capacity in your model you can chain rnn layers so long as the last one doesnt return sequences
51928856,valueerror cannot use sparse input in svc trained on dense data,python pythonx machinelearning scikitlearn textclassification,you get this error because your training test data are not of the same kind while you train in your initial xtrain set you are trying to get predictions from a dataset which has undergone count vectorization tfidf transormations first it is puzzling why you choose to do so why you nevertheless compute traincounts and traintfidf you dont seem to actually use them anywhere and why you are also trying to redefine predicted as classifierpredictxtest immediately afterwards normally changing your training line to and getting rid of your second predicted definition should work ok
51925123,nameerror name fitclassifier is not defined,python pythonx scikitlearn classification textclassification,you are calling a non existing function fitclassifierxtrain ytrain to fit your classifier you would use classifierfitxtrain ytrain instead youll get the same error when trying to predict your test data you need to change predicted predictxtest to predicted classifierpredictxtest your confusionmatrix should get your labels not your test data printconfusionmatrixytest predicted labels labels
51775834,nltk naivebayesclassifier classifier issues,python nltk textclassification naivebayes,training a sentiment model means that your model learns how words affect the sentiment thus its not about specifying which words are positive and which are negative its about how to train your model to understand that from a text by itself the simplest implementation is called bag of words which is usually used with tfidf normalization bag of words works this way you split your text by words and count occurrences of each word within the given text block or review in this way rows correspond to different reviews and columns correspond to the number of occurrences of the given word within the given review this table becomes your x and the target sentiment to predict becomes your y say for negative and for positive then you train your classifier after the model is trained you can make predictions further reading
50290782,r problems applying lime to quanteda text model,r text textclassification quanteda lime,corpus doesnt have to be run try redefining predictmodeltextmodelnbfitted as follows where the only modification is to add the dfmselect step as your traceback output shows corpus throws an error to debug i inserted printstrnewdata in the first line of the predictmodeltextmodelnbfitted function this shows that newdata is already a dfm object so it can be passed directly into predicttextmodelnbfitted after processing it with dfmselect in more recent versions of quanteda textmodelnb returns an object of classes textmodelnbtextmodel and list this would first require a corresponding method for modeltype we then also have to write a textmodelnb method for predictmodel notice that the second argument to dfmselect is different from that in predictmodeltextmodelnbfitted from the original version of the answer this is because the structure of the x object the output from textmodelnb has changed
50273919,r lime returns error on different feature numbers when its not the case,r textclassification quanteda lime,we can trace the error to predictmodel which calls predicttextmodelnbfitted i used only the first rows of trainraw to speed up computation the problem is that predicttextmodelnbfitted expects a dfm not a data frame for example predictnbmodel testraw gives you the same feature set in newdata different from that in training set error however explain takes a data frame as its x argument a solution is to write a custom textmodelnbfitted method for predictmodel that does the necessary object conversions before calling predicttextmodelnbfitted this gives us
50192763,python sklearn pipiline fit attributeerror lower not found,python machinelearning scikitlearn svm textclassification,either remove step vect countvectorizer or use tfidftransformer instead of tfidfvectorizer as tfidfvectorizer expects array of strings as an input and countvectorizer returns a matrix of occurances ie numeric matrix per default tfidfvectorizer lowercasetrue will try to lowercase all strings hence the attributeerror lower not found error message also parameter tokenizer expects either a callable function or none so dont specify it
49723152,unicodedecodeerror in python classification arabic datasets,python textclassification nearestneighbor naivebayes sklearnpandas,in the twitter data you are trying to load there are characters that are not recognized by utf try to load it with other encoding formats like
49547715,sklearn model data transform error countvectorizer vocabulary wasnt fitted,python machinelearning scikitlearn textclassification countvectorizer,you need to call vectorizerfit for the count vectorizer to build the dictionary of words before calling vectorizertransform you can also just call vectorizerfittransform that combines both but you should not be using a new vectorizer for test or any kind of inference you need to use the same one you used when training the model or your results will be random since vocabularies are different lacking some words does not have the same alignment etc for that you can just pickle the vectorizer used in the training and load it on inferencetest time
49415195,typeerror from multinomialnb float argument must be a string or a number,pythonx machinelearning scikitlearn textclassification naivebayes,change to the first line creates a list featuresets of pairs word true or word false ie the second element is a set sklearnclassifier does not expect this as a label the code looks very much like one from creating a module for sentiment analysis with nltk the author is using a tuple x in words there but its no different from just x in words
48946458,how to solve this error with lambda and sorted method when i try to make sentiment analysis pos or neg text,pythonx nltk sentimentanalysis featureextraction textclassification,if ive understood the format of your wordscores dictionary correctly that the keys are words and the values are integers representing scores and youre simply looking to get an ordered list of words with the highest scores its as simple as this if you want to use a lambda to get an ordered list of tuples where each tuple is a word and a score and they are ordered by score you can do the following the difference between this and your original attempt is that i have passed one argument x to the lambda and x is a tuple of length x is the word and x is the score since we want to sort by score we use x
48479867,attributeerror while designing naive bayes classifier,python textclassification naivebayes,what your code is trying to do is to build is a very simple classifier based on name features based on its name an item will be classified as a fruit or as a veggie the training set contains a few names with their respective classes the error youre getting is due to the wrong format of your training set and test set the training set is a list of featuresets one featureset for each training example and should have a structure of the form each featureset is a pair features class where features is a dictionary and class is some value for instance in your classifier the featureset for apple is here is the corrected code a slightly better classifier maybe this is what you were thinking of along the lines of the example in document classification here the classifier simply predicts whether a basket contains more fruits or veggies based on this you can construct more complex classifiers with better features and more training data
46027033,scikit text classification bad input shape error,python scikitlearn textclassification valueerror,thats because you are not using the actual data in the countvectorizer you are using reuterstrain whereas you should be using reuterstraindata change to also countvectorizer tfidftransformer tfidfvectorizer so i would recommend using that inplace of two objects on further reading of the description of rcv dataset here its given that the data contains nonzero values contains cosinenormalized log tfidf vectors so there is no need to actually do the countvectorizer and tfidftransformer on the data and you can directly use it like this but you will again encounter an error and this time due to the shape of target data you see multinomialnbfit only works with single dimension targets may be multiclass or binary but not with multilabel or multioutput data tldr so you need to remove countvectorizer and tfidftransformer from your code because its already done in the data and you need to change the classifier multinomialnb to any other which supports d in target y like maybe decisiontreeclassifier or others
44671194,inconsistent shape error multilabelbinarizer on ytest sklearn multilabel classification,numpy scikitlearn textclassification multilabelclassification,you should only call transform on test data never fit or its variations like fittransform or fitpredict etc they should be used only on training data so change the line ytest mlbfittransformytest to ytest mlbtransformytest explanation when you call fit or fittransform the mlb forgets its previous learnt data and learn the new supplied data this can be problematic when ytrain and ytest may have difference in labels as your case have in your case ytrain have different kinds of labels whereas ytest have only different labels but this doesnt mean that ytest is labels short of ytrain it can be possible that ytest may have entirely different set of labels which when binarized results in columns and that will affect the results
44033443,sklearn sgd partial fit error number of features does not match previous data,python machinelearning scikitlearn textclassification,this error is because before you pickled your classifier you trained it with features number of columns in x which in now only it should be equal to older features how you can do that by calling only countvectitemgrouptransform you are now again calling the fittransform on the countvectitemgroup which then forgets about the previously learned data and fits on the new data hence the number of features found are lesser than before change your code to
43864026,iabtaxonomeorg error code,classification categories taxonomy textclassification customtaxonomy,indeed i had the same issue after clearing this with taxonome support team i figure out there is a requirement for at least words per classification i have asked to add it to the api reference page double checking and editing my answer it is depends which framework is being used to send this data in case you are implementing the client and not encoding the url string it wont work for you eg space check the api example here
43530398,svm value error text classification,python scikitlearn textclassification,explanation a trained model can only predict on vectors the same size as the vectors it trained upon therefore in cases of vectorizing text by bag of words methods you must keep the original vocabulary of the train sample in order to create vectors according to the same vocabulary remarks used only two samples so no train test split just trained on both and therefore no cross validation no need to lower case the data sklearn vectorizer does that for you
43188504,lstm error python keras,python classification keras lstm textclassification,the problem lies in fact that you are adding an additional dimension connected with samples try this should work
43176300,stringtowordvectore error in java for text classification,java weka randomforest libsvm textclassification,i found these websites very useful to do text classification with filter stringtowordvector
41270436,cross validation classification error,python machinelearning scikitlearn crossvalidation textclassification,basically you have one very small class something around samples and in one of the splits you did not get any thus leading to errors you can use stratifiedkfold instead which guarantees that in each split you have a constant amount of samples from each class
39116088,typeerror in countvectorizer scikitlearn expected string or buffer,python pandas dataframe scikitlearn textclassification,you need convert column message to string by astype because in data are some numeric values
38768499,textvec classification with caret problems,r svm rcaret textclassification textvec,f you turn your s class dtmtrain into a simple matrix the code will work do not forget to do the same for your dtmtest otherwise the predict function will complain as well pred
38121451,tensorflow error using my own data for text classification,python machinelearning tensorflow textclassification,the axis parameter was just added to tfunpack on june and the example youre looking at was changed to use it so i suggest either use an older version of the example from before that commit eg build a newer tensorflow from github head i hope that helps
36959387,problems with naive bayes,r classification textclassification naivebayes,seemingly the problem is that the tdm needs to get rid of so much sparsity so i added and it started working thank you all for your ideas thank you chelsey hill
35265843,getting attributeerror on nltk textual entailment classifier,python nltk textclassification,take a look at the type signatures type this into the python shell tells you x is of type list now type which tells you param rtepair a rtepair from which features should be extracted clearly x does not have the correct type for calling nltkrtefeatureextractor instead a single item of the list does have the correct type update as mentioned in the comment section extractortextwords shows only empty strings this seems to be due to changes made in nltk since the documentation was written long story short you wont be able to fix this without downgrading to an older version of nltk or fixing the problem in nltk yourself inside the file nltkclassifyrteclassifypy you will find the following piece of code if you run the same regexptokenizer with the exact text from the extractor it will produce only empty strings returns ie a list of empty strings
34372166,error using termdocumentmatrix and dist functions in r,r textmining textclassification textanalysis,change to dist requires as input a numeric matrix data frame or dist object and event though a termdocumentmatrix is a matrix it needs to be transformed here
31920199,nltk accuracy valueerror too many values to unpack,python nltk textclassification,the error message arises because items in gold can not be unpacked into a tuple fsl it is the same error you would get if gold equals since the tuple can not be unpacked into a tuple fsl gold might be buried inside the implementation of nltkclassifyutilaccuracy but this hints that your inputs classifier or testfeats are of the wrong shape there is no problem with classifer since calling accuracyclassifier trainfeats works the problem must be in testfeats compare trainfeats with testfeats trainfeats is a tuple containing a dict and a classification but testfeats is just a dict wordfeatstweetswordsfileidsf so to fix this you would need to define testfeats to look more like trainfeats each dict returned by wordfeats must be paired with a classification
31306390,sklearn classifier get valueerror bad input shape,python scikitlearn classification textclassification,thanks to meelo i solved this problem as he said in my code data is a feature vector target is target value i mixed up two things i learned that tfidfvectorizer processes data to data feature and each data should map to just one target if i want to predict two type targets i need two distinct targets targetc with all c value targetc with all c value then use the two targets and original data to train two classifier for each target
31228303,scikitlearns pipeline error with multilabel classification a sparse matrix was passed,python scikitlearn gaussian textclassification,you can do the following now as a part of your pipeline the data will be transform to dense representation btw i dont know your constraints but maybe you can use another classifier such as randomforestclassifier or svm that do accept data in sparse representation
31000098,predictionio train error tokens must not be empty,token textclassification trainingdata predictionio,so this is something that happens when you feed in an empty arraystring to opennlps stringlist constructor try modifying the function hash in prepared data as follows ive only encountered this issue in the prediction stage and so you can see this is actually implemented in the models predict methods ill update this right now and put it in a new version release thank you for the catch and feedback
27712040,text classifier with weka how to correctly train a classifier issue,java weka textclassification categorization,it seems like you changed the code from the website you referenced in some crucial points but not in a good way ill try to draft what youre trying to do and what mistakes ive found what you probably wanted to do in extractfeature is split each tweet into words tokenize count the number of occurrences of these words create a feature vector representing these word counts plus the class what youve overlooked in that method is you never reset your featuremap the line originally was at the beginning extractfeatures but you moved it to initialize that means that you always add up the word counts but never reset them for each new tweet your word count also includes the word count of all previous tweets im sure that is not what you wanted you dont initialize featurewords with the words you want as features yes you create an empty list but you fill it iteratively with each tweet the original code initialized it once in the initialize method and it never changed after that there are two problems with that with each new tweet new features words get added so your feature vector grows with each tweet that wouldnt be such a big problem sparseinstance but that means that your class attribute is always in another place these two lines work for the original code because featurewordssize is basically a constant but in your code the class label will be at index then then and so on but it must be the same for every instance this also manifests itself in the fact that you build a new attributelist with each new tweet instead of only once in initialize which is bad for already explained reasons there may be more stuff but as it is your code is rather unfixable what you want is much closer to the tutorial source code which you modified than your version also you should look into stringtowordvector because it seems like this is exactly what you want to do converts string attributes into a set of attributes representing word occurrence depending on the tokenizer information from the text contained in the strings the set of words attributes is determined by the first batch filtered typically training data
26367075,countvectorizer attributeerror numpyndarray object has no attribute lower,python numpy scikitlearn textclassification,check the shape of mealarray if the argument to fittransform is an array of strings it must be a onedimensional array that is mealarrayshape must be of the form n for example youll get the no attribute error if mealarray has a shape such as n you could try something like
22613364,theano classification task always gives validation error and test error,sentimentanalysis textclassification theano deeplearning,i asked the same question in theanos user groups and they answered that feature values should be between and so i used a normalizer to normalize feature values and it solved the problem
78525417,deadline error when embedding video with google vertex ai multimodal embedding modal,googlecloudplatform wordembedding googlecloudvertexai videoembedding,all of the code looks right this could be a service issue or something to do with the file on gcs does this error occur on any video thats sent to the multimodal embeddings api from your project or just this particular one can you try using this video in a public gcs bucket gscloudsamplesdatavideoanimalsmp
78271828,tensor size error when generating embeddings for documents using huggingface pretrained models,huggingfacetransformers largelanguagemodel wordembedding huggingface pretrainedmodel,the length of the text variable is while the pipeline accepts a maximum length of you can fix this by splitting your text into chunks of or you can use a model that accepts larger sequences
78270250,modulenotfounderror no module named llamaindexembeddingslangchain,python langchain wordembedding llamaindex,you have pip install llamaindexembeddingsopenai and official documentation has pip install llamaindexembeddingshuggingface and similar way you need
74567500,tensorflow unknownerror graph execution error jit compilation failed opinferencerestoredfunctionbody,tensorflow deeplearning wordembedding,i had the error too and just did it with my cpu and it worked
71863897,wordvecgensim runtimeerror you must first build vocabulary before training the model,python gensim wordvec wordembedding,your sentences list is likely empty the only line of code that adds anything to it requires line to be an empty string and sentence to be nonempty maybe thats never happening check the value of sentences before creating the model make sure it has the expecten length in number of texts and look at the st few say sentences to make sure they look ok each item in sentences should itself be a listofstrings if its not debug your code that reads the files and assembles the sentences sequence until it looks as expected if youre still having problems in either an edit to this question or a followup question be sure to show the entire error message youre receiving including all lines of traceback showing filenames linesofcode linenumbers describe more about your corpus files such as an example of some of its contents
71721694,tensorflowx keras embedding layer process tfdataset error,python tensorflow keras textprocessing wordembedding,you cannot feed a tfdatadataset directly to an embedding layer you can either use map or define your model and feed your dataset through modelfit
66921706,i am getting the following error when importing import texthero as hero,python wordembedding,seems like a potential conflict with a newer version of gensim in my case pip install gensim solved this for me
66852256,error while loading wikib embeddings from tensorflow hub,tensorflow wordembedding tensorflowhub,passing the text wrapped in a tfconstant to embed and setting the outputkey keyword should make it work tested with tf and tensorflowhub
65785949,valueerror need at least one array to concatenate in topvec error,python arrays concatenation wordvec wordembedding,you need to use more docs and unique words for it to find at least topics as an example i just multiply your list by and it works i had few long docs of up to characters so i just split them into smaller docs every characters
65406526,valueerror invalid vector on line while loading wikiarvec using gensimmodelskeyedvectorswordvec function,python arabic gensim wordvec wordembedding,this error indicates the file is not in the proper format at that specified linevector where did the file come from are you sure its a binaryformat file of the right format have you tried redownloading the file to ensure it hasnt been corrupted or truncated
60682634,issues while loading a trained fasttext model using gensim,python pythonx gensim wordembedding fasttext,if the model was saved with gensims native save method youd load it with load not loadfasttextformat which is only for models saved in the raw format used by facebooks original fasttext c code
59312001,wordvec how to rid of typeerror unhashable type list and attributeerror dlsymxfacbe attachdebuggertracing symbol not found,gensim wordvec wordembedding,gensims wordvec expects its corpus sentences to be a sequence where each individual item is a list of string tokens that is those string tokens are words instead you have a list which is acceptable as a sequence where each of its items is a list which is also acceptable but then each of those lists instead has as each item yet another list when for wordvec training each of those items should be a string token word ive edited your example data to be be structurallyindented to make the levels of nesting clearer if those innermost listsofstrings are your real individual sentences you need to be sure theyre the items in your outermost list if on the other hand you really want a cluster like intentactionphonestate intentactionmain to be a single word in your model youll want to change that list into a single string token so it can look like a word and thus hashable key to wordvec and python
59042165,valueerrorlayer convd was called with an input that isnt a symbolic tensorall inputs to the layer should be tensors,keras convneuralnetwork sequential wordembedding tfkeras,error is thrown because inputlayer is layer and not tensor you are passing embedding layer as input to convd in this case you have not provided any input to embedding layer change this one and add input tensor to this layer also i think you are trying to concatenate outputs from three separate filters if that is the case then this part would come outside loop
58666699,wordvec keyerror word x not in vocabulary,gensim wordvec wordembedding,when you get a not in vocabulary error like this from wordvec you can trust it happy really isnt in the model even if your visual check shows happy inside your file a few reasons why it might not wind up inside the model include it doesnt occur at least mincount times the data format isnt correct for wordvec so its not seeing the words you expect it to see looking at how data is prepared by your code it looks like a giant list of all words in your file wordvec instead expects a sequence that has as each item a listofwords for that one text so not a listofwords but a list where each item is a listofwords if youve supplied happy birthday instead of the expected happy birthday those singlewordstrings will be seen a listsofcharacters so wordvec will think you want to learn wordvectors for a bunch of onecharacter words you can check if this has affected your model by seeing if the vocabulary size seems small lenmodelwv or if a sample of learnedwords is only singlecharacter words modelwvindexentity if you supply a word in the right format at least mincount times as part of the trainingdata it will wind up with a vector in the model separately size is a choice way outside the usual range of ive never seen a project using such highdimensionality for wordvectors and it would only be theoretically justifiable if you had a massivelylarge vocabulary and trainingset oversized vectors with smaller vocabulariesdata are likely to create uselessly overfit results
58497395,input shape error adding embedding layers to lstm,python keras lstm wordembedding,in regards to the number of samples keras automatically infers that from the input data shape xtrain in this case in terms of the use of the embedding layer the idea is to convert a matrix of integers into a vector in your case it seems like you might be essentially doing that already in the step where you populate x you might instead want to consider letting the embedding layer compute a vector for each index to do this i believe you would modify x to be of shape numofsentences numofcharspersentence where the value at each datapoint is the char index for that particular character also you might want to set the lstm returnsequences to false i believe you are only looking for the final result from that layer i hope this helps
57539872,runtimeerror expected object of backend cuda but got backend cpu for argument index,pytorch wordembedding,variable noise is available on cpu while selfembedding is on gpu we can send noise to gpu as well
57125306,valueerror cannot reshape array of size into shape,python deeplearning tokenize wordvec wordembedding,it looks like the intent of your wordvector method is to take a list of words and then with respect to a given wordvec model return the average of all those words vectors when present to do that you shouldnt need to do any explicit reshaping of vectors or even specification of size because thats forced by what the model already provides you could use utility methods from numpy to simplify the code a lot for example the gensim nsimilarity method as part of its comparision of two listsofwords already does an averaging much like what youre trying and you can look at its source as a model so while i havent tested this code i think your wordvector method could be essentially replaced with its sometimes the case that it makes sense to work with vectors that have been normalized to unitlength as the linked gensim code via applying gensimmatutilsunitvec to the average i havent done this here as your method hadnt taken that step but it is something to consider separate observations about your wordvec training code typically words with just or a few occurrences dont get good vectors due to limited number variety of examples but do interfere with the improvement of other morecommonword vectors thats why the default is mincount so just be aware your surviving vectors may get better if you use a default or even larger value here discarding more of the rarer words the dimensions of a dense embedding like wordvecvectors arent really independent variables or standalone individuallyinterpretable features as implied by your codecomment even though they may seem that way as separate valuesslots in the data for example you cant pick one dimension out and conclude thats the fooness of this sample like coldness or hardness or positiveness etc rather any of those humandescribable meanings tend to be other directions in the combinedspace not perfectly aligned with any of the individual dimensions you can sortof tease those out by comparing vectors and downstream ml algorithms can make use of those complicatedentangled multidimensional interactions but if you think of each dimensions as its own feature in any way other than yes its technically a single number associated with the item you may be prone to misinterpreting the vectorspace
56433993,getting error while adding embedding layer to lstm autoencoder,tensorflow keras lstm autoencoder wordembedding,i tried the following example on google colab tensorflow version and then trained the model using some random data this solution worked fine i feel like the issue might be the way you are feeding in labelsoutputs for mse calculation update context in the original problem you are attempting to reconstruct word embeddings using a seqseq model where embeddings are fixed and pretrained however you want to use a trainable embedding layer as a part of the model it becomes very difficult to model this problem because you dont have fixed targets ie targets change every single iteration of the optimization because your embedding layer is changing furthermore this will lead to a very unstable optimization problem because the targets are changing all the time fixing your code if you do the following you should be able to get the code working here embeddings is the pretrained glove vector numpyndarray
53767829,value error problem with multicell dimensions must be equal but are and,python tensorflow multidimensionalarray valueerror wordembedding,instead of using the basicrnncell instance multiple timesone instance per rnn layer should be created for example in this way in addition there are other mistakes on your codesrnnstates is a tuple containing cell state and hidden state and its shape is nonenone i assume you want to use hidden statereplace it thats ok
52352522,how does keras d convolution layer work with word embeddings text classification problem filters kernel size and all hyperparameter,python tensorflow keras convneuralnetwork wordembedding,i would try to explain how dconvolution is applied on a sequence data i just use the example of a sentence consisting of words but obviously it is not specific to text data and it is the same with other sequence data and timeseries suppose we have a sentence consisting of m words where each word has been represented using word embeddings now we would like to apply a d convolution layer consisting of n different filters with kernel size of k on this data to do so sliding windows of length k are extracted from the data and then each filter is applied on each of those extracted windows here is an illustration of what happens here i have assumed k and removed the bias parameter of each filter for simplicity as you can see in the figure above the response of each filter is equivalent to the result of its convolution ie elementwise multiplication and then summing all the results with the extracted window of length k ie ith to ikth words in the given sentence further note that each filter has the same number of channels as the number of features ie wordembeddings dimension of the training sample hence performing convolution ie elementwise multiplication is possible essentially each filter is detecting the presence of a particular feature of pattern in a local window of training data eg whether a couple of specific words exist in this window or not after all the filters have been applied on all the windows of length k we would have an output of like this which is the result of convolution as you can see there are mk windows in the figure since we have assumed that the paddingvalid and stride default behavior of convd layer in keras the stride argument determines how much the window should slide ie shift to extract the next window eg in our example above a stride of would extract windows of words instead the padding argument determines whether the window should entirely consists of the words in training sample or there should be paddings at the beginning and at the end this way the convolution response may have the same length ie m and not mk as the training sample eg in our example above paddingsame would extract windows of words pad mmm mm pad you can verify some of the things i mentioned using keras model summary as you can see the output of convolution layer has a shape of mkn and the number of parameters ie filters weights in the convolution layer is equal to numfilters kernelsize nfeatures onebiasperfilter n k embdim n
51983456,invalidargumenterror sentiment analyser with keras,pythonx keras deeplearning sentimentanalysis wordembedding,the error complains about a nonexistent word index thats because you are only limiting the number of emedding features ie there is a word with index which is not in the range which refers to the vocabsize you have set to resolve this you also need to pass the vocabsize as numwords argument of loaddata function like this this way you are limiting the words to the most frequent words ie top vocabsize words with the most frequency in the dataset with their indices in range vocabsize
51896013,error when checking model input keras when predicting new results,python tensorflow machinelearning keras wordembedding,either set the inputlength of the embedding layer to the maximum length you would see in the dataset or just use the same maxlen value you used when constructing the model in padsequences in that case any sequence shorter than maxlen would be padded and any sequence longer than maxlen would be truncated further make sure that the features you use are the same in both train and test time ie their numbers should not change
51456059,pytorch nnembedding error,pytorch wordembedding,when you declare embeds nnembedding the vocab size is and embedding size is ie each word will be represented by a vector of size and there are only words in vocab lookuptensor torchtensorwordtoixhow dtypetorchlong embeds will try to look up vector corresponding to the third word in vocab but embedding has vocab size of and that is why you get the error if you declare embeds nnembedding it should work fine
50196608,torchnnembedding has run time error,python pytorch wordembedding,if we change this line with this the problem is solved
49429971,how can i load wordvec with gensim without getting an attributeerror,python wordvec gensim wordembedding,are you sure your xxxxmodelwv file was a saved full wordvec model object that error suggests it was instead a euclideankeyedvectors just the vectors and not a full model with all properties like negative so you might need to load it as that instead
47363698,keras dense vs embedding valueerror input is incompatible with layer repeatvector expected ndim found ndim,keras embedding keraslayer wordembedding keras,the output shape of the dense layer is none embeddim however the output shape of the embedding layer is none inputlength embeddim with inputlength itll be none embeddim you can add a flatten layer after the embedding layer to remove axis you can print out the output shape to debug your model for example embeddim left sequential leftadddenseembeddim inputshapeencodedim printleftoutputshape none left sequential leftaddembeddingencodedim embeddim inputlength printleftoutputshape none leftaddflatten printleftoutputshape none
37857957,how to give fixed embedding matrix to embeddinglayer in lasagne,lasagne wordembedding,the above problem can be solved by adding the trainablefalse tag to the weight parameter of the custom layer defined to work as the embedding layer
79026693,numpy error implicit conversion to a numpy array is not allowed please use to construct a numpy array explicitly,python numpy gpu spacy,from the reference mapping word vector to the most similarclosest word using spacy reference you need to convert cupy arrays to be explicitly converted to numpy arrays before operations on the cpu edit lets make reshape the word vector into a d array with one row and multiple columns
77970163,error message while installing spacy microsoft visual c or greater is required even though build tools is already installed,python pythonx pip spacy buildtools,spacy is compatible with you can install by or here is the complete link
77524569,importerror dll load failed while importing numpyops the specified module could not be found when importing spacy in python,python spacy importerror,you just need to run the following line on the command prompt python m pip install msvcruntime then the error goes away
77354502,attributeerror module transformers has no attribute berttokenizerfast,python pip spacy,try downgrading spacy to with and then try
77172474,error could not install packages due to an environmenterror errno disk quota exceeded,pip spacy,in case this would be useful to someone in the future i ended up doing things that helped me in the sh file that run this code added a path to hfcach file reboot the computer inside my code downloaded the file again while running
77037891,typeerror issubclass arg must be a class,python pythonx pip spacy spacy,you can add below to your requirements there appears to be a bug in pydantic v and earlier related to the recent release of typingextensions v that causes errors for import spacy and any other spacy commands for python and python for spacy v and v we have published patch releases with fixes for the typingextension requirement upgrade to spacy v or v pydantic should be v or higher here is the complete solution in this link
76301180,problem with spacyloadencorewebmd in python,python pythonx pycharm spacy,once you download encorewebmd you need to restart runtime or ctrlm from the menu bar
76216596,i want to use errant with spacy verrant is not working with spacy v,python spacy,we can use errant with spacy v by just making changes or editing the errant class reference
75643803,installing chatterbot it raise spacy installation error in python pip install chatterbot not working,python django visualstudiocode spacy chatterbot,it seems that the chatterbot package no longer supports the new version of python you can download and install python or python or earlier versions to use the chatterbot package after testing i successfully installed the chatterbot package with python and python
75571167,download model en for spacy produces typeerror can only concatenate list not tuple to list,python pip concatenation spacy chatterbot,one solution is to use the newest v release currently v since v includes a fix for this bug the other option is to specify the package directly instead of using spacy download for example your requirements could include see
75258945,oserror e could not read metajson from modelbestzip,machinelearning model spacy namedentityrecognition,for spacy you can load models from sources pretrained models downloaded via a command like python m spacy download yourmodel custom models you have trained via spacy train loaded models via nlpfromdisk usually any of these models is stored as a folder or directory with an structure similar to this one for a ner model which it seems it is what you are attempting to load this discards loading directly from a zip file as a valid option i think you may want to try the following try to unzip modelbestzip and see if you find a similar directory structure than the one shown above if you are in a linuxbased system here is how if the previous structure is confirmed then proceed with step otherwise your file may be corrupted or not a spacy model as such and you wont be able to load the model try nlpner spacyloadpathtoyourmodelbest nlpner spacyloadmodelbest in your case and see if it works hope it helps
74914461,spacy addalias typeerror,python cython spacy spacy,that looks like a bug in the api docs knowledgebaseaddalias has type iterableunionstr int for entities but the code above the actual error is actually one line below only works for str and not int values the marked line should have selfvocabstringsasintentity that said the value is probably not going to be the right value here no matter what and the simplest solution is to use strings instead like or q which should currently work as expected you also need to add the entity before adding aliases this snippet is not going to work even with a string value
74078342,attributeerror type object englishdefaults has no attribute createtokenizer,python machinelearning deeplearning spacy attributeerror,have you considered using the buildin class tokenizer that according to the documentation we can use to create new tokenizer import spacy from spacytokenizer import tokenizer nlp spacyloadencorewebsm tokenizer tokenizernlpvocab printtokenizer result python mainpy
74070201,spacy isdigit or likenum not working as expected for certain chars,spacy,it may be helpful to just check which tokens are true for likenum like this here youll see that sometimes the tokens you have are split in two and sometimes they arent the tokens you match are only the ones that consist just of digits now why are m g and t split off while h j and v arent this is because they are units as for mega giga or terabytes this behaviour with units may seem inconsistent and weird but its been chosen to be consistent with the training data used for the english models if you need to change it for your application look at this section in the docs which covers customizing the exceptions
73569160,valueerrore unable to merge the doc objects because they do not all share the same,python spacy spacy,provide the existing vocab from the first model for any subsequent models when theyre loaded nlp spacyblanken vocabnlpvocab
73156496,regex spacy matcher not working as expected in spacy,regex spacy matcher,convert fstring to normal pnum text regexrd in fstrings and must be used as literal braces
73063306,python streamlit and yfinance issues,python pandas spacy streamlit yfinance,there is an issue in your logic in stockinfo function because of which same symbol is getting different values and when you are cleaning the duplicate based on occurrence of the symbol its retaining the row with first occurrence of symbol the below code will solve both of your issues
73008946,typeerror argument other has incorrect type expected spacytokenstokentoken got str,python spacy,while the problem lies in the fact that conjunction is a string and sentence is a span object and to check if the sentence text contains a conjunction you need to access the span text property you also reinitialize the coordsents in the loop effectively saving only the last sentence in the variable note a list comprehension looks preferable in such cases so a quick fix for your case is def getcoordinatesentsfiletoexamine conjunctions and but for nor or yet so text langmodelfiletoexamine return sentence for sentence in textsents if anyconjunction in sentencetext for conjunction in conjunctions here is my test import spacy langmodel spacyloadencorewebsm texttolook a woman is looking at books in a library shes looking to buy one but she hasnt got any money she really wanted to book so she asks another customer to lend her money the man accepts they get along really well so they both exchange phone numbers and go their separate ways filetoexamine texttolook conjunctions and but for nor or yet so text langmodelfiletoexamine sentences textsents coordsents sentence for sentence in sentences if anyconjunction in sentencetext for conjunction in conjunctions output however the in operation will find nor in north so in crimson etc you need a regex here import re conjunctions and but for nor or yet so rx recompilefrbjoinconjunctionsb def getcoordinatesentsfiletoexamine text langmodelfiletoexamine return sentence for sentence in textsents if rxsearchsentencetext
72588288,oserror e could not read config file from homexxxxlocallibpythonsitepackagespyresparserconfigcfg,pythonx spacy,i just figured it out after being stuck with the same error for a while its a version issue pip install nltk pip install spacy pip install pip install pyresparser does the trick also try different spacy versions and models because they produce different results havent tested any further myself hope this helps answer from quppi
72507686,oserror e could not read config file from cusers,spacy,normally the way you would install the spacy model and use it is like this first in the shell then in python i am not exactly sure what is causing your particular error maybe because you are using pip install with a url directly something is getting set up oddly it is also possible you are getting weird interactions between conda and pip
72235951,issue installing spacyr on r,r spacy failedinstallation,i solved my problem i tried more things like installminiconda see the following code block but i was still unable to use spacy i deleted my miniconda folder in my home directory and then ran the above code and i was able to use spacy
71654207,how to solve the spacy latin language import error,python pythonx spacy spacy,spacy doesnt have builtin support for latin so you need to load the pipeline a bit differently see the spacystanza docs modifying the coptic example there slightly should work
71635441,retrain custom language model with the current spacy version compatibility issues,python pythonx spacy spacy,in nearly all cases spacy v models are forwardscompatible with newer versions of spacy v so download jacorenewstrf and then install the vietnamese model with pip install nodeps so that pip doesnt install an older version of spacy as a dependency youll get a warning on load that an older model might be incompatible but test it on your data and as long as the performance is the same as with the older version of spacy it should be fine to use see you can only retrain the model if you have access to the original training data
70976353,after installing scrubadubspacy package spacyloadencorewebsm not working oserror e could not read configcfg,python python spacy azuremachinelearningservice oserror,taking the path from your error message you have a model for v but its looking for a configcfg which is only a thing in v of spacy it looks like you upgraded spacy without realizing it there are two ways to fix this one is to reinstall the model with spacy download which will get a version that matches your current spacy version if you are just starting something that is probably the best idea based on the release date of scrubadub it seems to be intended for use with spacy v however note that v and v are pretty different if you have a project with v of spacy you might want to downgrade instead
70880056,how to fix spacy entraining incompatible with current spacy version,python spacy,for spacy v models the underconstrained requirement means in effect and as a result this model will only work with spacy vx there is no way to convert a v model to v you can either use the model with vx or retrain the model from scratch with your training data
70720126,oserror e could not read configcfg spacy on colab,python spacy,i solve the problem by using this installation guide
70600787,error installing chatterbot stuck on spacy,python spacy chatterbot,install chatterbot from its source then unzip the file after open up cmd and type in cd chatterbotmasterdirectory finally just type python setuppy install from this post
70579115,spacy problem during the training of my model it seems to block at epoch,pythonx spacy namedentityrecognition,its likely you just have too much data and your training is slow how much data do you have how much ram what does spacy debug data show
70387763,spacy library to extract noun phrase valueerror e expected a string or doc as input but got,python spacy phrase,i faced a similar issue and i fixed it using the use of this code will fix the problem as you have to convert all the data values to str format usually it happens as comment might be number or nan or null
69887344,readtimeouterror when downloading space encoreweblg,python spacy,githubs servers appear to be having issues here is a thread on the issue tracker until github sorts things out you can download the models from their mirrors on the huggingface hub and install them with pip though you cant do this through the spacy download command unfortunately
68919242,why am i facing permissionerror while installing spacy encorewebsm on ssh server,python linux ssh pip spacy,as you suspect this is happening because you dont have permission to install to usr normally pip would install there but it looks like your pip is basically running with user which will install to your user home directory instead the spacy models cant be installed directly via pip because they are large data files so they cant be hosted on pypi like ordinary code an unfortunate side effect of this is that some of the options around pip configuration are ignored you can pass extra arguments to the pip install command by appending them to your command so in your case you can do this and everything should work that said i would strongly recommend you learn how to use virtual environments with the virtualenv tool which will make working with python projects easier and allow to you avoid this problem as anything you install will just go in the local virtualenv rather than your global pip install
68682465,pattern match issue with spacy,spacy,put the entityruler before ner in the pipeline so that its matches have priority over the cardinal spans from ner or alternatively you can set it to overwrite overlapping entities with the overwriteents setting
68562959,oserror e cant find model encoreweblg it doesnt seem to be a shortcut link a python package or a valid path to a data directory,python googlecolaboratory spacy,for google collab you will get a message saying download and installation successful you can now load the package via spacyloadencoreweblg
68297377,importing spacytextblob shows attributeerror,python spacy attributeerror sentimentanalysis,i found the workaround the issue is the compatibility of spacy version with the spacytextblob i was using spacy so i downgraded my spacytextblob to a lower version using
68208653,spacy rule based matching issue,python spacy,when you have such issues first make sure you understand how spacy tokenizes your string look t for t in doc probe hiv dna amp probe hiv dna quant hiv dna dir probe hiv dna so your hiv is a single token now you need to add another pattern that would account for the fact that lower hiv ispunct true text regexd can be a single token for example it can look like lower regexhivwd where the lowercased token text must match a hivwd regex you can use patterns lower hiv ispunct true text regexd lower dna lower quant lower regexhivwd lower dna lower quant matcher matchernlpvocab matcheraddhelloworld patterns doc nlpdata printdocstartendtext for startend in matcherdoc hiv dna quant the hivwd regex means start of the string here token hiv hiv text w any nonalphanumeric char d one or two digits end of the string here token see the regex demo
68013740,getting modulenotfounderror no module named pkgresourcesextern while importing spacy,python pythonx pip spacy,i think in python pkgresourcesinitpy should not contain line from pkgresourcesextern import six this is python related somehow you have an older version of setuptools targeted at python i suggest you trying to install specific setuptools version eg pip install setuptools
67987569,jupyter notebook python error while importing spacy no module named clickbashcomplete,python jupyternotebook spacy,in my case i installed the lower version of click and after that i get an error like exit module not found and installed it also restarted my notebook and spacy gets imported without any errors
67906945,valueerror nlpaddpipe now takes the string name of the registered component factory not a callable component,python spacy matcher,for spacy v the normal way to add an entity ruler looked like this ruler entityrulernlp nlpaddpiperuler ruleraddpatterns for spacy v you just want to add it with its string name and skip instantiating the class separately ruler nlpaddpipeentityruler ruleraddpatterns see
67789171,memoryerror unable to allocate mib for an array with shape and data type int,python dataframe outofmemory spacy,you havent really provided enough information here but it looks like you cant hold all the spacy docs in memory a very simple workaround for this would be to split your csv file up and process it one chunk at a time another thing you can do since it looks like youre just saving some words is to avoid saving the docs by changing your for loop a bit this way youll just keep the strings you want and not the docs so it should reduce memory usage a couple of other comments about your code whatever youre trying to do with the hashtag function its not working if you call strtextsplit the output is really weird itll turn i like cheese into i like cheese and it will cause spacy to give you nonsense output i recommend just not using that function spacy expects to deal with puncutation you seem to be using spacy to remove words based on part of speech for the most part but thats generally not a good idea modern text processing doesnt need that kind of prefiltering it was still common practice like years ago but you should just be able to give whole sentences to any reasonable model and theyll be better than overly filtered text
67720450,keyerror packaging autofill the config file spacy bert model,python config googlecolaboratory spacy,sorry you ran into that weve had one report of that error before it seems like something is weird with cupy on colab specifically based on the previous report you should start with a clean python environment and should not install cupy directly i think colab uses a special version or something
67646070,attributeerror spacytokensspanspan object has no attribute string,python spacy,as tim roberts said you want to the text attribute
67540692,error updating ner model in spacy any advice,python spacy spacy,i think this code should work for you to break it down a little bit further in spacy there are two changes they got rid of entity in nlpentitycreateoptimizer we dont pass texts and annotations directly to nlpupdate but with example
67484484,error when loading pipelines in spacy,importerror spacy spacytransformers,it looks like this is fixed in newer versions of transformers try upgrading both transformers and spacytransformers
67441897,how to resolve spacy pos attribute e error,python errorhandling spacy,you have a model for spacy v the model version starts with but you are using spacy v the models are not compatible with different major versions you need to uninstall the model and then download the new model
67252292,docker build python app error no matching distribution found for spacy on apple m,python nodejs docker spacy applem,as a last resort i bumped all the packages in the requirements file to their latest versions and now it builds successfully
67212594,attributeerror pathdistribution object has no attribute name,pythonx celery spacy celerytask,the problem is probably related to importlibmetadata try adding a requirement to your venv to restrict it to an earlier version in a similar case importlibmetadata worked for me the next release of spacy v should fix this problem at least if its only related to spacy by removing importlibmetadata as a requirement
67159937,importerror e cant import language customen from spacylang no module named spacylangcustomen,python spacy spacy,see the sidebar on the right in this section python m spacy train configcfg code codepy
67127453,why am i getting modulenotfounderror after installing negbio,pythonx django dockerfile spacy,as per your comment it wouldnt install with pip and hence not installing via pip firstly to make sure the is properly installed via python setuppy install you need to install its dependencies via pip install r requirements first so either ways you are doomed to have pip inside docker for example this is the sample dockerfile that would install the negbio package properly so wouldnt harm if you actually install it via requirementstxt i would do it like this requirementstxt have all your requirements added here and make sure its installed on the fly inside the docker using run pip install r requirementstxt
67100601,python spacy help needed environment inconsistency issue,pythonx anaconda spacy murmurhash,spacy and neuralcoref are currently not compatible the cython api of spacys v has changed too much this might be causing conflicts in your environment
66764880,importerror loading spacy in jupyter notebook,python pythonx jupyternotebook spacy spacy,it sounds like you have an old version of thinc somewhere try uninstalling and reinstalling thinc another thing to check is if youre running in the right python environment sometimes jupyter notebooks pull in a different environment than the one youre expecting in nonobvious ways there was a thread in spacy discussions about this recently you can run this command to check which python executable is being used in the notebook and make sure its the one you think it is
66654470,spacy importerror cannot import name deque in jupyter notebook,python pythonx jupyternotebook pycharm spacy,so for future references i just solved this problem by making a new environment with python since afaik jupyter notebook doesnt support python versions highter than x as of now yet set up a new venv and now both pycharm and jn use the same python version and i could successfully import spacy so im assuming the different python versions really were the problem maybe its not even necessary to create a new venv and everything but i wanted to start clean again to not have further problems
66439851,error parsing config overrides sections does not exist,spacy,theres nothing wrong with that tokenizer setting encorewebsm already contains its own config including a tokenizer which is what you cant override you want to load your config starting from a blank pipeline instead of a pretrained pipeline nlp spacyblanken configconfig be aware that the language en needs to match here with the spacy init config language setting or it wont be able to load the config
66433496,how do i fix valueerror when doing nlpaddpipelanguagedetector namelanguagedetector lasttrue with spacy,python spacy languagedetection,you can also use a languagefactory decorator to achieve the same result with less code import scispacy import spacy import encorescilg from spacylangdetect import languagedetector from spacylanguage import language languagefactorylanguagedetector def languagedetectornlp name return languagedetector nlp encorescilgloaddisabletagger ner nlpmaxlength nlpaddpipelanguagedetector lasttrue
66367475,oserror e cant find model encorewebsm it doesnt seem to be a shortcut link a python package or a valid path to a data directory,python anaconda spacy oserror,if youre using anaconda you need to activate your conda environment before downloading the encorewebsm model if you dont have an anaconda environment first run conda create n envname replacing envname with whatever name you want activate your environment with conda activate envname again replace the variable with whatever you put in step then install spacy with conda install spacy finally run python m spacy download encorewebsm now when you import spacy and try to load the model it should work
66099105,spacy named entity recognition on dates not working as expected,python date spacy namedentityrecognition,you need a more featurerich model type the one with md or lg suffix with spacy x and trf with spacy x for example you may install python m spacy download encorewebtrf then you may use import spacy nlp spacyloadencorewebtrf text university of a university of b college a college b doc nlptext for ent in docents printenttext entlabel output
66087475,chatterbot error oserror e cant find model en,python windows spacy chatterbot,make sure you actually have the right spacy model installed for example install encorewebsm with the python m spacy download encorewebsm command in the terminal next fix this error that is open the cusersuserappdatalocalprogramspythonpythonlibsitepackageschatterbottaggingpy file go to line replace selfnlp spacyloadselflanguageisolower with if selflanguageisolower en selfnlp spacyloadencorewebsm else selfnlp spacyloadselflanguageisolower you will need to add more conditions for other languages you need to support
66034842,importerror cannot import name deque with spacy,python visualstudiocode spacy,it is recommended that you reinstall the module spacy in the currently selected python environment the python environment shown in the lower left corner of vs code you could use the command pip show spacyor pip show spacy to check the installation location of the module debug if it still doesnt work please try to reinstall the python extension and reload vs code
65745253,spacy model installation failed due to an environmenterror,python ubuntu spacy,what i did was which then i installed using pip install locally downloaded file worked like a charm i think this error occurs because of the garbage collector which cleans the tmp folder prematurely because of the long download time of the file
65719547,import spacy on rpi in pycharm and other ide gives error,python pythonx pycharm spacy,you probably have a bit os instead you will need a bit os with bit python for spacy v eg as of spacy v you should be able to install spacy in a venv for linux aarch with python m venv venv source venvbinactivate pip install upgrade pip setuptools pip install spacy alternatively there are binary linux aarch packages on condaforge theres a linux aarch miniforge installer and the install command would be conda install spacy if youre not using the miniforge installer then you need to add c condaforge or otherwise add the condaforge channel see
65612362,typeerror argument string has incorrect type expected str got float,python typeerror spacy,covert chunkroottext to a string during list comprehension join requires strings in the iterable
65549200,error in installing spacy encoreweblg on heroku app,python django heroku spacy,your error is because there is no pip library named encoreweblg keep in mind that you cannot put encoreweblg in the requirementstxt since it is not a library that you can install using pip they are models that spacy uses the accepted answer is fine however if you want a complete automatic deployment you can also add something like this into your code try nlp spacyloadencorewebmd except if not present we download spacyclidownloadencorewebmd nlp spacyloadencorewebmd this will download the models automatically if they are not present and it will not require you to use the heroku terminal only spacy on the requirementstxt file will be necessary finally keep in mind that spacy models are loaded in memory ram the lg model about mb is too big to be fitted in a free hobby or standard x dynos mb of ram so it will fail to load and the dyno will be killed with r more info about dynos
65235652,installed python package spacy throws following error,python pythonx package spacy,this is a bug for python and spacy v or v it should be fixed in v unless you have a particular reason to be using python it would be better to switch to python
65134056,ner should i include common prefixes in labeled entities,spacy namedentityrecognition,it depends on the neural network architecture you use lets assume you use spacy v and its default neural architecture which is a cnn in this case the architecture is going to slide through your text according to a specific window ie x number of words before the date entity and x number of words after the date entity with this approach every time the token date appears in the text it is likely that the neural network will recognize that the entity date sits next to it in this case my suggestion would be to include only the annotate the xxxxxxxx date as an entity it will give the model more flexibility in determining what is a date entity however testing is always the best way to find out whats best so give it a try
65087252,spacy installation error in python with pip command,python pip spacy,there may be a problem with your pip cache andor your pip version try this delete your pip cache as it may appear to be causing troubles on windows it is located on cusersyourusernameappdatalocalpipcache then update pip to the latest version for the installation to work python m pip install upgrade pip after this process try installing spacy again python m pip install spacy or else you could also try to uninstall python bit and install python bit
64750336,with spacy how to indicate that a part of a fixed pattern can be seperated by one or multiple words from the last part of the pattern,python spacy,you will need to replace lines of with one line like in regex spacys op can be one or more zero or more not
64715848,getting error while loading spacy encorewebsm library,python spacy,you need to install the encorewebsm model separately by using the following command after installing spacy or i have tried the below steps in my jupyter notebook just now and it works good as below step result step result
64392302,scipy numpy django docker issue,python django docker scipy spacy,even though oomkilled is false killed due to out of memory i increased the memory from to gb on docker desktop and voila my app worked
64035821,how to fix spacy enmodel incompatible with current spacy version,pythonx model spacy namedentityrecognition,solved by downgrading spacy to
63712729,modulenotfounderror no module named encorewebsm,python module package spacy pipenv,use now if you run for example you will get
63681625,pandas udf pyspark incorrect type error,pandas apachespark pyspark userdefinedfunctions spacy,you need to see the input as pdseries instead of single value i was able to get it working by refactoring the code a bit notice xapply call which is pandas specific and applies function to a pdseries
63405961,valueerror when using columntransformer in an sklearn pipeline using custom class of spacy for glovevectorizer,python pandas machinelearning scikitlearn spacy,the error message tells you what you need to fix valueerror the output of the titleglove transformer should be d scipy matrix array or pandas dataframe but what you are returning with your current transformer spacyvectortransformer is a list you can fix it by turning the list into a pandas dataframe for instance like this next time please also provide a minimal reproducible example in your provided code there are no imports as well as no dataframe called df
63020929,function not returning in multiprocess no errors,python spacy multiprocess,it turns out this is a known issue with pytorch running multithreading in child processes causing deadlocks a workaround is to add the following
62950757,spacy convert and train utf encoding cli issues,encoding utf spacy,the escaped utf will be read in correctly by the library srsly which is using a fork of ujson internally if youre worried you can doublecheck with srslyreadjsonfilejson you provide python strings python str as input
62796437,spacy french langage gives nonetype error,python spacy french,this seems to be a bug at spacy downgrade to and it should work pip install spacy
62642100,spacy model in cloud function not working,python googlecloudfunctions spacy,your requirementstxt should look like this the link will directly download the spacy model and install it when you install the dependencies from requirementstxt the spacy models are valid python packages check this link for more details
62076017,python spacy keyerror e cant retrieve string for hash,python raspberrypi spacy raspberrypi,after three days of testing problem was solved by simply installing an older version of spacy
62057053,dockerized python scripts having issues accessing files stored to tmp,python django docker celery spacy,i figured this out well mostly the problem is spacy and other similar libraries and tools that download data files put them into local directories on their file system and possibly create symlinks to them in a docker container however these files and symlinks are not persistent however unless the parent directory is in a docker volume what i ended up doing is creating docker volumes for the folder that spacy or whatever library uses to store data files libraries in my case spacy is always called from via celery which has its own docker my dockercompose stack so i needed to attach volumes for each of my spacy data directories to my celeryworker like so all of this said ive noticed there are situations where at first pass the installation of a data file like a spacy model in my worker container throws an error that the data file is still not accessible yet when this happens and its not all the time i can just run the install again and of the time this appears to fix the issue i have not had time to try to troubleshoot that perhaps someone else can figure that part out
62003962,text classifier training data not properly loaded via spacy debugdata cli,pythonx commandlineinterface spacy,the spacy debugdata command expects data in spacys internal json training format described here there are some examples here the conversion script in the same directory shows how to convert from a jsonl format thats very similar to the traindatatype format used in the example scripts
61984151,persistent environmental issues in python installed python packages fail to load,python pythonx jupyter spacy,spacy is lowercase for import and installation but yeah if you have access to the command line thats the way to go you can spend some time with the conda docs theyre wellwritten
61778810,can i apply custom token rules to tokens split by prefixes in spacy,python tokenize spacy prefix,unfortunately theres no way to have prefixes and suffixes also analyzed as exceptions in spacy v tokenizer exceptions will be handled more generally in the upcoming spacy v release in order to support cases like this but i dont know when the release might be at this point i think the best you can do in spacy v is to have a quick postprocessing component that assigns the lemmasnorms to the individual tokens if they match the orth pattern
61768494,python code for training arabic spacy ner model not giving result or errors,machinelearning spacy trainingdata namedentityrecognition,spacy doesnt allow overlapping entitiesyou should remove the overlapping entities your code it will be
61410527,oserror e cant find model en it doesnt seem to be a shortcut link a python package or a valid path to a data directory,python conda spacy rasacore,you need to download and link the spacy model you want to use eg also preconfigured pipelines will be deprecated see the new guide to choosing a pipeline and pass the components you want explicitly
61383523,running spacy in pyspark but getting modulenotfounderror no module named spacy,pythonx apachespark pyspark apachesparksql spacy,coming back to this it appears that i needed to restart my spark session in the end so i close this i dont really understand what was happening but its not an open issue anymore
61269954,attribute error using neuralcoref in colab,pythonx googlecolaboratory spacy,update since the previous helped solving the first problem but created another problem i have updated the answer according to neuralcoref page for our version of spacy we need to manually install it from the source also try each of the following blocks in new cell in colab and restart runtime after installation
61210066,spacy special token overriding suffix rule causing annotation misalignment,python tokenize spacy rules,tokenizer exceptions also special cases rules have priority over the other patterns so you would need to remove the special cases you dont want nlptokenizerrules contains the special cases which you can modify remove all exceptions with periods as an example
61109879,problem with spanasdoc method in spacy,spacy,you can iterate over a sentence which is a span just like a doc to access the tokens import spacy nlp spacyloadencorewebsm doc nlpshe gave the dog a bone he read a book they gave her a book dativesents for nc in docnounchunks if ncrootdep dative and ncrootheadpos verb dativesentsappendncsent for dativesent in dativesents printsentence with dative dativesenttext for token in dativesent printtokentext tokenpos tokendep print output sentence with dative she gave the dog a bone she pron nsubj gave verb root the det det dog noun dative a det det bone noun dobj punct punct sentence with dative they gave her a book they pron nsubj gave verb root her pron dative a det det book noun dobj punct punct
61085648,attributeerror example object has no attribute insult when build vocab using buildvocab from torchtext,python pytorch googlecolaboratory spacy torchtext,your script fetched the html file instead of the actual dataset it is because the url you used is not a direct url to the csv file it is rather in html format because the provided url is of google sheets to solve this you can download the dataset to your computer and upload it to colab this is the content of the datacsv that you fetched
60953898,is this supposed to be a suffix infix or prefix rule,spacy,it doesnt matter in the end whether a token was a prefix infix or suffix and there can often be multiple ways to get the same result i think the best way would be to add a regex that has as an infix between date and a digit you could also add both date and as prefixes instead if this doesnt cause any side effects in other cases which maybe slightly easier to do because you can just add items to tokenizerprefixes without writing any regexes
60534999,how to solve spanish lemmatization problems with spacy,python spacy lemmatization,unlike the english lemmatizer spacys spanish lemmatizer does not use pos information at all it relies on a lookup list of inflected verbs and lemmas eg ideo idear ideas idear idea idear ideamos idear etc it will just output the first match in the list regardless of its pos i actually developed spacys new rulebased lemmatizer for spanish which takes pos and morphological information such as tense gender number into account these finegrained rules make it a lot more accurate than the current lookup lemmatizer it will be released soon meanwhile you can maybe use stanford corenlp or freeling
60517164,error of preparation of data before fitting,scikitlearn spacy,you just have to change the text to text because the vectorizers take d iterator of strings only you can get more discussions on this here some reproducible example for your code from sklearndatasets import fetchnewsgroups import pandas as pd import numpy as np import spacy load english tokenizer tagger parser ner and word vectors nlp spacyloadencorewebsm from sklearnfeatureextractiontext import countvectorizer tfidftransformer from sklearnsvm import linearsvc from sklearnmodelselection import traintestsplit from sklearncompose import columntransformer from sklearnpipeline import makepipeline from sklearnpreprocessing import ordinalencoder cats altatheism scispace newsgroupstrain fetchnewsgroupssubsettrain categoriescats df pddataframetext newsgroupstraindata target newsgroupstraintarget dfsetting nprandomchoiceplayiottransport news play calendar lendf printdfhead def bowtokenizertext doc nlptext tokens token for token in doc if not tokenisstop or tokenispunct tokens tokenlemmalower if tokenlemma pron else tokentextlower for token in tokens return tokens bowvec countvectorizertokenizerbowtokenizer tfidfvec tfidftransformer lsvc linearsvc x dftextsetting y dftarget xtrain xtest ytrain ytest traintestsplitx y testsize randomstate preproc columntransformerbow tfidf makepipelinebowvec tfidfvec text ordinalencoder ordinalencoder setting pipe makepipelinepreproc lsvc pipefitxtrain ytrain predicted pipepredictxtest
60481221,spacy oserror e cant find model on google colab python,python googlecolaboratory spacy lemmatization,you first need to download the data then restart the runtime after which your code will run correctly
60434789,python virtual environment error module not found error for flask and spacy libraries,python flask anaconda conda spacy,do you have a line in your flask apppy that is attempting to import from spacy import spacy if so im not sure thats a valid spacy import edit run python apppy instead of conda run apppy
60061024,training spacy ner on custom dataset gives error,python spacy namedentityrecognition,the problem is you are feeding training data to model optimizer as mentioned in use the following function to remove leading and trailing white spaces from entity spans then use the following function for training
59920225,installing spacy with pip failed with error failed building wheel for blis,pip spacy powerpc,you may install version of spacy compiled for ppcle in the powerai supplementary channel that was recently added conda install c powerai spacy you may also git clone and modify to be version and then condabuild the package yourself if you want that specific version you would then run condabuild from the condarecipesspacyfeedstock directory to generate the conda package
59768365,getting modulenotfounderror while using import spacy in jupyter notebook and in ubuntu terminal,python jupyternotebook anaconda ubuntu spacy,seems like spacy is dependent on tqdm please install tqdm and restart the jupyter kernal installation for conda installation for pip
59645155,spacy filenotfounderror errno no such file or directory thincneuralcustomkernelscu in pyinstaller,pythonx pyinstaller spacy,the filenotfound error is because pyinstaller isnt packaging thinc properly thinc needed a hook ive found that as script containing from spacy import will work with the hook file below the command i used was this worked because i added a hook for spacy and its submodules in the same directory as the testspacypy script simply copy the below text into a file called hookspacypy thats in the same directory as your script hook file for spacy from pyinstallerutilshooks import collectall spacy data collectallspacy datas data binaries data hiddenimports data thinc data collectallthinc datas data binaries data hiddenimports data cymem data collectallcymem datas data binaries data hiddenimports data preshed data collectallpreshed datas data binaries data hiddenimports data blis data collectallblis datas data binaries data hiddenimports data this hook file is a bit of a hack really all of the libraries should be in seperate hook files eg hookblispy with the blis part of the hook ask in the comments if you need any more help
59183052,beginner typeerror got an unexpected keyword argument funcnames,pythonx pandas scikitlearn spacy sklearnpandas,a generic way to do this is to store the functions in a dictionary where you use keys to find the function you want here is an example below where i create a function combinefunctions which takes a list of strings as an argument this allows you to pick which function should be run in which order technically this also allows you to run the same function multiple times def funcx printi am func return x def funcx printi am func return x def funcx printi am func return x def combinefunctionsx funcnames functions func func func func func func for funcname in funcnames x functionsfuncnamex return x
59179404,how can i solve an attribute error when using spacy,python spacy,the problem here lies in the fact that spacys tokenlemma returns a string and that strings have no text attribute as the error states i suggest doing the same as you did when you wrote nonumbers tok for tok in nopunct if not rematchd tok the only difference with this line in your code would be that youd have to include the special string pron in case you encounter english pronouns please tell me if this solved your problem as i may have misunderstood your issue
59094116,python spacy installation error no module preshedbloom,python installation spacy,this is probably a version mismatch in one of spacys dependencies you shouldnt need to download wheels from a thirdparty site installing with pip or conda should just work you can try pip install u spacy in your current environment to update all the dependencies or if that doesnt help try installing spacy from scratch in a new virtual environment the answer you linked is a bit outofdate but as mentioned in the other answers spacy only works with bit python so doublecheck that too
58955013,attributeerror english object has no attribute vocal,python spacy,first of all i think you meant vocab instead of vocal second of all you are trying to access the wordvector and vocab has nothing to do with that finally you are using the encorewebsm model which doesnt support wordvectors according to spacy official documentation here my suggestion is to use encorewebmd instead you can download it using the following command and you can change your code to be
58847147,error when installing spacy using pip exit status,python pythonx pip setuptools spacy,the issues was caused by anaconda being bit whereas the os was bit reinstalled bit version of anaconda and everything is working smoothly
58843519,lemmatization issue using spacy in pandas series and dataframe,python pandas dataframe series spacy,
58779371,importerror cannot import name lemmaindex from spacylangen,python spacy lemmatization,this is due to a change from v to v to move the large lookup tables out of the main library the lemmatizer data is now stored in the separate package spacylookupsdata and the lemmatizer is initialized with a lookups object instead of the individual variables see the second section here about initializing lemmatizers if you install the package spacylookupsdata you can access the default english lemmatizer like this it automatically loads the data from spacylookupsdata if its available if its not available the lemmas will be the same as the tokens from the text if you use an english model like encorewebsm the lookup tables are included with the model so you dont need the additional package spacylookupsdata
58776141,spacy with zappa showing error on aws lambda,awslambda spacy zappa,your error is that there is no space left on the lambda function spacy requires quite a lot of space therefore follow the following suggestions aws lambda no space left on device error
58475716,skitlearn with spacy parallelization error with randomizedsearchcv,python pythonx scikitlearn parallelprocessing spacy,apparently the issue lies with the parallel backend of sklearn which uses loky by default changing the backend to multiprocessing solves this problem as mentioned here here more info on sklearn parallel backend can be found here first import this when running the fit do this to overwrite the parallel backend
58291958,getting error while installing backend dependencies for spacy,python pip installation spacy,i was facing the same issue having same configuration bit windows and bit python actually the problem is we cant build blis with bit python so you have to use bit python i have done the same and now it is working check out official spacy issues reference failed building wheel for blis when installing spacynightly through pip
57702805,typeerror string indices must be integers in entityruler,python spacy,i think your patterns should be like this
57679852,spacy showing import module error while it is already installed,python jupyternotebook spacy,i was able to run the spacy in python console so i assumed the problem was with jupyter notebook i followed what i did is i added pip install ipykernel then ipython kernel install user nameprojectname at this point you can start jupyter create a new notebook and select the kernel that lives inside your environment
57607392,error while importing matcher from spacymatcher,python pythonx spacy,the problem is that you called your file spacypy while youre also trying to use the package spacy never name your script the same as an existing module solution rename your file to something different eg mainpy or apppy
57536044,add multiple entityruler with spacy valueerror entityruler already exists in pipeline,python spacy,you can add another custom entity ruler to your pipeline by changing its name to avoid name collision here is some code to illustrate but please read the remark below we can verify that the pipeline does contain both entity rulers remark i would suggest using the simpler and more natural approach of making a new entity ruler which contains the rules of both entity rulers finally concerning your question about best practices for entity labels it is a common practice to use abbreviations written with capital letters see spacy ner documentation for example org loc person etc edits following questions if you do not need spacys default named entity recognition ner then i would suggest disabling it as that will speedup computations and avoid interference see discussion about this here disabling ner will not cause unexpected downstream results your document just wont be tagged for the default entities loc org person etc there is this idea in programming that simple is better than complex see here there can be some subjectivity as to what constitutes a simpler solution i would think that a processing pipeline with fewer components is simpler ie the pipeline containing both entity rulers would seem more complex to me however depending on your needs in terms of profiling adjustability etc it might be simpler for you have several different entity rulers as described in the first part of this solution it would be nice to get the authors of spacy to give their view on these two different design choices naturally the single entity ruler above can be directly created as follows the other code above shown for constructing rulerall is meant to illustrate how we can query an entity ruler for the list of patterns which have been added to it in practice we would construct rulerall directly without first constructing rulerplant and ruleranimal unless we wanted to test and profile these rulerplant and ruleranimal individually
57504608,spacy custom infix regex rule to split on for patterns like mailtojohndoegmailcom is not applied consistently,tokenize spacy,theres a tokenizer exception pattern for urls which matches things like mailtojohndoegmailcom as one token it knows that toplevel domains have at least two letters so it matches gmailco and gmailcom but not gmailc you can override it by setting then you should get if you want the url tokenization to be as by default except for mailto you could modify the urlpattern from langtokenizerexceptionspy also see how tokenmatch is defined right below it and use that rather than none
57490832,spacy valueerror operands could not be broadcast together with shapes,python pytorch spacy multiclassclassification,as milla well already commented the answer can be found here the bug fix on github from syllogism
57117775,encorewebsm module error serverless deployment aws lambda,amazonwebservices configuration awslambda serverlessframework spacy,take the advantage of the model being a separate component to the library and uploaded the model in an s bucket before initialising spacy i download the model from s this is accomplished by the method below and the code using spacy looks like this
56979376,error could not find a version that satisfies the requirement preshed,python pythonx pip version spacy,you have pip set up to look for packages in your private repository myartifactrepocom which is missing the package either upload the preshed package and its eventual dependencies to the private repo or install preshed from pypi
56884020,spacy with joblib library generates picklepicklingerror could not pickle the task to send it to the workers,python pythonx parallelprocessing spacy joblib,same issue i solved by changing the backend from loky to threading in parallel
56800959,facing error while trying to use the english package of spacy,spacy,please try the below steps open cmd using run as administrator use the command pip install u spacy to download the english package python m spacy download en to load it import spacy spacyloaden
56446478,spacy en model issue,spacy,so each model is a machine learning model trained on top of a specific corpus a text dataset this makes it so that each model can tag entries differently especially because some models were trained on less data than others currently spacy offers models for english as presented in according to a model can be downloaded in several distinct ways probably when you downloaded the en model the best matching default model was not encorewebsm also keep in mind that these models are updated every once in a while which may have caused you to have two different versions of the same model
56272350,valueerror could not broadcast when using word vectors how to fix,python arrays numpy spacy valueerror,the encorewebsm model doesnt include word vectors you can download the encorewebmd or encoreweblg models instead which do reference output
55993124,attributeerror list object has no attribute similarity,python pythonx spacy,be aware that you are using german model for english phrases in your case you would need to glue back the remaining tokens and create a spacy object again in your case you remove all the tokens by this condition lentoken anyway
55864933,spacy lemmatizer issueconsistency,python spacy lemmatization,the issue was analysed by the spacy team and theyve come up with a solution heres the fix basically when the lemmatization rules were applied a set was used to return a lemma since a set has no ordering the returned lemma could change in between python session for example in my case for the noun leaves the potential lemmas were leave and leaf without ordering the result was random it could be leave or leaf
55832506,unable to pip install spacy model due to proxy issue,python spacy,you are behind the proxy and are you able to download the model directly from the release in your browser first download the tar file the targz archive is the same file thats downloaded during spacy download and its an installable python package so if you have the file you can also do the following you should then be able to use the model like this you can also download other spacy model in same way or you can also use proxy in pip install but it not work in my case
55534487,how to debug msgpack serialisation issue in google cloud dataflow job,googleclouddataflow apachebeam spacy msgpack,are you able to run the same code from a regular python program not from a beam dofn if not check whether you are storing any nonserializable state in a beam dofn or any other function that will be serialized by beam this prevents beam runners from serializing these functions to be sent to workers hence should be avoided
55280666,spacy language model installation in python returns importerror from mklinit importerror dll load failed the specified module could not be found,python pip conda importerror spacy,thanks to ines montani for pointing this out it did seem that a quick reinstallation of numpy helps solve the problem however what i did realize is that using pip uninstall numpy and simple pip install numpy solves the problem however using conda remove force numpy and conda install numpy does not solve the problem for me
54798604,attributeerror english object has no attribute nounchunks,python pythonx spacy,how did you come up with that code the loaded nlp processing object doesnt have a property nounchunks instead you want to be accessing the noun chunks of a processed document nlp spacyloadencorewebsm load the english model doc nlpthere is a big dog process a text and create a doc object for chunk in docnounchunks iterate over the noun chunks in the doc printchunktext a big dog for more details see the documentation here
53690935,download spacy model and get attributeerror nonetype object has no attribute ndarray,python spacy,this happened to me once during development and the reason was that for some reason my code tricked spacy into thinking i was on gpu on gpu spacy uses cupy instead of numpy and if cupy is not installed it defaults to none its likely that the code should be calling numpyndarray but its calling cupyndarray ie nonendarray which results in that error if youve intended to run spacy on gpu make sure its available and youve installed the correct dependencies for your cuda version if youre running spacy on cpu here are some things to try check whats installed in your environment and make sure you didnt accidentally end up with a halfbroken install of cupy or something like that also make sure numpy is installed correctly unsatisfying answer but often helps uninstall spacy and its dependencies and reinstall the latest version ideally in a clean virtual environment
53561787,valueerror exceeds maxbinlen when attempting spacyload,spacy,try pip install msgpack
53289839,python m spacyendownload connection refused urlerror,spacy,please try the below steps open cmd using run as administrator use the command pip install u spacy to download the english package python m spacy download en to load it import spacy spacyloaden
53122687,spacy fails to run with error cymemcymem has no attribute pymalloc,python spacy,uninstalling thinc and cymem and then reinstalling spacy fixed this issue for me
52436726,deep copying phrasematcher object in spacy is not working,python pythonx nltk spacy,the root of your problem is that phrasematcher is a cython class defined and implemented in the file matcherpyx and cython does not work properly with deepcopy referenced from the accepted answer to this stackoverflow question cython doesnt like deepcopy on classes which have functionmethod referenced variables those variable copies will fail however there are alternatives to that if you want to run phrasematcher to multiple documents in parallel you can use multithreading with the pipe method of phrasematcher a possible workaround for your problem hope it helps
52345387,python spacy typeerror unpackb got an unexpected keyword argument raw,python spacy,note that my prompt happens to underscored that regex version is incompatible with spacy which wants regex however my spacy still works normally now btw great thanks to ines montani
51639941,output result to csv file typeerror writerow takes positional arguments but were given,python csv typeerror spacy namedentityrecognition,a csvdictwriter requires a dictionary for writing if your ent object doesnt have a todict method you will have to make one
51542570,error sockettimeout the read operation timed out while installing a python module,python pip spacy,try increasing pip timeout pip install defaulttimeout pkgname
51298807,error installing spacy using pip,pythonx spacy,update july spacy has been updated for python spacy hasnt been updated for python yet from matthew honnibal author of spacy a few days ago python just came out and while i dont think theres any deep incompatibilities i think we do need to add specifiers for the new version to our various dependencies could you try installing with python source
51045347,installation of spaccy language model in python not working,python spacy rasanlu rasacore,latest spacy has a requirement of thinc as double check the requirementstxt and check which modules are installed with pip list if the issue is with incorrect version of thinc then run
50841484,raisefirstseterror in spacy topic modeling,lda spacy,i had this same issue and i was able to resolve it by uninstalling regex i had the wrong version installed and then running python m spacy download en again this will reinstall the correct version of regex
50553201,importerror when importing spacy using spyder,python python spyder importerror spacy,spyder maintainer here this problem seems related to an already reported bug in spyder which was fixed in our version released in march
50017423,spacy phrasematcher value error pattern length phrasematchermaxlength,python spacy,currently a single rule cannot exceed tokens in length you can try to set the limit higher ie selfmatcher phrasematchernlpvocab maxlength but iirc in the current release version of spacy is a hard limit see relevant documentation at and source at
49547647,attributeerror nonetype object has no attribute get on rasacom and tensorflow backend,pythonx tensorflow anaconda spacy rasanlu,in your case slotdictslotname is none make sure it is not none but some object here is the same error in a simplified way reflecting what happened in your scenario
48572541,spacy sentence tokenization error on hebrew,python spacy,spacys hebrew coverage is currently quite minimal it currently only has word tokenization for hebrew which roughly splits on white space with some extra rules and exceptions the sentence tokenizationboundary detection that you want requires a more sophisticated grammatical parsing of the sentence in order to determine where one sentence ends and another begins these models require a large amount of labeled training data so are available for a smaller number of languages than have tokenization heres the list the initial message is telling you that it can do tokenization which doesnt require a model and then the error youre getting is the result of not having a model to split sentences do ner or pos etc you might look at this list for other resources for hebrew nlp if you find enough labeled data in the right format and youre feeling ambitious you could train your own hebrew spacy model using the overview described here
48059201,importerror no module named thincabout,windows pythonx spacy,i found this link from where i downloaded all of the required whls one by one to install spacy
47582609,trivial example using spacy matcher not working,spacy,when using the matcher keep in mind that each dictionary in the pattern represents one individual token this also means that the matches it finds depends on how spacy tokenizes your text by default spacys english tokenizer will split your example text like this stays one token which objectively is probably quite reasonable an ip address could be considered a word so the match pattern that expects parts of it to be individual tokens wont match in order to change this behaviour you could customise the tokenizer with an additional rule that tells spacy to split periods between numbers however this might also produce other unintended side effects so a better approach in your case would be to work with the token shape available as the tokenshape attribute the shape is a string representation of the token that describes the individual characters and whether they contain digits uppercaselowercase characters and punctuation the ip address shape looks like this you can either just filter your document and check that tokenshape dddddddd or use the shape as a key in your match pattern for a single token to find sentences or phrases containing tokens of that shape
47561572,assertionerror on trying to add new entity using matcher on spacy,namedentityrecognition spacy,i believe your first code fails because you have not added an entity label for email the second code works because event is a preexisting entity type the documentation is not very clear on what the first argument of the matcheradd method actually does but it adds an entity label for you here are two alternatives that should work and clear up the confusion alternative alternative im not sure why youd want to do it this way because you end up with two entity labels serving essentially the same purpose but provided just for illustration purposes
47388438,named entity recognition upper case issue,namedentityrecognition spacy,the xxentwikism model was trained on wikipedia so its very biased towards what wikipedia considers and entity and whats common in the data it also tends to frequently recognise i as an entity since sentences in the first person are so rare on wikipedia so posttraining with more examples is definitely a good strategy and what youre trying to do sounds feasible the best way to prevent the model from forgetting about the uppercase entities is to always include examples of entities that the model previously recognised correctly in the training data see the catastrophic forgetting problem the nice thing is that you can create those programmatically by running spacy over a bunch of text and extracting uppercase entities see this section for more examples of how to create training data using spacy you can also use spacy to generate the lowercase and titlecase variations of the selected entities to bootstrap your training data which should hopefully save you a lot of time and work
47295316,importerror no module named spacyen,python spacy,yes i can confirm that your solution is correct the version of spacy you downloaded from pip is v which includes a lot of new features but also a few changes to the api one of them is that all language data has been moved to a submodule spacylang to keep thing cleaner and better organised so instead of using spacyen you now import from spacylangen however its also worth mentioning that what you download when you run spacy download en is not the same as spacylangen the language data shipped with spacy includes the static data like tokenization rules stop words or lemmatization tables the en package that you can download is a shortcut for the statistical model encorewebsm it includes the language data as well as binary weight to enable spacy to make predictions for partofspeech tags dependencies and named entities instead of just downloading en id actually recommend using the full model name which makes it much more obvious whats going on python m spacy download encorewebsm nlp spacyloadencorewebsm when you call spacyload spacy does the following find the installed model named encorewebsm a package or shortcut link read its metajson and check which language its using in this case spacylangen and how its processing pipeline should look in this case tagger parser and ner initialise the language class and add the pipeline to it load in the binary weights from the model data so pipeline components like the tagger parser or entity recognizer can make predictions see this section in the docs for more details
46897484,spacy lemmatizer help deciphering generic error message,python pythonx pandas spacy,it looks like youre combining the forloop syntax for x in iterable with the list comprehension syntax x for x in iterable the only times ive seen colons inside list comprehensions has been in lambda functions eg lambda x xx for x in range here the colon shows up without a lambda expression so the interpreter chokes hopefully this is what youre looking for dfnewcol toklemmalowerstrip if toklemma pron else toklower for tok in col
44176829,python spacy error runtimeerror language not supported,python dictionary entity spacy,the problem here is that spacyload currently expects either a language id eg en or a shortcut link to a model that tells spacy where to find the data because spacy cant find a shortcut link it assumes that mymodel is a language which obviously doesnt exist you can set up a link for your model like this this will create a symlink in the spacydata directory so you should run it with admin permissions alternatively if youve created a model package that can be installed via pip you can simply install and import it and then call its load method with no arguments in some cases this way of loading models is actually more convenient as its cleaner and lets you debug your code more easily for example if a model doesnt exist python will raise an importerror immediately similarly if loading fails you know theres likely a problem with the models own loading and meta btw im one of the spacy maintainers and i agree that the way spacyload currently works is definitely unideal and confusing were looking forward to finally changing this with the next major release were very close to releasing the first alpha of v which will solve this problem more elegantly and will also include a lot of improvements to the training process and documentation
44095147,how to fix a python spacy error undefined symbol pysliceadjustindices,python spacy,i cannot explain it but it works when i run idle under sudo i use python shell and run it from terminal by command idle later when i compile the code i get the aforementioned mistake but when i start from terminal sudo idle it works perfect without any mistakes so i think its a question from access in ubuntu thank you for all
43833845,how to fix unicodedecodeerror ascii codec cant decode byte,python unicode beautifulsoup spacy,when you get an decoding error with the ascii codec thats usually an indication that a byte string is being used in a context where a unicode string is required in python python wont allow it at all since youve imported from future import unicodeliterals the string s is unicode this means the string youre trying to strip must be a unicode string too fix that and you wont get the error anymore
43459437,spacy link error,python models spacy,i had this same issue when i tried this on windows the problem was the output of python m spacyendownload all said linking successful but above that was the message that the symbolic link wasnt actually created due to permissions running python m spacyendownload all as an adminstrator fixed the problem
42664276,cant install spacy on winpython modulenotfounderror no module named semver,python semanticversioning spacy,if your on winpython try this find the pythonpth file next to the pythonexe file of winpython rename it as zpythonpth it looks related to
42161113,spacy nlp library issue with dependency parse,python pycharm spacy,below code solved the issue
38853776,spacy urlliberrorurlerror during installation,python pythonx pip nltk spacy,the problem was caused by a serverproblem of spacy itself and is fixed now spacyioblogannouncement
34842052,import error with spacy no module named en,python spacy,for windows open cmd with admin right then you should see the shell prompt stating you can now load the model via spacyloaden
78435465,getting nltk certificate verify failed error with visual studio code with python,python nltk errno,used a vpn and then added some certificate code at the top
77468918,winerror connection timeout error when downloading punkt in nltk,python pythonx jupyternotebook nltk,a very strange error it seems to be that on some networks ive been told jio is one of them rawgithubusercontentcom is not accessible eg
76836010,nltkdownloadwordnet is giving parseerror mismatched tag line column on python,python nltk,there may be other solutions to my issue what i ended up doing to solve this problem was manually downloading wordnet from and saving the file where the documentation tells you to cnltkdatacorporawordnet
76206458,header issue in generated tdm via python,python pandas numpy nltk,you can use output
74921920,i can this specific error while download nltk,python pythonx windows nltk,changing the network to another wifi connection worked for me the previous connection had some sort of blockers for me hope this helps
73999391,stderr output from native app classifier modulenotfounderror no module named nltk,javascript pythonx reactnative firefox nltk,actually i got it the path that python was pointing to did not have nltk and keras the plugin was using python from system path not from the virtual environment as i thought
73466767,attributeerror list object has no attribute hypernyms,python nltk wordnet,i took a look at the documentation for nltk at it looks like the culprit is wnsynsetsword in your invocation of getdistractorswordnet an easy typo wnsynset returns a single synonym set whereas wnsynsets returns a list of synonym sets youre getting that error because youre trying to use a member function of a synset on a list of synsets hence list object has no attribute hypernyms all you have to do is either iterate over the list or refactor to use wnsynset instead hope this helps
72814534,error when passing argument through function for converting pandas dataframe of tweets into corpus files,python pandas function nltk,problem you are passing myfolder as a variable to your function which you have not defined in your code and hence it raises a nameerror solution just replace it with myfolder pass it as a string
72377231,typeerror argument of type wordlistcorpusreader is not iterable,python pandas nltk,refer to the following wordlistcorpusreader is not iterable you just need to define a variable for the stopwords that reads from the stopwords object that you import from nltk corpus stopwords setstopwordswordsenglish
72002617,how to solve nltk lookuperrorresourcenotfound it exists in path python,python nltk,it seems you are not properly assigning a file path to the nlktdata module i also noticed a very similar issue try specifying the path using tempfilegettempdir and download it import tempfile import string import nltk from nltk import wordtokenize from nltkcorpus import stopwords from collections import counter downloadstopwords downloaddirtempfilegettempdir nltkdatapathappendtempfilegettempdir stopwordsstopwordswordsturkish
71999817,nltk is installed but nltkutils returns modulenotfounderror,python pip nltk modulenotfounderror,nltkutils is nothing that comes shipped with nltk did you mean nltkutil which is described here otherwise nltkutils is used in some examples using nltk where it is a custom file that contains useful functions in interacting with nltk eg in this chatbot example so if you are following some tutorial or similar check if they mention somewhere what nltkutils should contain
71850125,im getting this error filter object at xaf error when i run this program listed below,python nltk,thats not an error thats how filter objects print to get the results of the filter you have to iterate over the filter object the easiest way to do that is to pass it to list
70601433,having error after deploying django in heroku lookuperror,django heroku nltk,it looks like you installed the nltk python package but not any data for it if you want a quick fix you can connect to your heroku heroku psexec and run a django shell python managepy shell and then install the relevant nltk modules from the python shell since you want to be able to redeploy it better create a management command to do this and run it similar to how you run migrations management command should be under appmanagementcommands a command like this from djangocoremanagementbase import basecommand commanderror class commandbasecommand help install nltk stuff def handleself args options import nltk nltkdownload replace the dots with the relevant corpora
69610572,how can i solve the below error while importing nltk package,python nltk,just ran into this i found that the following fixes it xcrun codesign sign yourpathtodylibhere in my case the error was like so importerror dlopenusersuserdevcrlikesvenvlibpythonsitepackagesregexregexcpythondarwinso no suitable did find usersuserdevcrlikesvenvlibpythonsitepackagesregexregexcpythondarwinso code signature in usersuserdevcrlikesvenvlibpythonsitepackagesregexregexcpythondarwinso not valid for use in process using library validation trying to load an unsigned library by running xcrun on the shared object in this case usersuserdevcrlikesvenvlibpythonsitepackagesregexregexcpythondarwinso the error is now gone
69602779,keyerror when using re module,python nltk pythonre,the problem is that i was working with a csv file and i dropped some rows but i didnt reset the index thus when we reach the iteration nb ie the index we dont find it because it was dropped
69579151,error loading certain nltk modules in google colab,python nltk googlecolaboratory,it seems colab is downloading the package correctly just like it claims to be but the nltk modules are all downloaded as zip files thats the case for both stopwords and comtrans in the case of stopwords it unzips it after downloading while that unzip step is skipped for comtrans the difference here is that locally nltk is willing to grab the comtrans data directly from the zip file but in colab it isnt so because that data is only available in zip form it rejects the operation with a resource not found all the nltk zips i checked contain a single folder at the root level with all the specific files for the module contained within that folder needs to be extracted into the same location as the zip file in this case unzipping just needs to be done manually import nltk nltkdownloadcomtrans data is downloaded to rootnltkdatacorporacomtranszip from zipfile import zipfile fileloc rootnltkdatacorporacomtranszip with zipfilefileloc r as z zextractallrootnltkdatacorpora data nltkcorpuscomtransalignedsentsalignmentenfrtxt printdata reprise de la sessio
69210889,i get lookup error in google cloud run for my flask python app when using nltk,python nltk googlecloudrun,for anyone interested putting inside the dockerfile fixed the issue like this
69012588,nameerror name senttokenize is not defined,python nltk,import senttokenize
68927809,i compare two identical sentences with ribes nltk and get an error why,python nltk metrics machinetranslation,youre getting to a division by zero on this line this is because numpossiblepairs is when lenworder is all of this is because youre calling sentenceribes with two lists when the first parameter should be a list of lists a list of sentences where each sentence is a list of words try calling it like this instead
68909637,how to fix the python chatbot code problem attempted to use a closed session,python tensorflow nltk chatbot,why this happened follow the link when exception happens in selftrainerrestore the selftrainersession can not be assigned to dnn session and dnn session is closed before exception happening add this line model tflearndnnnet in exception when modelload failed the result is d list so the correct code should be like this
68378380,list index out of range error after a lot of intents,python deeplearning nltk chatbot,ive been having this error in a similar program and i was able to directly trace the issue to having an empty list being returned i fixed this by adding a try and except to my code perhaps you could do this or another thing i tried was retraining my model but that may be timeconsuming
67821111,download nltk corpus as cmdclass in setuppy files not working,python nltk setuptools setuppy,pass the class not its instance no to avoid instantiating the class
67675976,countvectorizer fittransform error typeerror expected string or byteslike object,python machinelearning scikitlearn nltk,so i ended up finding a solution or at least a work around to this problem instead of importing the files first into a pandas frame i imported them directly through the countvectorizer function i am not sure what the original issue is but hopefully this can help anyone who has a similar issue note this exact code requires files to be in the py file you are runnings folder but this can be easily changed if not i have a suspicion it could be because of differences between pandas and numpy dataframes and commands
67527721,python trying to using a for to create a set and then apply another for that will interate and generate a dict but its is returning error,python forloop set classification nltk,i is an element of testedtabela because of that change to
66624212,nltk cfg valueerror grammar does not cover some of the input words,python parsing nltk grammar contextfreegrammar,it looks like converting the tokens to nltkpostag is not quite right comment that line out and the script works output
66170024,issue,python nltk tokenize,tweettokenizer is the constructor of the tweettokenizer class and therefore returns a tokenizer object you shall then call tokenizertokenizesentence
65815430,iterating through python list getting typeerror nonetype object is not iterable,python list loops nltk listcomprehension,you can remove the none values and use itertools
65454401,can someone help me with error in using nltk wordtokenize function,python nltk,you need to download the punkt module as stated open terminal on your mac execute python then the below commands nltk uses pretrained word and sentence tokenizers which needs to be downloaded seperately if in case the download fails use below reference
65156554,nltk ngrams typeerror iotextiowrapper object is not callable,python nltk ngram,your bug is the you overload ngrams you use it as file and as ntlk function a fix can be
64836501,unboundlocalerror local variable referenced before assignment doesnt work in command line call,python commandline nltk global,add some error checking to the function to help you debug i simplified the implementation a bit just for my own benefit in making it easier to understand im pretty sure it does the same thing with less confusion note that adding type annotations also means that if you had a line of code like say if you were to run mypy it would tell you about the error no assert required
64460941,nltk typeerror unhashable type list,python nltk lemmatization,tokenizedwords is a column of lists the reason its not a column of strings is because you used the split method so you need to use a double for loop like so lem joinwnllemmatizeword for wordlist in tokenizedwords for word in wordlist
64402907,error in importing from nltkcorpus import wordnet in android studio using chaquopy,java python androidstudio nltk chaquopy,as i already said in my previous answer the wordnet pip package apparently has nothing to do with nltk so you can remove it because of an emulator bug you may need to call nltkdownload in a loop as described here
64399616,typeerror list indices must be integers or slices not str on a windows,python artificialintelligence nltk frequency wordfrequency,you define idfs as a list if udfs is a list then in this assignment word must be an integer because it specifies an index or position within the list but it appears that words is a list of str and so inside the iteration word is a str since a str is not an integer the line causes the error youre getting for exactly the reason that it explains maybe idfs should be a dict rather than a list defined like this then the line interprets word as a key in the dictionary and assigns idf as the value of that key in the dict dictionary keys can be any object and are most often strings so this makes good sense
64252434,architecture not supported error when installing nltk with pip on mac,python pip nltk macoscatalina,since the error message shows error architecture not supported maybe you want to tell pip which architecture you are using please note the x argument you specified should match your systems real architecture i fixed a similar issue when i tried to install pycairo
63598027,chaquopy problems with nltk and download,python androidstudio kotlin nltk chaquopy,i was able to reproduce something similar on the emulator in my case the root cause was that the download failed with a decryptionfailedorbadrecordmac error leaving behind an incomplete zip file this appears to be a lowlevel problem with the emulator which isnt specific to python if you can confirm you have the same problem by seeing decryptionfailedorbadrecordmac in the nltkdownload logcat output then please add a star on the android issue tracker here to help encourage them to fix it you can work around this by calling nltkdownload repeatedly in a loop until it returns true to save time you should probably only download the data files you need you can find out what these are by simply calling the corresponding function and looking at the error message eg nltkpostagsentshello world lookuperror resource maveragedperceptrontaggerm not found please use the nltk downloader to obtain the resource m import nltk nltkdownloadaveragedperceptrontagger then you can add this to your code this succeeded after a few iterations and i was then able to call nltkpostagsents successfully
62735030,string methods typeerror column is not iterable in pyspark,python pyspark nltk apachesparkml lemmatization,the error message is accurate you can iterate through a dataframe column like a standard python iterator for applying a standard function such as summean etc we need to use the withcolumn or select function in your case you have your own custom function so you need to register your function as a udf and use it along with withcolumn or select below is the example of a udf from spark documents
62634368,value error when called printing function in pairs in python chatbot,python pythonx nltk,the error occurs because the pair list has in the first index items and the recompilex reignorecase y for x y in pairs statement expect to items so you can try or
62594180,attributeerror list object has no attribute allhypernyms what is this error,python nltk wordnet lcs sentencesimilarity,so i am not sure of what you are trying to acheive with this code but i see why the code is breaking what is happening is that in the line that is breaking you are comparing two lists of synsets this is problematic because the function cannot handle list of synsets so if you want to compare all the synsets that are created you will need to use an additional for loop to extract all the synset in the list however if you want to compare the first synset of the first word with the first synset of the secon word you can simply do this also you need to be careful in the if statement of the wordcheck rangeis becasue you are comparing a list with a value while you should check the similaryty outcome itself
62468476,error accessing tree in semcortaggedsents,pythonx nltk lookup,my goal is to create a function that takes as input a sentence from semcor and extracts a list which contains for each token of the sentence either the corresponding wordnet lemma eg lemmafridaynfriday or none example as for the error and its meaning see nltk was unable to find the gs file
62040101,i have installed tensorflow and tflearn for creating chatbots during importing it shows errors,python tensorflow nltk tflearn,did you run pip install tensorflow you should try running this
61894629,valueerror length of values does not match length of index in nested loop,python pandas forloop nltk listcomprehension,read the error message carefully valueerror length of values does not match length of index the values in this case is the stuff on the right of the values word for new in datanew for word in new if word not in stopwords the index in this case is the row index of the dataframe index dataindex the index here always has the same number of rows as the dataframe itself the problem is that values is too long for the index ie they are too long for the dataframe if you inspect your code this should be immediately obvious if you still dont see the problem try this datatexttokenized wordtokenizerow for row in datatext values word for new in datatexttokenized for word in new if word not in stopwords printn rows datashape printn new values lenvalues as for how to fix the problem it depends entirely on what youre trying to achieve one option is to explode the data also note the use of map instead of a list comprehension datatexttokenized datatextmapwordtokenize flatten the token lists without a nested list comprehension tokensflat datatexttokenizedexplode join your labels w your flattened tokens if desired dataflat datalabeljointokensflat add a nd index level to track token appearance order might make your life easier dataflattokenid datagroupbylevelcumcount dataflat dataflatsetindextokenid appendtrue as an unrelated tip you can make your csv processing more efficient by only loading the columns you need as follows data pdreadcsvrdpython projectsreadfilesspamcsv encodinglatin usecolsv v
61808752,need help for valueerror substring not found,python nltk valueerror,you are changing your sent i recommend creating a new variable it will solve the issue output nhace calorantithesissde todas formas no salgo a casa nadems va a venir peterantithesisssin embargo no lo s a qu hora llegar exactamente new variable will have your desirable output in the form of list
60602630,partial keyword match not working when i am trying to create a new column from a pandas data frame in python,python regex pandas dataframe nltk,extractall will do the job but you must first build the pattern you would get pattern is here dinnergovernagententertain
60540605,pickling error while using stopwords from nltk in pyspark databricks,pyspark nltk stopwords,just change your function this way and it should run databricks is pain when it comes to nltk it doesnt allow stopwordswordsenglish to run inside a function while applying udf
60378173,installed matplotlib using terminal on mac but modulenotfounderror in python,python matplotlib nltk,from the look of your traceback it looks like you have installed matplotlib inside a virtualenv so your choices are run your idle from your virtualenv myprojectbinpython then you should be able to import matplotlib run sudo pip install matplotlib this will install matplotlib into your system python so you can run from your system python usrbinpython hope this helps
60291151,why do i get typeerror unhashable type when using nltk lemmatizer on sentence,python nltk lemmatization,lets walk through the code and see how to get your desired output first the imports you have and then you were using since youre already using from nltk import postag the postag is already in the global namespace just do idiomatically the imports should be cleaned up a little to look like this next actually keeping the list of tokenized words and then the list of pos tags and then the list of lemmas separately sounds logical but since the function finally only returns the function you should be able to chain up the postagwordtokenize function and iterate through it so that you can retrieve the pos tag and tokens ie out now we know that theres a mismatch between the outputs of postag and the pos that the wordnetlemmatizer is expecting from there is a function call pennmorphy that looks like this an example and if we use these converted tags as inputs to the wordnetlemmatizer and reusing your ifelse conditions out hey what did you do there your code works but mine doesnt okay now that we know how to get the desired output lets recap first we clean up imports then we clean up the preprocessing without keeping intermediate variables then we functionalized the conversion of pos tags from penn morphy lastly we applied the same ifelse conditions and run the lemmatizer but how is it that my code doesnt work okay lets work through your code to see why youre getting the error first lets check every output that you get within the findtag function printing the type of the output and the output out at sentence wordtokenizesentence you have already overwritten your original variable to the function usually thats a sign of error later on now lets look at the next line out now we see that the sentence istrip for i in sentence is actually meaningless given the example sentence q but is it true that all tokens output by wordtokenize would have no trailingheading spaces which istrip is trying to do a yes it seems like so then nltk first performs the regex operations on strings then call the strsplit function which would have removed headingtrailing spaces between tokens see lets continue out q wait a minute where is the poslabel only a single string q and what is pos tag p a lets look closer whats happening in nltkpostagsentencelower usually when you have to do such nested index retrieval its error prone we need to ask whats we know that sentence now after sentence wordtokenizesentence has became a list of strings and postagsentence would return a list of tuples of strings where the first item in the tuple is the token and the second the pos tag ie out now if we know thing postagwordtokenizei love running angrily outputs the above lets work with that to see what is accessing so thing outputs the tuple of token pos for the first token and thing outputs the pos for the first token achso the looks for the first character of the pos of the first token so the question is that the desired behavior if so why are you only looking at the pos of the first word regardless of what im looking at your explanation still that doesnt tell me why the typeerror unhashable type list occurs stop distracting me and tell me how to resolve the typeerror okay we move on now that we know thing postagwordtokenizei love running angrily and thinglower p given your ifelse conditions we find that p value would have gone to the else ie printlemlemmatizesentence but wait a minute remember what has sentence became after youve modified it with so what happens if we just ignore all the rest of the code and focus on this out ah ha thats where the error occurs its because wordnetlemmatizer is expecting a single string input and youre putting a list of strings example usage q why didnt you just get to the point a then you would miss out on how to debug your code and make it better
60261080,issue in user input or text file data in sentiment analysis,python nltk sentimentanalysis corpus,the userinput variable is a string so iterating over it is iterating over the chars what you want to do is remove the for loop and treat userinput as a review assuming it holds review otherwise you could define a separating char between reviews and iterate like so
60116885,python with nltk shows error at senttokenize and wordtokenize,python nltk,this error usually appears when theres a module missing this can be solved by using the download method and specifying the module furthermore you can pass all and just download everything the code would be
60096180,error in loading nltk resources please use the nltk downloader to obtain the resourcenn,python nltk tokenize wordvec,then nltk tokenizer expects the punkt resource so you have to download it first also you dont need a lambda expression to apply your tokenizer function you can simply use
60087408,python pandas and nltk type error int object is not callable when calling a series,python pythonx pandas dataframe nltk,in the first code snippet the way you applied the function to the column is the root of the issue this line caused the problem datawords tweetsapplynltkfreqdisttweets lets say you get this simple dataframe after cleaning up the tweets and want to apply nltkfreqdist to compute word frequencies in each of the tweets the function takes any callable import pandas as pd df pddataframe tweets hello world i am the abominable snowman i did not copy this text the dataframe looks like this now lets find out the word frequencies in each of the three sentences here import nltk define the fdist function def findfdistsentence tokens nltktokenizewordtokenizesentence fdist freqdisttokens return dictfdist apply the function on column dfwords dftweetsapplyfindfdist the resulting dataframe should look like this
59937943,error when using stringpunctuation to remove punctuation for a string,python nltk punctuation,the problem here is that utf has different encodings for left and right quotation marks single and double rather than just the regular quotation mark that is included in stringpunctuation i would do something like this adds the utf values for the nonascii quotation marks to a list called punctuation and then decodes the text to utf and replaces those values note this is python if youre using python the formatting of the utf values will likely be slightly different
59526282,im getting typeerror expected string or byteslike object,python nltk,range produces an iterable of integers so when you feed i into nltkwordtokenize youre feeding it an integer obviously an integer is not stringlike i dont personally know how nltkwordtokenize is supposed to work but based on context clues it would seem you might want to pass the sentence object at the index i instead of just the index i
59499039,problems with invokings functions from module using python and nltk,pythonx nltk,add the following statement to the top of esempiopy full file then issue the following commands in the python interactive shell i copied them from your question good luck
59040636,attributeerror module object has no attribute devnull,python nltk,subprocessdevnull is new in python make sure you use a version of nltk which stil supports python from their changelog
59019104,import error when trying to import nltk module,python nltk,wrong import use this ref
58858431,pythonpath error windows with pyspark when use import lazy like nltk or pattern duplicate label disk ccsparkcorejar,python windows apachespark pyspark nltk,i had the same issue using pytest i do not have a proper solution for malformed path in windows you can apply a quickfix to it as such you will at least get rid of the error
58825230,importerror cannot import name escape from cgi,import tree pycharm nltk,cgiescape has been removed in python quoting from here parseqs parseqsl and escape are removed from the cgi module they are deprecated in python or older they should be imported from the urllibparse and html modules instead since you are importing a thirdparty module try using a lower python version
57426505,saving results to csv file typeerror writerows argument must be iterable,pythonx csv nltk,couple of changes
57228272,typeerror expected string or byteslike object occurred at index when calling processextract,python pythonx pandas nltk fuzzywuzzy,i think you need to change getaltnames to look more like the following version output this code runs but you may still need to modify it to get the exact result you desire specifically im not sure if you want altnames to be a list of lists or just a list
57128735,using pyinstaller with nltk results in error cant find nltkdata,python nltk exe pyinstaller,it seems that it is a known bug to the hook of pyinstaller named nltk an easy way to fix it is to edit this file and comment the lines iterating over nltkdata remember to replace pathtonltkdata with your currrent path for nltkdata
55852234,pyechant and webtext library issue,python nltk libraries,add from nltkcorpus import webtext and then also make sure you download it nltkdownloadwebtext
55833527,dictionary comprehension typeerror builtinfunctionormethod object is not iterable,dictionary tags nltk stopwords dictionarycomprehension,you have multiple issues here first the error you get note that ditems without parentheses is a function and a function is not iterable but ditems with parentheses is an dictitems object which is iterable that means you can call iter on it second your dict comprehension where would the k come from its a key from dlemmaitems note the square brackets positions is incorrent because k does not live out of the list comprehension whereas is correct third the double if can be a if and fourth stick with pep stopwords should be stopwords dfilteredwords should be dfilteredwords this is really important
55806624,why i am getting runtimeerror generator raised stopiteration and how to solve it,python nltk,i was not able to resolve this error not sure why nltkbigramsdoctokeni is generating this but i was able to create bigrams by using the following code
55796607,error tokenizing with nltk from array data in file excel sequence item expected str instance list found,python nltk tokenize,instead of use
55792897,too many values to unpack valueerror while training classifier,python nltk,the reason is pretty simple naivebayesclassifier expects an iterable of tuples comprising a featureset and a label for example in your context the positive word featureset would be something like this accordingly the data you should be feeding to naivebayesclassifier should be of this form however if you look at the wider context of what youre doing i am not sure this really makes sense for a few reasons the principal one is that you actually dont have a way to decide what those scores are in the first place you seem to be doing sentiment analysis the simplest and most common way is to download a pretrained mapping from words to sentiment scores so you could try that the second is that the featureset is meant as a mapping from feature values to labels if you look at the nltk official example the featureset looks something like this the workflow here takes a name generates a single feature from it the last letter and then uses the last letter of each name in conjunction with whether it is male or female the label to determine the conditional probability of a names gender given its last letter on the other hand what youre doing is attempting to decide if a sentence is positive or negative which means that you need simplifying here to tell if each individual word is positive or negative however if so then both your feature and your label mean the exact same thing
55775131,nltk package returns typeerror lazycorpusloader object is not callable,python pythonx nltk virtualenv virtualenvironment,you get an error from first line change your code into
55244152,problem with my forloop combined with yield,python forloop nltk yield,both the extra words i assume are in your dictionary and therefore being yielded a second time after iterations of the for loop because they meet the case when they become w in the lines a redesign of your joinasterisk function seems to be the best way to do this as any attempt to modify this function to skip these would be incredibly hacky the following would be a way to redesign the function so that you can skip words that have already been included as the second half of a word seperated by a if you want this to fit closer to your original function it can be modified to
55201299,word tokenization nltk abbreviation problem,python nltk,the nltk regexptokenize module splits a string into substrings using a regular expression a regex pattern can be defined which will build a tokenizer that matches the groups in this pattern we can write a pattern for your particular usecase which looks for words abbreviationsboth upper and lower case and symbols like etc the regex pattern for abbreviations is azaz the matches the in a forward lookup containing characters in az or az on the other hand the full stop is matched as an independent symbol in the following pattern which is not bound to a positive or negative lookahead or containment in a set of alphabets
55190327,nltk importerror dll load failed the specified module could not be found,python pycharm anaconda nltk conda,its not a nltk problem but rather a sqlite problem error shows that the required sqlite dll file is not found in your system a simple workaround solution would be to download the required dll file as per your system configuration windowslinux x or x accordingly from here and place them at anacondadlls directory make sure anacondadlls is added to your path variables as well
55170966,importerror no module named sqlite,python linux sqlite nltk,i solved this issue by commenting out import sqlite in the panlexlitepy file present inside nltk library folder and also commented out sqlite connection string present inside this file and the code works now this solution will only work if you are intented to use nltk only but not sqlite
55168908,python textblob translate issue,python nltk sentimentanalysis textblob,take a look at the package langdetect you could check the language of the page you are feeding in and skip translation if the page language matches the translation language something like the following
54718017,giving an error when extracting cities from a text using geograpypython,python windows nltk geography,this should be a bug in the package geograpy in geograpyplacespy revise to and the problem of encoding should go away
53920226,python feature extraction attributeerror list object has no attribute lower,python scikitlearn nltk featureextraction,without knowing what the type of combitidytweet actually is this is likely because fittransform expects an iterable of strings and youre supplying it a series combitidytweet should actually be a list of strings for fittransform to work currently it looks like it is a series of a list of strings so your best bet is to concatenate the tokens within each row list into one string package these strings into one list and then use fittransform on it
53910512,clobbererror when trying to install the nltkdata package using conda,pythonx anaconda nltk,answering que first there have been similar issues all across windows machines its better to use the ntlkdownload function if you want to use punkt or a similar module the lookup error can easily be resolved it was because of a typo instead of it should be
53786024,nltk python typeerror module object is not callable,python nltk,you are attempting to call the module ospath when you write ospathusrlocalsharenltkdata path is a module within the os module and you cannot call a module like you can a function you might have meant to call a method within os or ospath
53728305,python error with str and byte code object,python string pythonx byte nltk,you must change your last line return textencodeascii errorsreplacereplace to return textencodeascii errorsreplacereplaceb b because after the encode youre operating on bytes and must replace bytes with other bytes
53698679,unhashable type list error for stopwords,python pandas nltk dataanalysis stopwords,tldr in long please see why is my nltk function slow when processing the dataframe for more detailed explanation on tokenize text in a dataframe remove stopwords other related cleaning processes for better twitter text processing then use this from nltkcorpus import stopwords
53692117,attributeerror list object has no attribute isdigit specifying pos of each and every word in sentences list efficiently,python pythonx list nltk postagger,try this then after do you will get your desired results hope this helped
53685093,expected string or byte like object error,python pandas nltk dataanalysis,lets assume this dataframe then when you run your script you should get an error because you are passing a series and not a string typeerror expected string or byteslike object wordtokenize will accept strings thats why wordtokenizesome text will work so you need to iterate through your series if you still get a type error then most likely not every value in datatext is a string lets assume this dataframe now performing the list comprehension on this dataframe will not work because you are trying to pass an int in wordtokenize however if you change everything to a string it should work you check your types by printtypetexti for i in rangelentext
53593314,python error modulenotfounderror no module named nltk,python pythonx ubuntu nltk,try downcasing it like this from nltkcorpus import stopwords
53528074,nltk making bigrams and trigrams sequentially error doing both at same time,python nltk,filter returns an iterator once you iterate through it it becomes empty you must convert the iterator into a list if you want to use it more than once
53502624,python attributeerror nonetype object has no attribute start,python pythonx jupyternotebook nltk nltkbook,print returns none so after this line matchindex is none try assigning and printing on separate lines result
53486955,when i applied nltk stop words to a data frame it showing an error,python pythonx jupyternotebook nltk nltktrainer,if you want to apply a function elementwise to the dataframe use applymap a simplified example if you want to reassign the values without stopwords into your dataframe use
52866988,python nltk stanford ner tagger error message nltk was unable to find the java file,python nltk stanfordnlp namedentityrecognition,found the solution on the web replace the path with your own or source
52811369,python attributeerror with no isclass when importing pandas,python pandas nltk,i think that you are shadowing tokenize module with cpythonworking scriptstokenizepy could you try and change its name internally inspect imports python builtin module tokenize but it appears that you have tokenizepy next to script you try to run python will search for modules to import in current directory first and in your case it finds one where in reality it requires the one of python builtin modules
52787562,necessary condition to fix weird lemmas,python nltk textanalysis lemmatization,your pennwordnet function assigns the noun pos tag to us although postagus returns us prp this makes wordnetlemmatizer to treat us as a noun you have to add an additional condition to handle personal pronouns
52535788,nltk download not working inside docker for a django service,django pythonx docker nltk,having faced that same problem before and having done almost the same thing you did id assume what youre missing here is configuring the nltkdatapath by adding to the path wherever your osgetcwd is
52315632,python nltk wordnetlemmatizer an error has occurred,pythonx nltk postagger,the purpose of lemmatisation is to group together different inflected forms of a word called lemma for example a lemmatiser should map gone going and went into go thus we have to lemmatize each word separately
52288745,cyclic imports to fix r from pylint,python nltk pylint cyclicdependency,not sure why pylint reports these in the nltknltkccglexiconpy file but the cyclic imports themselves can be seen on the right in the error message taking the first error cyclic import nltk nltkinternals the root initpy has an import from nltkinternals and internalspy has an import from the package root which is obviously a cyclic import
52186716,nltk typeerror unhashable type list,python list tuples nltk hashable,the expected type for hypothesis is liststr from documentation type hypothesis liststr candidate is a listliststr you can compute the bleuscore like this output
52029253,modulenotfounderror no module named ntlk,python pip anaconda nltk,you have a typo in your import code its nltk ie import nltk not ntlk
51427111,importerror no module named nltkclassify,python django dockercompose nltk,nltkclassify is not a package but it is contained in the ntlk package as the command run pip install r requirementstxt will probably fails none of the packages will be installed try to delete nltkclassify from your requirementstxt and try again
51166149,python flask application on ibm cloudbluemix with textblob library throwing exception textblobexceptionsmissingcorpuserror,python flask ibmcloud nltk textblob,it seems that you do not have a nltktxt in the root directory of your deployed app the cloud foundry python buildpacks have builtin support for nltk the text file holds information about which corpora need to be installed during deployment sample content of a nltktxt make sure that it is a single line no duplicates and no strange characters
50838132,movies reviews category error ntlk,python nltk,the following line doesnt make any sense its simply equivalent to writing you can try following and this line will find nothing other than empty string it is trying to produce a substring of pos starting at index which doesnt exist
50784278,modulenotfounderror no module named sentimentmod,python twitter nltk sentimentanalysis,theres no such module in ntlk judging by the name its some private modification of ntlksentiment consult wherever you got this code from how to use it maybe there are some additional requirements or its simply obsolete
50760744,nltk postag error in windows anaconda,pythonx anaconda nltk nltktrainer nltkbook,you need to install nltks corpora ie data your code tries to look up pos tags and tokenize data this should solve your issue reference nltk data edit after nikkis suggestion in case you have had previous installation of nltkdata it will download it to the same location in that case you should do the below that is the conflict which is causing you the issue or set nltkdata environment variable
50622897,stanford ner tagger and nltk not working oserror java command failed,nltk stanfordnlp namedentityrecognition,download stanford named entity recognizer version see download section from the stanford nlp website unzip it and move files nertaggerjar and englishallclassdistsimcrfsergz to your folder open jupyter notebook or ipython prompt in your folder path and run the following python code i tested this on nltk and ubuntults
50442202,nltk error oserror no such file or directory,python anaconda nltk,it should be a format issuetext vs txt as highlighted already as the below steps work fine in my linux systemsee screenshot
50260158,error from importing textblob,python pythonx laravel nltk textblob,i got my answer from this post valueerror could not find a default download directory of nltk what i did is to initialize a nltkdata environment variable within the py script this solved the problem
50104095,getting name error name sentencestream is not defined,python nltk nameerror,i have solved this problem by rewriting the code as
49537804,running stanford ner tagger in pycharm is not working,python pycharm nltk stanfordnlp,nevermind i got it the location for stanfordclassifier and stanfordnerpath were wrong and need to pass first two arguments in standfordnertagger no need of ascii
49482417,how to solve a notimplementederror from nltkclassify classifieri,python nltk,as noted in the comments theres some bad spaghetti like code in the classiferi api that has classify calling classifymany when overriden it might not be a bad thing when considering that the classifieri is strongly tied with the naivebayesclassifier object but for the particular use in the op the spaghetti code there isnt welcomed tldr take a look at in long from the traceback the error is starts from nltkclassifyutilaccuracy calling the classifiericlassify the classifiericlassify is generally used to classify one document and the input is a dictionary of featureset with its binary values the classifiericlassifymany is supposed to classify a multiple documents and the input is a list of dictionary of featureset with its binary values so the quick hack is to overwrite how the accuracy function so that the votedclassifier wont be dependent on the classifieri definition of classify vs classifymany that would also mean that we dont inherit from classifieri imho if you dont need other functions other than classify theres no need to inherit the baggage that classifieri might come with now if we call the new myaccuracy with the new votedclassifier object out note theres certain randomness when it comes to shuffling the document and then holding out a set to test for the classifier accuracy my suggestion is to do the following instead of simple randomshuffledocuments repeat the experiments with various random seed for each random seed do a fold cross validation
49263269,to prevent memory errorhow to one hot encode word list to a matrix of integer in keras using tokenize class,python text keras nltk tokenize,taking a look at the source code the result matrix is created here using npzeros with no dtype keyword argument which would result in dtype being set to default value set in function definition which is float i think the choice of this data type is made to support all forms of transformation like tfidf which results in noninteger output so i think you have to options change the source code you can change add a keyword argument to definition of textstomatrix like dtype and change the line where matrix is created to use another tool for preprocessing you can preprocess your text using another tool and then feed it to keras network for example you can use scikit learns countvectorizer like
49216339,tokenizing sentences from a txt file and getting the expected string or byteslike object error,python nltk tokenize,you need a read command between open and tokens
49097979,valueerror too many values to unpack nltk classifier,python machinelearning nltk naivebayes,nltkclassifier doesnt work like scikit estimators it requires the x and y both in a single array which is then passed to train but in your code you are only supplying it the xtrain and it tries to unpack y from that and hence the error the naivebayesclassifier requires the input to be a list of tuples where list denotes the training samples and the tuple has the feature dictionary and label inside something like you need to change your input to this format note the above for loop can be shortened by using dict comprehension but im not that fluent there then you can do this
49018135,syntaxerror when pip install install nltk,python installation pip nltk,pip is your python package manager it is a command line tool and not a python function object or method this means you cant call pip from within python at least not by just typing pip into a python interpreter pip should come along with your installation of python so you dont need to install it as long as you have python installed you need to call pip from your command line on a mac this would most likely be from terminal so you need to open your terminal type pip install nltk which should install your package then you can start python by using the command python in terminal you can then import nltk using import nltk only once youve followed those steps and successfully installed and imported the nltk package can you use nltkdownload to download nltk data nltkdownload in itself has nothing to do with installing the package i would recommend following a python tutorial such as the one linked in order to gain an understanding of how to use the python interpreter this should explain how to install packages and use basic python functionality
48984000,conda ssl error while installing nltk,python tensorflow machinelearning anaconda nltk,i have found the solution i have modified the condarc file and set the sslverify attribute false it looks like this now i can add those packaged without any problem an alternate way is to modify the configuration file from command line this will edit the file for you wherever it might be located
48872564,i use python and i want to make sentiment analysis but i have an error in nltkmetrics package,python nltk precision metrics sentimentanalysis,no big deal just not calling the correct method try nltkmetricsscoresprecisionreference test ctrlf for precision will get you to documentation corrected code printpos precision nltkmetricsscoresprecisionrefsetspostestsetspos printpos recall nltkmetricsscoresrecallrefsetspostestsetspos
48872128,helpercommand not working after importing nltk,pythonx nltk helper,not directly related but if your plan is to import all of nltk just use import nltk no need for the looking at the nltkhelp module you would need to use one of the functions defined there as nltkhelp itself is not a function but a library location see so if that is the module you want to use try import nltk nltkhelpupenntagset
48768084,nltk unigramtagger typeerror unhashable type list,pythonx nltk,thanks to patrick artners answer i managed to resolved my problem like this
48708135,memory error when training multinomial naive bayes model,python scikitlearn outofmemory nltk,disclaimer this code has many potential flaws so i expect few iterations to get to the root cause parameters mismatch suboptimal word lookup suboptimal feature computing suboptimal feature storage
48521208,nltk sentiment classifier issues with install,python pythonx nltk sentimentanalysis,it is missing some files needed for it to work and no those files arent downloaded when you install the package using pip you can download the repository for the library from and then copy paste the files inside the srcsenticlassifierdata into your librarys directory which is cusersacanacondalibsitepackagessenticlassifierdata directory
48352048,cannot import name defaultdict error for nltk,python python nltk,i can see from the traceback that you have a file called tokenizepy in the current directory rename that file and delete the tokenizepyc so theyre not shadowing the import of the standard librarys tokenize
48340974,how to resolve the error attributeerror generator object has no attribute endswith,python nltk preprocessor wordnet lemmatization,the error message and traceback points you to the source of the problem in preprocessingtext lemmatization lmtzrwordnetlemmatizer tokenslmtzrlemmatizeword for word in tokens preprocessedtext jointokens return preprocessedtext anacondalibsitepackagesnltkstemwordnetpy in lemmatizeself word pos def lemmatizeself word posnoun obviously from the functions signature word not words and the error has no attribute endswith endswith is actually a str method lemmatize expects a single word but here you are passing a generator expression what you want is nb you mentions i believe the error is coming from the source code for nltkcorpusreaderwordnet the error is indeed raised in this package but it is coming from in the sense of caused by your code passing the wrong argument hope this will help you debug this kind of problems by yourself next time
48323393,attributeerror unicode object has no attribute wupsimilarity,python unicode nltk wordnet,the similarity measures can only be accessed by the synset objects not lemma nor lemmanames ie str type out when you get the lemmas and access the names attribute from the synset object you are getting str out you can use the hasattr function to check what objectstypes can access a certain function or attribute out most probably you want a similar function to which maximizes the wupsimilarity across two synsets but note that there are many caveats like prelemmatization that is necessary so i think thats where you want to avoid it by using the lemmanames perhaps you can do this but most probably the results are uninterpretable and unreliable since theres no word sense disambiguation going up prior to synset look up bot in the outer and inner loop
47982117,type error unhashable type list when using python set of strings,python list nltk,words is a list of lists since wordtokenize returns a list of words when you do word for word in words if word not in customstopwords each word is actually of a list type when the word not in customstopwords is in set condition needs to be checked word needs to be hashed which fails because lists are mutable containers and are not hashable in python these posts might help to understand what is hashable and why mutable containers are not in python why is a tuple hashable but not a list what do you mean by hashable in python hashable immutable
47962848,chunkdraw not working in python idle,python nltk,python version do not provide such facility i installed python and now chunkdraw is working fine
47857147,nltk consecutivenpchunker threw valueerror,python nltk,you have an unpacking error it is because you dont have the zip method which takes n number of iterables and returns list of tuples so in your code in def parse methodfunction conlltags wtc for wtc in taggedsents this should be this will yield more than one value to unpack
47741315,showing typeerror unhashable type list while using it in nltkfreqdist function,python pythonx nltk,this error occurs when you are trying to use a list as a dict key lists are not hashable and cant be used for keys eg tuples can be used in your case in this line you are using degree as an index for worddegree that doesnt make sense cause degree itself is a list
47614975,error typeerror str object is not callable python,python python nltk ngram,the code you posted is correct and work with both python and for you have to put parenthesis around the print statement however the code as is has a spaces indent which should be fixed to spaces here how to reproduce your error there must be somewhere where you redefine str builtin operator as a str value like the example above here a shallower example of the same problem
47312432,attributeerrorlinearsvc object has no attribute predictproba,python scikitlearn nltk,according to sklearn documentation the method predictproba is not defined for linearsvc workaround use svc with linear kernel with probability argument set to true just as explained in here
46231574,sentimentanalyser error bytes object has no attribute encode using,python pythonx nltk,from the unicodedatanormalize docs the method is convert a unicode string into a common format string it will get so the issue is here dfstockslocdate articles is not a unicode string
46105180,typeerror expected string or byteslike object with pythonnltk wordtokenize,python pythonx pandas dataframe nltk,the problem is that you have none na types in your df try this
45763803,python nltk parsing error str object has no attribute checkcoverage,python pythonx parsing nltk,going by this link you might want to parse those rules first using nltkparsecfg
45711104,tokenize not working with any string input,python string nltk typeerror tokenize,its the problem in nltk package itself as in the picture it is not the parameter passed in but literal in nltkdatapy which is considered to be list and converting to string reinstall nltk package may help show the th line of nltkdatapy it should be path
45622509,nltk json data loading error,python json pythonx nltk,after hours of research it turns out nltk does not accept json files if the highest order is a list rather than a dict in order to access the data the upper most structure must be a dictionary structure with keys this allows one to access the first element of the list which includes a dictionary inside similar to every other element of the list one solution is to seperate the elements of the list and load the dicts one at a time i also suspect that while encoding the json sortkeys true might make the upper most structure a dictionary
45588980,error in calling a function in python function not defined,python pythonx function nltk,since splitpun is an instance method in the urducorpusreader class you need to call it from an instance splitpunword should be wordlistssplitpunwordjust like you used wordlistswordsinfile the line above it
45407786,error when importing the imaplib package in python,python nltk python pythonimport imaplib,pythons imaplib doesnt depend on the nltk but it looks like you have a script called tokenizepy in your directory which is imported instead of an expected dependency look carefully at the paths in the trace you included
45342388,http error forbidden when using nltk,python nltk,as alvas suggested on this page this is an easy fix getting error while trying to download nltk data
45342305,http error forbidden when downloading nltk data,pythonx nltk,the problem is coming from the nltk download server if you look at the guis config its pointing to this link if you access this link in the browser you get this as a message so i was going to file an issue on github but someone else already did that here a workaround was suggested here based on the discussion on github it seems like the github is downblocking access to the raw content on the repo the suggested workaround is to manually download as follows people also suggested using an laternative index as follows
45143097,issue with installing python newspaper package,python pycharm nltk pythonnewspaper,according to newspapers github documentation page they have deprecated the python version of the api and have termed it as buggy the exact statement as follows reference it shows that we should not use the python version anymore for now i switched to python for this specific use case
44815059,using nltkwordtokenize generates error expected string or byteslike object in pandas data frame,python pandas nltk,first print second print by the way if you used inplacetrue explicitly you dont have to assign it to your original df again
44752571,wordnetlemmatizer error all alphabets are lemmatized,python pandas nltk wordnet lemmatization,tldr lemmatizer takes in any string as an input if datasetcontent columns are strings iterating through a string would be iterating through the characters not words eg so you would have to first word tokenize your sentence string eg another eg but to get a more accurate lemmatization you should get the sentence word tokenized and postagged see
44690399,importerror no module named simplify,python nltk,it used to be in nltk not in the newer version see the docs here also look at this as well
44539224,using findall method in a tokenized text and prefix r,python regex nltk,in this particular case r makes no difference as this string does not contain any sequences which could be misinterpreted however it is a good habit to use r when writing regular expressions to avoid misinterpretation of sequences like n or t with r they are treated literally as two characters backslash followed by a letter without r they evaluate to newline and tab respectively
44390198,if form in exceptions typeerror unhashable type list in python nltk,python nltk tokenize lemmatization,seems like you have the variables mixed up it should be
44306860,error while predicting sentiment analysis tensorflow nltk,python python tensorflow nltk sentimentanalysis,looks like your features are of the wrong shape please try this your model accepts batch data so if you want to run just one prediction you need to reshape it as a batch of good luck
44238864,importerror cannot import name punktwordtokenizer,python nltk importerror,punktwordtokenizer was previously exposed to user but not any more you can rather use wordpuncttokenizer the difference is punktwordtokenizer splits on punctuation but keeps it with the word where as wordpuncttokenizer splits all punctuations into separate tokens for example given input thiss a test
44078259,import of nltk version fails with importerror,python python windows nltk,in the latest version of nltk v theres an issue with optional dependency see the importerror would happen in any os windows linux mac since its a python dependency issue this is due to the additional dependency that nltkparsecorenlp needs but it isnt elegantly imported and the imports were exposed at the top level at to install nltk with requests to patch this problem for fuzzfree installation installs all packages that all nltk submodules would require alternatively you can install the request package separatedly hopefully issue gets resolved soon and a minor patched version of the release will be rereleased soon
43769655,error installing nltk on win,python nltk,after three days of research finally i have solved the problem so for anyone else to safe you the time i invested do the following steps install anaconda bit after sucessfull installation run python and use the following command import nltk run the command nltkdownload dont forget the parantheses
43727583,resub erroring with expected string or byteslike object,python pandas regex typeerror nltk,as you stated in the comments some of the values appeared to be floats not strings you will need to change it to strings before passing it to resub the simplest way is to change location to strlocation when using resub it wouldnt hurt to do it anyways even if its already a str
43535987,issue with geography library in python,python pythonx nltk,there is a python versionfork geograpy for install instructions check
43442372,error installing nltk python,python pythonx nltk,nltk itself is os independent but the windows msi installer is not its specifically for bits pythons alternatively you can use pip to install nltk which will install the os independent source file simply in cmd type this
43292867,ignore unicodeencodeerror when saving utf file,python pythonx pythonrequests nltk,decoding and reencoding a file just to save it to disk is pointless just write out the bytes you have downloaded and you will have the file on disk this is the only reliable way to write to disk exactly what you downloaded sooner or later youll want to read in the file and work with it so lets consider your error you didnt provide a full stack trace always a bad idea but your error is during encoding not decoding the decoding step succeeded the error must be arising on the line filewriteraw which is where the text gets encoded for saving but to what encoding is it being converted nobody knows because you opened file without specifying an encoding the encoding youre getting depends on your location os and probably the tides and weather forecast in short specify the encoding
43270972,unicodedecodeerror ascii codec cant decode byte in textranking code,python nltk summarization,please try if the following works for you
43251888,error when extract nounphrases from the training corpus and remove stop words using nltk,python nltk stopwords,you are getting this error because the function wordtokenize is expecting a string as an argument and you give a list of strings as far as i understand what you are trying to achieve you do not need tokenize at this point until the printall noun phrasenouns you have all the nouns of your sentence to remove the stopwords you can use of course in this case you have the same result with nouns because none of the nouns was a stopword i hope this helps
43216610,nltk naive bayes classifier training issues,python nltk sentimentanalysis naivebayes nltktrainer,there is a typo in your code featureset findfeaturesallwords sentiment for allwords sentment in documents this causes sentiment to have the same value all the time namely the value of the last tweet from your preprocessing step so training is pointless and all features are irrelevant fix it and you will get
43194726,naive bayes for text classification python data structure issue,python datastructures scikitlearn nltk naivebayes,thanks to help from both vivek lenz who explained to me the problem i was able to reorganise my training set and thankfully it now works thanks guys the problem was very well explained in viveks post this is the code that reorganised the train data into the correct format
43169079,nltk is not working in docker,docker nltk dockercompose,final dockerfile final dockercompose
42908625,nltk download error permission denied mac,nltk,your permissions on this file are wrong you either need to execute the script with sudo or preferably change the permissions by running this command also if you want to change all of the permissions in the directory you can run
42870572,nltk typeerror not supported between instances of str and int,python nltk,my best guess is that npversion the value referenced at the bottom of the stack trace has become corrupted somehow and is now a tuple of strings or some other datatype that is not a tuple of ints which is what the code is comparing it against in the line if npversion i would suggest reinstalling nltk though that might not be a definite fix its possible that a version mismatch has occured in which a newer version uses a tuple of strings to store the version number if this is so it might be a good idea to try to install an older version of nltk though its possible a reinstall will fix the problem hope this helps
42819535,nltk plaintextcorpusreader sents and paras functions not working,python pythonx nltk,youre missing a data file resource needed by the sentence tokenizer fix the problem by downloading the punkt resource under models in the interactive downloader or noninteractively by running this code once to avoid running into this kind of problem repeatedly as you explore the nltk i recommend downloading the book bundle now it contains everything youre likely to need for a while
42690716,error using nltk wordtokenize,python nltk,you have to convert html which is obtained as byte object into a string using decodeutf
42677143,python natural language processing vadersentiment import sentiment error,python text import nltk package,vadarsentiment does not have sentiment module the import should be source
42638098,programming a discord bot in python ran into the error string index out of range,pythonx nltk markovchains discord,the fact that wordisdigit throws the error implies that word is an empty string the most likely cause of this is that your regex split produces empty strings on occasion the solution would be to after put the line
42428390,nltk french tokenizer in python not working,python nltk tokenize,tokenizertokenize is sentence tokenizer splitter if you want to tokenize words then use wordtokenize reference
42370497,nltk unknown url error,python nltk,the data loader is mistaking the c prefix in your path for a protocol name like i thought this had been fixed already to avoid the problem add the file protocol at the start of your path eg there are better ways to structure your code but thats up to you technically this is not a bug since nltkdataload expects a url not a filesystem path but really it ought to be fixed its not that hard to handle windows paths
42321701,nltk stopwords returns error lazycorpusloader is not callable,python nltk typeerror stopwords,try
41850809,getting error while trying to use ground method in nltkcontribtimex,python nltk,the timex module has a bug where a global variable is referenced without assignment in the ground function to fix the bug add the following code which should start at line def groundtaggedtext basedate
41522842,nltk error when installing using pip command on ubuntu,ubuntu pip nltk,take a look at the last line ioerror errno permission denied try it again but put sudo at the front sudo pip install nltk
41461599,indexerror string index out of range errors,python pycharm nltk,the data that you are processing is different on the two machines there is probably a word in the data set which has only one letter on your machine
41348621,ssl error downloading nltk data,python macos ssl sslcertificate nltk,you dont need to disable ssl checking if you run the following terminal command in the place of put your version of python if its an earlier one then you should be able to open your python interpreter using the command python and successfully run nltkdownload there this is an issue wherein urllib uses an embedded version of openssl that not in the system certificate store heres an answer with more information on whats going on
41047362,python networkx error module networkxdrawing has no attribute graphvizlayout,python nltk graphviz networkx,the answer is courtesy bonlenfum and code from heres the code that worked and heres the adjusted code from the nltk book
40893874,encoding issue using nltk,python python encoding nltk stopwords,
40883333,typeerror list object is not callable,python nltk,well to start debugging i would run the program by using this will launch a pdb debugging console once there press c to run the program after awhile supposing the program crashes youll be stopped at the exact location where the error occurred from there you can refer to this or this just google python pdb to figure out exactly whats going on in your code good luck
40458145,elementtreeparseerror while downloading nltk corpus,python nltk elementtree centos,lets see your downloader opens the xml document that lists the available downloads tries to parse it and gets an error either very unlikely the nltk site is no longer compatible with python or youre not actually receiving the expected xml document because theres something wrong with your connection are you behind a proxy if not something else is probably wrong with your connection
40361488,importerror no module named nltktokenize nltk is not a package,python pycharm nltk,this usually happens because you have another file called nltkpy check your directory cpython where you are running this script and remove or rename it if its there i suppose the stray nltkpy might be somewhere else on your pythonpath too
40359272,nltk unicodedecodeerror connected with the ntpathpy file,python python unicode nltk textblob,i have found a solution i tried to insert this part into ntpathpy so here is the part of the code in this file it works perfectly if you have another language in your system settings play with them and replace cp
40346561,keyerror documentclass,python nltk,your frequencies dictionary is empty you get key error right from the start that is expected i suggest that you use collectionscounter instead it is a specialized dictionary a bit like defaultdict which allows to count occurrences edit that answers your question without using ntlk package at all i answered just like nltk package was just a string tokenizer so to be more specific and allow to go further in your text analysis without reinventing the wheel and thanks to the various comments below you should just do this youll get with my text so not a dictionary with word number of elements directly but a more complex object bearing this information a lot more frequencydistitems you get the wordscount and all classical dict methods frequencydistmostcommon prints the most common words frequencydistthe returns the number of occurrences of the
39805675,valueerror could not find a default download directory of nltk,python nltk,problem nltk package tries to find the osenvironappdata variable to load its contents xampp or any other cgi server doesnt load all of the os variables which are generally available on windows hence we must explicitly provide the appdata set variable this can be done via methods solution inside python itself before loading anything from nltk package by adding the appdata folder path import os osenvironappdata rcusersyouruserappdataroaming set the environment variable in xampps file by adding this line to it setenv appdata appdata
39622121,nltk perceptron tagger typeerror lazysubsequence object does not support item assignment,classification nltk anaconda python perceptron,debugging doing some greping in the nltk source code found the answer in the file sitepackagesnltkutilpy the class is declared after another quick test from the interpreter i see the following details about the type of the taggedsentences i see in the file sitepackagesnltkcorpusreaderutilpy a final test with the random package proves the problem exists in the way i am creating the taggedsentences solution to work around the error just explicitly create a list of the sentences from the nltkcorpusbrown package then random can shuffle the data properly now the tagging works as desired
39525684,unicodedecodeerror while reading a custom created corpus in nltk,python characterencoding nltk,you need to understand that in pythons model unicode is a kind of data but utf is an encoding theyre not the same thing at all youre reading your raw text which is apparently in utf cleaning it then writing it out to your new corpus without specifying an encoding so youre writing it out in who knows what encoding dont find out just clean and generate your corpus again specifying the utf encoding i hope youre doing all this in python if not stop right here and switch to python then write out the corpus like this
39321495,attributeerror list object has no attribute isdigit,python nltk postagger,you can actually pass series object to the postag method directly prints
39316345,error while stemming arabic text in file using isristemmer,python nltk arabic,try decoding the lines from the file to unicode before passing it to the stemmer i am assuming that your input file is encoded as utf seems likely looking at the error however you can change the encoding as suits alternatively you can use ioopen specify the encoding and python will decode the incoming stream to unicode
39187042,nltk cfg recursion depth error,python nltk,the function generate as its docstring states generates an iterator of all sentences from a cfg clearly it does so by choosing alternative expansions in the order they are listed in the grammar so the first time is sees an np it expands it with the rule np np pp it now has another np to expand which it also expands with the same rule and so on ad infinitum or rather until pythons limits are exceeded to fix the problem with the grammar you provide simply reorder your first two np rules so that the recursive rule is not the first one encountered do it like this and the generator will produce lots of complete sentences for you to examine note that the corrected grammar is still recursive hence infinite if you generate a large enough number of sentences you will eventually reach the same recursion depth limit
39177769,error in data pickle in python,python machinelearning nltk pickle naivebayes,you can use pickle you can save your model like this and for loading later
38997459,permission error when installing textblob,python python pip nltk,try with permission
38927230,panda assertionerror columns passed passed data had columns,python pandas dataframe nltk azuremachinelearningservice,try this
38768052,recursion error maximum recursion depth exceeded,python recursion scikitlearn nltk stemming,your error is in analyzer superstemmedcountvectorizer selfbuildanalyzer here you are calling the function buildanalyzer before the super call which cause a infinite recursive loop change it for analyzer superstemmedcountvectorizer selfbuildanalyzer
38617910,python nltk word tokenize unicodedecode error,python nltk pythonunicode,looks like this text is encoded in latin so this works for me you can test for different encodings by eg looking at the file in a good editor like textwrangler you can open the file in different encodings to see which one looks good and look at the character that caused the issue in your case that is the character in position which happens to be an accented word from a spanish review that is not part of ascii so that doesnt work its also not a valid codepoint in utf
38488431,how with python the nltk wordnet can avoid a nondescript error message,python nltk wordnet,im not sure what your code really looks like or what you are trying to do but the nltk wordnet howto shows how to create a synset if you already know its identifier if this doesnt clear things up for you please edit your question and add some actual python code that creates the synset that gives you problems
38420922,receiving indexerror string index out of range when using apply,python nltk,your input contains two or more consecutive spaces in some places when you split it with xsplit you get zerolength words between adjacent spaces fix it by splitting with xsplit which will treat any run of consecutive whitespace characters as a token separator
38392407,typeerror must be unicode not str in nltk,python nltk crf,in python regular quotes or create byte strings to get unicode strings use a u prefix before the string like udfd to read from a file youll want to specify an encoding see backporting python openencodingutf to python for options most straightforwardly replace open with ioopen to convert an existing string use the unicode method though usually youll want to use decode and supply an encoding too for much more details ned batchelders pragmatic unicode slides are recommended if not outright obligatory reading
38363747,python unicodedecodeerror for a file in utf language is ang old english,python pythonx utf nltk,your conversion technique didnt work because you never read and wrote the file back out again x is not a valid byte in utf or any iso character set it is valid in windows codepages but only unicode can support old english characters so you have some very broken data to convert utf with bad bytes do if you dont care about losing data you may get away using the encoding argument on mtecorpusreader which will make x a euro symbol
38358511,nltk error using php,php python nltk,nltkdownload should be run from a python shell which was started with root permissions on debian for example su yourrootpassword python and nltkdownload you should be able to download everything there the import for nltk is not enough to use it you need to download all the corpora and tokenizers to make use of it
38220624,attributeerror list object has no attribute text,python attributes nltk,you dont show wherehow you created the function wordsonly but the self argument indicates that you patterned it on a class method youre evidently using it as a standalone function like this i would advise you not to tackle classes yet if you can avoid it write your function like this you should also know that your function is designed to process one string not a list of them in addition dont use builtin names like list as variable names youre asking for a very confusing error in a day or two use a more informative name or an abbreviation like lst since you actually want to work with the list of lines apply the revised function to each one like this if you had wanted to work with the entire contents of the file you would read in your text like this
37239532,wordnet synset strange list index out of range error,python nltk wordnet,most likely wnsynsetsplacementitem a returned you an empty list this can happen if placementitem isnt in wordnet therefore when you did iterationset it throws an out of range exception instead you can change your check to be instead of
37187500,nltk sentence tokenizer gives attributeerror,python pythonx nltk tokenize textmining,the line thats giving you trouble is correct thats how youre supposed to use the sentence tokenizer with a single string as its argument youre getting an error because you have created a monster the punkt sentence tokenizer is based on an unsupervised algorithm you give it a long text and it figures out where the sentence boundaries must lie but you have trained your tokenizer with a list of sentences the first elements in commentslist which is incorrect somehow the tokenizer doesnt notice and gives you something that errors out when you try to use it properly to fix the problem train your tokenizer with a single string you can best join a list of strings into one like this ps you must be wrong about it working successfully when you tokenized a literal string certainly there were other differences between the code that worked and the code in your question
37155210,nltktwitter is giving error,python nltk twitteroauth dataanalysis,there is a few things that may have caused this but i bet it is the time issue as nltk is trying to use the streamer and the time of your computerserver is out of sync also make sure you install nltk completely try
37148550,nltk api to stanford postagger works fine for ipython in terminal but not working in anaconda with spyder,python nltk stanfordnlp anaconda spyder,the problem has nothing to do with python or the nltk its a consequence of how os x starts gui applications basically the classpath environment variable is set in your profile or its kin but this file is only executed when you are starting terminal gui applications inherit their environment from your login process which doesnt know classpath there are numerous so questions about how to deal with this see here or here but in your case there are also a couple of workarounds that ought to work start spyder from the terminal command line not via the launchpad just type spyder or your python program can also set its own environment which will be inherited by child processes prior to launching the stanford parser like this
37059269,attributeerror list object has no attribute copy,python list nltk,the listcopy method does not work both in python x and python x i wonder why it is still in the documentation to achieve the results of copying a list user the list keyword optionally you can copy a list by slicing it
36884334,ioerror when loading nltk perceptron tagger,python nltk ironpython,i had to dig into the code and finally found the problem nltk determines the operating system with if sysplatformstartswithwin extremely professional way to determine by the way however if you are using ironpython your platform is cli i suspect this is causing lots of problems for ironpython users so next time any python package acts like its unix counterpart just check modules for this code edit my fix for it is to replace the check code with sysplatformstartswithwin or sysplatformstartswithcli
36739843,celery raised unexpected lookup error,django celery nltk javahome postagger,my environment is mac the problem is the javapath should be
36727005,python loaded nltk classifier not working,python nltk pickle sentimentanalysis naivebayes,the most likely place a pickled classifier can go wrong is with the feature extraction function this must be used to generate the feature vectors that the classifier works with the naivebayesclassifier expects feature vectors for both training and classification your code looks as if you passed the raw words to the classifier instead but presumably only after unpickling otherwise you wouldnt get different behavior before and after unpickling you should store the feature extraction code in a separate file and import it in both the training and the classifying or testing script i doubt this applies to the op but some nltk classifiers take the feature extraction function as an argument to the constructor when you have separate scripts for training and classifying it can be tricky to ensure that the unpickled classifier successfully finds the same function this is because of the way pickle works pickling only saves data not code to get it to work just put the extraction function in a separate file module that your scripts import if you put in in the main script pickleload will look for it in the wrong place
36669895,sitepy attributeerror module object has no attribute moduletype upon running any python file in pycharm,python pythonx intellijidea pycharm nltk,there is a file named typespy in your current directory this file shadows the types module that is part of the standard library the nltk program very indirectly tries to import types getting your module which lacks a moduletype attribute causing an importerror which never gets caught and produces that traceback
36538591,nltktokenize executing properly from shell but getting error as a script file,python nltk,this would happen if you would name your python script nltkpy rename your script
36288024,categorizedplaintextcorpusreader how to specify categories with regex nonetype object has no attribute group error,python regex nltk,you need to look through the files that contain pos or neg only where is a nongreedy match for any characters any number of times negpos is a capturing group it has to be capturing for the category extractor to work that would match either neg or pos works for me
36121123,python nltk align import error,python nltk pythonimport pythonmodule,as of nltk version the align module has been renamed to translate therefore use
36038827,import nltk ununderstandable error,python centos nltk python dictionarycomprehension,since version nltk drops supports for python nltk released october add support for python drop support for python sentiment analysis package and several corpora improved pos tagger twitter package multiword expression tokenizer wrapper for stanford neural dependency parser improved translationalignment module including stack decoder skipgram and everygram methods multext east corpus and mtecorpusreader minor bugfixes and enhancements for details see since dictionary comprehension is a feature from python using nltk will lead to error when a dictionary comprehension occurs strongly encouraged to upgrade to python or using conda would simplify the problem too but if python is really necessary
35992743,syntaxerror unexpected eof while parsingfor python and nltk,python nltk jupyter,python is looking for the rest of the compound try statement eg a finally or except block as you didnt provide one python complains about this with no other blocks at a lower indentation level it only knew for certain the rest was missing when the parser reached the end of the file because the parser was expecting to find another part of the statement finding an eof end of file instead is unexpected
35861482,nltk lookup error,python python nltk,use to install the missing module the perceptron tagger check also the answers to failed loading englishpickle with nltkdataload
35827859,python nltk postag throws urlerror,pythonx nltk,edited this issue has been resolved from nltk v upgrading your nltk would resolve the issue eg pip install u nltk you might be using nltk verion downgrade it to version and it will work fine i myself used the undermentioned method right now and the url error is gone seems like an issue with nltkversion browse to this directory on your computer basically the aim is to go into sitepackages directory which holds the installed packages search and delete these files and directories after deleting force install older version as
35795472,tagger perceptron tagger traceback error,python python nltk,change the nltk version to and delete previous avareage perceptron tagger in nltkdownload and install it again
35780684,error when using python textblob library tagger,python python nltk textblob,this seems to be a problem with nltk version until its fixed in the release you can use this hack nltk v unable to nltkpostag
35741627,memory error when train tbl pos tagger in python,python memory nltk postagger,this happened because your pc doesnt have enough ram when you train your large corpus it requires a lot of memory install more ram then you can get it done
35740529,persistent import error for nltk corpus twittersamples,python python twitter nltk spyder,the command from nltkcorpus import twittersamples is correct according to the nltks twitter howto so the most likely reason for the import error is that your version of the nltk is out of date the nltks twitter package is quite new it was added in september with version but improved in various ways since then right now the nltk is at version but the current anaconda distribution comes with nltk which one do you have you can check the nltk version by printing out nltkversion to update to the latest version distributed by anaconda start an anaconda command prompt and run this command anaconda updated to the current version of the nltk within days of its release so i would expect them to continue to do so with future versions unless some compatibility issue arises
35529215,valueerror with nltk,python nltk,instead of this do this
35525598,error message claims that i am passing two arguments into a function that takes only one,python nltk,the first argument to a classmethod should be cls which will be filled edit by the python runtime not you with the specific class on which the method is being called to be pythonic though these look like they really could be modulelevel functions since they are not using the mechanisms of a class in any way
35315068,q can a machine learning model solve rulebased problems,machinelearning nltk wordvec,got too long for a comment yes it can however it is freakingly complicated this kind of reasoning and analysis is done by watson for example ibm is calling these cognitive computing as you wrote rule based or logical reasoning systems can solve such tasks so the question you should ask yourself is how you can extract the required facts from text nlp part of speech named entity however the task is extremely hard because not more then times a day is not contradicting the sentence so reasoning would require rich background knowledge as said it is an extremely broad topic you would have to sketch the solution and then pick a tiny piece which would be called a phd thesis which is illustrated in this nice image so looking with the right keywords for phd thesiss turned up this one might provide you a few nights of reading if you want to try something hands on with nltk i would generate parse trees for the sentences you want to analyse afterwards you could try to align these and check for overlaps and deviations however im not sure how to draw conclusions a slightly simpler version would be to match word by word something along levenstein distance calculations
35273090,how to solve iteration issue on data set in python when using wordnet,python python nltk,according to the documentation lemmanames is a method you should call it in order to retrieve names
35100967,nltk cant using importerror cannot import name compat,python twitter nltk,youre trying to import the entirety of nltk when you probably only need a small portion try editing your code to
34960312,typeerror classify missing required positional argument featureset,python ubuntu nltk sentimentanalysis,i suspect you have not trained your classifier note the following error you need to train it first then you can classify features
34640116,nltk ngrammodel error,python nltk,this is an open issue because of bugs this is noted in the issue if youre currently using the version from github you can switch to the model branch which includes the ngrammodel code though its currently significantly behind the develop branch and hasnt picked up all the newest bug fixes the link to the model branch is here
34574340,getting attributeerror while printing stopwords from nltk toolbox in python,python nltk sentimentanalysis stopwords,as mentioned in the comment try that should fix the error
34055347,unicodeencodeerror ascii codec cant encode character uxa in position ordinal not in range,python unicode utf nltk,e or url is a unicode string which makes the resulting string passed to logdebug a unicode string examples likely logdebug doesnt expect unicode strings so python implicitly converts it to a byte string using the default ascii codec the unicode string generated isnt asciicompatible hence unicodeencodeerror
33969173,attributeerror wordnetcorpusreader object has no attribute getsynsetsfromword,python nltk arabic wordnet,this arabic wordnet has no relation to wordnet but the name wordnet has not that function wngetsynsetsfromword you should download arabic wordnet from here
33398282,attributeerror module object has no attribute scores,python nltk,in short in long this is tricky the issue occurred because of how nltk was packaged if we look at dirnltkmetrics theres nothing inside it other than alignmenterrorrate btw in the bleeding edge version of nltk alignmenterrorrate has been moved to nltktranslatemetrics see the nltktranslate package is a little unstable because its still underdevelopment going back to the metrics package from we see this basically this means that the functions from the metrics package has been manually coded and pushed up to nltkmetricsinitpy so if the imports stop here dirmetrics would have listed all the metrics imported here but because on the higher level at nltkinitpy the packages was imported using now all metrics score has been imported to the top level meaning you can do but you can still access any intermediate level modules that are within nltkmetrics that are not imported in nltkmetricsinitpy but you have to use the correct namespaces as how the functions are saved in their respective directory note that these will not show in dirnltkmetrics but are valid ways to import a function
33068269,attributeerror module object has no attribute logicparser with nltklogicparser,pythonx nltk,it sounds like youve spotted the problem but just in case you are reading the first edition of the nltk book but evidently you have installed nltk which has many changes look at the current version of chapter for the correct usage
32930545,attributeerror cant set attribute from nltkbook import,python nltk python,i am not sure if you worked our your issue just in case same issue also reported here solution i was able to fix this problem by going into the internalspy file in the nltk directory and removing the line parsedpatternpatterngroups my rationale behind this was after doing a bit of code reading the original version of sreparsepy that nltk was designed to work stored groups as an attribute of an instance of the sreparsepattern class the version that comes with python stores groups as a property which returns im not too familiar with properties but this is what i presume it does the length of a subpattern list the code im talking about is here at about line what i dont know is what the long term effects of doing this will be i came up with this solution just by tracing through the code i havent looked at what bugs this may cause in the long run someone please tell me if this would cause problems and if theres a better solution the above works for me without any issues so far
32616359,index error in loop to separate body of text by speaker in python,python nltk,break the text into paragraphs with resplitrnsn text then examine the first word of each paragraph to see who is speaking and dont worry about the nltk you havent used it yet and you dont need to
32469712,unicodedecodeerror ascii codec cant decode byte nltk,python python nltk,provide an explicit encoding when you open your file you said its utf so tell python
32446892,the function bigrams in python nltk not working,python nltk,the function bigrams has returned a generator object this is a python data type which is like a list but which only creates its elements as they are needed if you want to realise a generator as a list you need to explicitly cast it as a list
32332134,i am getting an index error as list out of range i have to scan through many lines,python indexing nltk,just add a list length check would solve the problem
31868545,python not working with nltk,python nltk,on mac os x on ubuntu then
31482620,decoding error in paths using nltkcorpusgutenbergfileids,python python nltk anaconda,im guessing python nltk has some issues with nonascii paths using python is probably the simplest fix here at least assuming you dont have too much code that doesnt work in it its hard to say for sure since you didnt include the full traceback but likely nltk would have to be patched to fix this for python otherwise you would need to avoid paths with nonascii characters meaning avoiding your user directory or changing your username
31295957,nltkwordtokenize giving attributeerror module object has no attribute defaultdict,nltk attributeerror defaultdict,i just checked it on my system fix then everything worked fine
31270361,why shows error import nltk,python python nltk,try to install the package in your computer ubuntu
31078817,nltk unicodedecode error,python nltk,i believe this problem is fixed using nltk and the lastest maxenttreebankpostagger model to install nltk use make sure the pip you are calling is for python once nltk is installed open the python interpreter type and use the gui to install maxenttreebankpostagger its located under the models tab
30908903,lookuperror from nltkbook import,python import ipython nltk anaconda,your missing the gutenberg corpora in nltkbook hence the error the error is self descriptive you need to use nltkdownload to download the corpora once the corpora is downloaded rerun your command and check if the error comes up again if it does it would be for another corpora download that corpora too from nltkbook import is not the preferred method it is advisable to only import the corpora which you would be using in your code you could use from nltkcorpus import gutenberg instead see reference on link
30577389,typeerror init got an unexpected keyword argument shuffle,python nltk,here is the source for your version of sklearn ive linked to the actual line for the init on stratifiedkfold which shows that there is no shuffle keyword argument upgrade to v which does have shuffle as seen here im going to assume that your version of sklearn on py is
30577199,valueerror when using a variable to call a function,python variables nltk,this does not have anything to do with using variables or not the problem seems to be here you execute this part of code only when lenword ie you do randint you probably just mistakenly used instead of or shorter
30331801,python importerror cannot import name abstractlazysequence,python nltk importerror,we determined that this was being caused by a namespace conflict with nltktokenize and the users tokenizepy after renaming tokenizepy everything worked properly
30283217,unicodedecodeerror unexpected end of data while stemming over dataset,python unicode pandas nltk stemming,read up on unicode string processing in python there is the type str but there is also a type unicode i suggest to decode each line immediately after reading to narrow down incorrect characters in your input data real data contains errors work with unicode and u strings everywhere
30194886,unicodedecodeerror for medieval characters,python unicode encoding utf nltk,your file isnt valid utf maybe its partly utf and partly some other junk you could try to replace nonutf sequences with question marks instead of raising an error which might give you a chance to see where the problem lies in general if you have a mix of encodings in a single file youre pretty much doomed as they cant be reliably picked apart
30008555,bad zip file error while using nltk pos tagger,python nltk,try these the first two are for pos tagging and named entities respectively the third youre not using in your code sample but youll need it for nltksenttokenize which breaks up plain text into sentences since youll be working with pos tags id also download these theyre tiny if you do have a bit of space downloading the entire book collection will give you everything you need to explore the nltk
30003957,error when accessing synonyms in python using nltk,python python nltk,in nltk the lemmanames has been changed to a method from an attribute so you have to call the method other minor changes required are synonymlist is not defined list will not have an add method even if synonymlist is defined you better name your variable synonymset
30002972,nltk cant open files unicodedecoreerror,python nltk,very likely to be a file encoding issue since i cant see your code or the file i suggest you try specify an encoding when you open the file before passing it to nltk
29582351,how to solve the unicodedecodeerror when using stanford parser api in nltk for python,python unicode characterencoding nltk stanfordnlp,if the osenviron or export paths are set properly as described in this stanford parser and nltk then it should be an issue of specifying the encoding in the nltk api and the encoding of your input string so the solution would be update nltk to the latest stable version ie sudo pip install u nltk use python or specify the encoding for your string if youre somehow unable to update your python or nltk then specify the encoding when using stanford api in nltk because of specify the encoding for your string see how to output nltk chunks to file it is strongly recommended that you use python especially when handling text inputs if all else fails and you only have the old version of nltk and you must somehow use py then see six docs
29262678,python nltk importerror,python nltk importerror,according to the documentation it looks like it should be see
28791328,nltk pos tagset help not working,nltk postagger,as nltk is telling you it searched for the file helptagsetsupenntagsetpickle in the directories and could not find it is it there if not use nltkdownload to get it and make sure its in one of these directories
28526888,exception error while installing nltk in ubuntu and python,python python ubuntu nltk,most likely youve cut and pasted this line from some web page or pdf the web page probably changed the ascii hyphen minus sign into a u hyphen therefore you need to type the command as do not cutandpaste the error message implies that line is the string xexxu this utf encoded string looks like a hyphen and a u but note that the hypen is not the usual ascii hypen chr rather it is the utf encoded u hyphen as opposed to the usual hyphen chr which is the utf encoded ud hyphenminus
28509263,nltk import error,windows python bit nltk,i solve this by changing my operating system locale for nonunicode programs go to control panel click clock language and region click regional and language options go under administrative tab current system locale for nonunicode is displayed to change it click change system locale then just choose english after a reboot done
28323575,errors nltkgaacdemo is run,nltk,this appears to be an nltk module compatibility issue between python x and x i explain below you can hack a solution which is in the last section explanation on my machine in python nltkgaacdemo yields whereas in python i see the exact behaviour op reports for python i have raised a bug report with the nltk developers here this blog on migrating python to python notes that unorderable types cmp and cmp under python the most common way of making types sortable is to implement a cmp method that in turn uses the builtin cmp function since having both cmp and rich comparison methods violates the principle of there being only one obvious way of doing something python ignores the cmp method in addition to this the cmp function is gone this typically results in your converted code raising a typeerror unorderable types error so you need to replace the cmp method with rich comparison methods instead to support sorting you only need to implement lt the method used for the less then operator solution to get yourself going add a lt function to the dendrogramnode class open cpythonlibsitepackagesnltkclusterutilpy in an editor of your choice find the line class dendrogramnodeobject line in my installation add a less than function so your code looks like my version of the hacked utilpy file is available as a github gist
28265862,processing a corpus so big im getting runtime errors,python runtimeerror nltk pickle sentimentanalysis,you could use csvfieldsizelimitint for example you can try changing the value to something better maybe on the comment about pandas also you might want to check out cpickle here x faster check out this question answer too another relevant blog post here
28254261,unicodeencodeerror how to encode xml tree parsed with elementtree,python xml encoding utf nltk,so twotext should be a unicode string and you want to print it why not just check and then if appropriate without the laborious stringification if your terminal is properly set it will tell python which encoding to use to send it bytes properly representing that string for display purposes its usually best to work uniformly in unicode thats why str has become unicode in python and only decode on input encode on output and often the io systems will handle the decoding and encoding for you quite transparently depending on your version of python which you dont tell us you may need to do some explicit encoding as soon as possible not late in the day eg if youre stuck with python and wow is a unicode string depends on your version of nltk i think then might work better if wow is already a utfencoded byte string as it comes from nltk then obviously you wont need to encode it again to remove such doubts printreprwow or thereabouts will tell you more and printsysversion will tell you what version of python so you can in turn tell us as so few people appear to do even though its most often absolutely crucial info
27750608,error installing nltk supporting packages nltkdownload,python pythonx nltk,try below code it has downloaded package as expected looks before link was broken whicvh been fixed by ssl note mac been used
27680118,recurring issue with importing nltk,python nltk,the version of python being run when you enter python in terminal is the version from pythonorg whereas pip is using homebrews version of python youll need to edit your bashrc or profile to change the order of your path so that homebrews python is run instead to do this open terminal and enter ls al and check the output to see if bashrc andor profile exist next use cat to check the contents of each file looking for the presence of lines that start with export path if only one file exists or if both exist and only one defines path then open that file in your favorite editor on the last line enter the following export pathusrlocalbinpath save the file completely close terminal and reopen it if everything worked as expected which python should now return usrlocalbinpython you can now run python and once in the interpreter running the command import nltk should import the module with no errors edit setting up the pythonorg version of python to be your default is easier as its already in your path the version of pip youre using is both outdated and installed for use with homebrew so well need to install a new version first though well change the permissions of your installation so you dont need to use sudo every time you run pip to do this run sudo chown r user libraryframeworkspythonframeworkversionslibpythonsitepackages sudo chown r user libraryframeworkspythonframeworkversionsbin this allows pip to install modules in sitepackages and scripts in the bin directory next well copy the contents of your homebrew sitepackages directory to the pythonorg sitepackages directory so you can use the modules youve already installed with pip to do this run cp r usrlocalcellarpythonframeworkspythonframeworkversionslibpythonsitepackages libraryframeworkspythonframeworkversionslibpythonsitepackages finally download getpippy change to the directory you downloaded it in and run python getpippy this is assuming you havent changed your path as instructed above this will set up the current version of pip as of this writing its for use with pythonorg python you can now run pip install modulename to install packages
27658409,downloading error using nltkdownload,python python ubuntu nltk spyder,to download a particular datasetmodels use the nltkdownload function eg if you are looking to download the punkt sentence tokenizer use if youre unsure of which datamodel you need you can start out with the basic list of data models with it will download a list of popular resources ensure that youve the latest version of nltk because its always improving and constantly maintain edited in case anyone is avoiding errors from downloading larger datasets from nltk from and if anyone wants to find nltkdata directory see and to config nltkdata path see
27522015,how to solve this weird python encoding issue,python encoding nltk decoding,in your example st is a str list of bytes to do that it was encoded in some form utf by the looks but think of it as a list of bytes and you need to know how it was encoded in order to decode it though utf is always generally a good first punt so stencode is nonsensical here its already encoded as utf by the interpreter by the looks of things for some mad reason in python strencode will first decode into a unicode and then encode back to a str it chooses to decode as ascii by default but your data is encoded as utf so the error youre seeing is in the decode step of your encode operation its looking at that list of bytes e and saying hmmm those arent real ascii characters lets start with unicode data instead notice the u really all this is pythons fault python wont let you get away with these shenanigans of thinking of unicode and str as the same thing the rule of thumb is always work with unicode within your code only encodedecode when youre getting data in and out of the system and generally encode as utf unless you have some other specific requirement in python you can ensure that data in your code is automatically unicode udata
27471177,cleaning text files in python typeerror coercing to unicode,python unicode nltk textmining,tfilereadlines gives you a list of lines which you are appending to another list in result you have a list of lists in mylist the following should fix the problem this will give you a list of strings in mylist
27280661,nltk import error python,python nltk,i think you want notice the corpus namespace
27246917,my maxent classifier works fine with gis algorithm but does not work with iis algorithm it is not throwing any error just some warnings,python nltk maxent,firstly the way youre importing your libraries unsorted is too confusing also there are lot of unused imports after some googling so lets cut down the imports and stick with this then i found that featx is some example module the jacob perkins was using for his book this is a better source so lets heres a documented version with some explanation of what the functions are doing now lets go through the process of training the model and testing it first the feature extraction lets see what we get after calling labelfeatsfromcorpus out so we get a document with the neg label and for each word in our document we see that all words are true for now each document only contains the feature ie the word that it has lets move on now we see that the splitlabelfeats change the key value structure such that each iteration of trainfeats gives us a document with a tuple of the features label out so it seems like the error can only be caused by your last two lines of code when you run the line you get these warnings but do note that the code is still building the model so its just warnings due to underflow see what are arithmetic underflow and overflow in c it takes a while to build the classifier but fear not just wait till its finish and dont ctr c to end the python process if you kill the process you will see this so lets understand why the warning occurs there are warnings given all of them points to the same function used to calculate delta in nltks maxent implementation ie and you find out that the this delta calculation is specific to iis improved iterative scaling algorithm at this point you need to learn about machine learning and supervised learning to answer your question the warming is merely an indication that delta is hard to calculate at some point but its still reasonable to deal with possibly because of some super small values when calculating delta the algorithm is working its not hanging its training in order to appreciate the neat implementation of maxent in nltk i suggest you go through this course or for more hardcore machine learning course go to training a classifier takes time and computing juice and after you wait long enough you should see that it does out you can see that the accuracy is bad as expected since delta calculation is going too far is your baseline go through the courses as listed above and you should be able to produce better classifiers after knowing how they come about and how to tune them btw remember to pickle your classifier so that you dont have to retrain it the next time see save naive bayes trained classifier in nltk and pickling a trained classifier yields different results from the results obtained directly from a newly but identically trained classifier heres the full code
27216038,stanford tagger not working,java python nltk stanfordnlp,the latest version of the stanford tagger requires java if you arent able to upgrade use an older version of the tagger
27212050,problems traversing ne chunks in nltk,python nltk namedentityrecognition,it seems you are iterating over sentences i assume you want to iterate over the individual nodes contained in sentences it should work like this edit for future reference what tipped me off to the fact that you are iterating over sentences is simply that the root node for a sentence is commonly labeled s
27178492,unicode error when using nltk to find trigrams for entire corpus and print to csv,python unicode nltk trigram,try see why should we not use syssetdefaultencodingutf in a py script its a pesky issue in py and nltk
27074322,nameerror global name isdigit is not defined,python macos python nltk,no its a method of strings i suspect you want yisdigit
26969747,code to replace emoticons by sad or happy not working properly,python nltk textprocessing,you are turning each word and each emoticon to a set this means you are looking for overlap of individual characters you probably wanted uses exact matches at most you can iterate over dictionaries directly no need to call keys there you dont actually appear to be using the dictionary values you could just do and then perhaps use sets instead of dictionaries this then can be reduced to using the dictionary view on the keys as a set still it would still be better to use sets then if you wanted to remove the emoticon from the text youll have to filter the words or better still combine the two dictionaries and use dictget here i pass in the current word both as the lookup key and the default if the current word is not an emoticon the word itself is printed otherwise the word sad or happy is printed instead
26893417,nltktextblob in flasknginxgunicorn on ubuntu error,ubuntu flask nltk textblob,the sudo user admin declared in the appconf supervisor file created to run this app was not able to read at the site root level the inaccessible nltk corpora downloaded at a rootnltkdata were causing my original i discovered this problem after having reconfigured gunicorn logging and receiving fatal supervisor crashes on supervisorctl restart app for the newly pointed gunicornlog not having permissions to write my updated and working supervisor config sans user declaration is as follows i am not sure what the full security implications are for this configuration however and not sure why the sudo group admin user was not accessing the directories correctly bonus points to anyone with that answer
26727511,getting nonetype error when parsing xml file in python,python nltk,remove the line it is done automatically when using the with open syntax and also the name f is only valid inside the with block
26693736,nltk and stopwords fail lookuperror,python nltk sentimentanalysis stopwords,you dont seem to have the stopwords corpus on your computer you need to start the nltk downloader and download all the data you need open a python console and do the following in the gui window that opens simply press the download button to download all corpora or go to the corpora tab and only download the ones you needwant
26582284,encoding error in pos tagging with nltk on python,python nltk postagger,in the current version of nltkdata they provide two versions of the pickle files one for python and one for python for example there is one englishpickle at nltkdatataggersmaxenttreebankpostagger and one at nltkdatataggersmaxenttreebankpostaggerpy the newest nltk handles this automatically by a decorator pydata in short if you download the newest nltkdata but dont have the newest nltk it may load the wrong pickle file raising the unicodedecodeerror exception note suppose you already have the newest nltk you may encounter some path error where you can see two pys in the path of the pickle file this may mean some developers were not aware of the pydata and have handled the path redundantly you can removerevert the redundancy by yourself see this pull request for an example
26551232,pyspark textblob from nltk used in map missingcorpuserror,apachespark nltk emr textblob pyspark,so the problem was that spark has internally set home to home hack to make this work with python is to add before call of textblob line it is connected to this spark issue
26411779,error when stripping punctuation from corpus,python string nltk,but i didnt think string had to be defined when used like that like all the other modules in python you need to import string before it is used does that mean raw is not a string do not confuse the string module with the type string yes probably raw is of type string how can solve this add import string at the beginning of the file
26394748,nltk python error typeerror dictkeys object is not subscriptable,python pythonx dictionary key nltk,looks like you are using python in python dictkeys returns an iterable but not indexable object the most simple but not so efficient solution would be in some situations it is desirable to continue working with an iterator object instead of a list this can be done with itertoolsislice
26126579,iterate over bigramstuple given by nltk typeerror nonetype object is not iterable python,python list tuples nltk,for iterating inside a tuple you need just use variables with the number of bigram indexes not tuple like thisfor a b in bigrams and if you just want each bigram use one variable in your loop for better understanding see the below demo
25998742,nltk download url authorization issue,python python nltk,nltk have moved from their googlecode site as noted in this question nltks new data server is located at just update the url to the new location and it should hopefully work
25827058,attributeerror freqdist object has no attribute inc,python python attributes nltk,you should do it like so but usually freqdist is used like this also look at the examples here
25590089,nltk postag throws unicodedecodeerror,pythonx nltk,ok i found the solution to it looks like a problem in the source itself check here i opened up datapy and modified line as below
25315566,unicodedecodeerror in nltks wordtokenize despite i forced the encoding,python encoding utf nltk pdfminer,you are turning a piece of perfectly good unicode string back into a bunch of untyped bytes which python has no idea how to handle but desperately tries to apply the ascii codec on remove the encodeutf and you should be fine see also
25226848,error in nltk udhr module,python python ubuntu nltk,youve made a typo it should be germandeutsch not germandeutsh see section here note that the last line of the stack trace indicates the corpus that couldnt be loaded which should be a hint if you run into this again
25225898,error in inaugural corpus in nltk,python nltk,this code is from natural language processing with python by bird klein and loper page indeed theres a typo in the book it has been fixed in the online version change file to fileid as moose suggested
25215887,babelizeshell not working in nltk package,python nltk,the issue isnt with your code the problem is that the babelfish translation service is no longer in operation so the example code no longer works more details at
25182140,dispersionplot not working inspite of installing matplotlib,python nltk,the examples of the online book are not quite right you may try this you may also call it from a text directly this way you import the function directly instead of calling the funcion from text object i realized this watching at the source code directly hope this work for you
24470390,keyerror in python despite having keyword,python python nltk,fellow exists in text but not in cnt which is an empty dictionary with no keys the minimal fix is but its probably easier to use collectionscounter
23995456,error installing nltk in python,nltk python,sorry sometimes its the most obvious things that work in this case simply double clicking the setuppy file from the nltk folder allowing windows to execute it with python and its installed beautifully
23872413,fatal error while installing nltk,python installation nltk fatalerror qgis,i dont know if this will help but i have often just downloaded the binaries from his page provides and bit windows binaries of many scientific opensource extension packages for the official cpython distribution of the python programming language
22976731,attributeerror list object has no attribute split when i try to split a row from csv file,python csv split nltk stopwords,your data file is not a csv the words are separated by whitespace not commas so you dont need the csv module for this instead just read each line from the file and use row linesplit to split the line on whitespace by the way checking membership in a set is an o operation while checking membership in a list is an on operation so its advantageous to make cachedstopwords a set
22930328,error using stanford pos tagger in nltk python,python nltk stanfordnlp postagger,note just posting it as answer to help in case others face this issue in future i finally found out what i did wrong it turned out to be a blunder tagger file name is not englishbidirectionaldistimtagger but englishbidirectionaldistsimtagger
22919275,error while printing gloss definitions in python,python nltk wordnet,question why do you have synsets objects as strings native string objects in python doesnt have definition attribute they only have these functionsattributes what you need is a synset object from nltk see going back to your code what you need is the key to access the synsets eg bankn then with the key cast it into a synset object then you can access the definition from the wnsynsetxdefintion
22861795,nltk download error getadderinfo failed,python nltk,i was facing the same issue the issue in my case was that when the nltk downloader started it had the server index as this needs to be changed to you can change this by going into the nltk downloader window and the filechange server index
22191619,nltk data installation issues,python nltk,have you tried to check if the downloads work try a few of the corpora that you have downloaded eg if the corpora are not installed properly you will see something like this
21823057,error in python program in subprocespopen,python perl nltk,try to change this line cmd perlsampleplab to cmd perlsamplepl stra strb
21812661,python import nltk error on mac,python nltk,try reinstalling it using pip or homebrew or whatever you used to install it if there is anything that says error and clang sorry i cannot remember the exact error that means that it did not install correctly which could be because you dont have permission use the sudo command to run the command you use to install it just in case you dont have permissions
21809935,cant install nltk urllib error,ubuntu installation urllib nltk,i am having the same error when trying to install nltk except that instead of connection refused i get the error have you tried using the following syntax
21274309,nltk created string regex not working,python regex nltk,rematch matches the pattern at the beginning of the input string you should use research instead see search vs match to make the program robust check the return value of the call btw if you want to check indicates is in the string using in operator is more preferable
20735733,nltk stopword removal issue,python nltk,i would do this by avoiding adding them to the freqdist instance in the first place depending on the size of your corpus i think youd probably get a performance boost out of creating a set for the stopwords before doing that if thats not suitable for your situation it looks like you can take advantage of the fact that freqdist inherits from dict
20403876,nltk sklearnclassifier error,python classification nltk scikitlearn,you havent trained the classifier call its train method before attempting to classify anything as the author of this code i admit the error message could be friendlier
20402460,import error when i import nltkcorpusframenet in nltk python,python nltk,in your link it import as so have you tried that edit version and above of nltk has framenet in the nltkcorpusreader package so it should be
19980498,typeerror map object is not subscriptable error in python,python nltk typeerror,in python keys returns an iterator which you cant slice convert it to a list before slicing
19975301,freqdist with nltk valueerror too many values to unpack,python nltk frequencydistribution,you were so close in this case you changed your taggedsent from a list of tuples to a list of lists of tuples because of your list comprehension taggedsent nltkpostagsentfor sent in words heres some things you can do to discover what type of objects you have this shows you that you have a list in this case of sentences you can further inspect one of those sentences like this you can see that the first sentence is another list containing items well what does one of those items look like well lets look at the first item of the first list if your curious to see the entire object which i frequently am you can ask the pprint prettyprint module to make it nicer to look at like this so the long answer is your code needs to iterate over the new second layer of lists like this of course this just returns a nonunique list of items which look like this you can uniqueify this list in many different ways but you can quickly by using the set data structure like so for an examination of other ways you can uniqueify a list of items see the slightly older but extremely useful
19916449,how to get rid of import nltk error on mac,python macos nltk,looks like you do not have nltk importerror raised when an import statement fails to find the module definition or when a from import fails to find a name that is to be imported see try opening terminal and running if you do not have pip installed take a look at how do i install pip on macos or os x
19675106,fixing words with spaces using a dictionary look up in python,python python dictionary nltk textsegmentation,take a look at word or text segmentation the problem is to find the most probable split of a string into a group of words example the most probable segmentation should be of course heres an article including prototypical source code for the problem using google ngram corpus the key for this algorithm to work is access to knowledge about the world in this case word frequencies in some language i implemented a version of the algorithm described in the article here example usage using data even this can be reordered note that the algorithm is quite slow its prototypical another approach using nltk as for your problem you could just concatenate all string parts you have to get a single string and the run a segmentation algorithm on it
19479432,attributeerror str object has no attribute dispersionplot nltk,attributes matplotlib nltk,note that you have to make it into an nltk text object after tokenizing it also your text variable as used in your code is the string texttesttxt not the text inside the file called texttesttxt assuming that you have matplotlib and numpy installed which are necessary for dispersionplot to work your file is at homemyfiletxt your file is simple text like the ones they use then this should do it
19233967,nltk import problems,python nltk importerror,from the interpreter type and then create a script with the same contents and run please post the output
19062969,python error in comparing strings,python string compare nltk,i suspect youre intending to acces the name property of the lemma object to print everything except flabbergasted you could try which gives the output
18987117,unicodedecodeerror in textblob tutorial,python nltk textblob,release fixes this issue which means its time for a the problem was that the enlexicontxt file used for partofspeech tagging opened the file using windows default platform encoding cp the file apparently had characters that python could not decode from this encoding this was fixed by explicitly opening the file in utf mode
18687879,error in computing text similarity using scikit learn,python machinelearning nltk cosinesimilarity,its not your fault its because of different formula used in current sklearn and the one used in the tutorial the current version of sklearn uses this formula source where nsamples refers to the total number of documents d in the tutorial and df refers to the number of documents in which the term appears dt in d in the tutorial to deal with zero division they by default use smoothing option smoothidftrue in tfidfvectorizer see documentation that changes the df and nsamples values like this so those values would be at least while the one in the tutorial uses this formula so you cant get the exact same result as the one in the tutorial unless you change the formula in the source code edit strictly speaking the right formula is lognsamplesdf but since it causes the zerodivision problem in practice people try to modify the formula to allow it to be used in all cases the most common one is like you said lognsamplesdf but its not wrong also to use the formula lognsamplesdf given that youve already smoothed it beforehand but reading the code history it seems that they did that so that they wont have negative idf value as discussed in this pull request and later updated in this fix another way to remove negative idf value is simply by converting negative values to i have yet to find which one is the more commonly used method they did agree that the way they do it is not the standard way so you can safely say that lognsamplesdf is the correct way to edit the formula first i must warn you that this will affect every user that uses the code make sure you know what youre doing you can just go to the source code in unix its at usrlocallibpythondistpackagessklearnfeatureextractiontextpy in windows im not using windows now but you can search for the file textpy and edit the formula directly you might need administratorroot access depending on the platform you use additional note as an additional note the order of terms in the vocabulary is also different at least in my machine so to get the exact same result if the formula is the same you also need to pass in the exact same vocabulary as shown in the tutorial so using your code
18295772,unicodedecodeerror ascii codec cant decode byte python,python unicode nltk,overall you have these strategies treat input as sequence of bytes then both input and grammar are utfencoded data bytes treat input as sequence of unicode code points then both input and grammar are unicode rename unicode code points to ascii that is use escape sequences nltk that is installed with pip in my case doesnt accept unicode directly but accepts quoted unicode constants that is all of the following appear to work note that i quoted unicode text and not normal text vs bar
17631510,nltk with flask import error,python flask nltk importerror,it looks like the activation of your virtualenv is causing the problem did you activate the virtualenv before running sudo pip install u pyyaml nltk if not they were installed globally remember that by default when you create a virtualenv environment it will ignore all packages not installed directly into the environment itself in other words it will ignore the packages you installed globally using aptget install so you have two options install your dependencies into your virtualenv by activating the virtualenv then doing pip install nltk if nltk depends on any development libraries you will need to install those development libraries as well those can be installed using your package manager aptget rebuild your virtualenv this time using the systemsitepackages option this will allow you to use packages installed outside of the virtualenv environment
17532128,python assertion error during nltkconditionalfreqdistribution,python text python nltk,i can reproduce the error by creating an empty file foo and then calling textwordsfoo so to avoid empty files you could do this
17184225,cherrypy gives an error when using nltk natural language tool kit,python nltk,youve got the wrong function name nltkwordtokenize is actually called nltkwordtokenize you use it like so if you use the wrong name you get the error you got
16967654,typeerror map object is not subscriptable nltk book with python,pythonx nltk,probably the interface changed and you need to do listvocabulary or something like that see helpmap
16678500,python nltk snowball stemmer unicodedecodeerror in terminal but not eclipse pydev,python python pydev nltk snowball,this works in pydev because it configures python itself to work in the encoding of the console which is usually utf you can reproduce the same error in pydev if you go to the run configuration run run configurations then on the common tab say that you want the encoding to be ascii this happens because your word is a string and youre replacing with unicode chars i hope the code below sheds some light for you this is all considering ascii as the default encoding but if you do it all in unicode it works you may need to encode it back afterwards to the encoding you expect if you expect to deal with strings and not unicode so you can make your string unicode before the replace or you can encode the replace chars note however that you must know whats the actual encoding youre working on in any place so although im using cp or utf in the examples it may be different from the encodings you have to use
16609242,import nltk not working on xampp,apache python cgi nltk,from looking at the code here it appears that the library has platformspecific path handling and that error is thrown in the nix branch the code to detect windows looks like this i hope that sysplatform isnt getting messed up so i think its most likely that apache isnt propagating the appdata environment variable down to your code this question explains why that is and gives a workaround
15320894,python importerror maxentclassifier,errorhandling python nltk,you need to have numpy installed is behind you should install numpy and be able to do import numpy without error
15220961,attribute error while using scikitlearn,python nltk scikitlearn featureextraction,since im running the development pre version where the featureextractiontext module got overhauled i dont get the same error message but i suspect you can solve this issue with the mindf parameter causes countvectorizer to throw away any term that occurs in too few documents because it wont have any predictive value by default its set to which means all your terms get thrown away so you get an empty vocabulary
14525637,issue with conditional frequency distribution,nltk,this error message is telling you that the rematch returned none in other words there was no match when you look for group it throws up the error you have a few options in terms of moving forward simplify your match command be defensive first use an if to check if there was a match then look for its group in general when you get stuck with errors like this keep simplifying your code to see what is causing the problem python can be written in very terse ways but i find it better to be more descriptive when learning this so question should give you a few more options hope that helps you move forward
14506969,nltk pos tagger not working,python nltk postagger,it seems that the saved word tokenizer requires numpy youll need to install it
14463368,some problems with nltk,python tokenize nltk counting corpus,replace with explanation what you were doing was you were appending all the words tokens produced by corpuswordsfileids if the number of words was at least so i suppose always for your corpus what you really wanted to do was to filter out the words shorter than characters from the tokens set and append the remaining long words to longtokens your function should return the result tokens having characters or more i assume the way you create and deal with categorizedplaintextcorpusreader is ok here is an answer to the question you asked in the comments
14311601,importing libraries issue importerror no module named,python numpy libraries pip nltk,try changing the pythonpathenvironment variable if you are using bash the below should work other linux shells will be slightly different in how they assign environment variables
14110030,issue while importing nltk package in python,python python syntaxerror nltk pythonimport,you have a local file named newpy check your current directory and rename it or delete it you can see this in the traceback the preceding module has a full filepath but the newpy file does not making it a local file that is shadowing the relative import in scipystatsdistributions
13584629,ocaml compile error usrbinld cannot find lstr,linux ubuntu ocaml nltk,the instructions given here allow me to compile with no error it boils down to which tells me that libcamlstr can be found in usrlibocaml ymmv so i do then im able to compile the project
13504424,naive bayes classifier error,python nltk,change to otherwise the classifier only knows about words of the form contains and is therefore clueless about the words in i love this city yields
13503660,python nltk ngrams error,python nltk assertion ngram,are you sure youre calling ngrammodel with the right arguments looking at the source for the current version of nltk ngrammodel looks like this which doesnt seem to match up to how youre calling the function what is estimator in your code because youre currently passing estimator as the padleft argument
13269543,why am i getting error valueerror chunk structures must contain tagged tokens or trees,python nltk textmining,the parse function can only handle one sentence at a time this works result
12702839,what is the error in following python code,python nltk stopwords,the problem with what you are doing is listremovex only removes the first occurrence of x not every x to remove every instance you could use filter but i would opt for something like this
12122163,rubypython error during importing nltk,python jruby nltk rubypython,looks like rubypython does its magic with ffi if there is a problem with ffi binary which comes with jruby andor jvm youre using there is not much you can do when jvm segfaults try a newer jruby version andor jvm but beyond that i am afraid you are not going to get much help here to wit it works on my mac
10792611,python nltkdownload tclerror cant download corpus fedora,python tcl fedora nltk,tclerror is a python exception that is defined by the tkinter module iirc tcl itself doesnt generate it indeed its actually meaningless from a tcl perspective have you tried importing tkinter yet
10179011,nltk error not show some word,python nltk stopwords,if you change wordlist so that it is a list of words it works fine wordlist will contain the words you are after
10031470,python frequency distribution freqdist nltk issue,python nltk frequencydistribution,im not completely certain what you want but the error message is saying that it wants to hash the list which is usually a sign its putting it in a set or using it as a dictionary key we can get around this by giving it tuples instead but we still have if we make the subsequences into tuples though is that what youre looking for
9853227,tokenizing large mb txt file using python nltk concatenation write data to stream errors,python nltk tokenize,problem n you are iterating the file char by char like that if you want to read every line efficiently simply open the file dont read it and iterate over filereadlines as follows problem n the wordtokenize function returns a list of tokens so you were trying to sum a str to a list of tokens you first have to transform the list into a string and then you can sum it to another string im going to use the join function to do that replace the comma in my code with the char you want to use as glueseparator if instead you need the tokens in a list simply do hope that helps
8190545,pip nltk install issue on ubuntu using virtualenv,python ubuntu virtualenv nltk pip,it looks like pip is grabbing the first targz package from pypi for nltk this is a macosx binary you will have to explicitly point pip to the correct package the easiest way to do this is to just provide the full path to the package the other solution is to download the package to a known directory and install it from there for example say you download the package to downloads the command would be
5754492,valueerror occurs when i try to use cg algorithm of maxentclassifier in nltk,python classification nltk,it works if you set the algorithm note you missed one line of the training corpus edit several nltk algorithms fail including cg the problem is probably the same as the one reported here if this is the case it probably will be solved in nltk next releases you could also report a bug to nltk to help the developpers and yourself as the reported bug seems related with numpy broadcasting and outdated uses of numpy maybe you could try with an older version of numpy
5512765,removing punctuationnumbers from text problem,python nltk,change to assignment to word in the forloop above simply changes the value referenced by this temporary variable it does not alter wordlist
3549910,project gutenberg python problem,python regex text nltk,given a list l of words and a target word t tells you whether l has word t in a caseinsensitive way its faster of course to do since python does not hoist the constant computation out of the loop and unless you hoist it yourself it will be performed repeatedly given a list of lists lol the longest sublist including t can be found by if multiple sublists include t and are of the same maximal length this will give you the first one as it happens
818691,python replace string with prefixstringsuffix keeping original case but ignoring case when searching for match,python regex search replace nltk,this ok the key to the whole thing is using word boundaries groups and the rei flag
78142012,onnx export of seqseq model issue with decoder input length,pytorch onnx seqseq,ok thats a problem in pytorch x onnx exporter there is no such a problem with pytorch x
77692725,pytorch nnlstm runtimeerror for unbatched d input hx and cx should also be d but got d d tensors,python pytorch neuralnetwork lstm seqseq,you cant send a tensor of ints into the lstm model the y tensor needs to be d like the x tensor you also need to iterate over the decoder inputs you can check out this example of seqseq in pytorch
63566232,runtimeerror the size of tensor a must match the size of tensor b at nonsingleton dimension,python pytorch transformermodel seqseq,i took a look at your code which by the way didnt run with seqlen and the problem is that you hard coded the batchsize to be equal line in your code it looks like the example you are trying to run the model on has batchsize just uncomment the previous line where you wrote batchsize queryshape and everything runs fine
63203500,tensorflow keras lstm performs bad on seqseq problem with clear solution,tensorflow keras deeplearning lstm seqseq,so in case this helps anyone in the future the model did exactly what i asked it to do but you need to be careful that your data preprocession does not lead to ambiguity so you have to prevent something like while improving one equatation the other one will lose that was my problem see this example one the one hand the models increases the accuracy for eq on the other hand it decreases the loss for eq so is kind of a compromise the model found to solve that i had to add another factor which describes the data more specifically so i added an extra condition describing whether y is written out or just written as date type so now i have a data structure like the following and my accuracy grew to
62822737,oserror e cant find model de it doesnt seem to be a shortcut link a python package or a valid path to a data directory,pythonx jupyternotebook pytorch tensorboard seqseq,so after whole one month trying on other things and exploring issues and questions related to this topic i found a way to do so import spacycli spacyclidownloadencorewebmd with this method you can use and import any spacy model whether mediumsized or larger size datasets also which always gives an error if you try to import the dataset using spacyload because it is not effective for loading datasets other then sm or smallest size datasets in google colab or kaggle notebook or any other online notebook
61683953,typeerror init got multiple values for argument axes,python tensorflow keras seqseq attentionmodel,the first argument to dot is called axes so you need to decide what value you want to pass for axes either decoderoutputs encoderoutputs or
60665202,valueerror error when checking target expected dense to have dimensions but got array with shape,python tensorflow keras deeplearning seqseq,after experimenting around a little i realised that id been trying to use a d input while the actual code was using d input i referred this question which had an almost similar query and the solution to my query
59047014,invalidargumenterror when training a seqseq model,seqseq,so few mistakes in your code vocabulary sizes encoder and decoder if you do a printxdict you will see that your vocabularies start at and go up to some value lets say n now you set the inputvocab as lenxdict this leaves your embedding layer lacking a row for the last word in your vocabulary so whenever your model encounters that last words you get that embeddinglookup type error so you need to set inputsizelenxdict having two none in your output shapes this is personally something i always try to avoid it is okay to leave your batch dimension none but having more than one none in your output shape is dangerous for example tensorflowkeras sometimes does reshaping in layers if you have more than one none you cannot recover the original shape of the tensor or probably not even allowed to perform reshaping either way it is not the best practice so ive set the sequence lengths in your input shapes so after the changes your code looks like this
50886684,dimension issue with tensorflow stackbidirectionaldynamicrnn,python tensorflow deeplearning seqseq,use the following method to define a list of cell instances you can see the difference when you print two lists above code snippet prints something similar to following as you could see method outputs a list of same cell instance which is not expected hope this helps
50369758,nmt tensorflow error with notfounderror,python tensorflow machinelearning seqseq,i just solved this problem with deleting log data
75809216,calculate cosine similarity sentences valueerror expected d array got d array instead,python similarity embedding cosinesimilarity sentencesimilarity,what i wanted to do if i was you which should speed it up is normalize your sentence vectors such they have unitnorm calculate the matrixdot product between your sentences with sentencestsentences the result is an nxn matrix where ij is the similarity between sentence ij
67301250,pandas calculate overlapping words between rows only if values in another column match issue with multiple instances,python pandas list combinations sentencesimilarity,do you want this
61806882,tensorflow session error in universal sentence encoder,python tensorflow tensorflow tensorflowserving sentencesimilarity,these two lines are intended to make tensorflow x to tensorflow x for tensorflow x this is common issue while serving with flask django etc you have to define a graph and session for inference import tensorflow as tf import tensorflowhub as hub the input request can be handled through for details for tensorflow x session and graph is not required
47632262,typeerror fetch argument array has invalid type numpyndarray must be a string or tensor can not convert a ndarray into a tensor or operation,python tensorflow lstm sentencesimilarity,the problem is that you are replacing the value of sim which i suppose initially contains a reference to a tensorflow tensor or operation with the result of evaluating it which is a numpy array so the second iteration fails because sim is not a tensorflow tensor or operation anymore you can try something like this
75026054,simple ner indexerror string index out of range error,namedentityrecognition,nltknechunk expects its input to be tagged tokens rather than just plain tokens so i would recommend adding a tagging step between the tokenization and ne chunking via nltkpostag ne chunking still would give you every token chunked by entities if there are any detected since you want only the entities you can check for if there is a tree in a particular chunk like the following please note that this code doesnt give exactly the output you want instead it gives
74192948,attributeerror list object has no attribute ents in building ner using bert,python pandas bertlanguagemodel namedentityrecognition,it seems you mix code from different modules ents exists in module spacy but not in transformers in transformers you should use directly nlpv but it gives directory with ententity entscore entindex entword entstart entend
73931787,sklearncrfsuitecrf unicodeencodeerror,python scikitlearn namedentityrecognition,i figure out that i cannot use the chinese symbols in ner tag from reference after changing the tagdictionary with int in the value it works
73358347,how to resolve the error namename label if label in featureskeys else labels in hugging face ner,pythonx token huggingfacetransformers namedentityrecognition,i think the object tokenizedinputs that you create and return in tokenizeandalignlabels is likely to be a tokenizersencoding object not a dict or dataset object check this by printing typemyobject when in doubt and therefore it wont have keys you should apply your tokenizer to your examples using the map function of dataset as in this example from the documentation
71629167,valueerror e labels for component tagger not initialized,namedentityrecognition spacy,i just meet the same problem the picture of setting the config file is misleading you if you just want to run through the tutrital you can set the config file like this only click the check box on ner
70799226,ner classification deberta tokenizer error you need to instantiate debertatokenizerfast,python tokenize bertlanguagemodel namedentityrecognition roberta,lets try this
68086528,pytorch with cuda throws runtimeerror when using packpaddedsequence,python pytorch namedentityrecognition customtraining,within padsequence function which acts as a collatefn which gathers samples and makes a batch from them you are explicitly casting to cuda device namely you dont need to cast your data when creating batch we usually do that right before pushing the examples through neural network also you should at least define the device like this or even better leave the choice of device for youuser in some part of the code where you setup everything
67956814,spacy custom ner training attributeerror docbin object has no attribute todisk,python namedentityrecognition spacy,make sure you are really using spacy in case you havent you can check this from the console by running python c import spacy printspacyversion by issuing via command line pip install spacy in a python env and then running in the python console import spacy from spacytokens import docbin nlp spacyblanken load a new spacy model db docbin create a docbin object omitting code for debugging purposes dbtodisktrainspacy save the docbin object you should get no errors
67894649,valueerror with nerda model import,python huggingfacetransformers namedentityrecognition,take a look at the source code of the used huggingfacehub lib they comparing the version of your python version to do different imports but you uses a release candidate python version this tells the value rc that caused the error because they didnt expecthandle this you get the intparsevalueerror solution update your python version to a stable version no release candidate so you have an intonly version number solution monkeypatch sysversion before you import the nerda libs
67370416,catalogueregistryerror e could not find function spacycopyfrombasemodelv in function registry callbacks,namedentityrecognition spacy,copyfrombasemodelv is a new function introduced in spacy v are you perhaps running an older version of spacy if so can you try updating it this will likely resolve your error see also
63928965,error with join parsing txt for named entity recognition in nlp google api,python googlecloudplatform namedentityrecognition googlecloudautoml googlenaturallanguage,well it means that gcsfile has type bytes so you need to make it a string str type for example
56139932,fixing a custom opennlp ner model,java opennlp namedentityrecognition,our end goal was to be able to train a model on certain words that we classified and have to correctly classify each word regardless of sentence structure in opennlp we werent able to accomplish that im guessing our training questions are to similar to each other and now its assuming whatever follows what was is a department is that a correct assumption and is there a better way of training these models based on my testing and results im concluding yes the sequence and pattern of the words plays a part i dont have any documentation to back that though also i cant find anything to get around that with opennlp is the best bet to break each entity into its own model based on experience and testing im resolving that separate models as much as possible is the best way to train unfortunately we still havent been able to accomplish our goals even with this approach ultimately what weve done to switch to stanfordnlp ner models you can still do custom implementations around domain specific language and have the option of turning off sequencing in the properties file reference for custom ner in stanfordnlp stanford corenlp training your own custom ner tagger
55663594,valueerror setting an array element with a sequence on keras modelfit,python tensorflow keras lstm namedentityrecognition,the error definitely originates from you passing in an object array as your predictive variable your ytr should be of shape as far as i can tell from your snippet maybe some of your rows in ytr is not long or maybe you have specifically used an object array to generate the data if the latter you can try to convert it back like this replace float with whatever type suits your needs this should give an error if the rows are of different size as well
54211431,nameerror name nechunk is not defined,python namedentityrecognition,you need to download the below packages the named entity chunker will give you a tree containing both chunks and tags
48094827,typeerror not supported between instances of nonetype and str using pyner for name entity recognition,string pythonx stanfordnlp namedentityrecognition,the issue here is that ner is setup so that when the output is set to slashtags it output a dictionary format however the text is parsed with slash characters where a named entity occurs and this character is then used to separate dictionary entity before the dictionary is generated as a result if any slashes occur in your text data you need to parse this out something like this shouldnt be an issue in nlp terms as dates should still be picked out with this format but if some key part of your analysis requires this tag to be there this solution might not be suitable i cant verify if this issue exists in the java implementation but its possible
47198333,stanford nlp ner sentiment sutime performance issue,stanfordnlp sentimentanalysis namedentityrecognition sutime,i ran this command on your example text and i got this timing information i see sec for the processing time make sure you dont rebuild the pipeline each time it appears your code is not rebuilding the pipeline in the main method my command uses multithreading for ner and parse note that i am also using the shiftreduce parser which is substantially faster than the default parser all of the pipeline settings can be set in java api code by assigning them to the properties object you use to build the pipeline here is exhaustive documentation for using the java api you will have to convert this into scala java api command line you dont need to build a separate nerclassifiercombiner you can use the ner annotator which will also run sutime i should note the time will be dominated by the parser you can choose not to parse really long sentences with parsemaxlen n and set n to whatever token length youd like if you want to get character offsets for full entity mentions make sure to add the entitymentions annotator to the end of the annotators list each sentence has a list of entity mentions in it each entity mention is a coremap you can get access to the begin and end character offsets of the entity mention with this code please let me know if you have any questions about converting this into scala code
35588529,error loading ner bin file as model argument for opennlpmaxententityannotator,r apache opennlp namedentityrecognition,resolved the error the r function opennlpmaxententityannotator was not compatible with the named entity recognition ner model being produced by opennlp building the ner model using opennlp resulted in opennlpmaxententityannotator running without error
25504216,named entities in encapsulated xml cause parsing errors,xml perl namedentityrecognition,ok i think i found a solution xmlentities will do the job of reencoding the entities in the string however i need to limit the characters that are encoded to only a few otherwise the encoded string will have entities that the xml parser does not recognize so at the moment i use to only encode the ampersand and a few special xml chars
76790316,replacing field values in a fixedposition file using awk,awk textprocessing,with your shown attempts please try following gnu awk code using match function in awk program where using regex which creates capturing group and it contains value into array arr and then printing st and nd element of array arr
75104021,how to concatenate lines only if they begin with same prefix,regex concatenation notepad textprocessing,you can try this solution find what sss replace with this cannot be done with a single replacement since the match number is not fixed you need to hit the replace all button again and again until there are no more changes for this example you need to replace it twice to get the desired output first replacement second replacement it also works if something else is in between
73165681,how to build a for loop that prints the sentiment score of each string and does not produce a key error,python pandas list textprocessing,at your sentiment functions you can use tryexcept concept so you can define what to do if an exception raises its not going to be perfect example because dont know what your functions do actually but your can try
72695200,issue converting githubcomraw urls to rawgithubusercontentcom urls using awk,awk textprocessing unixtextprocessing,your only real problem is that awk fields arrays and strings all start at not so your loop should have started at not as written first time through your loop print i is doing print having said that i think what you want is the following with a couple of other things tidied up the only slightly tricky part in that is subrawrs subofs rs which is how you remove a midrecord field in awk first convert the field to a string that matches rs since that cant be present in the input we can use rs directly when its a string like n rather than a regexp so we changed raw to n in the th field which meant the record now contained n and then removed n thereby removing the th field and preceding
69789448,prefixing numerically a string without change count status up to matchs in a text file with multiples matchs per line using awk sub,awk gsub textprocessing prefix,assumptions for every occurrence of the string dog prefix said string with an integer pfx said integer pfx starts and increments after each n uses one awk idea note the in x and x refers to the length of the search string dog if op were to search for a different string then the s would need to be updated accordingly eg if searching for dogs then change the two s to s this generates fwiw with n and a single line of input dog dog dog dog dog dog dog dog dog dog this generates
69783031,repeat nth number until the nth match while prefixing these number at the pattern using awk sub,awk gsub textprocessing,you can try this awk by splitting every match of dog into or this will generate matches within an integer int suppresses the float decimals so there may be a better approach for this output
68284662,removing duplicates in fixed width file which has multiple key columns except the last occurrence,linux shell awk textprocessing,please try following awk code written and tested with shown samples explanation simple explanation would be creating arr with index of st characters and th to st characters and having current line value in it keep doing same till whole inputfile is done with reading in end block of this program printing all elements of array which will basically provide all elements last occurrence only nd solution using gnu awks fieldswidth option you can try following
62866213,how to fix notimplementederror,python textblob textprocessing,a quick look on correct function source code will reveal the answer def correctself correct the spelling of the word returns the word with the highest confidence using the spelling corrector versionadded textblob return wordselfspellcheck raise notimplementederror as you can see the code of the function raise immediately this error in simple words textblobde doesnt support correct method yet textblobde is an extension of textblob for german language so obviously it should implement correct method relevant to german language unfortunately developers didnt do so yet maybe in the future
60948259,assertionerror some objects had attributes which were not restored,pythonx tensorflow keras textprocessing,a really idiotic mistake i made which was so minor that i doubt anyone could have picked it up in this line though the file is named to have the index prefix for some reason attaching that extension to the variablecall function has caused it to panic for some reasonmaybe a bug what would have been more helpful was an error to point out the incorrect extension so for anyone else having this problem just change your checkpoint directory to this
59397301,how to get all the words around a word within a fixed proximity,python pythonx textprocessing,instead of looking at characters i will look for words in that way you will say find my target and add n words before and after it using regex we can replace found variable with found for x in mytext if researchrbbx foundappend else foundappend the only think i do is split the string to a list
58861116,issues removing punctuation when preprocessing text using stm in r,r textprocessing tm topicmodeling,here is an example with quanteda i find this package very useful even more so when not working the english languange and it works in parallel i put your example text in a text file in my r directory im showing all steps for clarity a few steps could be done inside each function
58511066,csv column values going to new line causing errors loading in pandas,python pandas csv textprocessing,fix your files use m refindall to find cases like green ngrape the pattern will find alpha nalpha and ignore alpha nnumeric m will be a list of all the matches eg ng replace ng g which results with green grape find all of the files with pathlib rglob looks in all subdirectories use glob if all the files are in one directory pathlib treats paths like objects instead of strings as such pathlib objects have many methods stem returns the filename suffix returns the file extension eg csv this will not overwrite your existing files it will create a new file adding fixed to the name import re from pathlib import path list of all the files files listpathrcsomepathrglobcsv iterate through each file for file in files create new filename namefixed newfile filewithnameffilestemfixedfilesuffix read all the text in as a string text filereadtext find and fix the sections that need fixing m refindallazazsnazaz text for match in m text textreplacematch f match textlist textsplitn textlist xstrip for x in textlist write the new file with newfileopenw newline as f w csvwriterf delimiter wwriterowsxsplit for x in textlist example with the following content in a csv orderidfruitcountperson applepeter green grape mary watermelon paul applepeter green banana mary watermelon paul applepeter green apple mary watermelon paul new file orderidfruitcountperson applepeter green grape mary watermelon paul applepeter green banana mary watermelon paul applepeter green apple mary watermelon paul create dataframe import pandas as pd newfiles listpathfcsomepathglobfixedcsv df pdconcatpdreadcsvf for f in newfiles
55133157,python issue writing txt file,python pythonx pythonrequests textprocessing,update your code to match the following
54878981,sed to replace all line on the first match not working,sed commandline textprocessing,try explanation listen is a range it matches all lines from the beginning of the file to the first line that matches the regex listen thus this matches the first two lines of your sample file slistenxyz is a substitute command it is executed for any line in the range it replaces any line with listenxyz this means that it replace server as well as listen by changing the substitute command to slistenlistenxyz we make sure that it only replaces lines that contain listen
52596419,python dataframe attributeerror float object has no attribute replace,python pandas dataframe textprocessing,two things to fix first when you apply a lambda function to a pandas series the lambda function is applied to each element of the series what i think you need is to apply your function to the entire series in a vectorized manner second your function has multiple return statements as a result only the first statement return sourcereplaceazaz will ever run what you need to do is make your preprocessing changes on the variable source inside your function and finally return the modified source or an intermediate variable at the very end to rewrite your function to operate on an entire pandas series replace every occurrence of source with sourcestr the new function definition then instead of this try this
52394161,bash shellscript column check error handling,bash shell awk textprocessing,awk can report out which files meet this condition then you can do your mv based on that example using a pipe to xargs with smaller variable names since you get the idea by now
51185547,how to solve this error error recognize featurenotlicensed at ocrxpress std,nodejs imageprocessing npm ocr textprocessing,use these keys provided by accusoft after adding these three keys my issue has been resolved
47877125,using v option to invert multiline grep results with grep a not working,linux grep textprocessing,try how it works jrgetlinenext this selects the lines containing jr for those lines this instructs awk to get the next line getline and then to skip the rest of the commands and start over on the following line next for any lines that dont have jr in them this prints the line
41566939,splitting text lines whilst appending prefix,regex awk sed textprocessing,this might work for you gnu sed replace each v by a newline the first field and a tab print and delete the first line and repeat edit as per the new question remove any single double quotes replace double double quotes by single double quotes and replace the semi colon by a tab character then replace any vs by a newline and the first field and a tab and repeat
40133779,error in reading multple text files from directory in r,r textprocessing dataextraction,this seems to work fine for me make changes as per code comments in case files have headers answer edited to reflect new information posted by op rmlistls clean memory if you can afford to mydir desktopa change as per your path read full paths myfiles listfilesmydirpattern regionalvolfullnamest myfiles check that files listed correctly initialise the dataframe from first file change header tf depending on presence of header make sure sep is correct df readcsv myfiles header f skip nrows sep c check that first line was read correctly df read all the other files and update dataframe we read lines to read the header correctly then remove ans lapplymyfiles functionx readcsv x header f skip nrows sepc ans update dataframe lapplyans functionxdfrbinddfx this should be the required dataframe df also if you are on linux a much simple method would be to simply make the os do it for you
39668845,error at creating word cloud in r error in simpletripletmatrix i j v different lengths,r textprocessing tm wordcloud,i think the problem is that wordcloud is not defined for tm corpus objects install the quanteda package and try this
38709392,qdap package bug in converting zero digits to zero words,r numbers textprocessing qdap,if you dig into the guts of replacenumber you can see that the problem occurs in qdapnumsub digging within that function the issue occurs in numbword which has internal codes which convert zero values to blanks if i were facing this problem myself i would fork the qdap repo go to replacenumberr and try to change this in a backward compatible way so that replacenumber could take a logical argument blankzerostrue which got passed down to numbword and did the right thing eg in the meantime i have posted this on the qdap issues list
35778753,what is the fastest most errorfree method of extracting and cleaning the html body text in python,python html beautifulsoup lxml textprocessing,youve done a lot to make it fast the soup strainer and the lxml parser are usually the first things to try when optimizing the parsing with beautifulsoup here are some improvements to this particular code remove the body existence check and use find instead replace the if text is none or lentext with just if not text strip via gettextstriptrue the improved code these are just microimprovements and i dont think they are gonna change the overall performance picture what i would also look into running the script via pypy beautifulsoup is compatible but you would not be able to use lxml parser try it with htmlparser or htmllib you might win a lot without even modifying the code at all
30839392,why this exclusion not working for long sentences,textprocessing perl,youre using the wrong syntax is used to match a character class while here youre trying to match a number of occurences of which can be done using also as suggested by salva in the comments the pattern can be shortened to
29991354,error while executing an if statement in r,r textprocessing,for j in lengthwords doesnt do what you expect it to j is only ever equal to the full length of the list this also means that j runs off the end of the list so that wordsj returns na change your for loop line to now returns as noted in the comments a better computationally more efficient and more concise approach would be
29137914,r split function size increase issue,r memory textprocessing categoricaldata logfileanalysis,try this what happenned in your case is that since the id was numerical the number was used as an index position in the created list which is obviously not right since ids go pretty high in numbers r filled the gaps with as many empty lists hence the huge object size by making the id a character variable we avoid this another way which would leave the id variable intact inside the line dataframes would be to access the elements in the newly created list youll need to quote the numbers if you use the operator another option would be to add characters in front of the numerical id for instance which makes accessing listitems more convenient
22462855,cell array add suffix to every string,string matlab textprocessing cellarray,strcat operates on cell arrays
21834743,sed match pattern ttextt not working,regex bash sed textprocessing,it appears that your sed command is correct but you have some null characters in your text file run this sed command to remove nulls first
17562094,when the code to find the phrase file error in the file removes the whole line,java textprocessing,you can do with the following steps create a temporary file open both the files original file for reading and temp file for writing read from the original file line by line and write it into temp file if the line contains text file error skip that line once reading is finished delete the original file and rename the temp file to original file
15711286,how to list only the log files with todays date in their name and error in their body in unix,date unix textprocessing,you get todays date with you find name of files with certain pattern with as l pattern will show the files matching the pattern so you can even directly grep files which have today in their name
13199237,finding syntaxerror in a file that have a custom format using rexex,php regex syntaxerror textprocessing,detecting error is easy imagine logtxt simple scanner output
7721331,problems with strtok,php textprocessing bbcode strtok,although i dont think this is really the solution it seems this is the only way im going to get my point across it could be something with the way strtok works that to get the results you want although not perfect i was able to obtain results close to what you were expecting with this as i said it isnt perfect but maybe seeing this will help you on your way to handling this solution i personally have never handled bbcode with this type parsing instead using pregmatch
7484407,how to deal with unicode character encoding issues while converting documents from pdf to text,pdf unicode characterencoding textprocessing pdfconversion,pdf is at its heart a print format and thus records text as a series of visual glyphs not as actual text originally it was never intended as a digital archive format and that still shows in many documents with complex scripts such as arabic or indic scripts that require glyph substitution ligation and reordering you often get a mess basically what you usually get there are the glyph ids that are used in the embedded fonts which do not have any resemblance to unicode or an actual text encoding fonts represent glyphs some of which may be mapped to unicode code points but some are just needed for fontinternal use such as glyph variants based on context or ligatures you can see the same with pdfs produced by latex especially with nonascii characters and math pdf also has facilities to embed the text as text alongside the visual representation but thats solely at the discretion of the generating application i have heard word tries very hard to retain that information when producing pdfs but many pdf generators do not it usually works somewhat for latin thats probably why nearly no one bothers i think the best bet for you if the pdf doesnt have the plain text available is ocr on the pdf as an image
3006922,script to fix broken lines in a txt file,perl textprocessing,there are a bunch of ways to do it i would recommend something along the lines of perl python or ruby if youre looking to do this with a quickanddirty oneliner perl has an edge in that department for example this will do what you asked for but this is probably better if paragraphs are separated by newlines
2099471,add a prefix string to beginning of each line,linux scripting textprocessing,if you want to edit the file inplace sed i e sprefix file if you want to create a new file sed e sprefix file filenew if prefix contains you can use any other character not in prefix or escape the so the sed command becomes soptworkdir or soptworkdir
1393489,using awk to process a file where each record has different fixedwidth fields,linux unix awk textprocessing gawk,hopefully this will lead you in the right direction assuming your multiline records are guaranteed to be terminated by a cc type row you can preprocess your text file using simple ifthen logic i have presumed you require fields and on one row and a sample awk script would be create an awk script file called programawk and pop that code into it execute the script using
77201376,problem with text find and replacement in python,python replace extract textmining,your substrmod was a regex escaped string is converted to now originalsubstr can not be found because originalsubstr has no backslash next time use a debugger removed re and do all with literal string find removed the else because the if test is always true def cleanstrs str return slowerreplace replace replace def findandsavesubstr mainstr convert both strings to lowercase and remove spaces commas and hyphens for caseinsensitive matching substrmod cleanstrsubstr mainstrmod cleanstrmainstr find the substring in the modified main string start mainstrmodfindsubstrmod if start return substr none returns substr as it was and none if no match is found end start lensubstrmod count originalstart originalend for i c in enumeratemainstr if c not in count if count start originalstart i if count end originalend i break originalsubstr mainstroriginalstartoriginalend if the whole substr is matching with some part of mainstr return an empty string as modifiedsubstr modifiedsubstr if cleanstroriginalsubstr substrmod always true modifiedsubstr return modifiedsubstr originalsubstr returns the modified substr and the matched string in its original form output of the cases international workshop on grapheneceramic composites wgcc isnnm international symposium on fractography of advanced ceramics fractography from macro to nanoscale zjazd chemikov zbornk abstraktov
74659657,inner join not working for r sentiment analysis,r textmining sentimentanalysis,sounds like an interesting project try adding by cword word
71557300,tibble error tibble columns must have compatible sizes,textmining tidymodels,the tmpluginwebmining package has not been updated in a long time and no longer connected to the resources it needs unfortunately the code in the book still runs because we recorded what this package did return at the time that we wrote the book the book uses that older result we are tracking this needed update on github and thinking about a good replacement
67825501,error in ldacdes k k method gibbs control listverbose l each row of the input matrix needs to contain at least one nonzero entry,r dataframe textmining quanteda topicmodels,it looks like some of your documents are empty in the sense that they contain no counts of any feature you can remove them with
67823934,memory problems when using lapply for corpus creation,r memory lapply textmining corpus,you can write a function which has series of steps that you want to execute on each pdf using lapply apply the function on each file
66846209,textblob error too many values to unpack,python pandas dataframe textmining textblob,naivebayesclassifier expects a list of tuples of the form text label train listzipdftext dftext recommended abc yes def no cl naivebayesclassifiertrain
61201861,r tidytext unnesttokens error when using a txt file as source,r textmining tidytext,first input is the column name of output column that you want and second one is that of input
58545968,error while finding topics quantity on latent dirichlet allocation model using ldatuning library,textmining lda textvec,ldatuning expects input dtm matrix in a different format format from topicmodels package you need to convert dtm sparse matrix from matrix package to a format which ldatuning can understand
58528772,perplexity issues using textvec,forloop textmining textvec,this is because you need to recalculate newdoctopicdistr inside the loop
57921647,r randomforest undefined column issue,r randomforest textmining,replace train with traindata
56854594,problem with adist function in text comparison,r text comparison textmining levenshteindistance,as others have already pointed out it looks like a bug using the source from and looking at lines in file srcmainagrepc there is code that is reversing a buffer walking through what happens in gdb then executing the following r code continue at the break to reach the last diagonal entry examine the buffer before reversal stepping through the code shows that buf and buf are copied from the end of the buffer exiting the loop k and k is examining the reversed buffer shows that buf was not set to nul this results in replacing bufk with bufk appears to put the nul in the correct location resulting in the expected output after the fix your second example results in the result appears to be consistent with the other attributes for ins sub and del
55352498,how to output in r all possible deviations of a word for a fixed distance value,r textmining tidyverse stringr quanteda,im going to assume that you want all actual words not just permutations of the characters with an edit distance of that would include nonwords such as zat we can do this using adist to compute the edit distance between your target word and all eligible english words taken from some word list here i used the english syllable dictionary from the quanteda package you did tag this question as quanteda after all but this could have been any vector of english dictionary words from any other source as well to narrow things down we first exclude all words that are different in length from the target word by your distance value distfn functionword distance select eligible words for efficiency eligibleywords namesquantedadataintsyllables wordlengths nchareligibleywords eligibleywords ncharword distance wordlengths ncharword distance compute levenshtein distance distances utilsadistword eligibleywords return only those for the requested distance value eligibleywordsdistances distance distfncat at bat ca cab cac cad cai cal cam can cant cao cap caq car cart cas cast cate cato cats catt cau caw cay chat coat cot ct cut dat eat fat gat hat kat lat mat nat oat pat rat sat scat tat vat wat to demonstrate how this works on longer words with alternative distance values distfncoffee caffee coffeen coffees coffel coffer coffey cuffee toffee distfncoffee caffey calfee chafee chaffee cofer coffees coffelt coffers coffin cofide cohee coiffe coiffed colee colfer combee comfed confer conlee coppee cottee coulee coutee cuffe cuffed diffee duffee hoffer jaffee joffe mcaffee moffet noffke offen offer roffe scoffed soffel soffer yoffie yes according to the cmu pronunciation dictionary those are all actual words edit make for all permutations of letters not just actual words this involves permutations from the alphabet that have the fixed edit distances from the input word here ive done it not particular efficiently by forming all permutations of letters within the eligible ranges and then computing their edit distance from the target word and then selecting them so its a variation of above except instead of a dictionary it uses permuted words distfn functionword distance result character start with deletions for i in maxncharword distance ncharword result c result combnunliststrsplitword fixed true i paste collapse simplify true now for changes and insertions for i in ncharwordncharword distance all possible edits edits applyexpandgridreplistletters i paste collapse remove original word edits editsedits word get all distances add to result distances utilsadistword edits result cresult editsdistances distance result for the op example distfncat ca ct at caa cab cac cad cae caf cag cah cai caj cak cal cam can cao cap caq car cas aat bat dat eat fat gat hat iat jat kat lat mat nat oat pat qat rat sat tat uat vat wat xat yat zat cbt cct cdt cet cft cgt cht cit cjt ckt clt cmt cnt cot cpt cqt crt cst ctt cut cvt cwt cxt cyt czt cau cav caw cax cay caz cata catb catc catd cate catf catg cath cati catj catk catl catm catn cato catp catq catr cats caat cbat acat bcat ccat dcat ecat fcat gcat hcat icat jcat kcat lcat mcat ncat ocat pcat qcat rcat scat tcat ucat vcat wcat xcat ycat zcat cdat ceat cfat cgat chat ciat cjat ckat clat cmat cnat coat cpat cqat crat csat ctat cuat cvat cwat cxat cyat czat cabt cact cadt caet caft cagt caht cait cajt cakt calt camt cant caot capt caqt cart cast catt caut cavt cawt caxt cayt cazt catu catv catw catx caty catz also works with other edit distances although it becomes very slow for longer words d distfncat setseed cheadd sampled taild c a t ca ct at aaa baa daa eaa faa gaa haa iaa jaa kaa laa maa naa oaa paa qaa raa saa taa uaa vaa waa xaa yaa zaa cba aca bca cca dca eca fca gca hca ica jca kca lca mca nca oca pca qca rca cnts cian pcatb cqo uawt hazt cpxat aaet ckata caod ncatl qcamt cdtp qajt bckat qcatr cqah rcbt cvbt bbcat vcaz ylcat cahz jcgat mant jatd czlat cbamt cajta cafp cizt cmaut qwat jcazt hdcat ucant hate cajtl caaty cix nmat cajit cmnat caobt catoi ncau ucoat ncamt jath oats chatz ciatz cjatz ckatz clatz cmatz cnatz coatz cpatz cqatz cratz csatz ctatz cuatz cvatz cwatz cxatz cyatz czatz cabtz cactz cadtz caetz caftz cagtz cahtz caitz cajtz caktz caltz camtz cantz caotz captz caqtz cartz castz cattz cautz cavtz cawtz caxtz caytz caztz catuz catvz catwz catxz catyz catzz this could be speeded up by less brute force formation of all permutations and then applying adist to them it could consist of changes or insertions of known edit distances generated algorithmically from letters
54910980,error in aggregatedataframeasdataframex arguments must have same length,r textmining topicmodeling,i am not sure what you want to achieve with the command as far as i see you produce only one decade with with all the preprocessing from tweets to textdata youre producing a few empty lines this is where your problem starts textdata with its new empty lines is the basis of your corpus and your dtm you get rid of them with the lines at the same time youre basically deleting the empty columns in the dtm thereby changing the length of your object this new dtm without the empty cells is then your new basis for the topic model this is coming back to haunt you when you try to use aggregate with two objects of different lengths tweetsdecade which is still the old length of with theta that is produced by the topic model which in turn is based on dtmnew remember the one with fewer rows what i would suggest is to first get an idcolumn in tweets later on you can use the ids to find out what texts later on get deleted by your preprocessing and match the length of tweetdecade and theta i rewrote your code try this out
53455834,typeerror test missing required positional argument,python pycharm pytest textmining nose,running tests with nose or unittest is not trivial if youre new at this i think you would have an easier time getting started with something like pytest
53166000,r dplyr text mining error in evalrhs env env object score not found,r function dplyr textmining,you can only reference column variables like score in dplyr functions like select innerjoin and so on you tried to reference to score outside of a dplyr function so r is looking for a variable called score and can not find it the solution is to use score inside a dplyr function here is an alternative way that should achieve your result by grouping by word and using summarise to get the word count
52199374,problem with analysing turkish text while using stopwords tr with r,r textmining,you are almost there you just need to change the source of where the stopwordsstopwords get the language from tldr for running your code you need explanation these are the languages available in the default source snowball to get turkish you just need to change the source to source stopwordsiso below you can see all the stopwords available in this source which means that for running your code you need
50275752,regular expression not working in r but works on website text mining,r regex textmining,i think you can do this with regexpr function for an example this matches mr from the manual regexpr returns an integer vector of the same length as text giving the starting position of the first match or if there is none with attribute matchlength an integer vector giving the length of the matched text or for no match the match positions and lengths are in characters unless usebytes true is used when they are in bytes as they are for an asciionly matching in either case an attribute usebytes with value true is set on the result if named capture is used there are further attributes capturestart capturelength and capturenames gregexpr returns a list of the same length as text each element of which is of the same form as the return value for regexpr except that the starting positions of every disjoint match are given in your case i think you need to paste the lines together by using then apply regexpr on fullline
50006289,text mining error in r nonnumeric argument to binary operator,r ggplot dplyr textmining tidyr,your problem is easily overlooked you need backticks around jane austen not quotes jane austen is not a name in this case but a column name in frequency and column names with a space need backticks it should be not
49628211,attributeerror module urllib has no attribute urlretrieve,python urllib textmining,extending from comments section as stated by documentation you can access urlretrieve like this
48821866,matching documens with textvec scaling problems,r textmining textvec,the issue in step c is that matsim is sparse and all the apply calls make columnrow subsetting which are super slow and convert sparse vectors to dense there could be several solutions if matsim is not very huge convert to the dense with asmatrix and then use apply better you can convert matsim to sparse matrix in a triplet format with asmatsim tsparsematrix and then use datatable to get indices of the max elements here is an example also as a side suggestion i recommend to try chartokenizer with ngrams for example of the size c to fuzzy match different spelling and abbreviations of addresses
48022087,stemcompletion is not working properly,r textmining tm stemming,why is the unstemmed corpus not showing the right content since you got a simple corpus object you are effectively calling which yields due to stemcompletion awaiting a character vector of stems as a first argument conce we have not a character vector of stemmed texts conce we have if you want to complete the stems in your corpus whatever this is supposed to be good for you have to pass a character vector of single stems to stemcompletion ie tokenize each text document stemcomplete the stems then paste them together again
46948275,error in creating termdocumentmatrix using tm package in r,r textmining tm,you did everything right just in your last line you accidentally passed a character data note the quotation marks to the function termdocumentmatrix instead of the object data
46148182,issues in getting trigrams using gensim,python datamining textmining wordvec gensim,i was able to get bigrams and trigrams with a few modifications to your code i removed the threshold parameter from the bigram phrases because otherwise it seems to form weird digrams that allow the construction of weird trigrams notice that bigram is used to build the trigram phrases this parameter would probably come useful when you have more data for trigrams the mincount parameter also needs to be specified because it defaults to if not provided in order to retrieve the bigrams and trigrams of each document you can use this list comprehension trick to filter elements that arent formed by two or three words respectively edit a few details about the threshold parameter this parameter is used by the estimator to determine if two words a and b form a phrase and that is only if where n is the total vocabulary size by default the parameter value is see docs so the higher the threshold the harder the constraints for words to form phrases for example in your first approach you were trying to use threshold so you would get human computerinteraction is as digrams of out of your sentences that begin with human computer interaction that weird second digram is a result of the more relaxed threshold then when you try to get trigrams with default threshold you only get human computer interaction is for those sentences and nothing for the remaining two filtered by threshold and because that was a gram instead of a trigram it would also be filtered by if tcount in case that for example you lower the trigram threshold to you can get human computer interaction as trigram for the two remaining sentences it doesnt seem easy to get a good combination of parameters heres more about it im not an expert so take this conclusion with a grain of salt i think its better to firstly get good digram results not like interaction is before moving on as weird digrams can add confusion to further trigrams gram
46137572,error in extracting phrases using gensim,python datamining textmining wordvec gensim,the technique used by gensim phrases is purely based on statistics of cooccurrences how often words appear together versus alone in a formula also affected by mincount and compared against the threshold value it is only because your training set has new and york occur alongside each other twice while other words like machine and learning only occur alongside each other once that newyork becomes a bigram and other pairings do not whats more even if you did find a combination of mincount and threshold that would promote machinelearning to a bigram it would also pair together every other bigramthatappearsonce which is probably not what you want really to get good results from these statistical techniques you need lots of varied realistic data toysized examples may superficially succeed or fail for superficial toysized reasons even then they will tend to miss combinations a person would consider reasonable and make combinations a person wouldnt why because our minds have much more sophisticated ways including grammar and realworld knowledge for deciding when clumps of words represent a single concept so even with more better data be prepared for nonsensical ngrams tune or judge the model on whether it is overall improving on your goal not any single point or adhoc check of matching your own sensibility regarding the referenced gensim documentation comment im pretty sure that if you try phrases on just the two sentences listed there it wont find any of the desired phrases not newyork or machinelearning as a figurative example the ellipses imply the training set is larger and the results indicate that the extra unshown texts are important its just because of the rd sentence youve added to your code that newyork is detected if you added similar examples to make machinelearning look more like a statisticallyoutlying pairing your code could promote machinelearning too
45597235,how to use traceback and debug to fix broken r code,r function debugging textmining traceback,lets examine the traceback pile from the bottom your error is in quickclean its on the corp line luckily you only have one inside tmmap the function itself is calling the method tmmapvcorpus as your corp object is of class vcorpus and tmmap is a wrapper for different methods this function itself is calling tmparlapply etc from the time you hit a reliable function in traceback its usually not so useful to go much further it means that the input you gave to the functions isnt good we learnt that you gave a vcorpus object as a first parameter so this one seems to be ok though we may check later if its format is not problematic but lets check the other parameter removepunctuation the doc tmmap says it requires a function if you use debug debugonce or browser look them up youll see that their boolean at the time you execute the line and theyre boolean because you named your function parameters just like those functions so rename your function parameters and hopefully it will run fine heres how you may use browser define this function spot the added line execute the line that triggered the error type classcorp to confirm what we already know type classremovepunctuation ooops its a boolean type q or the escape key to get out of the browser debug is like browser but starts at the first line of the function
45565164,tfidf document term matrix and lda error messages in r,r matrix textmining lda tidytext,if you explore the documentation for lda topic modeling using the topicmodels package for example by typing lda in the r console youll see that this modeling procedure is expecting a frequencyweighted documentterm matrix not tfidfweighted so the answer is no you cannot use a tfidfweighted dtm directly in this function if you have a tfidfweighted dtm already you can convert it using tmweighttf to get to the necessary weighting if you are building a documentterm matrix from scratch then dont weight it by tfidf
45043296,lda topic model issue,r text textmining,you need to keep track of which ones youre removing or reconstruct the index used to remove them
42822442,r rsentimentcalculatescore returns error arguments imply differing number of rows,r textmining sentimentanalysis,it turns out that the problem was caused by special characters in the sentence after removing them i could successfully run the sentiment analysis i incorporated the datacleaning step in the function
41862690,problems with non english letters using wordcloud by twitter mined text,r textmining wordcloud,managed to solve it by first encoding the vector to utfmac since im on osx then using the gsub function in order to manually change the hex codes for the letters i had problems with to the actual letters for example gsubxc x x gsubxc xa x since case sensitivity lastly changing the argument for the tmmap function from utfmac to latin that did the trick for me hopefully someone else will find this useful in the future
40236989,agrep function of r is not working for text matching,r textmining agrep,it is because of your maxdistance parameter see agrep for instance if you want only the closest match see best match
40109181,coreference resolution by corenlp corefchainannotationclass not working,parsing semanticweb textmining stanfordnlp,look into this complete example i guess you are missing something as i am unable to understand the exact reason from the code you provided
39217789,r tidytext and unnesttokens error,r textmining,try with the underscore in unnesttokens unnesttokens is the standard evaluation version of unnesttokens and allows you to pass in variable names as strings see nonstandard evaluation for a discussion of standard vs nonstandard evaluation
37497070,tolower function of the corpus package throws an error,r textmining tm corpus,basetolower chokes on special characters this is often a problem when mining tweets you could try catching errors or just use stringis tolower pendant
37021799,alternatives for wget giving error forbidden,webscraping wget textmining,set custom user agent like this
36456518,hashingvectorizer and multinomial naive bayes are not working together,python python scikitlearn textmining,if the nonnegative argument isnt available just like my version try putting vectorizer hashingvectorizeralternatesignfalse
34581021,is there a more efficient way to append lines from a large file to a numpy array memoryerror,python csv numpy outofmemory textmining,since your word counts will be almost all zeros it would be much more efficient to store them in a scipysparse matrix for example x is now an ndocs nwords scipysparselilmatrix and words is a list corresponding to the columns of x you could pass x directly to ldaldafit although it will probably be faster to convert it to a scipysparsecsrmatrix first
34211537,path error for tree tagger with korpus r package,r windows path textmining postagger,there are several issues here first the asfactorsalut ca va should be a file with that text in it you are also missing a preset value inside of ttoptions you will want to put presetfr after the path argument finally the path itself should point to the root directory the documentation here states ttoptions a list of options to configure how treetagger is called you have two basic choices either you choose one of the predefined presets or you give a full set of valid options path mandatory the absolute path to the treetagger root directory that is where its subfolders bin cmd and lib are located you are pointing the path variable inside of the bin directory to the exe file run the following code to point to the root directory where the bin directory is located as follows
32929351,error in substring arguments when calling mapply with regmatches as argument,regex r textmining mapply,a classmate of mine came up with an answer turns out that the last argument of mapply should be a list not a dataframe so lapply does the trick
32513513,rtexttools creatematrix got an error,r classification textmining,run this in the source code box that pops up line will have a misspelling of the word acronym change the a to an a and hit save it should work fine after that
32509140,font issue on ubuntu machine in parsing pdf file,java ubuntu textmining apachetika,i would do a three step approach to fix this issue analyse what files are searched for and not found using strace use aptfile to search for the package providing these files install the missing package install strace if its not already installed sudo aptget install strace check what files are used by your app strace grep open you can further filter this for enoent errors strace grep open grep enoent now you should know what files are missing check what package is providing this file dpkg s only works for already installed packages install that package using aptget install ive no ubuntu here but the ms fonts are normally available in a package called mscorefont or similar
29777415,fatal error in tm text mining document term matrix creation,r textmining tm,you might consider a few changes in your code especially removestopwords and creating a corpus below worked for me
29253913,error while instaling open grm thrax,c ubuntu textmining textanalysis openfst,during installation of openfst you have to type then you should install thrax and while on it type in terminal worked for me for openfst and thrax when i got i added to configure for openfst and i did the same for mpdt error if you get other errors you can try doing the same
28250961,big document term matrix error when counting the number of characters of documents,r matrix textmining tm,you might be able to work around this if you keep your data in the dtm which uses a sparse matrix representation that is much more memory efficient than a regular matrix the reason why the apply function gives an error is because it converts the sparse matrix into a regular matrix the matrix object in your q btw its poor style to give data objects names that are also names of functions especially base functions this means that r has to allocate memory for all the zeros in the dtm which are typically mostly zeros so thats a lot of memory with zeros in it with a sparse matrix r doesnt need to store any of the zeros heres the first few lines of the source for apply see the last line here for the conversion to regular matrix so how to avoid that conversion heres one way to loop over the rows to get their sums while keeping the sparse matrix format subsetting this way preserves the sparse format and does not convert the object to the more memory expensive common matrix representation we can check the representation so in the sapply function we are always working on a sparse matrix and even if sum or whatever function you use there does some kind of conversion its only going to be converting one row of the dtm rather than the entire thing the general principle when working with largish text data in r is to keep your dtm as a sparse matrix and then you should be able to keep within memory limits
27478161,error in encutfx argumemt is not a character vector,r textmining,you have to refer to the content of the corpus ie the character vector in samplecontent here i replaced encutfx with encutfxcontent
25604121,issues with dates in apache opennlp,apache textmining opennlp dataextraction,i am not an expert in opennlp but i know that the problem you are trying to solve is called temporal expression extraction because i do research in this field p nowadays there are some systems which can greatly help you in extracting and unambiguously representing the temporal meaning of such expressions here are some references mantime online demo software heideltime online demo software sutime online demo software if you want a broader overview of the field please have a look at the results of the last temporal information extraction challenge tempeval task a i hope this helps
25551514,termdocumentmatrix errors in r,r textmining tm corpus termdocumentmatrix,the link provided by jazzurro points to the solution the following line of code must be changed to
25086185,how to avoid too many redirects error when using readlinesurl in r,r url textmining,it seems like the nyt site forces redirects for cookie and tracking purposes looks like the builtin url reader isnt able to deal with them correctly not sure if it supports cookies which is probably the problem anyway you might consider using the rcurl package to access the file instead try
24612080,tmpluginsentiment issue error could not find function dmetadata,r text textmining tm,looks like its caused by the removal of the dmetadata function from the tm package refer to this issue on github upgrading to the latest version of tmpluginsentiment from github using devtools fixed this for me
23506964,rtexttools package error,r textmining,it appears that you might not be specifying the sizes correctly the operator has a very high priority you might try
22470059,text mining clustering analysis in r error two dimensional array,arrays r kmeans textmining,i cant reproduce your error it works fine for me update your question with a reproducible example and you might get a more useful answer perhaps your input data object is empty what do you get with dimdtm here it is working fine on the data that comes with the tm package
22128047,how to avoid weird umlaute error when using datatable,r datatable textmining sparsematrix,i couldnt reproduce either but there was another object not found error that arun found that im hoping is this one too now in v commit from news o an error object name not found could occur in some circumstances particularly after a previous error reported with nonascii characters in a column name a red herring we hope since nonascii characters are supported in column names in datatable fix implemented and tests added if it happens again please let us know your test has been added verbatim to the test suite thanks
19790721,problems with termdocumentmatrix function in r,r textmining,the error suggests something is wrong with your choice of minimum and maximum document frequency in the bounds for example the following works note that in the latest versions of tm to specify a weighting you need to use weighting weighttfidf rather than weight weighttfidf similarly you should use stemmingtrue in your control list to stem words im not sure that maxwordlength is an option currently tm will silently ignore invalid options in the control list so you wont know that something is wrong until you go back to inspect the matrix
17041981,an issue with vectorising gsub,r function forloop textmining gsub,try using mapply
15479393,findassoc function in tm package of r giving error,r textmining,you get an error beacause the word does not exist in the terms sets you can check this using which should return false
8073106,package tm problems with kmeans,r statistics clusteranalysis kmeans textmining,the tdm is stored as a sparse matrix as described in termdocumentmatrix this can also be seen from just inspecting the object like strwstdm that old data function was just a way to access the contents as a regular matrix it is not needed anymore just do kmeanswstdm and youll see that the output is as expected with clusters identified for observations terms on features documents good luck
75972320,unimplementederror in python for ann implementation,python jupyternotebook neuralnetwork sentimentanalysis,input data for modelfit could be source looks like a problem with datatypes of xy you are passing to annfit method
73963008,problem completing bert model for sentiment classification,python tensorflow keras sentimentanalysis bertlanguagemodel,op was using bertbasecased for their model and bertbaseuncased for their tokenizer causing issues during training when the vocab size of the model and the tokenized data differed
72545249,attributeerror series object has no attribute encode in sentimentintensityanalyzer,python sentimentanalysis,it appears that your library returns a pandas dataframe object with your data see with the syntax dataframestring you get back a pandas seriesobject which is a subset of a dataframe object a column to be exact in your example you get the contents of the dataframe data specifically the contents of the column text this pandas series object does not have the method encode depending on what you want to do you may have to encode every item of that series object eg
72363741,pytorch dataloader runtimeerror stack expects each tensor to be equal size but got at entry and at entry,pytorch sentimentanalysis pytorchdataloader,quick answer you need to implement your own collatefn function when creating a dataloader see the discussion from pytorch forum you should be able to pass the function object to dataloader instantiation def mycollatefndata todo implement your function but i guess in your case it should be return tupledata return dataloader ds batchsizebatchsize numworkers collatefnmycollatefn this should be the way to solving this but as a temporary remedy in case anything is urgent or a quick test is nice simply change batchsize to to prevent torch from trying to stack things with different shapes up
71177046,how to remove unexpected parameter and attribute errors while importing data for sentiment analysis from twitter,python twitter tweepy sentimentanalysis textblob,langen should be inside of the value of search tweetnode should be tweetmode the fulltext will only exist if the tweetmodeextended parameter is correct and the tweet is more than characters in text length
70949018,errors in counting combining bing sentiment score variables in tidytext,r dplyr sentimentanalysis tidytext,i dont understand what is the point of counting there if the columns are numeric by the way that is also why you are having the error one solution could be the result you should get its
70661279,sentiment analysis fitting a model result in value error shapes incompatible,python tensorflow keras deeplearning sentimentanalysis,you are currently having a sparse tensor for your yvalues this sould have the shape which you can check with ytrainshape one thing that should be working is simply changing the size of your output layer to optional after this you can also change your loss to sparsecategoricalcrossentropy also you should consider flattening your data after the embedding layer so that you get the output shape none instead of none
68383634,cuda error cublasstatusinvalidvalue error when training bert model using huggingface,python pytorch sentimentanalysis bertlanguagemodel,i suggest trying out couple of things that can possibly solve the error as shown in this forum one possible solution is to lower the batch size of how you load data since it might be a memory error if that does not work then i suggest as shown in this github issue to update to a new version of pytorch cuda that fixes a matrix multiplication bug that releases this same error that your code could be doing hence as shown in this forum you can update pytorch to the nightly pip wheel or use the cuda or conda binaries you can find information on such installations on the pytorch home page where it mentions how to install pytorch if none of that works then the best thing to do is to run a smaller version of the process on cpu and recreate the error when running it on cpu instead of cuda you will get a more useful traceback that can solve your error edit based on comments you have a matrix error in your model the problem stems in your forward func then the model bert outputs a tensor that has torchsize which means if you put it in the linear layer you have it will error since that linear layer requires input of bc you initialized it as nnlinear in order to make the error disappear you need to either do some transformation on the tensor or initialize another linear layer as shown below sarthak jain
67977030,what are the cons and potenzial problems of using textblob to perform sentiment analysis how could they be solved,python sentimentanalysis textblob,most of the challenges in nlp sentiment analysis tasks are semantic ones like irony and sarcasm ambiguity in th textmultipolarity thay why textblob may not yield the best resulat depending on your text and if it contains multiples languges you can add new models or languages through extensions
67907780,valueerror number of coefficients does not match number of features mglearn visualization,python machinelearning sentimentanalysis valueerror,youve defined featurenames in terms of the features from a countvectorizer with the default stopwordsnone but your model in the last bit of code is using a tfidfvectorizer with stopwordsenglish use instead featurenames gridbestestimatornamedstepstfidfvectorizergetfeaturenames
67833669,valueerror columns must be same length as key from sentiment analysis,python split sentimentanalysis,edit changed case from dfsentiment to dfsentiment the string methods wont work because its not a string but a set stored in the cell you can do this to create a new column or
65757730,unable to retrieve tweets from tweepy no errorcolumns with no result in output,pythonx pandas twitter tweepy sentimentanalysis,in order to dump the output of tweepys cursor api into a pandas dataframe you need to pass pddataframe a list of dictionaries and the fields youd be interested in as column names tweepy has methods for structuring the data from the cursor items method into a dictionary in your case and then you can do
64381457,valueerror shapes are incompatible in lstm model,python tensorflow keras lstm sentimentanalysis,your ytrain is coming from a single column of a pandas dataframe which is a single column this is suitable if your classification problem is a binary classification problem then you only need a single neuron in the output layer
64240572,how can i handle invalidargumenterror in keras while testing my model,python tensorflow keras sentimentanalysis,i was scrimmaging this error about weeks finally by omitting inputlength from piece of code for designing model i avoid that error code below shows how i created the model hope to be useful for someone else who is tired of this error
62720723,issue applying textblob to a dataframe series,python sentimentanalysis,try to remove rows contains a float value or use isnasum rather than using dropna
61978549,sentiment analysis and fasttext import error,python sentimentanalysis fasttext,running your code on a clean python conda environment should work after installing fasttext with pip pip install fasttext if you do that you should see in a linux console with that your fasttext version is the current one today in addition upon installing the wget package with pip the code below should get you started for sentiment analysis using one of the trained models amazon reviews in the page that you linked if model size is an issue try replacing the model with a compressed one you can also refer to to train a model on a custom dataset instead
61773231,error with tunegrid function from r package tidymodels,r datascience sentimentanalysis hyperparameters tidymodels,found a solution in the comments section of the post this worked for me windows user and made grid tuning nearly x faster additional documentation can also be found here and here
61589497,arguments should have same length error in r,r datascience lapply sentimentanalysis tapply,your problem seems to arise from the use of unlist avoid this as it drops the null values and concatenates list entries with multiple values your organization list has entries two of which are null and one is length you should have entries if this is to match the newsus dataframe so something is out of sync there lets assume the first entries in organization are correct i would bind them to your dataframe to avoid further sync errors newsusorganization organization then you need to do the sentiment analysis on each row of the dataframe and bind the results to the organization values the code below might not be the most elegant way to achieve this but i think it does what you are looking for this code drops any rows where there were no detected organization values it should also duplicate sentiment scores in the case of more than one organization being detected the results will look like this which i believe is your goal the mean scores for each organization can then be collapsed using by aggregate or similar edit examples of by and aggregate
60929241,what is the error in this code using dataframe and matplotlib,python pandas matplotlib sentimentanalysis,it is not a dataframe it is a numpy array the result of your predict method is a numpy array which cannot be indexed like you are trying to why not just use the dataframe that you append the prediction to dftweetpredspredictions tweetpreds then you can do all sorts of indexing
60686336,valueerror dimension mismatch while predicting new values sentiment analysis,python machinelearning twitter sentimentanalysis naivebayes,it seems i made a mistake fitting x after i already fitted trainx i found out there is no use of doing that repeatedly once you the model is fitted so what i did is i removed this line and it worked perfectly
59352235,laravel errorexception enotice undefined variable class,php laravel eloquent sentimentanalysis,php namespace app use illuminate use appmodelsdatatraining use appmodelswordfrequency use appmodelssentimen use appmodelstwitterstream use illuminate use db class controllertraining extends controller public function index title data training sentimen sentimenall class datatraining ifemptysentimen classclass foreachsentimen as key stm classclasskey stmkategori datatrainingstmkategori wordfrequencywhereidsentimenstmidsentimenget total wordfrequencycount foreachclassclass as cls sum dbtabletermfrequencyselectdbrawsumjumlah as jumlahtermjoinsentimen sentimenidsentimen termfrequencyidsentimenwheresentimenkategoriclswherenotnullidtrainingfirst datasum kelas cls jumlah sumjumlahterm distinct dbselectselect count as total from select kata from termfrequency where termfrequencyidtraining is not null group by kata as x foreachdistinct as dst distinctwords dsttotal uniquewords distinctwords i foreachclassclass as cls count dbtabledatatraining joindatacrawling datatrainingidcrawling datacrawlingidcrawling joinsentimen sentimenidsentimen datacrawlingidsentimen selectsentimenkategori as kategori wheresentimenkategori cls count count datatrainingwherekategoriclscount totalcount datatrainingcount prior kelas cls nilai count totalcount return viewdatatraining compacttitledatasumprioruniquewordsdatatrainingtotal else echo sentimen is empty public function hapustrainingkategori datatraining twitterstreamwhereidsentimenkategoridelete return redirecttraining public function datasentimen datatraining sentimenall return responsejsondatatraining
59189603,sentiment analysis using azure error resource not found,python azure sentimentanalysis azurecognitiveservices,it is not recommended sharing your subscription key here pls revoke this subscription key asap for your issue try this result
59043293,load data typeerror cannot convert the series to,python pandas sentimentanalysis,does your rate column in metroparktransdemojicsv file contain something that doesnt belong to numbers you should make sure that the content of the rate column can all be converted to int first suppose the rate columns data all belong to int then you can write this converts the content of the rate column to int then compares it with the type of reviewdfrate is it cannot be converted to int directly
58931303,i get a typeerror while doing sentiment analysis how can i fix this issue,python anaconda jupyter typeerror sentimentanalysis,you need to debug your code you are passing none value xdescription might have some none value in there make sure in your preprocessing stage you dont have any none or nan value in your dataframe
58344723,error when checking input expected denseinput to have shape but got array with shape,machinelearning deeplearning sentimentanalysis,as the error message suggests your xtrain should be a vector of shape as you have given inputdim in your first layer but looks like you are passing the xtrain vector with shape you have to correct the shape of xtrain you are passing to the model show the code where you are reading the data and storing it as xtrain that will help to see where the error is
57095032,how to fix no package called textdata error,r sentimentanalysis tidytext,this means that the package is missing from your libraries you need to install it with installpackagestextdata
57002944,error in usemethodtype no applicable method for type applied to an object of class factor sentiment analysis,r sentimentanalysis,the problem is at stringcount if the second argument is a factor you get an error for example convert that to character and you will be fine so in your case
56746874,error of tfidfvectorizer on cleaned text dataset,python datamining sentimentanalysis tfidfvectorizer,without a proper error trace we can only guess since the error involves stop my guess is that your variable english that isnt in the code you shared at all is inappropriately set up and not a set of words you probably meant to use stopwordsenglish instead
56496475,why do i get a typeerror when importing a textfile line by line for sentiment analysis instead of using a sentence hardcoded,python stanfordnlp sentimentanalysis pycorenlp,i did not install the stanfortlib so i couldnt test with its system but the way it is returning let me thing that your resultsvariable is of type list of dicts or some nested type anyway i made a test then i build your loop and tweaked it a little to fit my needs like what printed me the following so basically the code works but you have to figure out of what type the returning value is after it gets back from that stanfort api typeresults for example when you have this info you can start with a loop that goes through the values and if you dont know of what type the nested value is you call anotehr print of type go all the way down until you reach the layer with the items you want to work with one last thing to point out in the description you linked in the notes there he informs about how to pass text into the api and there he explains that the api gets rid of slicing and formatting you shall only send the whole text in case you get no results back to keep that in mind
55431356,typeerror init got an unexpected keyword argument nfoldssentimentanalysiswithsvm,pythonx scikitlearn svm sentimentanalysis sklearnpandas,as you can see in the documentation for modelselectedstrafiedkfold there is no keyword argument called nfolds and you should indeed use nsplits note however that the data should not be passed as an argument to the validator and by doing so youre effectively passing likedtrain as the argument for nsplits which wont work rather you should pass the data only to the fit of your gridsvm after initialization
54871465,scikit learn valueerror found array with dim estimator expected,machinelearning sentimentanalysis,the error you get is not about the number of samples but the number of features and this comes from those line of code you need to encode the test and the train the same way you fit the count vectorizer on all the datas and then apply it to the test and train if not you dont have the same vocabulary and thus not the same encoding edit you just dont use ct only cv
54870167,countvectorizer error valueerror setting an array element with a sequence,machinelearning sentimentanalysis,the error comes from the way x as been done you cannot use directly x in the fit method you need first to transform it a little bit more i could not have told you that for the other problem as i did not have the info right now you have the following which is enough to do a split we are just going to transform it you can understand and so will the fit method what we do is convert the sparse matrix to a numpy array take the first element it has only one element and then convert it to a list to make sure it has the right dimension now why are we doing this x is something like that so we transform it to see what it realy is and then as you see there is a slight issue with the dimension so we take the first element going back to a list does nothing its just for you to understand well what you see you can dump it for speed your code is now this
54274236,sample example of sentiment feature of watson nlu failing with error code,ibmcloud sentimentanalysis watsonnlu,heres how it worked for me explaining in an elaborate way to help others first of all you have to create a file named parametersjson and paste the below code pointing to the folder in which this json file is on a terminal or command prompt and replacing the apikey and url with the nlu service values run the below command the url in my case is then should see the below output
54253465,how to fix object not found in the following code,r sentimentanalysis,try removing corpus just replace your code with this snippet
54176657,problem with countvectorizer from scikitlearn package,python scikitlearn classification sentimentanalysis textrecognition,suppose you happen to have a dataframe separate into features and outcomes then following your pipeline you can prepare your data for any ml algo like this this newx can be used in your further pipeline as is or converted to dense matrix rows in this matrix represent rows in the original reviews column and columns represent counts of words in case youre interested in what column refers to what word you may do where key is a word and value is column index in the above matrix you may infer actually that column index correspond to ordered vocabulary with awesome responsible for th column and so on you may further proceed with your pipeline like this finally you can feed your preprocessed data into randomforest this code runs without error on my notebook please let us know if this solves your problem
53681949,training issue in keras,pythonx keras sentimentanalysis,total params trainable params nontrainable params your model has more than million trainable parameters which is too much for your machines configuration cpu ram etc thus it cant handle it what are the options use smaller model use a better more powerful computer with gpu of course consider using an online cloud solution like crestle or paperspace
53249636,workaround for python memoryerror,python keras sentimentanalysis,your array appears to be k x k which is million elements of bits each because the default dtype is float so thats million bytes aka megabytes if you use float it will cut the memory usage in half or if you only care about and this will cut it by
52822022,tensorflow valueerror cannot feed value of shape for tensor inputdatax which has shape,python tensorflow sentimentanalysis trainingdata valueerror,the error comes from trainx tocategoricaltesty nbclasses this needs to be changed to testy tocategoricaltesty nbclasses also setting the batch size to none means it should expect the batch to be any size in your case you set the batch size to so you could also set the input shape to
51664903,request error google cloud nlp api with swift,swift googlecloudplatform alamofire sentimentanalysis googlenaturallanguage,sorry solved it my jsonrequest should be of type parameters according to alamofire
49379615,runtimeerror no active exception to reraise with tweepy,python twitter tweepy sentimentanalysis,you are encoding the string and then try to run refindall on it try without the encode call
48886181,python indexerror list index out of range with csv file,pythonx csv twitter sentimentanalysis,amended code the the following and problem solved
45325336,sklearn pipeline is not working,python scikitlearn pipeline sentimentanalysis,this worked for me
44480077,issue with spark mllib that causes probability and prediction to be the same for everything,python hadoop apachespark apachesparkmllib sentimentanalysis,tldr ten iterations is way to low for any real life applications on large and nontrivial datasets it can take thousand or more iterations as well as tuning remaining parameters to converge binomial logisticregressionmodel has summary attribute which can give you an access to a logisticregressionsummary object among other useful metrics it contains objectivehistory which can be used to debug training process
42976413,typeerror tuple indices must be integers or slices not str python sentiment tweet,python json tweepy sentimentanalysis urllib,here x is a tuple of your dictionarys keyvalueso you have to pass index if you simply want the data of text key you can write tweettext
42839404,error in if allo missing value where truefalse needed r,r sentimentanalysis,i think youre introducing problems when you lapply try checking dimsubtrainnotereco if that evaluates to true then you may have a different problem than i describe below from the glmnet github i am seeing if allo as part of glmnetrlognetr which i am assuming is called by cvglmnet in the lognet function we see earlier on line we see nc defined the rest of the code depends on the values of nc so i would suggest determining the results of dimsubtrainnotereco for starters you could try accessing those data differently also if you construct a dataframe we can copypaste to work with it will be much easier to debug
42244860,gridsearchcvfit returns typeerror expected sequence or arraylike got estimator,python machinelearning scikitlearn sentimentanalysis gridsearch,this error is because you are passing wrong parameter by using scoringfscore into the gridsearchcv constructor have a look at documentation of gridsearchcv in scoring param it asks for a string see model evaluation documentation or a scorer callable object function with signature scorerestimator x y if none the score method of the estimator is used you are passing a callable function with signature ytrue ypred which is wrong thats why you are getting the error you should use a string as defined here to pass in scoring or pass a callable with signature estimator x y this can be done by using makescorer change this line in your code to this i have answered for same type of problem in this answer here
41338978,text translation issue,r googletranslate sentimentanalysis rpackage,this paper documents the creation of a sentiment lexicon in norwegian and says all the sentiment lexicons are publicly available for those that are interested so i would suggest contacting the authors
41091487,twitter sentiment package issues npm,angularjs nodejs express npm sentimentanalysis,your regular expressios is weird first there are no quotations around regex in javascript also you are not escaping special charaters should be notice the escape of you may want to add in there too so for example text hello there text textreplaceg consolelogtext
39449644,performance issue while trying to match a list of words with a list of sentences in r,r sentimentanalysis sentimentr,i was able to use david arenburg answer with some modification here is what i did i used the following suggested by david to form the data frame the problem with the above approach is that it does not do the exact word match so i used the following to filter out the words that did not exactly match with the words in the sentence after applying the above line the output data frame changes as follows now apply the following filter to the data frame to remove those words that are not an exact match to those words present in the sentence now my resulting data frame will be as follows stridetectfixed reduced my computation time a lot the remaining process did not take up much time thanks to david for pointing me out in the right direction
37639516,does the ratio of two classes matter in classification problems,machinelearning sentimentanalysis,yes yes it can there are two things to consider of is thus you will try to model data distribution of your class based on just samples this might be orders of magnitude to small for neural network consequently you might need x more data to have a representative sample of your data while you can easily reduce the majority class through subsampling without the big risk of destroing the structure there is no way to get more structure from less points you can replicate points add noise etc but this does not add structure this just adds assumptions class imbalance can also lead to convergence to naive solutions like always false which has accuracy here you can simply play around with the cost function to make it more robust to imbalance in particular train splits suggested by purew is nothing else like black box method of trying to change the loss function so it has bigger weight on minority class when you have access to your classifier loss like in nn you should not due this but instead change the cost function and still keep all the data
36563597,error while using stanford core nlp,stanfordnlp sentimentanalysis,actually the problem was with different version usedsome of the jar files like xomejml where missingi fixed it by downloading the complete jar files from here stanford core nlp site with version and one of the import statements was changed to sentimentcoreannotationsannotatedtreeclass needs to be changed to sentimentcoreannotationssentimentannotatedtreeclass this changes have resolved my all errors
36483137,naive bayes in r sentiment analysis leads to cannot coerce class error,r sentimentanalysis,i believe you can wrap asmatrix with asdataframe or directly with asmatrixdataframe
34724246,attributeerror float object has no attribute lower,python twitter sentimentanalysis tweets,thank you dick kniep yesit is pandas csv reader your suggestion worked following is the python code which worked for me by specifying the field datatype in this case its string
34469227,error insertingretrieving tweets into mongolite db,r mongodb twitter sentimentanalysis mongolite,i think you wanted to use sapply which flattens the list of status object that searchtwitter returns in any case this works note that you need to install and then start mongodb for this to work yields the following scored bad scored neutral scored good
34064705,stanford nlp no annotator named sentiment error,java stanfordnlp sentimentanalysis,turns out that stanfordcorenlp v was left out from a previous implementation since august when i removed it the code started working fine
33921575,corenlp training model issue,stanfordnlp sentimentanalysis scoring,i think the short answer is no a difference in wording always has a chance of changing the sentiment of a sentence you can try mitigating the problem by retraining on new data really if youre running on anything but movie reviews you should expect the model to degrade in performance at least a little and occasionally a lot if you have the training data its worth retraining
31003716,function decodeshorturl from twitter package not working,r twitteroauth sentimentanalysis tweets urlshortener,update i wrapped this functionality in a package and managed to get it on cran sameday now you can just do you can pass in a vector of urls and get a dataframedataframe back in that form that particular bitly url gives a error heres a version of decodeshorturl that has an optional check parameter that will attempt a head request and throw a warning message for any http status other than you can further modify it to return na in the event the expanded link s i have no idea what you need this to really do in the event the link is bad note that the addd head request will significantly slow the process down so you may want to do a first pass with checkfalse to a separate column then compare which werent expanded then check those with checktrue you might also want to rename this to avoid namespace conflicts with the one from twitter
30060157,sentiment package installation from local zip file issue,r sentimentanalysis,please try this should allow you to select the file itself and install the package subsequently also please note that the package version may not be compatible with the current r version on your machine thanks
29829380,where i can find the file sentimenttreesdebugtxt,stanfordnlp sentimentanalysis,you can download them here a simple devtxt would look like hth
28494869,unicodedecodeerror on python,python python unicode sentimentanalysis,if you have input data that is malformed id not use codecs here to do the reading use the newer ioopen function and specify a error handling strategy replace should do i set the newline handling to to make sure the csv module gets to handle newlines in values correctly instead of passing in an open file just pass in the filename this wont let you skip faulty lines it instead will handle faulty lines by replacing characters that cannot be decoded with the ufffd replacement character you can still look for those in your columns if you want to skip the whole row
23819862,failed with error package sentiment was built before r please reinstall it,r sentimentanalysis snowball roauth,first i notice that url actually offers to download rstemzip not sentimentzip i think what your error message would be more helpful saying is youve downloaded a binary package for r go and find the binary package for r or later and download and install that instead here are the packages so perhaps you can try downloading and installing if that doesnt work you could try finding the source package and installing it that way but on windows that might be nontrivial
70771864,search is not working in elasticsearch for words ending with s y e,java elasticsearch stemming,root cause of the issue is stemmer as per elastic search docs algorithmic stemmers apply a series of rules to each word to reduce it to its root form for example an algorithmic stemmer for english may remove the s and es suffixes from the end of plural words you can refer to following sites for more detail in your application mappingjson file you can remove if any stemmer configuration is already present if your application does not have any searchable description field which can have plural words then you can remove stemmer from your configuration and it should work fine
69259983,stemming a text file to remove suffixes given linewise in another file using sed,bash sed stemming suffix,kinda hairy but sed and unix tools only the generates the substitution script of this requires gnu sed for b it would be easier with perl ruby awk etc here is a gnu awk both produce
67066233,porterstemmer stemming in python is not working,python stemming porterstemmer,two things jump out traindata in your question is a list containing one string consult change wait rather than a list of three strings consult change wait stemming converts to lowercase automatically if you intended for the list to contain one string this should work fine from nltkstem import porter stemmer porterporterstemmer list of one string stringinlist consult change wait for word in stringinlist printstemmerstemword print if you wanted a list of three strings then modify to include quotes between commas list of three strings individualwords consult change wait for word in individualwords printstemmerstemword print handling the upper vs lowercase at the start of the word requires passing a parameter but can make sense if youre trying to handle proper nouns eg distinguish stemmed change from the name chang stem but do not convert first character to lowercase for word in individualwords printstemmerstemword tolowercasefalse expected output when all three run
66377264,elasticsearch spanish stemming not working with rojo color,elasticsearch kibana stemming,seems like the stemmer type has a minimum token lengh i tried with rojos instead of rojo and stems to roj you can try with another approach like snowball stemming
52856329,problems in stemming in text analysis swedish data,r tm stemming snowball,using tidytext see issue the wordstem function is from the snowballc package which comes with multiple languages see getstemlanguages
40311334,get the longest common string prefix of all words in a list,python stemming,
37886430,snowball stemmer is not working,elasticsearch nest stemming snowball snowballanalyzer,i found out the issue analyser is not set to the attachment field and returns
37597984,python attributeerror list object has no attribute split,python arabic stemming lsa,titles is already a list do this instead
36354648,stemming process not working in python,python stemming,from the stemmer docs it looks like the stemmer is designed to be called on a single word at a time try running it on each word in ie before calling join eg edit comment response i ran the following and it seemed to stem correctly i dont think there is a stemmer that can be applied straight to text but you can wrap it in your own function that takes ps and the text
24191686,fastvectorhighlighter phrase highlighting not working with stemming,java solr lucene stemming fastvectorhighlighter,turns out hlfragsize wasnt set to a large enough value to include the entire highlighted sequence the silly problems are often the worst
23426533,lucene project fatal error,java lucene stemming porterstemmer,upgrade your jvm its well documented on the lucene website that you cannot use java because of a bug in the oracle jvm
21095637,elasticsearch snowball not working,elasticsearch querystring stemming snowball,your problem is you are using querystring and not defining a defaultfield so its searching against the all field which is using your default analyzer standard most likely to fix this do this i try to stay away from querystring searching though unless i really cant avoid it sometimes people coming from solr like this method of searching over the search dsl in this case try using match but either way yields the correct results see documentation here for the querystring
16147029,sphinx morphology stemen not working,sphinx stemming,turns out sphinx will not read modifications to config files for a realtime index after its been created see here for info the way around this is to then delete all index files for me held in varlibsphinxsearchdata then restart service this means you lose your existing index which will then have to be manually rebuilt from your database
13953146,solr wildcards configuration issue,solr wildcard stemming,since you use englishporterfilterfactory it eliminates the er suffix from the zeitalter and index it as zeitalt so if you want to find this keyword then you should either change your query as qvolltexteitalt or you should remove englishporterfilterfactory from the field definition and search using qvolltexteitalter
6192242,lucenenet stemming problem,lucenenet stemming,are you sure you used lucenenetanalysissnowballsnowballanalyzerenglish to write your index you have to use the same analyzer to write and query the index
4531433,data integration problem how to integrate similar entities,java python stemming editdistance dataintegration,considered something like refine
76749399,how to solve an attribute error when lemmatizing a listlower,pythonx list lowercase lemmatization,wordpatterns lemmatizerlemmatizewordlower for word in wordpatterns attributeerror list object has no attribute lower the error here occurs before lemmatize is even called the problem is already with wordlower you are expecting wordpatterns to be a list of strings but its not its a list of lists i followed the source of word to wordpatterns to wordpatterns documents this looks very suspicious and i think it is clearly meant to be wordpatterns document as document is your actual loop variable
63110020,error when using lemmatization and tf idf calculation on twitter data frame in python,python tfidf lemmatization,tfidfvectorizerfit takes string input not listyour dftweetlemmatized data should contain strings not lists for the better lemmatization you can use nltkpostag to get parts of speech and then lemmatize words based on their tag for example this way it will lemmatize word considering it is a verb
60508383,type error during text lemmatization in pandas dataframe,python pandas text lemmatization,one idea is apply solution only for non missing and no none values
58517007,cannot install lemmatizerprebuiltcompact library for net visual studio error,aspnet visualstudio dll lemmatization sharpnlp,i had solved the the reference is invalid or unsupported error by putting the dll library into the main directory of the project
52961019,why do i encountered attributeerror wordlistcorpusreader object has no attribute word in python,python lemmatization,its a typo the method you should be calling is stopwordswords change that into and that should fix this issue more information on the nltk documentation page
49205410,python name error name is not defined while using pandas dataframe,python python lemmatization,the exception says it all name temp is not defined so the variable temp is not defined before it is used the problem with your code is here if tag r is true and tokdociendswithly is not true then temp never gets defined consider adding an else clause like the one i inserted and commented out
26196036,r error in lemmatizzation a corpus of document with wordnet,r wordnet lemmatization,this answers your question but does not really solve your problem there is another solution above different answer that attempts to provide a solution there are several issues with the way you are using the wordnet package described below but the bottom line is that even after addressing these i could not get wordnet to produce anything but gibberish first you cant just install the wordnet package in r you have to install wordnet on your computer or at least download the dictionaries then before you use the package you need to run initdictpath to wordnet dictionaries second it looks like gettermfilter expects a character argument for x the way you have it set up you are passing an object of type plaintextdocument so you need to use ascharacterx to convert that to its contained text or you get the java error in your question third it looks like gettermfilter expects single words or phrases for instance if you pass this is a phrase to gettermfilter it will look up this is a phrase in the dictionary it will not find it of course so getindexterms returns null and getlemma fails so you have to parse the text of your plaintextdocument into words first finally im not sure its a good idea to remove punctuation for instance its will be converted to its but these are different words with different meanings and they lemmatize differently rolling all this up as you can see the output is still gibberish this is lemmatized as thistle and so on it may be that i have the dictionaries configured improperly so you might have better luck if you are committed to wordnet for some reason i suggest you contact the package authors
25643617,problems with solr tokenizer adding a lemmatizer,java solr lucene lemmatization,finally i did i modified the patterntokenizer and then i used the standardtokenizer to use the lemmatizer in brief i lemmatize the string from input and then create an stringreader with the lemmatized text here is the code hope it can be useful for somebody modifying the standardtokenizer script
79401652,checkpoints valueerror with downloading huggingface models,python terminal model huggingfacetransformers huggingface,following up on this question i asked i found the solution here basically deepseek is not a model supported by huggingfaces transformer library so the only option for downloading this model is through importing the model source code directly as of now
79338718,do i have to write custom automodel transformers class in case typeerror nvembedmodelforward got an unexpected keyword argument inputsembeds,deeplearning huggingfacetransformers largelanguagemodel peft,this is similar to this issue the reason is that tasktypefeatureextraction expects an additional inputembeds input which this model does not provide neither does clip for example remove the tasktypefeatureextraction basically setting tasktypenone so that you receive a plain peftmodel instance instead of a peftmodelforfeatureextraction and this problem should vanish
79309854,valueerror exception encountered when calling layer tfbertmodel type tfbertmodel,tensorflow tensorflow huggingfacetransformers bertlanguagemodel transformermodel,tldr use this explanation transformers package uses keras objects current version is keras packed in tensorflow since version fastest fix without downgrading tensorflow is to set legacy keras usage flag as above more info can be found here
79184146,error bus error running the simplest example on hugging face transformers pipeline macos m,huggingfacetransformers huggingface,using device the first gpu solved this classifier pipelinesentimentanalysis device
79082850,importerror using the with requires but i have version already installed,import pytorch huggingfacetransformers accelerate,just create a new conda environment and install everything from scratch as to the cause of your issue it could be that you were using the wrong python environment by accident it could also be a failed pip upgrade a lot of mistakes can lead to this outcome creating a new environment step by step from scratch can fix anything
78999652,error during the compilation of the tokenizers package when trying to install transformers,artificialintelligence huggingfacetransformers largelanguagemodel,chatgpt suggested you can try using python or to see if the issue is resolved since my python version was i downgraded to and reran pip install this successfully resolved the problem ive noticed that gemini flash only suggests me to update rust and cargo while gpto mini additionally mentions the issue of python version i have been using gemini before it seems i should compare these two models more in the future
78992237,berttokenizerfrompretrained raises unicodedecodeerror,python huggingfacetransformers,frompretrained take as input the path to the directory containing model weights saved using savepretrained not the bin file you can save your model modelsavepretrainedmymodeldirectory then you can load it berttokenizerfrompretrainedmymodeldirectory
78886512,outofmemoryerror cuda out of memory while using computemetrics function in hugging face trainer,python deeplearning pytorch huggingfacetransformers huggingface,did you already try to reduce perdeviceevalbatchsize you could also set evalaccumulationsteps to a low number and see if that helps check out this thread if nothing works you could use a smaller validation set and run a custom evaluation using smaller chunks of a test set although this might influence how well your model learns or you use a smaller model like from my experience in google colab you might also randomly get a gpu assigned that has a bit more or less vram eg gb vs gb which could make the difference for whether you run out of memory or not check out the second point here
78877667,topp sampling not working cuda error deviceside assert triggered,pytorch artificialintelligence huggingfacetransformers sampling logits,the problem is the indexing youre doing at this line logitssortedindicessortedindicestokeep floatinf for reasons ill explain this is causing an index out of bounds error out of bounds indexing is a common cause of cuda error deviceside assert triggered errors consider the following import torch import torchnn as nn torchmanualseed topp logits torchrandn random logits sort logits sortedlogits sortedindices torchsortlogits descendingtrue calculate cumulative probs cumulativeprobs torchcumsumtorchnnfunctionalsoftmaxsortedlogits dim dim apply top p threshold to cumulative probs sortedindicestokeep cumulativeprobs topp ensure at least one index is kept sortedindicestokeep true this is the problem logitssortedindicessortedindicestokeep floatinf printlogitsshape sortedindicessortedindicestokeepshape torchsize torchsize when you index sortedindicessortedindicestokeep both inputs are of shape but the output is of shape or similar depending on the random seed for the dummy logits this happens because the sortedindicestokeep has an irregular number of true values in each row this means the indexing operation cant resolve the output into a clean d tensor where every row is the same size pytorch handles this situation by returning an unrolled vector of every true value from the indexing tensor this means when you try to compute logitssortedindicessortedindicestokeep you are using a long d tensor to index into a small d tensor if you run this on cpu you get an error like indexerror index is out of bounds for dimension with size when you run on gpu you get the cuda assert error to fix this use the scatter operation use something like this def toppfilteringlogits topp shiftindicestrue debugfalse filter the logits using topp nucleus sampling sort logits in descending order and get the sorted indices sortedlogits sortedindices torchsortlogits descendingtrue compute the cumulative probabilities of the sorted logits cumulativeprobs torchcumsumtorchnnfunctionalsoftmaxsortedlogits dim dim create a mask for the tokens to keep sortedindicestokeep cumulativeprobs topp optional shift indices to the right this results in keeping the first token above the topp threshold skip this line to ensure that all token probs are strictly below the topp threshold if shiftindices sortedindicestokeep sortedindicestokeep clone ensure that at least one token is kept the first token which has the highest logit sortedindicestokeep true use scatter to create topp mask mask sortedindicestokeepscatterdim indexsortedindices srcsortedindicestokeep optional debug check to make sure topp is being honored note we need to compute probs before masking because applying softmax after masking will result in a distribution that sums to if debug probs torchnnfunctionalsoftmaxlogits dim probsmask printprobssum use mask to set logit vals to inf logitsmask floatinf return logits
78863932,runtimeerror numpy is not available transformers,python pythonx numpy huggingfacetransformers,try then restart the kernel
78812089,received server error while deploying huggingface model on sgaemaker,huggingfacetransformers amazonsagemaker endpoint huggingface amazonsagemakerstudio,it seems most of the doc on the topic including huggingface doc was out of date you no longer need the repackage the modeltargz with codeinferencepy all i had to do was pass the s path to my initial modeltargz after training to the estimator and pass the location of inferencepy and requirementstxt in the sourcedir and entrypoint
78803529,error while running hugging face models on kaggle notebook,python pythonx huggingfacetransformers kaggle,try setting the following backends to false source
78697835,deepspeed attributeerror dummyoptim object has no attribute step,python huggingfacetransformers largelanguagemodel huggingfacetrainer deepspeed,adding this solves the issue
78638576,error runtimeerror cuda error operation not supported when tried to locate something into cuda,python pytorch huggingfacetransformers,i had this same issue when trying to run cuda on vm with a vgpu the problem appears to be that either vgpu is not configured properly or it does not have a licence you can verify if it is licensed with nvidiasmi q the issue was resolved when the vm maintainers changed the gpu access from vgpu to full gpu passthrough more info on that here
78474448,oserror model does not appear to have a file named configjson,python machinelearning deeplearning huggingfacetransformers huggingface,it seems this model is an openclip only model right now you can not load it the usual way you should first install openclip with pip install opencliptorch then use this code
78451428,python accelerate package thrown error when using trainer from transformers,python huggingfacetransformers,seems like you have to force update accelerate with the specific version simply installing accelerate wont work as it will pick the latest to be as listed below you will have to force install by specifying it as the version
78347434,problem setting up llama in google colab cellrun fails when loading checkpoint shards,python huggingfacetransformers largelanguagemodel llama,the issue is with colab instance running out of ram based on your comments you are using basic colab instance with gb cpu ram for llama model youll need for the float model about gb but youll need both cpu ram and same gb gpu ram for the bfloat model around gb and still not enough to fit basic colab cpu instance given that youll also need to store intermediate calculations from the model check this link for the details on the required resources huggingfaceconousresearchllamabchathfdiscussions also if you want only to do inference predictions on the model i would recommend to use its quantized bit or bit versions both can be ran on cpu and dont need a lot of memory
78284637,attributeerror module torch has no attribute version,python pytorch huggingfacetransformers,according to the information given in your post you have two requirements the transformers package you are trying to use requires a pytorch version of the maximum cuda version supported by your gpu is so the cuda toolkit installed via pytorch must be that cuda version was decommissioned on the release of pytorch see release notes so a viable solution for you would be to see whether you can update your cuda driver to or with that version you will be able to install pytorch with the appropriate cuda toolkit
78280443,google colab error when importing tfbertmodel,tensorflow keras googlecolaboratory huggingfacetransformers,for the runtime python cpu created today i was able to run the following tfbertmodel just fine in google colab today i noticed that our tensorflow versions differ other than that i dont believe that tfbertmodel is being deprecated transformers version tensorflow version example working tfbertmodel code output
78199269,conversationalretrievalchain raising keyerror,python huggingfacetransformers langchain largelanguagemodel,if youre using transformerspipeline then make sure that this returntensorspt parameter is not passed
78164569,error while loading a model from huggingface,python huggingfacetransformers,the model file which you shared does not have tokenizer file hence its throwing error if you just load the model the below code works fine output the repo used huggingfaceautoformertourismmonthly is for timeseries forecasting hence it wont contained any tokenizer file if you are using to perform some time series prediction you can refer the below hugging face snippet here
78155250,langchainhuggingface pipeline error about modelkwargs which i did not include,pipeline translation huggingfacetransformers langchain largelanguagemodel,your code doesnt have any errors the reason for the error is that as of as of version of langchaincommunity only the tasks texttextgeneration textgeneration summarization are supported with huggingfacepipeline your task is translation which is not supported as of yet as to why the error occurs langchain passes the argument returnfulltext see this line to the underlying huggingface model however marianmtmodel the model youre using doesnt take this as a parameter youre better off using the base huggingface model directly this is the easiest solution translation entodepipelinehello how are you printtranslation output it returns without error
77993164,runtimeerror cuda error no kernel available for execution on the device for cuda and torch,python pytorch cuda huggingfacetransformers llama,as mentioned in the warning the main issue was that torch was trying to run code on the gt and due to it being not supported the program crashed to fix this i added an environment variable so torch just ignores the gt now and it runs perfectly fine for me now
77880191,runtimeerror expected issm issm to be true but got false,pytorch cuda nvidia huggingfacetransformers largelanguagemodel,installing a specific version of pytorch fixed my issue i used this from the github issue comment installing the nightly thanks to palonix for the hint to the github issue
77877783,running through this error attributeerror cant set attribute when finetuning llama,huggingfacetransformers llama,the error attributeerror cant set attribute is raised when you attempt to change a property see more here from the error message this line is the cause of the problem selfcansaveslowtokenizer false if not selfvocabfile else true the cansaveslowtokenizer is updated to be a property in this commit the line does not exist in transformers it was replaced with property def cansaveslowtokenizerself bool return ospathisfileselfvocabfile if selfvocabfile else false as you mentioned you may have dependencies conflict consider creating a new virtual environment and install transformers
77792137,how to fix the learningrate for huggingfaces trainer,machinelearning deeplearning huggingfacetransformers huggingfacetrainer learningrate,a warmup is in general an increase of the learning rate it starts at and then increases linearly over here step to the specified learning rate of e afterwards by default a linear in other cases a cosine learningrate scheduler decays your learningrate to disable the decay add lrschedulertypeconstant if i recall correctly this also disables the warmup if you want warmup and afterwards a constant rate use constantwithwarmup instead edit valid scheduler types are defined in trainerutilspy in the class schedulertype class schedulertypeexplicitenum scheduler names for the parameter in by default it uses linear internally this retrieves scheduler from scheduler types linear getlinearschedulewithwarmup cosine getcosineschedulewithwarmup cosinewithrestarts getcosinewithhardrestartsschedulewithwarmup polynomial getpolynomialdecayschedulewithwarmup constant getconstantschedule constantwithwarmup getconstantschedulewithwarmup inversesqrt getinversesqrtschedule reducelronplateau getreduceonplateauschedule cosinewithminlr getcosinewithminlrschedulewithwarmup warmupstabledecay getwsdschedule linear linear cosine cosine cosinewithrestarts cosinewithrestarts polynomial polynomial constant constant constantwithwarmup constantwithwarmup inversesqrt inversesqrt reduceonplateau reducelronplateau cosinewithminlr cosinewithminlr warmupstabledecay warmupstabledecay
77656467,attention mask error when finetuning mistral b using transformers trainer,python huggingfacetransformers mistralb,experiencing the same issue downgrading transformers to instead of latest version seems to work fine
77628127,transformers crossentropy loss masked label issue,python huggingfacetransformers gpt,you get the same result because you do not actually modify targetids this set all values to except the last seqlen that mean you exclude all the result is an empty tensor tensor size dtypetorchint to get different result use a value less than seqlen using for example output
77555030,how to resolve bert hf model valueerror too many values to unpack expected,python tensorflow machinelearning huggingfacetransformers bertlanguagemodel,the preparetfdataset function does not require the dataframe to have tensor value types removing returntensorstf should solve the problem def tokenizedatasetdf keys of the returned dictionary will be added to the dataset as columns return tokenizer dftextcolumn dftextcolumn paddingtrue truncationtrue maxlength
77269591,nvidia driver too old error when loading bart model onto cuda works on other models,huggingfacetransformers azuremachinelearningservice,installing pytorch through transformers extras probably not the best way to get compatible torch to your environment based on the cuda drivers in your base can try to install torch recommended way it should be fine with transformers that only pin a lower bound alternatively you can try more recent cuda nvcrio if you have an option to specify it posting comment on the question as answer
77261337,aws sagemaker endpoint error while deploying llm model,python amazonwebservices amazons huggingfacetransformers amazonsagemaker,if you are using the lmi container djl you need a servingproperties and an optional modelpy for exampe llama b with customized preprocessing octocoder
77253558,localentrynotfounderror while building docker hugging face model,python docker awslambda dockerfile huggingfacetransformers,so the issue was because the environment variable was set to use transformers in offline mode env transformersoffline
77107785,fastapi custom validator error fastapipydantic not recognizing custom validator functions runtimeerror no validator found for,fastapi huggingfacetransformers pydantic peft,the issue arises likely due to the fact that pydantic tries to validate all datatypes of your model is an int really an int etc and you have some custom datatypes that pydantic does not know how to handlevalidate out of the box you could tell it to allow those datatypes nevertheless and validate them yourself though as you already do with your validator class saicmodelfortextgenbasemodel model customtype yourmodel class config arbitrarytypesallowed true you can find more info on this and other model config options here
77061667,resume from checkpoint gives device error in huggingface transformers trainer,pytorch huggingfacetransformers,as pointed in huggingface github this is a known issue of loading optimizer changing the line in trainerpy from to will fix the issue huggingface transformer v pytorch v
77006745,oserror metallamallamabchathf is not a local folder,python huggingfacetransformers huggingface llama,the pretrainedmodelnameorpath may the model repo or the model path in your case the model repo is metallamallamabchathf which is right according to you must agree to the terms and conditions in the above link in order to access the model
76956484,how to fix this runtime error in this databricks distributed training tutorial workbook,pyspark gpu databricks huggingfacetransformers distributedcomputing,turns out i did need to configure the cli i never expected i would have to do that from within a databricks notebook but spark is a harsh mistress to do so click on the icon that sort of looks like a terminal on the bottom right when you mouse over it it will say open bottom panel then type in databricks configure and from there follow the steps do not close the panel it is ephemeral so only close it after finetuning is completed running it from there led me to another error though i had commented out the line reporttotensorboard remove mlflow integration for now that was a mistake mlflowexceptionsrestexception resourcedoesnotexist no experiment was found if using the python fluent api you can set an active experiment under which to create runs by calling mlflowsetexperimentexperimentname at the start of your program only after uncommenting out the line and configuring the cli did the code work
76875718,keyerror marketplace while downloading amazonusreviews dataset huggingface datasets,python huggingfacetransformers huggingface huggingfacedatasets,your syntax is correct i think the issue is on amazons side even the publiclyfacing root page which should contain a readme is now throwing access denied perhaps use a different dataset for now there is also amazonreviewsmulti update amazon has decided to stop distributing this dataset refer here
76863889,how does one fix an interleaved data set from only sampling one data set,python huggingfacetransformers huggingface huggingfacedatasets,the interleavedatasets function works correctly here its your conclusion that is incorrect what happens is that when two datasets are interleaved their features are combined these are the features of c and wikitext printccolumnnames text timestamp url printwikitextcolumnnames text when you combine the datasets all examples in the new dataset will have features text timestamp url even if they come from wikitext dataset since wikitext dataset does not have features timestamp and url these will be none dummy example from datasets import dataset interleavedatasets d datasetfromdict feature a b c d datasetfromdict feature dataset interleavedatasetsd d probabilities seed printfeatures datasetcolumnnames for e in dataset printe output
76857722,huggingface sft for completion only not working,python pytorch huggingfacetransformers huggingface huggingfacetrainer,i have a similar issue i think youre forgetting to add formattingfunc function also by default setting datasettextfield overrides the use of the collator so try without that argument heres how i call it it runs and stores things to wandb but my problem is my loss is always nan lemme know if you found the issue
76744939,importerror using the trainer with pytorch requires accelerate,python pytorch artificialintelligence googlecolaboratory huggingfacetransformers,i had the same error in colab with what helped was and restarting the runtime afterwards ps this was not my idea of course credits go to
76740367,how to resolve the error importerror cannot import name generationconfig from transformers,pytorch huggingfacetransformers,it looks like torchvision and torchaudio are not compatible with your current torch version try to install pytorchs stable version using this command or generate the desired combination from see also my answer to a similar question
76735605,cudapytorch transformer model import error,pytorch huggingfacetransformers,you should install the function from the transformers package you could use the following code this should load the model i used this version of the package
76617863,flantxxl valueerror need either a or a containing offloaded weights,huggingfacetransformers,for who need create a folder name savefolder for example then update to
76447153,how to use a llama model with langchain it gives an error pipeline cannot infer suitable model classes from huggingface,python huggingfacetransformers langchain chromadb largelanguagemodel,before using the langchain api to the huggingface model you should try to load the model in huggingface and thatll throw some errors then looking into the model files it looks like only the adapter model is saved and not the model so the automodel is throwing tantrums to load an adapted model you have to the base model and the peft adapter model separated first the installs restart after installs if needed then to load the model take a look at the guanaco example trying to install guanaco pip install guanaco for a text classification model but getting error you will need a gpu runtime now you can load the model that youve adaptedfinetuned in huggingface transformers you can try it with langchain before that we have to dig the langchain code to use a prompt with hf model users are told to do this but when we look at the huggingfacehub object it isnt just a vanilla automodel from transformers huggingface when we look at we see that its trying to load the llm argument with some wrapper class so we dig deeper into langchains huggingfacehub object at the huggingfacehub object wraps over the huggingfacehubinferenceapiinferenceapi for the textgeneration texttextgeneration or summarization tasks and huggingfacehub looks like some spaghetti like object that inherits from llm object to summarize this a little we want to load a huggingfacehub with langchain api and the huggingfacehub is actually a wrapper over the huggingfacehubinferenceapiinferenceapi and the huggingfacehub object is a subclass of llmbasellm given that knowledge on the huggingfacehub object now we have several options opinion the easiest way around it is to totally avoid langchain since its wrapper around things you can write your customized wrapper that skip the levels of inheritance created in langchain to wrap around as many tools as it canneed ideally ask the langchain developermaintainer to load peftadapter model and write another subclass for them practical lets hack the thing and write our own llm subclass practical solution lets try to hack up a new llm subclass phew langchain didnt complain and heres the output epilogue apparently this llama model doesnt understand that all it needs to do is to reply waffles waffles waffles tldr see
76307106,how to fix gpu out of memory in pytorch,python pytorch huggingfacetransformers,so i found some info that might be relevant this person said that they were able to solve their memory problems in the wavvec by preventing longer sound clips as input try to experiment with excluding clips that exceed a certain length
76300212,problems combining tensorflow with hugginggpt transformers on a python project,python tensorflow huggingfacetransformers,i recommend using the latest stable version and not a release candidate in this case install tf and keras also i dont recommend using brew python install conda and create a virtual env to experiment with installations there if you are not very experienced with packages and versions final the error that you mention tfbertforsequenceclassification requires the tensorflow library but it was not found in your environment can be easily solved google is your friend so to wrap up install conda setup a virtual env install everything in that virtual env make sure to activate it before install stuff with conda and not with pip recommendation not mandatory make sure you are using gpu tf pip install tensorflowgpu for example the last error is related to this conda handles quite good gpu support conda getting started
76252227,importerror using the with requires,python pytorch huggingfacetransformers tensorrt accelerate,either installing pip install accelerate u and then restarting the runtime or downgrading the pytorch to version should work
76225595,nameerror name partialstate is not defined error while training hugging face wavevec model,python huggingfacetransformers,as of the error seems to be caused by an issue in the huggingfaceaccelerate library you can try following solutions reinstall transformers accelerate pip uninstall y transformers accelerate pip install transformers accelerate if you are using colabjupyter make sure to restart the notebooks runtime install dev version of accelerate pip install git reverse to previous version of transformers you might also need to uninstall transformers first pip uninstall y transformers pip install transformers
76199989,problem with custom metric for custom t model,python huggingfacetransformers pretrainedmodel huggingfacedatasets largelanguagemodel,it seems like the task youre trying to achieve is some sort of translation task so the most appropriate model is to use the automodelforseqseqlm and in the case of unspecified sequence it might be more appropriate to use bleu chrf or newer neuralbased metrics for translation rouge for summarization you can take a look at various translationrelated metrics on treating it as a normal machine translation task to read the data youll have to make sure that the models forward function sees the data point as text labels in your datasetsdataset object use the collator to do batch eg datacollatorforseqseq and heres a working snippet of how the code in parts can be ran data processing part from datasets import dataset import evaluate from transformers import automodelforseqseqlm trainer autotokenizer datacollatorforseqseq mathdata text x x x x x x x x x x target d x x d x x d x d x x d x x mathdataeval text xxy x target x xy x dstrain datasetfromdictmathdata model automodelforseqseqlmfrompretrainedtsmall tokenizer autotokenizerfrompretrainedtsmall datacollator datacollatorforseqseqtokenizer dstrain dstrainmaplambda x tokenizerxtext truncationtrue paddingmaxlength maxlength dstrain dstrainmaplambda y labels tokenizerytarget truncationtrue paddingmaxlength maxlengthinputids dseval datasetfromdictmathdataeval dseval dsevalmaplambda x tokenizerxtext truncationtrue paddingmaxlength maxlength dseval dsevalmaplambda y labels tokenizerytarget truncationtrue paddingmaxlength maxlengthinputids metric definition part trainer setup part that works and good but why is the output of the model still so bad most probably you need to tune some hyperparameter batchsize more data different learning rates or increase no of maxsteps it can also be that your vocab is pretrained for natural language but your data isnt in that case ill suggest to try modifying the tokenizer before training eg how to add new tokens to an existing huggingface tokenizer
76061747,huggingface infomer runtimeerror mat and mat shapes cannot be multiplied x and x,pythonx pytorch huggingfacetransformers,it looks like the data or pretrained models topology they provided in the tutorial cause some dimensionality error in the models informalvalueembedding layers which consists of a linear layer with input size and output size this layer in located and used in both informerencoder and informerdecoder parts of the model printmodel you can change the input dimension of these layers by but this will reset the pretrained weights in these layers you want to finetune the model for some time to learn the weights there before making predictions
75891072,valueerror unable to infer channel dimension format,python pytorch huggingfacetransformers,according to this discussion you need to load your image imageopenpathtoimageconvertrgb to fix this issue
75836953,how to resolve typeerror dispatchmodel got an unexpected keyword argument offloadindex,python pytorch huggingfacetransformers,please try updating your accelerate package for example
75713161,finetuning vision encoder decoder models with huggingface causes valueerror expected sequence of length at dim got,python huggingfacetransformers,update in my case i solved it by passing trunction true inside the tokenizer for more check
75587208,key error when importing hugging face model into aws lambda function,amazonwebservices awslambda huggingfacetransformers amazonefs huggingface,my best guess here on the issue is that i am using an older docker image huggingfacetransformerspytorchcpu if you look on docker youll see this t been updated in over a year so im going to save the model to my local machine then push this to efs so my lambda can access it from a mounted directory hope that works
75385142,tokenizerpushtohubreponame is not working,python pytorch huggingfacetransformers huggingfacetokenizers huggingface,i have the same problem it is somehow associated with version of transformers i have when i change environment to the one with transformers version the problem is that code tries to clone repository which i am going to yet create and there is an error remote repository not found checked more and it looks like issue with version with huggingfacehub library when it is downgraded to it should work
75237628,tokenizersavepretrained typeerror object of type property is not json serializable,python huggingfacetransformers gpt,the problem is on the line tokenizerpadtoken gpttokenizereostoken here the initializer is wrong thats why this error occurred a simple solution is to modify this line to tokenizerpadtoken tokenizereostoken for the reference purpose your final code will look like this from transformers import gpttokenizer gptlmheadmodel tokenizer gpttokenizerfrompretrainedgpt tokenizerpadtoken tokenizereostoken datasetfile xcsv df pdreadcsvdatasetfile sep inputids tokenizerbatchencodepluslistdfx maxlengthpaddingmaxlengthtruncationtrueinputids saving the tokenizer tokenizersavepretrainedtokenfile
75229395,typeerror not supported between instances of torchdevice and int,python machinelearning huggingfacetransformers torch,this was a bug with the transformers package for a number of versions prior to v given that particular line of code does not discern between the type of the device argument could be a torchdevice before comparing that with an int tracing through git blame we can find that this specific change made in changeset daab include the much needed if isinstancedevice torchdevice provided by line in the resulting file which will ensure this error wont happen checking the tags above will show that the release for v and after should include this particular fix as a refresher to update a specific package activate the environment and issue the following alternatively with a specific version eg
75209070,txtaidatabasesqlbasesqlerror no such function jsonextract,python huggingfacetransformers sentencetransformers,it looks like the version of sqlite packaged with centos doesnt have the json extension enabled there may be a rd party repository with a newer version the link below shows how sqlite could be recompiled with json enabled alternatively you can try with another linux distro almost all distros released within the last years have json support enabled
75158430,error img when applying increment with keras and transformers for,python image keras classification huggingfacetransformers,i guess the error is here you are trying to access examples dictionary using img key from some code above it looks like the key should be image
75059015,problem with trained model and load model,speechrecognition huggingfacetransformers transformermodel,its a little bit odd i encountered this issue and solved it via a strange path try to use the same directory for your inference as your training directory after wrapping up the model and running it with the python interpreter instead of conda i have never seen the bug again i do not know the reason for this maybe someone can help me be more accurate about the cause
74948551,indexerror index out of range in self when using summarization hugging face,python huggingfacetransformers huggingface,output aaron rodgers is fresh out of a victory over the super bowl champion los angeles rams and lambeau last evening on monday night football the backtoback nfl mvp says hes looking forward to a holiday party
74593644,how to fix no token found error while downloading hugging face,pythonx pytorch huggingfacetransformers,use generate token from and past it install python lib huggingfacehub if you are using notebooke past your genrated token
74437271,data collation step causing valueerror unable to create tensor due to unnecessary padding attempts to extra inputs,python pytorch huggingfacetransformers pytorchdataloader huggingface,i solved this by extending the datacollatorforseqseq class and overriding the call method in it to also pad my spkuttpos list appropriately
74304875,oserror there was a specific connection error when trying to load compvisstablediffusionv,python huggingfacetransformers huggingface stablediffusion,just go to the huggingface website and agree the terms you need to be logged into your hugging face account
73952853,getting an error install a package on the terminal to use hugging face in vs cod,tensorflow torch huggingfacetransformers huggingfacetokenizers huggingface,thats the primary error that youre having youre going to need to install the rustlang compiler in order to finish the install
73593136,typeerror dropout argument input position must be tensor not tuple,python pytorch huggingfacetransformers,the issue you face is that the output of selfbert is not a tensor but a tuple from transformers import bertforsequenceclassification berttokenizer bertmodelname bertbasecased t berttokenizerfrompretrainedbertmodelname m bertforsequenceclassificationfrompretrainedbertmodelname returndictfalse omttest test returntensorspt printtypeo output i personally do not recommend using returndictfalse as the code becomes more difficult to read but changing this parameter doesnt help in your case as you want to use the pooler output which is removed by the classification head of bertforsequenceclassification the output of bertforsequenceclassification is listed here you already wrote in your own answer that you dont intend to use the classification head of bertforsequenceclassification and you can therefore load bertmodel directly instead of initializing bertforsequenceclassification and only using bert as you did with bertforsequenceclassificationfrompretrainedbertmodelname returndicttruebert from torch import nn from transformers import bertmodel berttokenizer class bertclassificationmodelnnmodule def initself bertmodelname numlabels dropout superbertclassificationmodel selfinit selfbert bertmodelfrompretrainedbertmodelname selfdropout nndropoutdropout selfclassifier nnlinear numlabels selfnumlabels numlabels def forwardself inputids attentionmasknone tokentypeidsnone pooledoutput selfbertinputids attentionmaskattentionmask tokentypeidstokentypeidspooleroutput pooledoutput selfdropoutpooledoutput logits selfclassifierpooledoutput return logits m bertclassificationmodelbertbasecased o mttest test returntensorspt printoshape output
73433868,systemerror googleprotobufpyextdescriptorcc bad argument to internal function while using audio transformers in hugging face,python tensorflow pytorch huggingfacetransformers,import tensorflow lib first even if you are not using it before importing any torch libraries dont know the exact reason but after importing the lib code is working on the notebook you have shared refer to these links torchvision and tensorflowgpu import error
73428120,runtimeerror msecuda not implemented for long when training a transformertrainer,python pytorch huggingfacetransformers,changing the datatype of the labels column from int to float solved this issue for me if your dataset is from a pandas dataframe you can change the datatype of the column before passing the dataframe to a dataset
73415504,error importing layoutlmvfortokenclassification from huggingface,pytorch huggingfacetransformers,this is issue from importing torchfix flag check for symbolic trace and new commit error of detectron use aebbbdcbacbcabaee this checkout and install for temporary work or clone pytorch with new commit
73358850,valueerror no gradients provided for any variable tfdebertavforsequenceclassificationdebertaembeddingswordembeddings,python pandas tensorflow keras huggingfacetransformers,i would say its probably due to the fact that you are not adding a loss to the compilation thus no gradient can be computed wrt it
73247922,runtimeerror failed to import transformerspipelines because of the following error look up to see its traceback initialization failed,pythonx tensorflow jupyternotebook pytorch huggingfacetransformers,changed kernel condatensorflowp
73201858,no module named keras error in transformers,python keras tensorflow huggingfacetransformers,turns out a new version of the huggingfacetransformers library was released a few days ago so setting the transformers version to solved the issue maybe upgrading tensorflow to might work as well
72845812,bertbaseuncased typeerror tuple indices must be integers or slices not tuple,python machinelearning pytorch huggingfacetransformers bertlanguagemodel,each bertlayer returns a tuple that contains at least one tensor depending on what output you requested the first element of the tuple is the tensor you want to feed to the next bertlayer a more huggingfacelike approach would be calling the model with outputhiddenstates o modelinputids outputhiddenstatestrue printlenohiddenstates output the first tensor of the hiddenstates tuple is the output of your extractembeddings object token embeddings the other tensors are the contextualized embeddings that are the output of each bertlayer you should by the way provide an attention mask because otherwise your padding tokens will affect your output the tokenizer is able to do that for you and you can replace your whole texttoinput method with tokenizera sentence returntensorspt paddingmaxlength maxlength
72605644,mobilevit binary classification valueerror and must have the same shape received none vs none,tensorflow deeplearning pytorch huggingfacetransformers imageclassification,you need to change the numclasses instead of numclasses as you have used sigmoid activation function which returns the values between to for binary classification the values will be as class in between two binary classes please refer to the replicated gist for your reference
72280030,how to resolve transformer model distilbert error got an unexpected keyword argument specialtokensmask,python huggingfacetransformers transformermodel,i was not able to reproduce your errors on my environment ubuntu but from what i see id suggest to try adding the returnspecialtokensmaskfalse parameter tokenspt tokenizerencodeplus text addspecialtokenstrue truncationtrue paddingmaxlength returnattentionmasktrue returntensorspt returnspecialtokensmaskfalse if that fails try to remove it explicitly tokensptpopspecialtokensmask
71944781,error in trying to push model to huggingface,git googlecolaboratory huggingfacetransformers,to my knowledge all you have to do is delete a folder on your drive to the name of dialogptsmalltechnoblade otherwise i dont know see what the results are if you try it on a vm for linux i recommend virtualbox
71922261,typeerror setup got an unexpected keyword argument stage,python pytorch huggingfacetransformers pytorchlightning,you need to add an extra argument stagenone to your setup method ive played with pytorch lightning myself for multigpu training here although some of the code is a bit outdated metrics are a standalone module now you might find it useful
71401458,hugginface dataloader bert valueerror too many values to unpack expected ax hyperparameters tuning with pytorch,pytorch huggingfacetransformers bertlanguagemodel hyperparameters dataloader,your dataloader returns a dictionary therefore the way you loop and access it is wrong should be done as such train network for in rangenumepochs your dataloader returns a dictionary so access it as such for batch in traindataloader move data to proper dtype and device labels batchtargetstodevicedevice attenmask batchattentionmasktodevicedevice inputids batchinputidstodevicedevice zero the parameter gradients optimizerzerograd forward backward optimize outputs netinputids attentionmaskattenmask
71398882,cuda runtimeerror cuda out of memory bert sagemaker,python gpu amazonsagemaker huggingfacetransformers,a cuda out of memory error indicates that your gpu ram random access memory is full this is different from the storage on your device which is the info you get following the df h command this memory is occupied by the model that you load into gpu memory which is independent of your dataset size the gpu memory required by the model is at least twice the actual size of the model but most likely closer to times initial weights checkpoint gradients optimizer states etc things you can try provision an instance with more gpu memory decrease batch size use a different smaller model
71335585,huggingface valueerror connection error and we cannot find the requested files in the cached path please try again or make sure your internet con,pythonx tensorflow huggingfacetransformers valueerror gpt,i saw a answer in github which you can have a try pass forcedownloadtrue to frompretrained which will override the cache and redownload the files link at bypatilsuraj
71318599,bert classifier valueerror target size torchsize must be the same as input size torchsize,python machinelearning pytorch huggingfacetransformers bertlanguagemodel,in case anyone stumbles on this like i did ill write out an answer since there arent a lot of google hits for this target sizeinput size error and the previous answer has some factual inaccuracies unlike the previous answer would suggest the real problem isnt with the loss function but with the output of the modelnnbcewithlogitsloss is completely fine for multilabel and multiclass applications chiara updated her post saying that in fact she has a binary classification problem but even that should not be a problem for this loss function so why the error the original code has this means run the model then create preds with the row indeces of the highest output of the model obviously there is only a index of highest if there are multiple predicted values multiple output values usually means multiple input classes so i can see why shai though this was multiclass but why would we get multiple outputs from a binary classifier as it turns out bert or huggingface anyway for binary problems expects that nclasses is set to setting classes to puts the model in regression mode this means that under the hood binary problems are treated like a twoclass problem outputting predictions with the size batch size one column predicting the chance of it being a and one for the chance of it being the loss fucntion throws an error because it is supplied with only one row of onehot encoded labels targets dtargetstodevice so the labels have dimensions batch size or after the unsqueeze batch size either way the dimensions dont match up some loss functions can deal with this fine but others require the exact same dimensions to make things more frustrating for version nnbcewithlogitsloss requires matching dimensions but later versions do not one solution may therefore be to update your pytorch version would work for example for me this was not an option so i ended up going with a different loss function nncrossentropyloss as suggested by shai indeed does the trick as it accepts any input with the same length in other words they had a working solution for the wrong reasons
71166789,huggingface valueerror expected sequence of length at dim got,python deeplearning pytorch huggingfacetransformers bertlanguagemodel,i fixed this solution by changing the tokenize function to note the padding argument also i used a data collator like so
71147064,arrowtypeerror could not convert,python numpy googlecolaboratory huggingfacetransformers,if the img column is a list of pil images you need to change the features of img from array to image replace features with the following
71086923,transformers longformer classification problem with f precision and recall classification,python huggingfacetransformers,my guess is that the transformation of your dependend variable was somehow messed up this i think because all your metrics which depend on tp true posivites are both precision and sensitivityrecall depend on tp as numerator fscore depends on both metrics and therefore on tp as numerator if the numerator is because you have no tp the result will be as well a goodmoderate accuracy can also be achieved if you only got the tn right that is why you can have a valid looking accuracy and for the other metrics so peek into your testtraining sets and look whether the split was succesful and whether both possible outcomes of you binary variable are available in both sets if one of them is lacking in the training set this might explain a complete missclassification and lack of tp in the testset
71012012,modulenotfounderror no module named transformers,python artificialintelligence googlecolaboratory importerror huggingfacetransformers,probably it is because you have not installed in your new since youve upgraded to colabs pro session the library transformers try to run as first cell the following pip install transformers the at the beginning of the instruction is needed to go into terminal mode this will download the transformers package into the sessions environment
70850015,oserror you seem to have cloned a repository without having gitlfs installed please install gitlfs and run git lfs install followed by git lfs pul,python git huggingfacetransformers gitlfs oserror,ive now installed and initialised git lfs in cloned folder terminal
70754085,valueerror the state dictionary of the model you are trying to load is corrupted are you sure it was properly saved,python huggingfacetransformers bertlanguagemodel onnx huggingfacetokenizers,exactly what i was looking for textattackalbertbasevmrpc how to use from the transformers library from transformers import autotokenizer automodelforsequenceclassification tokenizer autotokenizerfrompretrainedtextattackalbertbasevmrpc model automodelforsequenceclassificationfrompretrainedtextattackalbertbasevmrpc or just clone the model repo git lfs install git clone if you want to clone without large files just their pointers prepend your git clone with the following env var gitlfsskipsmudge
70746737,typeerror nlllossnd argument input position must be tensor not tuple,python pytorch huggingfacetransformers bertlanguagemodel,this is what it returns loss torchfloattensor of shape optional returned when labels is provided classification or regression if confignumlabels loss logits torchfloattensor of shape batchsize confignumlabels classification or regression if confignumlabels scores before softmax hiddenstates tupletorchfloattensor optional returned when outputhiddenstatestrue is passed or when configoutputhiddenstatestrue tuple of torchfloattensor one for the output of the embeddings one for the output of each layer of shape batchsize sequencelength hiddensize hiddenstates of the model at the output of each layer plus the initial embedding outputs attentions tupletorchfloattensor optional returned when outputattentionstrue is passed or when configoutputattentionstrue tuple of torchfloattensor one for each layer of shape batchsize numheads sequencelength sequencelength
70709572,typeerror not a string parameters in autotokenizerfrompretrained,python tensorflow huggingfacetransformers onnx huggingfacetokenizers,passing just the model name suffices tokenizer alberttokenizerfrompretrainedalbertbasev list of modeltypes can be found here
70699247,typeerror an integer is required got type nonetype,python tensorflow huggingfacetransformers onnx huggingfacetokenizers,a dev explains this predicament at this git issue the notebook experiments with bert which uses tokentypeids distilbert does not use tokentypeids for training so this would require redeveloping the notebook removing conditioning all mentions of tokentypeids for this model specifically
70698407,huggingface autotokenizer valueerror couldnt instantiate the backend tokenizer,python tensorflow huggingfacetransformers onnx huggingfacetokenizers,first i had to pip install sentencepiece however in the same code line i was getting an error with sentencepiece wrapping str around both parameters yielded the same traceback i then had to swap out parameters for just the model name tokenizer alberttokenizerfrompretrainedalbertbasev this second part is detailed on this so post
70697470,valueerror unrecognized model in mrpc should have a key in its configjson or contain one of the following strings in its name,python tensorflow huggingfacetransformers bertlanguagemodel onnx,explanation when instantiating automodel you must specify a modeltype parameter in mrpcconfigjson file downloaded during notebook runtime list of modeltypes can be found here solution code that appends modeltype to configjson in the same format import json jsonfilename mrpcconfigjson with openjsonfilename as jsonfile jsondecoded jsonloadjsonfile jsondecodedmodeltype with openjsonfilename w as jsonfile jsondumpjsondecoded jsonfile indent separators configjson
70663782,valueerror layer weight shape not compatible with provided weight shape,tensorflow keras deeplearning huggingfacetransformers bertlanguagemodel,you are passing to setweights a list of list while you should simply pass a list of arrays
70621833,modulenotfounderror no module named nnpruningmodulesquantization,python machinelearning deeplearning huggingfacetransformers,an issue has since been approved to amend this
70619634,errors in loading statedict for robertaforsequenceclassification,pythonx pytorch huggingfacetransformers roberta,load with ignoremismatchedsizestrue then you can finetune the model
70607224,huggingface optimum modulenotfounderror,python huggingfacetransformers quantization modulenotfounderror pruning,pointed out by a contributor of huggingface on this git issue the library previously named lpot has been renamed to intel neural compressor inc which resulted in a change in the name of our subpackage from lpot to neuralcompressor the correct way to import would now be from optimumintelneuralcompressorquantization import incquantizerforsequenceclassification concerning the graphcore subpackage you need to install it first with pip install optimumgraphcore furthermore youll need to have access to an ipu in order to use it solution instead of from optimumintellpotquantization import lpotquantizerforsequenceclassification from optimumintellpotpruning import lpotprunerforsequenceclassification from optimumintelneuralcompressorquantization import incquantizerforsequenceclassification from optimumintelneuralcompressorpruning import incprunerforsequenceclassification
70577285,valueerror you have to specify either inputids or inputsembeds when training automodelwithlmhead model gpt,python pytorch huggingfacetransformers gpt,i didnt find the concrete answer to this question but a workaround for anyone looking for examples on how to finetune the gpt models from huggingface you may have a look into this repo they listed a couple of examples on how to finetune different transformer models complemented by documented code examples i used the runclmpy script and it achieved what i wanted
70532485,error when importing pytorch the filename or extension is too long,python tensorflow pytorch torch huggingfacetransformers,your problem is that the error ist not a too long path error it is a file not found error which mean that pytorch is not correctly installed
70520725,runtimeerror the expanded size of the tensor must match the existing size at nonsingleton dimension,huggingfacetransformers huggingfacetokenizers huggingfacedatasets,simply add tokenizer arguments when you init the pipeline
70518826,onnx runtime bert inference runtimeerror input must be a list of dictionaries or a single numpy array for input attentionmask,python deeplearning huggingfacetransformers onnxruntime,according to the docs the returntensorsnp not returntensorspt
70274841,streamlit unhashable typeerror when i use stcache,huggingfacetransformers streamlit,after searching in issues section in streamlit repo i found that hashing argument is not required just need to pass this argument allowoutputmutation true
70205979,modulenotfounderror no module named hpyutils,pythonx pyinstaller huggingfacetransformers,i solved my problem heres what i did before i start do not use onefile flag in your command i ran the command pyinstaller w iconlogoico hiddenimporthpydefs hiddenimporthpyutils hiddenimporthpyhac hiddenimporthpyproxy hiddenimporttensorflow hiddenimporttransformers hiddenimporttqdm collectdata tensorflow collectdata torch copymetadata tensorflow copymetadata torch copymetadata hpy copymetadata tqdm copymetadata regex copymetadata sacremoses copymetadata requests copymetadata packaging copymetadata filelock copymetadata numpy copymetadata tokenizers copymetadata importlibmetadata chatbotpy go to the libsitepackagescertifi folder and copy the cacertprem file when you try to run the exe file from the generated dist folder you will get an oserror about a missing tls ca certificate bundle because its pointing to a certifi folder that does not exist within the dist folder from the generated dist folder go to the main folder create a new folder and rename it certifi and paste the cacertprem file in it rerun your exe file and it should work it worked for me
70149699,transformers barttokenizeraddtokens doesnt work as id expect for suffixes,python huggingfacetransformers,the short answer is that theres behavior bug in the handling of added tokens for bart and roberta gpt etc that explicitly strips spaces from the tokens adjacent both left and right to the added tokens location i dont see a simple workaround to this added tokens are handled differently in the transformers tokenizer code the text is first split using a trie to identify any tokens in the added tokens list see tokenizationutilspytokenize after finding any added tokens in the text the remainder is then tokenized using the existing vocabbpe encoding scheme see tokenizationgptpytokenize the added tokens are added to the selfuniquenosplittokens list which prevents them from being broken down further into smaller chunks the code that handles this see tokenizationutilspytokenize explicitly strips the spaces from the tokens to the left and right you could manually remove them from the no split list but then they may be broken down into smaller subcomponents note that for special tokens if you add the token inside of the addedtoken class you can set the lstrip and rstrip behaviors but this isnt available for nonspecial tokens see for the else statement where the spaces are stripped
70102323,runtimeerror expected all tensors to be on the same device but found at least two devices cpu and cuda when predicting with my model,python pytorch huggingfacetransformers,you did not move your model to device only the data you need to call modeltodevice before using it with data located on device
69874436,pyinstaller problem making exe files that using transformers and pyqt library,python pyqt pyside huggingfacetransformers,first pip install tqdm if you havent already second specify the path to your libsitepackages you can do this by either adding an argument to pathex in your spec file venv for a virtual environment at some folder venv in your local directory or the absolute path to your global python install libsitepackages if youre not using a virtual environment specifying the path to libsitepackages from the commandline pyinstaller paths venvlibsitepackages myprogrampy from the pyinstaller docs pathex a list of paths to search for imports like using pythonpath including paths given by the paths option some python scripts import modules in ways that pyinstaller cannot detect for example by using the import function with variable data using importlibimportmodule or manipulating the syspath value at run time
69825418,nonmatchingsplitssizeserror loading huggingface bookcorpus,python dataset huggingfacetransformers huggingfacedatasets,bookcorpus is no longer publicly available here is a work around
69757539,deploying huggingface zeroshot classification in sagemaker using template returns error missing positional argument candidatelabels,python model amazonsagemaker huggingfacetransformers,the schema of request body for a zeroshot classification model is defined in this link
69616471,hugginface bert tokenizer build from source due to proxy issues,python tokenize huggingfacetransformers,you would need to download vocabulary and configuration files vocabtxt configjson put them into a folder and pass folders path to berttokenizerfrompretrained function here is the download location of vocabtxt for different tokenizer models location of configjson source transformers codebase steps
69480199,padtokenid not working in hugging face transformers,python pythonx tensorflow huggingfacetransformers,your code does not throw any error for me i would try reinstalling the most recent version of transformers if that is a viable solution for you
69473082,importerror cannot import name savestatewarning from torchoptimlrscheduler,import huggingfacetransformers bertlanguagemodel nlpquestionanswering,you need to update the transformer package to the latest version you can achieve it by running this code for me there is no error after updating refer to these links official resource and this
69364068,loading tfkeras model valueerror the two structures dont have the same nested structure,python tensorflow keras huggingfacetransformers bertlanguagemodel,check out the issue here on github it should help you to solve the problem with the shape
69343395,hugging face h load model error no model found in config file,python tensorflow keras huggingfacetransformers,you can load the tensorflow version of distilbertbaseuncasedfinetunedsstenglish with the tfautomodelforsequenceclassification class from transformers import autotokenizer tfautomodelforsequenceclassification tokenizer autotokenizerfrompretraineddistilbertbaseuncasedfinetunedsstenglish model tfautomodelforsequenceclassificationfrompretraineddistilbertbaseuncasedfinetunedsstenglish
69239925,typeerror in torchargmax when want to find the tokens with the highest score,python pytorch torch huggingfacetransformers bertlanguagemodel,bertforquestionanswering returns a questionansweringmodeloutput object since you set the output of bertforquestionanswering to startscores endscores the return questionansweringmodeloutput object is forced convert to a tuple of strings startlogits endlogits causing the type mismatch error the following should work
69195950,problem with inputs when building a model with tfbertmodel and autotokenizer from huggingfaces transformers,tensorflow keras huggingfacetransformers bertlanguagemodel huggingfacetokenizers,for now i solved by taking the tokenization step out of the model def tokenizesentences tokenizer inputids inputmasks inputsegments for sentence in sentences inputs tokenizerencodeplussentence addspecialtokenstrue maxlength padtomaxlengthtrue returnattentionmasktrue returntokentypeidstrue inputidsappendinputsinputids inputmasksappendinputsattentionmask inputsegmentsappendinputstokentypeids return npasarrayinputids dtypeint npasarrayinputmasks dtypeint npasarrayinputsegments dtypeint the model takes two inputs which are the first two values returned by the tokenize funciton def buildclassifiermodel inputidsin tfkeraslayersinputshape nameinputtoken dtypeint inputmasksin tfkeraslayersinputshape namemaskedtoken dtypeint embeddinglayer bertinputidsin attentionmaskinputmasksin model tfkerasmodelinputsinputidsin inputmasksin outputs x for layer in modellayers layertrainable false return model id still like to know if someone has a solution which integrates the tokenization step inside the modelbuilding context so that an user of the model can simply feed phrases to it to get a prediction or to train the model
69046964,can bert output be fixed in shape irrespective of string size,python pytorch huggingfacetransformers huggingfacetokenizers,when you call the tokenizer with only one sentence and paddingtrue truncationtrue maxlength it will pad the output sequence to the longest input sequence and truncate if required since you are providing only one sentence the tokenizer can not pad anything because it is already the longest sequence of the batch that means you can achieve what you want in two ways provide a batch from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedbertbaseuncased model automodelfrompretrainedbertbaseuncased inputs a a a abcede inputs tokenizerinputs paddingtrue truncationtrue maxlength returntensorspt printinputsinputids outputs modelinputs printoutputslasthiddenstateshape output set paddingmaxlength from transformers import autotokenizer automodel tokenizer autotokenizerfrompretrainedbertbaseuncased model automodelfrompretrainedbertbaseuncased inputs a a a abcede for i in inputs inputs tokenizeri paddingmaxlength truncationtrue maxlength returntensorspt printinputsinputids outputs modelinputs printoutputslasthiddenstateshape i leni output tensor torchsize a tensor torchsize aaaaaaaaaaaaaaaaaaaa tensor torchsize aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa tensor torchsize abcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcedeabcede
68988378,jupyter notebook progress bar issue in tensorflow method,python tensorflow jupyternotebook huggingfacetransformers,have you tried according to this thread seems to have solved the problem to many people also do not forget to restart the jupyter kernel each time you try a new solution in order to determine what actually worked
68925863,problem building tensorflow model from huggingface weights,python tensorflow keras tensorflow huggingfacetransformers,you can try the following snippet to load dbmdzbertbaseitalianxxlcased in tensorflow if you want to load from the given tensorflow checkpoint you could try like this
68875496,huggingface typeerror not supported between instances of nonetype and int,deeplearning datascience huggingfacetransformers huggingfacetokenizers huggingfacedatasets,seems to be an issue with the new version of transformers installing version worked for me
68850172,token indices sequence length issue,python huggingfacetransformers sentencetransformers,you need to add the maxlength parameter while creating the tokenizer like below texttokens tokenizertext paddingtrue maxlength truncationtrue returntensorspt reason truncationtrue without maxlength parameter takes sequence length equal to maximum acceptable input length by the model it is e or for this model you can check by printing out tokenizermodelmaxlength according to the huggingface documentation about truncation true or onlyfirst truncate to a maximum length specified by the maxlength argument or the maximum length accepted by the model if no maxlength is provided maxlengthnone
68835630,typeerror when trying to apply custom loss in a multilabel classification problem,python tensorflow machinelearning keras huggingfacetransformers,the issue is that you are using tfautomodelforsequenceclassification ie forsequenceclassification and if you were to see its summary you will find that it returns a dense output and hence it is not an encoder as you want it but you want to use it as encoder and hence you would have to do it like this as you can see now you have your encoder as the output from the bert now the below line in the createmodel makes sense but it will give an error cause of below line in createmodel function this is cause the output at the index is of shape batchsize tokenlength embedding but we want the value of cls token which should be batchsize embedding and that is at the index so we have to update to below line also as of now you are fixing the inputshape to but we should specify the value to none so that we can have variable length input as below after doing all these change below is the result of a sample run
68813979,bert transformer size error while machine traslation,pythonx translation huggingfacetransformers,in the problem described here credits to lysandrejik the problem appears to be the data type of a dict instead of tensor it might be the case that you need to change the tokenizer output from to
68698065,how can i fix cuda runtime error on google colab,python pytorch gpu googlecolaboratory huggingfacetransformers,maybe the problem comes from this line torchbackendscudnnenabled false you might comment or remove it and try again
68604289,attributeerror module transformers has no attribute tfgptneoforcausallm,python pytorch huggingfacetransformers gpt,try without using fromtftrue flag like below fromtf expects the pretrainedmodelnameorpath ie the first parameter to be a path to load saved tensorflow checkpoints from
68589222,how to run runsquadpy on google colab it gives invalid syntax error,googlecolaboratory stanfordnlp bertlanguagemodel huggingfacetransformers nlpquestionanswering,solved it was giving error because i was downloading the github link rather than the script in github once i copied and used raw link to download the script the code ran
68557028,setting causes error in huggingface trainer class,pytorch huggingfacetransformers huggingfacetokenizers huggingfacedatasets,it fails because the value in line is a list of str which points to hypothesis and hypothesis is one of the ignoredcolumns in trainerpy see the below snippet from trainerpy for the removeunusedcolumns flag there could be a potential pull request on huggingface to provide a fallback option in case the flag is false but in general it looks like that the flag implementation is not complete for eg it cant be used with tensorflow on the contrary it doesnt hurt to keep it true unless there is some special need
68494108,hyperparam search on huggingface with optuna fails with wandb error,python huggingfacetransformers hyperparameters optuna,please check running the code on the latest versions of wandb and transformers works fine for me with wandb and transformers
68322542,problem connecting transformer output to cnn input in keras,tensorflow keras convneuralnetwork huggingfacetransformers transformermodel,i think you are right the problem seems to be the input to the convd layer according to the documentation the outputslasthiddenstate has a shape of batchsize sequencelength hiddensize convd is expecting an input of shape batchsize sequencelength maybe you could resolve the problem by either changing your convd to convd or adding a convd layer in between
68196815,modulenotfounderror huggingface datasets in jupyter notebook,python jupyternotebook huggingfacetransformers huggingfacedatasets,i had faced similar problem but with another library this worked for me import sys syspathappendrpath to datasets in python env import datasetutils path in your case homeyogavenvstextembeddingslibpythonsitepackagesdatasets my guess is that the environment variable does not has the pythonpath is not set up correctly pythonpath is an environment variable those content is added to the syspath where python looks for modules you can set it to whatever you like this should work
68113075,problem with batchencodeplus method of tokenizer,python pytorch huggingfacetransformers huggingfacetokenizers huggingfacedatasets,you need a nonfast tokenizer to use list of integer tokens tokenizer autotokenizerfrompretrainedpretrainedmodelname addprefixspacetrue usefastfalse usefast flag has been enabled by default in later versions from the huggingface documentation batchencodeplusbatchtextortextpairs batchtextortextpairs liststr listtuplestr str listliststr listtupleliststr liststr and for notfast tokenizers also listlistint listtuplelistint listint
68006956,attributeerror type object wavvecforctc has no attribute frompretrained,python visualstudiocode jupyternotebook huggingfacetransformers huggingfacetokenizers,issue solved when i uninstalled both the libraries and installed their respective versions needed using following command after this the issue got resolved
67972661,hugging face nameerror name sentences is not defined,python bertlanguagemodel huggingfacetransformers huggingfacetokenizers huggingfacedatasets,this error is because you have not declared sentences now you need to access raw data using
67872803,huggingface scibert predict masked word not working,python bertlanguagemodel huggingfacetransformers,as the error message tells you you need to use automodelformaskedlm from transformers import pipeline autotokenizer automodelformaskedlm tokenizer autotokenizerfrompretrainedallenaiscibertscivocabuncased model automodelformaskedlmfrompretrainedallenaiscibertscivocabuncased unmasker pipelinefillmask modelmodel tokenizertokenizer unmaskerthe patient is a year old mask admitted with pneumonia output
67811619,this method is deprecated call should be used instead how to solve this problem in bert,python huggingfacetransformers huggingfacetokenizers,ive been working with this recently here is a good look at some documentation for the call function call inputsargskwargs a list or a list of list of dict parameters args str or liststr one or several texts or one list of prompts with masked tokens targets str or liststr optional when passed the model will limit the scores to the passed targets instead of looking up in the whole vocab if the provided targets are not in the model vocab they will be tokenized and the first resulting token will be used with a warning and that might be slower topk int optional when passed overrides the number of predictions to return returns a list or a list of list of dict heres an example of how i used it keep in mind that im using the fillmask mode of bert which is a one word prediction model and the python language the model will predict the word that is tokenized by mask the example output for this is
67785438,attributeerror nonetype object has no attribute tokenize,python huggingfacetransformers,i assume that from transformers import xlnettokenizerfast tokenizer xlnettokenizerfastfrompretrainedxlnetbasecased dolowercasetrue works in this case you are just missing the sentencepiece package pip install sentencepiece
67740498,huggingface electra load model trained with google implementation error utf codec cant decode byte x in position invalid start byte,python tensorflow pytorch bertlanguagemodel huggingfacetransformers,it seems that npit is right the output of the convertelectraoriginaltfcheckpointtopytorchpy does not contain the configuration that i gave hparamsjson therefore i created an electraconfig object with the same parameters and provided it to the frompretrained function that solved the issue
67257008,oserror libmklintellpso cannot open shared object file no such file or directory,pytorch googlecolaboratory kaggle huggingfacetransformers tpu,worked for me we will also try to fix the problem internally
67194634,error loading weights from a hugging face model,python errorhandling model huggingfacetransformers,add the third line to this section rename or remove the model folder you had already and try again this time it should indeed work more than once i hope this gets you rolling
67089849,attributeerror gpttokenizerfast object has no attribute maxlen,tokenize huggingfacetransformers transformermodel huggingfacetokenizers gpt,the attributeerror berttokenizerfast object has no attribute maxlen github issue contains the fix the runlanguagemodelingpy script is deprecated in favor of languagemodelingrunclm plm mlmpy if not the fix is to change maxlen to modelmaxlength also pip install transformers might fix the issue since it has been reported to work for some people
66824985,huggingface error attributeerror bytelevelbpetokenizer object has no attribute padtokenid,python pytorch tokenize huggingfacetransformers huggingfacetokenizers,the error tells you that the tokenizer needs an attribute called padtokenid you can either wrap the bytelevelbpetokenizer into a class with such an attribute and met other missing attributes down the road or use the wrapper class from the transformers library from transformers import pretrainedtokenizerfast your code tokenizersavesomewhere tokenizer pretrainedtokenizerfasttokenizerfiletokenizerpath
66797173,issue while using transformers package inside the docker image,python docker pytorch huggingfacetransformers,i was having a similar issue it seems that starting the app somehow polutes the memory of transformers models probably something to do with how flask does threading but no idea why what fixed it for me was doing the things that are causing trouble loading the models in a different thread first reply here i would be really glad if this helps
66767832,berttokenizerfrompretrained errors out with connection error,python ssl sslcertificate huggingfacetransformers,i could eventually make everything work sharing the same here just in case it will be useful for anyone else in future the solution is quite simple something that i had tried initially but had made a minor mistake while trying anyways here goes the solution access the url huggingfaceco url in my case from browser and access the certificate that accompanies the site a in most browsers chrome firefox edge you would be able to access it by clicking on the lock icon in the address bar save all the certificates all the way up to the root certificate a i think technically you can just save the root certificate and it will still work but i have not tried that i may update this if i get around to try this out if you happen to try it before me please do comment follow the steps mentioned in this stack overflow answer to fetch the ca bundle and open it up in an editor to append the file with the certificates downloaded in the previous step a the original ca bundle file has heading lines before each certificate mentioning which ca root the certificate belongs to this is not needed for the certificates we want to add i had done this and i guess an extra space carriage return etc may have caused it to not work for me earlier in my python program i updated the environment variable to point to the updated ca root bundle osenvironrequestscabundle pathcacertcrt one may think that since most python packages use requests to make such get calls and requests uses the certificates pointed by the certifi package so why not find the location of the certificates pointed by certifi and update that the issue with that it whenever you update a package using conda certifi may get updated as well resulting in your changes to be washed away hence i found dynamically updating the environment variable to be a better option cheers
66693724,while exporting t model to onnx using fastt getting runtimeerroroutput with shape doesnt match the broadcast shape,python tensorflow pytorch huggingfacetransformers onnx,ive checked the repository it looks like a known issue as reported here developer of the library has posted a solution and created a notebook file here solution is to modify modelingtpy file at line if you dont want to modify the file yourself you will need to wait until this pull request to be merged into transformers library
66656622,python importerror cannot import name version from packaging transformers,python bertlanguagemodel huggingfacetransformers transformermodel,i think this is one of those cases where you have a bad naming maybe your file or something inside your code has a name that is overlapping one of the references you are trying to get for example if you are importing a certain module named kivy and your file is named kivy then the code will go after your file instead of the actual package you are trying to import if thats the case try changing the name and the problem will be solved
66524542,attributeerror str object has no attribute shape while encoding tensor using bertmodel with pytorch hugging face,python string pytorch attributeerror huggingfacetransformers,the issue is that the return type has changed since xx version of transformers so we have explicitly ask for a tuple of tensors so we can pass an additional kwarg returndict false when we call the bertmodel to get an actual tensor that corresponds to the lasthiddenstate in case you do not like the previous approach then you can resort to
66207138,errors appear when training an xlnet model,python huggingfacetransformers transformermodel,you should use a tfrecord dataset instead of a text file
65854722,huggingface albert tokenizer nonetype error with colab,googlecolaboratory huggingfacetransformers huggingfacetokenizers,i found the answer after install import the alberttokenizer and tokenizer i received an error asking me to install sentencepiece package however after i install this package and run tokenizer again i started receiving the error above so i open a brand new colab session and install everything including the sentencepiece before creating tokenizer and this time it worked the nonetype error simply means it doesnt know what is albertbasev however if you install the packages in right order colab will recognize better the relationship between alberttokenizer and sentencepiece in short for this to work in colab open a new colab session install transformers and sentencepiece import alberttokenizer create tokenizer
65806586,valueerror shape mismatch the shape of labels received should equal the shape of logits except for the last dimension received,python tensorflow keras tensorflow huggingfacetransformers,is a number of sequences in one batch i suspect that it is a number of sequences in your dataset your model acting as a sequence classifier so you should have one label for every sequence
65779837,importerror caused by file with the same name in working dir and file from imported package,python pythonx huggingfacetransformers huggingfacetokenizers,i think ive understood what causes the issue it is shadowing file with the same name in package transformer that internally import another package called tokenizers with my local file called tokenizerspy it is so because my working directory is first on list of paths that will be searched to find imports it can be checked with and to proof that search for imports starts from directory in which you call script you can move first syspath to the end of the list and the following code will work
65683013,indexerror index out of range in self while try to fine tune roberta model after adding special tokens,bertlanguagemodel huggingfacetransformers robertalanguagemodel,you also need to tell your model that it needs to learn the vector representations of two new tokens from transformers import robertatokenizer robertaforquestionanswering t robertatokenizerfrompretrainedrobertabase m robertaforquestionansweringfrompretrainedrobertabase robertabase knows tokens printmrobertaembeddingswordembeddings specialtokensdict additionalspecialtokens toktok taddspecialtokensspecialtokensdict we now tell the model that it needs to learn new tokens mresizetokenembeddingslent mrobertaembeddingswordembeddingspaddingidx printmrobertaembeddingswordembeddings output
65517232,an error occurs when predict with the same data as when performing train expects inputs but it received input tensors,python tensorflow machinelearning bertlanguagemodel huggingfacetransformers,it was a tensor dimension problem
65140400,valueerror you have to specify either decoderinputids or decoderinputsembeds,python deeplearning huggingfacetransformers torchscript,update refer to this answer and if you are exporting t to onnx it can be done easily using the fastt library i figured out what was causing the issue since the above model is sequential it has both an encoder and a decoder we need to pass the features into the encoder and labels targets into the decoder the decoderinputids is tokenized ids of the question here the question is a label even though the torchscript model is created it does not have the generate method as the huggingface t do
65091635,valueerror logits and labels must have the same shape vs,python tensorflow keras huggingfacetransformers huggingfacetokenizers,seems like for a single example your labels have a shape of indicating data points instead you have data point with possible labels hence it should be you have to reshape the data accordingly
65023526,runtimeerror the size of tensor a must match the size of tensor b at nonsingleton dimension,python deeplearning pytorch bertlanguagemodel huggingfacetransformers,the issue is regarding the berts limitation with the word count ive passed the word count as where the maximum supported is have to give up more for cls sep at the beginning and the end of the string so it is only reduce the word count or use some other model for your promlem something like longformers as suggested by cronoik in the comments above thanks
63939072,loading saved ner transformers model causes attributeerror,torch huggingfacetransformers,try to save your model with modelsavepretrainedoutputdir then you can load your model with model frompretrainedoutputdir where is the model class eg bertfortokenclassification
63907100,valueerror cant convert nonrectangular python sequence to tensor when using tfdatadatasetfromtensorslices,python tensorflow huggingfacetransformers,turns out that i had caused the trouble by having commented the line i assumed it would truncate on the model max size or that it would take the longest input as the max size it does none of it and then even if i have the same amount of entries those entries vary in size generating a nonrectangular tensor ive removed the and am using as maxlenght which is the max that bert takes anyways see transformers tokenizer class for reference
63899305,docker error when containerizing app in google cloud run,googlecloudplatform dockerfile googlecloudrun huggingfacetransformers googlecloudsdk,the error is this is due to these two lines in your dockerfile this attempts to copy the local directory containing the dockerfile into the container and then install it as a python project it looks like the dockerfile expects to be run at the repository root of you should cloning the repo and move the dockerfile you want to build into the root and then build again
63676307,hugging face runtimeerror caught runtimeerror in replica on device on azure databricks,pytorch databricks azuredatabricks bertlanguagemodel huggingfacetransformers,the out of memory error is likely caused by not cleaning up the session and or freeing up the gpu from the similar github issue it is because of minibatch of data does not fit on to gpu memory just decrease the batch size when i set batch size for cifar dataset i got the same error then i set the batch size it is solved
63387831,memory issue while following lm tutorial,python pytorch googlecolaboratory huggingfacetransformers,i came to the conclusion that my text file for training was way to big from the other examples i found the training text was around mb not gb in both instances i was asking pycharm and collab to pull off a very resource expensive task
63211463,error expected object of device type cuda but got device type cpu for argument self in call to thindexselect,neuralnetwork pytorch gpu huggingfacetransformers,try explicitly moving your model to the gpu
63141267,importerror cannot import name automodelwithlmhead from transformers,python pytorch huggingfacetransformers,i solved it apparently automodelwithlmhead is removed on my version now you need to use automodelforcausallm for causal language models automodelformaskedlm for masked language models and automodelforseqseqlm for encoderdecoder models so in my case code looks like this
62746180,importerror cannot import name hfbucketurl in huggingface transformers,tensorflow pytorch huggingfacetransformers,it turns out to be a bug this pr solves the issue by importing the function hfbucketurl properly
62125405,runtimeerror expected object of device type cuda but got device type cpu for argument index in call to thindexselect sitestackoverflowcom,python pytorch huggingfacetransformers,you have a typo in your evaluation function barch tuplebtodevice for b in batch you assign the gpu data to barch instead of batch
61832308,transformerscli error the following arguments are required modeltype,huggingfacetransformers,found the error it needs to be modeltype not modeltype
60914793,argument neversplit not working on bert tokenizer,python tensorflow huggingfacetransformers,i would call this a bug or at least not good documented the neversplit argument is only considered when you use the basictokenizer which is part of the berttokenizer you are calling the tokenize function from your specific model bertbaseuncased and this considers only his vocabulary as i would expect in order to prevent splitting of certain tokens they must be part of the vocabulary you can extend the vocabulary with the method addtokens i think the example below shows what i am trying to say from transformers import berttokenizer text lol thats funny lool tokenizer berttokenizerfrompretrainedbertbaseuncased neversplitlol what you are doing printtokenizertokenizetext how it is currently working printtokenizerbasictokenizertokenizetext how you should do it tokenizeraddtokenslol printtokenizertokenizetext output
60867353,using lime for bert transformer visualization results in memory error,python machinelearning huggingfacetransformers lime,ended up solving this by reimplementing along the lines of this github post my code is now very different from the above probably makes sense if you look to the github post for guidance if youre running into similar issues
60345277,migrating from to issue regarding model output,python cosinesimilarity pretrainedmodel huggingfacetransformers,first of all the newest version is called transformers not pytorchtransformers you need to tell the model that you wish to get all the hidden states model bertmodelfrompretrainedbertbaseuncased outputhiddenstatestrue then youll find your expected output as the third item in the output tuple encodedlayers modeltokenstensor segmentstensors iirc those layers now also include the embeddings so items in total so you might need to update the index to get the second last layer might be better to use a negative index to be sure
60184117,how to check the root cause of cuda out of memory issue in the middle of training,gpu pytorch huggingfacetransformers,this can have multiple reasons if you only get it after a few iterations it might be that you dont free the computational graphs do you use lossbackwardretaingraphtrue or something similar also when youre running inference be sure to use otherwise the computational graphs are saved there as well and potentially never freed since you never call backward on them
58454157,pytorch bert typeerror forward got an unexpected keyword argument labels,python pytorch bertlanguagemodel huggingfacetransformers,as far as i know the bertmodel does not take labels in the forward function check out the forward function parameters i suspect you are trying to finetune the bertmodel for sequence classification task and the api provides a class for that which is bertforsequenceclassification as you can see its forward function definition please note the forward method returns the followings hope this helps
79324718,autoencoderfit raises keyerror exception encountered when calling functionalcall,python tensorflow keras deeplearning lstm,to solve the error message replace inputdim by inputs as kerasmodel parameter for the problem with the loss value becoming nan after epochs first check that there is no nan in your data but as the first epochs run it should be ok probably that you are dealing with exploding gradients several possible solutions to try replace relu by tanh decrease the learning rate add gradient clipping add weights regularisation
79257431,how can i fix this error valueerror x has features but minmaxscaler is expecting features as input,python lstm,the shape of your train data is having features minmaxscaler was fit initially on all columns of the dataset you can try using a different scaler for goldprice hope that helps
79078406,using tensorflow through keras valueerror only input tensors may be passed as positional arguments,r tensorflow keras lstm,the error seems to be a bug with current release of keras package please remove it and try keras instead as suggested here
78945029,in tesserocr when i initailize api with oem and psm options to run detectos it raises errorfatal python error aborted why,pythonx ocr lstm tesseract,this problem is indeed caused by the incompatibility between lstmonly and osdonly if you do need to use osd and lstm you need to use oemdefault and the another solution what sets the lstmonly through setvariable method will change oem to default internally to ensure compatibility rather than sets oem as lstmonly with pytessbaseapipathtessdatadirectory langchisimeng as api apisetvariabletesseditocrenginemode stroemlstmonly apisetpagesegmodepsmpsmosdonly apisetimagefiletestimageabsolutepath os apidetectorientationscript print orientation orientdegnorientation confidence orientconfnscript scriptnamenscript confidence scriptconfformat os printapioem oemdefault user thanks for your comments
78353278,tensorflowpythonframeworkerrorsimploperatornotallowedingrapherror iterating over a symbolic is not allowed,pythonx lstm tensorflow recurrentneuralnetwork,just found it never mind i did the following and it worked
78317660,difference between singlestep forecasting and multistep in lstm regression problem,keras timeseries lstm forecasting multistep,how do i go from singlestep to multistep i guess that i need to change the size of the dense output layer do i need also to change the format of the input data for the lstm yes you are right no it is not mandatory to change the input data as well is a multistep prediction of values better than a single stepforecasting of values define better if by better you mean wrt accuracy then singlestep forecasts are going to be likely more accurate than a step forecast since the singlestep forecasts will take into account more recent historical values the lookback values in your case in general longer forecasting is harder than shorter forecasting since in longterm forecasting the error tends to accumulate as you attempt to forecast the future values further into the future last but not least is an lstm the best way to anticipate this large drops that depends largely on your data and only by actually evaluating different models you can answer this you can try nhits good performance in various tasks tft good performance too nbeats explainability i had uploaded two examples here here timegpt foundation model llmtime uses a large language model to predict the next value timellm uses an llm too amazons chronos and googles timesfm are other options but maybe too hot out of the oven to try now apis not yet released etc
78276731,data set and loader issues stack expects each tensor to be equal size but got at entry and at entry,timeseries dataset lstm pytorchdataloader,this issue was the randomize exceeded the data len itself it was removed or update the get item
78096484,valueerror expected input data to be nonempty,python deeplearning lstm trainingdata,where is testdata defined is its length more than if not the code does not enter the loop and xtest remains empty
77496347,training lstm neural network with missing values in target data error optimstep,python lstm nan torch,one solution can be to mask the nan elements try the following
77284708,custom peephole lstm layer returns typeerror,pythonx tensorflow machinelearning keras lstm,managed to fix this myself there were two issues first the tensorflow shape object was needed to parse the dimensions second a tensorarray was necessary to retain time iterations working model is as follows
76971846,shap typeerror nonetype object is not callable,python tensorflow keras lstm shap,the issue seems to be with how youre creating the shap explainer object you need to provide it with a callable functionmodel object that takes input data and returns model predictions heres how you do it with shapexplainer solution or you can use one of the available explainer types eg shapkernelexplainer shapdeepexplainer etc and provide it with a callable function that takes input data and returns model predictions heres how you can use the shapkernelexplainer to create an explainer for your keras model solution you can find more information in the shap coreexplainer documentation
76137989,getting value error while enctransform where enc is onehotencodersparseoutputfalse in pandas,python pandas numpy lstm,you cant encode categories never seen during transform process output
75991377,i get this error valueerror must pass d input shape,python pandas lstm,the solution is for this problem is by making the shape of networkinput and networkoutput same to do it change the sequencelength variable in preparesequences function to and also change the sequencelength argument of the oversample to here is the changes i made to run it successfully preparesequences function def preparesequencesnotes nvocab prepare the sequences used by the neural network sequencelength get all unique pitchnames pitchnames sortedsetitem for item in notes numpitches lenpitchnames create a dictionary to map pitches to integers notetoint dictnote number for number note in enumeratepitchnames networkinput networkoutput create input sequences and the corresponding outputs for i in range lennotes sequencelength sequencein is a sequencelength list containing sequencelength notes sequencein notesii sequencelength sequenceout is the sequencelength note that comes after all the notes in sequencein this is so the model can read sequencelength notes before predicting the next one sequenceout notesi sequencelength networkinput is the same as sequencein but it containes the indexes from the notes because the model is only fed the indexes networkinputappendnotetointchar for char in sequencein networkoutput containes the index of the sequenceout networkoutputappendnotetointsequenceout npatters is the length of the times it was iterated for example if i then npatterns because networkinput is a list of lists npatterns lennetworkinput reshape the input into a format compatible with lstm layers reshapes it into a npatterns by sequencelength matrix printlennetworkinput networkinput numpyreshapenetworkinput npatterns sequencelength normalize input networkinput networkinput floatnvocab onehot encodes the networkoutput networkoutput nputilstocategoricalnetworkoutput return networkinput networkoutput nvocab lensetnotes networkinput networkoutput preparesequencesnotesnvocab npatterns lennetworkinput pitchnames sortedsetitem for item in notes numpitches lenpitchnames oversample function def oversamplenetworkinputnetworkoutputsequencelength npatterns lennetworkinput create a dataframe from the two matrices newdf pdconcatpddataframenetworkinputpddataframenetworkoutputaxis rename the columns to numbers and notes newdfcolumns x for x in rangesequencelength newdf newdfrenamecolumnssequencelengthnotes printnewdftail print printfdistribution of notes in the preoversampled dataframe newdfnotesvaluecounts oversampling oversampleddf newdfcopy maxclasssize npmaxoversampleddfnotesvaluecounts maxclasssize printsize of biggest class maxclasssize classsubsets oversampleddfquerynotes stri for i in rangelennewdfnotesunique range because it is a binary class for i in rangelennewdfnotesunique try classsubsetsi classsubsetsisamplemaxclasssizerandomstatereplacetrue except printi oversampleddf pdconcatclasssubsetsaxissamplefracrandomstateresetindexdroptrue print printfdistribution of notes in the oversampled dataframe oversampleddfnotesvaluecounts get a sample from the oversampled dataframe because it may be too big and we also have to convert it into a d array for the lstm sampleddf oversampleddfsamplenpatternsreplacetrue has to be equals to print printfdistribution of notes in the oversampled postsampled dataframe sampleddfnotesvaluecounts convert the training columns back to a d array networkin sampleddfx for x in rangesequencelength networkin nparraynetworkin networkin npreshapenetworkinput npatterns sequencelength networkin networkin numpitches printnetworkinshape printsampleddfnotesshape converts the target column into a onehot encoded matrix networkout pdgetdummiessampleddfnotes printnetworkoutshape return networkinnetworkout printnetworkinputshape printnetworkoutputshape networkinputshapednetworkoutputshaped oversamplenetworkinputnetworkoutputsequencelength networkoutputshaped nputilstocategoricalnetworkoutput
75280102,dlj lstm contradictory errors,java lstm recurrentneuralnetwork deeplearningj dlj,there are several problems youve got here if you read the documentation for predict it tells you usable only for classification networks in conjunction with outputlayer cannot be used with rnnoutputlayer cnnlosslayer or networks used for regression the error message therefore tells you that it only works with rank output in your attempted solution you try then to reshape the input and the network complains that it isnt getting the input it is expecting you want to either use rnntimestep for single stepping or output for the entire sequence to get the unprocessed output and then apply the argmax accordingly the output of rnntimestep is just a slice of output so in order to get the same output as predict you should be able to use outputargmaxtointvector on it the output of output will be a d matrix so youll need to specify the correct axes
74888688,cross validation using metricsmeansquarederror found array with dim estimator expected error,python scikitlearn lstm recurrentneuralnetwork metrics,if the ytrue and ypred shapes are reshape into and compute rmse with rmse meansquarederror ytruereshape ypredreshape squaredfalse set to false for root mean square error or ex d d ytrueshape ytrue ytruereshapeex dd
74669249,incompatible shapes mean squared error keras,python tensorflow keras lstm,try setting the parameter returnsequences of the last lstm layer to false i have also changed the activation function in the output layer to linear since a softmax layer does not make much sense in your case also refer to this answer
74432853,training a rnnlstm model got keyerror equal to the val of the length,python pandas lstm recurrentneuralnetwork,it was exhausting to find the cause due to the poor and misleading error message anyway the trouble was on the target data set form the timeseriesgenerator does not accept panda dataframes just nparrays therefore this shall have been written as in the case of just one target it was enough just one level of squared brackets not two
74423081,typeerror relu argument input position must be tensor not tuple i believe its because i have an lstm layer,pytorch lstm,move the lstm layer out of the sequential layer lstm returns a tuple of output hn cn where hn cn are the last hidden states for example your init function will contain something like class modulennmodule def initself supernnmodule selfinit selflstm nnlstm selfseq nnsequential and your forward function will be def forwardself x lstmout selflstmx out selfseqlstmout return out
74327328,modulenotfounderror no module named keraslayersrecurrent,pythonx keras neuralnetwork lstm recurrentneuralnetwork,if youre using the tensorflow version try this you can check it at the link bellow
73933184,use of lstm layer inside cnn provides valueerror,python tensorflow keras convneuralnetwork lstm,may be try using returnsequencestrue it may resolve the error bcz the dimensions lstm is expecting is none but right now it is getting only dimensions which are none
73814554,error to fit generator object to model no of arguments,python tensorflow keras deeplearning lstm,your shape in the yield is wrong you need to provide inputs not try this
73789343,valueerror found array with dim estimator expected raise value error while i use scalerinversetransform,python arrays scikitlearn lstm valueerror,i beleve your scaler applies to the whole dataframe on which you build x and y right i would make a scaler for the x values and one for the y values if necessary in each case so maybe operate on the numpy data i would also recommend you to fit on the train dataset in order to remove any use of the test dataset in the training process thus i would do and then here i removed the x scaling but you can put it if you want
73607112,typeerror exception encountered when calling layer lstm type lstm,python tensorflow keras lstm,whenever you are working try to use print statement very frequently like printtypetemp gives your answer you are reading all your data in string format so your temp data is still in string format use this will convert all data in your code to numeric hopefully it will work
73323205,problems with lstm neural network,python tensorflow keras neuralnetwork lstm,the problem was on the code related with the import of tensorflow i found this whil looking deeply for this type of error along stack overflow posts now i am using the following importing and it works correctly the difference is that it has python between tensorflow and keras
73296320,lstm custom loss function caued error valueerror too many values to unpack expected,python keras lstm recurrentneuralnetwork lossfunction,you have to use tfunstack to unpack the values likewxyz tfunstackypreditfnormypredi ordeuclidean axisnone keepdimsnone namenone axis
73153595,lstm model is giving an valueerror while predicting based on xtest data,python lstm valueerror,your last dense layer outputs values because you specified so the output is coherent with your model you give samples and it predicts labels with dimension if you only need one value just change the number of neurons in your last dense layer to
73014360,convert cnnlstm model to dcnn model dimension error and must have the same shape,python tensorflow keras lstm reshape,you actually need a d tensor with the shape batchsize features and using a flatten layer on none dimensions will not work rather remove the last timedistributed layer and add a globalmaxpoold or globalavgpoold layer and it will work
72937452,importerror dlopen library not loaded rpathpywraptensorflowinternalso,python tensorflow keras pip lstm,i solve this annoying issue with my environment is
72893225,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim full shape received,python tensorflow keras deeplearning lstm,lstm expects an input of three dimensions namely batch timestep features is each sample of yours a length sequence in that case youll need to set inputshape and reshape your data as xtrain xtrain none and xvalidation xvalidation none
72838901,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras timeseries lstm,the problem is that the lstm layer expects a d input while your rd and th lstm layers are receiving a d input of shape none since returnsequences is set to false the default in the nd and rd lstm layers while it should be set to true import numpy as np from tensorflowkerasmodels import sequential from tensorflowkeraslayers import lstm dense xtrain nprandomnormalsize ytrain npmeanxtrain axis model sequential modeladdlstm returnsequencestrue inputshapextrainshape modeladdlstm returnsequencestrue recurrentdropout modeladdlstm returnsequencestrue recurrentdropout modeladdlstm recurrentdropout modeladddense modelcompileoptimizeradam lossmse modelfitxtrain ytrain epochs batchsize
72788811,valueerror of input and output values during lstm training,python tensorflow keras lstm recurrentneuralnetwork,you are currently trying to do categorical classification with classes but y has the shape it does not work like that also when you use categoricalcrossentropy you need onehotencoded labels here is a working example as orientation
72541650,valueerror in lstm model,python pandas keras deeplearning lstm,i think changing your modelcompile this should do the trick
72535757,lstm model valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow machinelearning keras lstm,you need to change intputshape and use tfexpanddims on x to add one dimension at the end then you can use your model and start training full code output
72237364,dimension issues for lstm sequence model on keras,keras deeplearning lstm multiclassclassification,the units parameter in tfkeraslayersdense is the dimensionality of the output space since you have used units in the last dense layer the model will process the input and return a tensor of shape none and compare it with the labels of shape none which would be incompatible try changing the number of units to in the last dense layer to get a compatible model
72113667,lstm keras many to many classification value error incompatible shapes,python tensorflow keras timeseries lstm,the solution may be to have ytrain of shape instead of as you predict one day this does not change the data just its shape you should then be able to train and predict with modeladdlayersdenseytrainshape activationsoftmax of course
71986500,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras lstm sequence,the line as pointed out in stacktrace is missing the second piece of the tuple dimension i assume it should have a number instead of being empty eg
71958700,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,machinelearning deeplearning lstm imageclassification lstmstateful,according to the documentation page lstm layer input should have the following shape batch timesteps feature your lstm layer receives input of shape none this is why you obtain an error about dd shapes you need to reshape your tensor none none to do this you can insert reshape layer between your last maxpoolingd and lstm layers
71895263,pytorch runtimeerror expected scalar type double but found float,python pytorch lstm,your input data to the model is tensor of type double while the model expects a float tensor do this in the last line of mkrandombatch function you may or may not get a similar error for label tensor as well during loss calculation in that case try converting label tensor to float as well
71785855,pytorch lstm runtimeerror input must have dimensions got,python pytorch classification lstm,i got it the input data containing values i have used map function to convert negative integers into positive as following line tuplemapint linestripsplit please make it sure for classification your data should contain positive number
71479172,lstm regression issues with masking and intuition keras,python machinelearning keras regression lstm,when weights are random they contribute to concrete inpute calculations chaotically and we always get nearly same output did you train the model looks like not consider simple mnist solver output before training and after upd so training is presented but not accomplishes its target well a lot of things can be a reason besides of technical issues the task may be to complex for neural network for example if target function cant be learned with gradual improvement check datapathes try to simplify the task find some example solution that solves close problem examine and rework it
71302892,why my predictions is not correct and accuracy how can i train my data and fixe my problem,python tensorflow keras deeplearning lstm,the loss lossbinarycrossentropy is adapted for binary classification problems but not for time series prediction problems for time series prediction you should use mean square error also you cannot use accuracy for problems that are not classifiction so just remove the metrics from your code giving
71251340,keras stateful lstm error specified a list with shape from a tensor with shape,python tensorflow keras lstm lstmstateful,you just need to make sure that the number of training samples can be divided evenly by the batch size here is a working example and also explicitly set the batch size when predicting since according to the docs if left unspecified batchsize will default to also make sure your batch is the same everywhere
71239896,valueerror all the input array dimensions for the concatenation axis must match exactly but along dimension the array at index has size,python pandas numpy lstm forecasting,basically numpy is telling you that the shapes of the concatenated matrices should align for example it is possible to concatenate a x matrix with x matrix so that we get x matrix we added dimension the problem here is that numpy is telling you that the axis dont align in my example that would be trying to concatenate x matrix with x matrix this is not possible as the shapes are not aligned this usually means that the you are trying to concatenate wrong things if you are sure though try using npreshape function which will change the shape of one of the matrices so that they can be concatenated
71124890,valueerror can not squeeze dim expected a dimension of got for node squeeze squeezetdtfloat squeezedims,python tensorflow machinelearning keras lstm,you are missing the last dimension of ytrain just reshape your output to match the dimensions of ytrain
70924532,valueerror shapes are incompatible in tensorflow lstm using randomizedsearchcv,python tensorflow machinelearning keras lstm,as mentioned in the comments you can easily solve this with the kerastuner import numpy as np import tensorflow as tf from numpyrandom import standardnormal choice from tensorflow import keras from tensorflowkerasmodels import model from tensorflowkeraslayers import input dense lstm reshape lambda from tensorflowkerasactivations import softmax from tensorflowkerasutils import tocategorical import kerastuner as kt def buildmodelhp inp inputshape x lstm hpintnneuronsminvaluemaxvalue stepreturnsequencesfalseinputshapenone inp x densenpproduct x x reshape x out lambdalambda x softmaxx axisx model modelinp out optimizer tfkerasoptimizersadam loss tfkeraslossescategoricalcrossentropyaxis modelcompilelosslossoptimizeroptimizer return model nobservations sequencelength nfeaturesin nclasses targetclasses nfeaturesout lentargetclasses x standardnormalnobservations sequencelength nfeaturesin y tocategoricalchoicetargetclasses nfeaturesout replacefalset for i in rangenobservations y npstacky tuner ktrandomsearch buildmodel objectiveloss maxtrials tunersearchx y epochs bestmodel tunergetbestmodels you can use hyperparametersint to define a range you want to test out
70777445,valueerror can not squeeze dim expected a dimension of for node binarycrossentropyweightedlosssqueeze,python tensorflow keras lstm tensorflow,i dont think the problem is the masking layer since you set the parameter returnsequences to true in the lstm layer you are getting a sequence with the same number of time steps as your input and an output space of for each timestep hence the shape where is the batch size afterwards you apply a batchnormalization layer and finally a dense layer resulting in the shape the problem is your labels have a d shape and your model has a d output due to the returnsequences parameter so simply setting this parameter to false should solve your problem see also this post here is a working example from tensorflowkeraslayers import lstm dense batchnormalization masking from tensorflowkeraslosses import binarycrossentropy from tensorflowkerasmodels import sequential from tensorflowkerasoptimizers import nadam import numpy as np if name main define stub data samples timesteps features x nprandomrandsamples timesteps features y nprandomrandint sizesamples create model model sequential modeladdmaskingmaskvalue inputshapenone modeladdlstm returnsequencesfalse modeladdbatchnormalization modeladddense activationsigmoid optimizer nadamlearningrate loss binarycrossentropyfromlogitsfalse modelcompilelossloss optimizeroptimizer train model modelfit x y batchsize
70765843,valueerror expecting kerastensor which is from tfkerasinput error in prediction with dropout function,tensorflow keras lstm keraslayer dropout,to activate dropout at inference time you simply have to specify trainingtrue tf in the layer of interest in the last lstm layer in your case with trainingfalse with trainingtrue in your example this becomes and the kerasdropoutprediction
70635743,valueerror in trainerfit,python deeplearning pytorch lstm pytorchlightning,before the training loop actually starts pl trainer will run a sanity check of validation loop for two steps in that case these two steps may only have one type of label negative or positive and crash your metrics turn it off by setting numsanityvalsteps in your trainer
70500714,valueerror input of layer sequential is incompatible with the layer expected shapenone found shapenone,python lstm valueerror stock forecast,here is my attempt at a solution i am not very familiar with the ml stuff but i just approached it as an issue of making the dimensions consistent thus i changed the line approximately line from to i think there was an indentation issue somewhere on line or so changed to then after this adjustment i received the following graph and the following output at the end the graph
70416927,problem in lstm traintest split in time series data,python tensorflow lstm trainingdata,there is a problem with the data shape the input shape and the output shape of your network are the same but the shapes of xtrain and ytrain are not a simple model that would do the job
70412894,python running error on macbook pro m max running on tensorflow,python tensorflow deeplearning lstm applem,my sense is there are two distinct problems here the steps to install tensorflow on an m machine using the metal engine the requirementstxt file of the package to be used background when working on a new m processor you tend to hit problems on installing prem python packages due to certain naming conventions and versioning assumptions that tend to get included in the package requirementtxt file separately it should be noted that only python and above are supported on m processors alas python and all previous versions of python were developed before the release of m processors and there are apparently no plans to backport key patches tensorflowmacos and tensorflowmetal install the steps to install tensorflowmacos and tensorflowmetal are detailed here they can be summarized as follows using miniforge so we can test this worked the expected output for my test machine ok everything is assumed to be working binary bot install issue explanation now we turn to look at binary bot first we need to install via git hub ok now prior to installing lets look at the requirementstxt file it is important to note two key issues and tensorflow the in the requirements file implies an exact version match on the package version meanwhile we have not installed tensorflow we have installed tensorflowmacos and tensorflowmetal we can check this thus if we run we receive a host of errors why well because the requirementstxt file requires an exact package match rather than a compatible package match or downloading any version greater than or equal to the stipulated version to navigate this we can change to which hopefully should allow us to install the packages compatible with the apple m silicon though we might break compatibility if the developer relies on older assumptions in the packages further the requirementstxt will try to install the tensorflow package not the tensorflowmacosx or tensorflowmetal packages which as we can see creates a host of issues when we run the requirementstxt file unchanged so how to navigate well one option is to simply replace the with and replace or remove tensorflow with tensorflowmacosx resulting in now if we run we install the latest version of a package resulting in so we should now be able to test if things are working so the package now installs all that is required is to add your name and password to the iqpy file and check that the results align with expectations in reference to the point on deleting the model which is the accepted answer i presume this refers to the lstmbestmodel in the models directory in the binarybot directory alas i am not currently in a country that allows me to signup for the service to fully test but my sense is that things should work to a point where one can validate the results based upon the above hopefully this points people in the right direction in thinking about the hurdles involved in getting python packages to work with applem appleintel laptops using the tensorflow metal plugin stay safe and well
70406438,predict method gives error for created model,python tensorflow keras deeplearning lstm,first you should specify batchsize if you pass just one sample second you should reshape your sample to match input data shape for example if you wanna just pass an size you could reshape it before passing to models as follow
70402402,keras lstm dimension value errors,python tensorflow keras lstm,your lstm layer needs d inputs shaped this way to learn more about it it suggest you to take a look to this
70331238,valueerror found input variables with inconsistent numbers of samples,python tensorflow keras lstm,tfkeraslayerslstm expects inputs a d tensor with shape batch timesteps feature output if your input shape is of d use tfexpanddimsinput axis to add extra dimension
70321889,lstm nvidia gpu causes notimplementederror,pythonx lstm tensorflow,check your numpy version is it above heres issue with the same problem and they point to issue where they fixed this in tensorflow so either downgrade to numpy pip install numpy or upgrade tensorflow pip install upgrade tensorflow this should hopefully fix the issue
70240533,how to solve error when reshaping dataframe to lstm,python dataframe numpy deeplearning lstm,based on reshape documentation the format for calling the routine is therefore you should do assuming the shape above is what you want
70101855,tensorflow valueerror shapes are incompatible,python tensorflow keras deeplearning lstm,you should make sure you are using tfkeraslossessparsecategoricalcrossentropy as your loss function and that the last dense layer is wrapped around a timedistributed layer the decoderlstm lstm is returning a sequence with the shape none and you are applying a dense layer to it but as the docs mention if the input to the layer has a rank greater than then dense computes the dot product between the inputs and the kernel along the last axis of the inputs so the last dense layer is essentially disregarding the timesteps and being applied to the last dimension which is probably not what you want with a timedistributed layer you are simply applying a dense layer with a softmax activation function to each time step n to calculate the probability for each word in the vocabulary of the size here is a working example import tensorflow as tf vocabsize targetvocabsize encoderinputs tfkeraslayersinputshape nameencoderinputs decoderinputs tfkeraslayersinputshape namedecoderinputs embedding tfkeraslayersembeddinginputdimvocabsize outputdim maskzerotrue encoderembeddings embeddingencoderinputs decoderembeddings embeddingdecoderinputs encoderlstm tfkeraslayerslstm returnstatetrue nameencoderlstm lstmoutputs stateh statec encoderlstmencoderembeddings encoderstates stateh statec decoderlstm tfkeraslayerslstm returnsequencestrue returnstatetrue namedecoderlstm decoderoutputs decoderlstmdecoderembeddings initialstateencoderstates decoderdense tfkeraslayerstimedistributedtfkeraslayersdensetargetvocabsize activationsoftmax namedecoderdense decoderoutputs decoderdensedecoderoutputs modelencodertraining tfkerasmodelencoderinputs decoderinputs decoderoutputs namemodelencodertraining modelencodertrainingcompileoptimizeradam losstfkeraslossessparsecategoricalcrossentropy samples xtrain tfrandomuniformsamples maxvalvocabsize dtypetfint xdecoder tfrandomuniformsamples maxvalvocabsize dtypetfint ytrain tfrandomuniformsamples maxvaltargetvocabsize dtypetfint modelencodertrainingfitxtrain xdecoder ytrain epochs batchsize
70094392,how can i fix the problem with this lstm model,python keras neuralnetwork lstm,the expected input data shape is batchsize timesteps datadim but your xtrain numpyarray has dimensions so in order to solve the problem you should reshape your input data this way
69892944,indexerror tuple index out of range lstm,python tensorflow machinelearning keras lstm,the expected input shape of the lstm layer is batch timesteps features as you have only one feature you can reshape your training sequences as follows xtrain xtrainreshapextrainshape xtrainshape as you already did for the validation sequences import pandas as pd import numpy as np import yfinance as yf from kerasmodels import sequential from keraslayers import lstm dense from sklearnpreprocessing import minmaxscaler pdoptionsmodechainedassignment none download the data df yfdownloadtickersaapl periody split the data traindata dfcloseiloc validdata dfcloseiloc scale the data scaler minmaxscalerfeaturerange scalerfittraindata traindata scalertransformtraindata validdata scalertransformvaliddata extract the training sequences xtrain ytrain for i in range traindatashape xtrainappendtraindatai i ytrainappendtraindatai xtrain nparrayxtrain ytrain nparrayytrain extract the validation sequences xvalid for i in range validdatashape xvalidappendvaliddatai i xvalid nparrayxvalid reshape the sequences xtrain xtrainreshapextrainshape xtrainshape xvalid xvalidreshapexvalidshape xvalidshape train the model model sequential modeladdlstmunits returnsequencestrue inputshapextrainshape modeladdlstmunits modeladddense modelcompilelossmeansquarederror optimizeradam modelfitxtrain ytrain epochs batchsize verbose generate the model predictions ypred modelpredictxvalid ypred scalerinversetransformypred ypred ypredflatten plot the model predictions dfrenamecolumnsclose actual inplacetrue dfpredicted npnan dfpredictediloc ypredshape ypred dfactual predictedplottitleaapl
69617623,valueerror target size torchsize must be the same as input size torchsize,python pytorch lstm,try converting your ytr variable into a class one hot label this should have the shape of target torchsize
69608434,attribute error when i am try to modify a loaded lstm model parameter on colab,python tensorflow keras lstm,the error message is somewhat misleading you cannot directly change a parameter such as the number of units in a lstm as this changes the size of the layer weights you will have to build a new model with the same structure as the first one with a different number of units for the lstm layer then you will need to copy the model weights except for the lstm layer and also the next layer because the size of the lstm output is changed below is some code that does it the values of the parameters are arbitrary
69412519,problems calculating performance and plotting lstm predictions,python keras deeplearning lstm,the output layer of the network has a bad shape but the output value is suppose to be just one so changing the line modeladddensextrainshape into modeladddense solves the problem ps looking at the input data the dates are not strictly consecutive however you treat them as if they are i recommend here filling the sequence with interpolated numbers to make better predictions
69379787,valueerror when loading a text classification model in tensorflow,python keras lstm tensorflow tfkeras,okay i solved this by removing the maskzerotrue attribute from the embedding layer however im not sure why this works and why it did not work with maskzerotrue it would be helpful if someone can tell me the reason
69212537,keras modelpredict causing a valueerror,python tensorflow keras lstm tfkeras,problem is with lstm input shape lstm expects input of a d tensor with shape batch timesteps feature i could reproduce the issue output working sample code output
69045748,attributeerror kerasclassifier object has no attribute add,python keras scikitlearn deeplearning lstm,yes you forgot to add modelsequential right at the top your of createmodel
68631738,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,tensorflow keras neuralnetwork convneuralnetwork lstm,there is never a need to give a model a fixed value for the batchsize dimension tensorflow will handle this dynamically depending on the given data shape so in the construction of the model when executing summary this layers should have an input shape of none npast nfeatures
68630309,error function call stack trainfunction occurred in implementation of convlstmd,python tensorflow keras convneuralnetwork lstm,your model output is because you are passing tensor with more than dimension to dense layer so you will get all other dimensions at output not just dense you may resolve this issue by adding flatten layer before the last layer ps investigate your models output with modelsummary
68516848,problem in reshaping train and validation data for d cnn,python tensorflow keras convneuralnetwork lstm,your reshaping is wrong you are altering the number of samples so your data becomes incompatible with your labels as i understand you are trying to reshape into values which is impossible since if you increase your padding and make shape then it becomes possible since since its an integer you can do this reshaping as the following pseudocode arrs shape becomes numberofsamples
68515775,valueerror xtraindata not shaped in threedimensional,python numpy tensorflow keras lstm,your xtraindata needs to be dimensional and in shape number of observations time steps number of variables so try same applies for xtestdata you might need to apply npravelytraindata before fitting if other parts are ok then these changes should suffice
68323793,keras lstm input valueerror shapes are incompatible,tensorflow machinelearning neuralnetwork lstm recurrentneuralnetwork,it wont work except for the batch size every other input dimension should be the same also your inputs dimensions are all going crazy for example from what i see the first dimension is supposed to represent the total number of samples in xtrainytrain and xtestytest but in your case the total samples are represented by the second dimension the first dimension should be in second place so as to say the dimensions should this will put everything in the right place you just need to look at how you came to the wrong dimensions and change that part
68281041,invalidargumenterror on custom tensorflow model,python tensorflow lstm,the error is due to the batch size the fit function sets the batch size to by default but the batch size from the rnninputlayer layer is resulting in an error the error can be fixed by adding batch size to the fit function as shown below kindly refer to this gist for complete code thank you
68061611,lstm occurs valueerror shapes and are incompatible,python tensorflow machinelearning keras lstm,your ytrue and ypred are not in the same shape you may need to define your lstm in the following way update using returnsequences true would work because you define your trainingparis in that way which represent batchsize timestep inputlenght but note that you need to reshape or fulfill the input requirement of the lstm layer in your above model and not the ytrain however when you define your model you dont use the return sequence and it makes the last layer have only the three classifiers without timestep but your ytrain is defined in that way but if you set the return sequence to true and plot your model summary you would see that the last layer will have an output shape of none which exactly matches the shape of ytrain before understanding what the returnsequence is doing here you may need to know what timestep means in an lstm model check this answer afaik it depends on how many timesteps you need to set for your input i can make a single occurrence of the lstm cell or multiple times nth timestep and for nth timestep n n if i want from lstm to return all timestep output n numbers then i will set returnsequence true but else returnsequence false from doc returnsequences boolean whether to return the last output in the output sequence or the full sequence default false in short if it sets as true all sequences will return but if its false then only the last output will for example here is a oneway approach to your above code iris data took from here model inference
68060235,invalidargumenterror when making a stateful lstm,python tensorflow deeplearning lstm lstmstateful,when using stateful lstms your input must be evenly divisible by batch size in your case is not an integer since is a prime number you need to drop a sample make your input size when you have samples change to code according to model definition stays same
68036975,valueerror shape must be at least rank but is rank for node biasadd biasaddtdtfloat dataformatnchwadd bias with input shapes,python pythonx tensorflow keras lstm,i continue to see this problem in when using lstms or grus in sagemaker with condatensorflowp kernel heres my workaround early in your notebook before defining your model set i know it looks weird to set format when you arent processing pics but this somehow works around the dimension error to demonstrate that this isnt just a library mismatch in the default kernel heres something i sometimes add to the beginning of my notebooks to update to the latest library versions currently tf it did not solve the error above
67965364,tensorflow valueerror dimensions must be equal lstmmdn,python tensorflow lstm recurrentneuralnetwork mixturemodel,for mdn model the likelihood for each sample has to be calculated with all the gaussians pdf to do that i think you have to reshape your matrices ytrue and mu and take advantage of the broadcasting operation by adding as the last dimension eg
67896365,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras lstm,i was able to replicate your issue using sample code as shown below output fixed code lstm expects inputs a d tensor with shape batch timesteps feature output
67881732,attributeerror module sklearnmetrics has no attribute items,pythonx scikitlearn deeplearning lstm sklearnpandas,can you share more details about what is the purpose of your code as you can see here there isnt any attribute of sklearnmetrics named items items is used for dictionaries in order to get the values pertaining to different keys in that dictionary also you have defined ypred after it has been referenced so that will cause an error as well
67880921,attributeerror numpyndarray object has no attribute op,numpy tensorflow keras lstm,i made a mistake in the code itself while executing the model part of in the functional api version i have given the variables inside the above part of the code only while in this part we just define what our model is going to be hence here we will just write what is our input layer that needs to be considered and what will be my output layer here input layer while constructing my model will be inputlayer in the code and output layer will be output hence code should be and after that we can do this will work perfectly fine now
67876156,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim full shape received none,tensorflow machinelearning keras lstm imageclassification,convlstmd expects d tensor as inputs with shape samples time rows cols channels and mainly used for time series sequence type of data for you need to change this code from to also use sparsecategoricalcrossentropy as a loss function for please find this reference for detailed understanding on
67727890,rnn lstm valueerror while training,python numpy keras deeplearning lstm,change to basically keras is designed to take any number of examples in a single batch so it automatically puts none as the first parameter so when you mention the rest dimensions it gets a dimensional input in total but if you yourself mention the first dimension the number of the dimensions becomes ie none xtrainshapextrainshapextrainshape but again if you really want to hard code the batchsize you can still do it for this you have to use batchinputshape instead of inputshape like follow it will give you the power to control which specific batch size to set for the network your program has another flaw in this case you are setting the batch size xtrainshape which is but you are sending in fit but they should be equal also batch size is generally taken lesser than the data set size
67649606,error in reverse scaling outputs predicted by a lstm rnn,python tensorflow keras lstm recurrentneuralnetwork,this is because the model is predicting output with shape but your scaler was fit on data frame either create a new scaler on the data frame of the shape of y values or change your model dense layer output to dimensions instead of the data shape on which scaler is fit it will take that shape only during scalerinversetransform old code shape n trainyappenddffortrainingscaledi nfuture i nfuture updated code shape n use all outputs trainyappenddffortrainingscaledi nfuture i nfuture
67590787,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras lstm,in the stacked lstm layer lstm layers will not return sequences ie they will return d output which means that the second lstm layer will not have the d input it needs to address this you need to set the returnsequencestrue
67394357,valueerror logits and labels must have the same shape none vs none,python arrays numpy tensorflow lstm,add to my comments if you want to stick to softmax activation for any reason you may have more classes in future then modify the last line of code like this
67351013,disease risk prediction with lstm in python input shape problem,python tensorflow lstm recurrentneuralnetwork prediction,take the average of each patient i assume inputs are parameter and parameter therefore your input shape is numrows your input shape for lstm should be lstm inputshape
67329078,i failed to train cnn lstm model how can i solve this problem is it have problem in dataset or model python x,python tensorflow keras convneuralnetwork lstm,if possible try using pycharm and see whether error persists or not also check whether it is same error or not i ran vgg series models in google colab its quite faster
67301887,tensorflowkeras lstm vae cannot convert a symbolic tensor error on rhel airflow,python numpy tensorflow keras lstm,answering my own question this occured only because of numpy even though i downgraded to numpy i still got the error because airflow somehow cachedeither in memory or in airflowlocal previous installation of numpy and used it despite pip listing so i had to remove numpy in airflows local environment and rebooted the machine and this was resolved
67169344,unknown errorcrash tensorflow lstm with gpu no output after start of st epoch,python tensorflow keras lstm,i found the solution kinda so it works as it should when i downgraded tensorflow to cuda to and cudnn to at the time th combination from this list on tensorflow website i dont know why it didnt work at the newest version or at the valid combination for tensorflow its working well so my issue is solved nonetheless it would be nice to know why using lstm with cudnn on higher versions didnt work for me as i havent found this issue anywhere
67120386,keras lstm error logits and labels should have the same shape,python keras classification lstm keraslayer,you need to change the last layer to match the desired output shape from to by running the example code below you see how it work in practice
66983586,align feature map with ego motion problem of zooming ratio,image pytorch lstm pixel affinetransform,its caused by the function torchnnfunctionalaffinegrid i used i didnt fully understand this function before i use it these vivid images would be very helpful on showing what this function actually dowith comparison to the affine transformations in numpy
66907936,valueerror with shapes using bidirectional lstm,python tensorflow keras lstm bidirectional,simply setting returnsequencesfalse in your first bidirectional lstm and adding as before repeatvector works fine
66701895,error after attempting to train simple lstm with spy data,python pandas numpy multidimensionalarray lstm,you cannot range from to if you define your xtest the way you do if you change your code to the rest of the code will produce i only took epochs and might explain that the predictions arent what you expected with epochs youd get this
66574569,aws sagemaker keyerror smchanneltraining when tuning hyperparameters,pythonx deeplearning lstm amazonsagemaker hyperparameters,in your trainpy file changing the environment variable from parseraddargumentdatadir typestr defaultosenvironsmchanneltraining to parseraddargumentdatadir typestr defaultosenvironsmchanneltrain should address the issue this is the case with torchs frameworkversion but other versions might also be affected here is the link for your reference
66484529,issue passing concatenated inputs to lstm in keras,keras neuralnetwork lstm plaidml,to achieve the goal here you can use reshape layer that convert input into the target shape keras is integrated with tensorflow here is the working code in tensorflow version output
66201362,runtimeerror expected hidden size got,pytorch lstm,that is because of this line in your training loop modelhiddencell torchzeros modelhiddenlayersize torchzeros modelhiddenlayersize even though you correctly defined hiddencell in your model here you hard coded numlayers to be and replaced the one you did correctly to fix it you can change it to this modelhiddencell torchzerosmodelnumlayers modelhiddenlayersize torchzerosmodelnumlayers modelhiddenlayersize or even remove it totally as you are basically repeating what youve already done and i dont suppose hiddenlayersize will change midtraining maybe when you batch your data then leaving it here would make more sense but it is clear that your batchsize
66131870,pytorch gru error runtimeerror size mismatch m x m x,python machinelearning pytorch lstm,this might have to do with the fact that you are not passing the output of your nngru to the first linear layer in grunets forward function
66112042,valueerror dimensions must be equal but are and in timevec example,tensorflow keras deeplearning lstm wordvec,you have to change the parameters inside the tv layer and inside your network in order to correctly match the shapes create a dummy example
65906889,lstm error attributeerror tuple object has no attribute dim,python pytorch lstm,you wont be able to use a nnrnn inside a nnsequential since nnlstm layers will output a tuple containing the output features and hidden states and cell states the output must first be unpacked in order to use the output features in your subsequent layer nnlinear something as if your interested in the hidden states and cell states you could define a custom nnmodule and implement a simple forward function such that
65690765,error when checking target expected x to have dimensions but got array with shape,python tensorflow keras lstm,are you sure you posted the correct code i tried your code and did not have any issues code below you can copy and try it only difference is that i generated some random data
65683082,valueerror shapes none and none are incompatible,pythonx tensorflow lstm,looks like ytrainres comprise of integer indices not onehot vectors if so you have to use sparsecategoricalcrossentropy and change its shape to d
65659512,crossvalidation in lstm valueerror input of layer sequential is incompatible with the layer,python tensorflow keras lstm,i can see straight away some strange things in the cycle you aim to implement i think you can safely get rid of the before the for loop then you are not selecting the split istances but are just feeding the whole dataset xtrain this looks better
65485571,normalizing data in a time series classification problem recurrent neural networks,timeseries classification lstm recurrentneuralnetwork normalize,i would use the better performing network if that is the main priority goal of normalization is generally just so your loss doesnt explode during training so it often will improve results when values are very large however sometimes when the values are already small normalization will make it worse it is also possible that your range of values is too small you might want to try to normalize between or a larger range but if performance is already satisfactory without normalization i wouldnt bother
65470212,valueerror expected target size got torchsize lstm pytorch,pytorch lstm recurrentneuralnetwork,as a general comment let me just say that you have asked many different questions which makes it difficult for someone to answer i suggest asking just one question per stackoverflow post even if that means making several posts i will answer just the main question that i think you are asking why is my code crashing and how to fix it and hopefully that will clear up your other questions per your code the output of your model has dimensions n d c here n is the minibatch size c is the number of classes and d is the dimensionality of your input the cross entropy loss you are using expects the output to have dimension n c d and the target to have dimension n d to clear up the documentation that says n c d d dk remember that your input can be an arbitrary tensor of any dimensionality in your case inputs have length but nothing is to stop someone from making a model with say a x input in that case the loss would expect output to have dimension n c but in your case your input is one dimensional so you have just a single d for the length of your input now we see the error outputs should be n c d but yours is n d c your targets have the correct dimensions of n d you have two paths the fix the issue first is to change the structure of your network so that its output is n c d this may or may not be easy or what you want in the context of your model the second option is to transpose your axes at the time of loss computation using torchtranspose
65454942,getting nonbrodcastable error in my lstm,pandas machinelearning keras lstm prediction,you have a moment in your reshaping where you end up with a noninteger division take this example works well because results in an integer but in this example you end up with the error message valueerror cannot reshape array of size into shape because the division does not result in an integer so looping the way you do is bound to result in events of that type
65327942,lstm input shape error input is incompatible with layer sequential,python tensorflow keras lstm multiclassclassification,not sure why you are setting the input shape as for the first lstm as you mentioned this is what i need i have a sequence like this which are my xtrain and is labely so i mean timestep size is and each has one feature only if you have time steps and only a single feature then you should define each sequence as such also since the model will always output a length probability distribution but your ytrain is a single value out of unique classes you need to use the loss sparsecategoricalcrossentropy instead of categoricalcrossentropy read more here
65326913,pytorch lstm in onnxjs uncaught in promise error unrecognized input for node lstm,javascript python pytorch lstm,for anyone coming across this in the future i believe im getting this error because even though onnx supports pytorch lstm networks onnxjs does not support it yet to get around this instead of running in the browser i may use a simple web application framework called streamlit
65240830,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,tensorflow keras lstm recurrentneuralnetwork onehotencoding,looks like you are feeding onehot vectors to embedding layer you should feed sequences of integers shape should be if you really need to feed onehot vectors you have to convert onehot to integer to feed them into embedding layer try something like that
65236149,valueerror cannot reshape array of size into shape,python tensorflow timeseries convneuralnetwork lstm,you are trying to reshape xtrain to it is impossible because xtrain shape is x x x x x
65205506,lstm autoencoder problems,python neuralnetwork pytorch lstm autoencoder,okay after some debugging i think i know the reasons tldr you try to predict next timestep value instead of difference between current timestep and the previous one your hiddenfeatures number is too small making the model unable to fit even a single sample analysis code used lets start with the code model is the same what it does getdata either works on the data your provided if subtractfalse or if subtracttrue it subtracts value of the previous timestep from the current timestep rest of the code optimizes the model until e loss reached so we can compare how models capacity and its increase helps and what happens when we use the difference of timesteps instead of timesteps we will only vary hiddensize and subtract parameters no subtract small model hiddensize subtractfalse in this case we get a straight line model is unable to fit and grasp the phenomena presented in the data hence flat lines you mentioned iterations limit reached subtract small model hiddensize subtracttrue targets are now far from flat lines but model is unable to fit due to too small capacity iterations limit reached no subtract larger model hiddensize subtractfalse it got a lot better and our target was hit after steps no more flat lines model capacity seems quite fine for this single example subtract larger model hiddensize subtracttrue although the graph does not look that pretty we got to desired loss after only iterations finally usually use difference of timesteps instead of timesteps or some other transformation see here for more info about that in other cases neural network will try to simply copy output from the previous step as thats the easiest thing to do some minima will be found this way and going out of it will require more capacity when you use the difference between timesteps there is no way to extrapolate the trend from previous timestep neural network has to learn how the function actually varies use larger model for the whole dataset you should try something like i think but you can simply tune that one dont use flipud use bidirectional lstms in this way you can get info from forward and backward pass of lstm not to confuse with backprop this also should boost your score questions okay question you are saying that for variable x in the time series i should train the model to learn xi xi rather than the value of xi am i correctly interpreting yes exactly difference removes the urge of the neural network to base its predictions on the past timestep too much by simply getting last value and maybe changing it a little question you said my calculations for zero bottleneck were incorrect but for example lets say im using a simple dense network as an auto encoder getting the right bottleneck indeed depends on the data but if you make the bottleneck the same size as the input you get the identity function yes assuming that there is no nonlinearity involved which makes the thing harder see here for similar case in case of lstms there are nonlinearites thats one point another one is that we are accumulating timesteps into single encoder state so essentially we would have to accumulate timesteps identities into a single hidden and cell states which is highly unlikely one last point depending on the length of sequence lstms are prone to forgetting some of the least relevant information thats what they were designed to do not only to remember everything hence even more unlikely is numfeatures numtimesteps not a bottle neck of the same size as the input and therefore shouldnt it facilitate the model learning the identity it is but it assumes you have numtimesteps for each data point which is rarely the case might be here about the identity and why it is hard to do with nonlinearities for the network it was answered above one last point about identity functions if they were actually easy to learn resnets architectures would be unlikely to succeed network could converge to identity and make small fixes to the output without it which is not the case im curious about the statement always use difference of timesteps instead of timesteps it seem to have some normalizing effect by bringing all the features closer together but i dont understand why this is key having a larger model seemed to be the solution and the substract is just helping key here was indeed increasing model capacity subtraction trick depends on the data really lets imagine an extreme situation we have timesteps single feature initial timestep value is other timestep values vary by at most what the neural network would do what is the easiest here it would probably discard this or smaller change as noise and just predict for all of them especially if some regularization is in place as being off by is not much what if we subtract whole neural network loss is in the margin for each timestep instead of hence it is more severe to be wrong and yes it is connected to normalization in some sense come to think about it
65036744,valueerror no gradients provided for any variable when defining custom loss function,tensorflow keras lstm loss,as soon as you cast your variables to int or bool all gradient information is lost so the gradients are broken in this first line this is the reason why we usually use things like the binary crossentropy as it gives us a differentiable approximation to the nondifferentiable loss
64957777,lstm error logits and labels must have the same shape,tensorflow lstm,fixing the same shape issue i think you want to change your model to this and if we look at the model summary we get the following output we can see that it really looks like the data is flowing in the correct shape showing that this model works lets take the above model and train it returns the following training results looks like it trains up fantastic i think the main issue was the returnsequencefalse which basically got rid of your second dimension of data the flatten was unnecessary and if you fixed the returnsequence the flatten then caused another issue let me know if this solves your problem
64933711,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim full shape received,python tensorflow keras lstm,you should feed shape to the model instead of just add a dimension to your data
64408681,getting an error regarding input shape when calling predict function for a lstm model,python keras deeplearning lstm,it seems that the problem is with data preparation i think you should divide your samples instead of timesteps to train and test data and the shape of train and test samples should be the same and like none timesteps features since you have just one sample with observations timesteps you can divide your data into samples containing small size timestep series for example alternatively you can collect more data samples each containing time steps it depends on your application and your existing data also you can look at this and this that explained using lstm in keras comprehensively
64399760,error in lstm nlp multiclass model valueerror shapes none and none are incompatible,python tensorflow lstm valueerror,see my answer here in short convert your target variable to one hot its shape is batchsize and it should be batchsize
64351475,recurrent neural network valueerror found array with dim estimator expected,python lstm,what are you doing wrong lets assume your input data has shape xtrainshape ddd then after setting up your bilstmmodel like we can check the input and outputshapes your model expects by so your model expects input of shape nbatchdd where nbatch is the batch size of the data and returns a shape nbatchd thus a dtensor now if you provide a dtensor to your model the modelpredictionmethod will succesfully return a dtensor however sklearnpreprocessingstandardscalerinversetransform only works for ddata thats why it says on the other hand if you first reshape your data to be d then modelprediction complains because it is set up to expect a dtensor how can you fix it for further help on how to fix your code you will need to provide us with more detailled information on what you expect your model to do especially what outputshape you want your bilstmmodel to have i assume you actually want your bilstmmodel to return a scalar for each sample so an additional flattenlayer might do the trick
64277744,valueerror error when checking input expected lstminput to have shape but got array with shape,python tensorflow lstm recurrentneuralnetwork,when you change the shape of trainx using following line trainx npreshapextrain xtrainshape xtrainshape it becomes now when you have written following line modeladdlstmunits returnsequences true inputshape trainxshape the model expects inputshape to be batchsize you should have written modeladdlstmunits returnsequences true inputshape trainxshape now the model will expect inputshape to be batchsize which will correspond to your new tranx on a side note with sequence lengthlookbacklag time of and input features of using lstm does not really make sense because lstm is designed to extract temporal features and here you have no temporalhistorical data probably convd layer will be more relevant before reshaping the input to batchsize this is just a suggestion
63974261,invalidargumenterror assertion failed condition x y did not hold elementwise,python tensorflow machinelearning keras lstm,i have found my answer here tensorflow invalidargumenterror assertion failed condition x y did not hold elementwise i added this line before the dense layer
63783513,keras functional api issue with input layer and first lstm layer,python tensorflow keras lstm,you describe passing a batchsize parameter via the functional api and getting an error suggesting passing a batchshape argument to your input layer if you try changing batchsize batchsize in your input layer to does that solve it
63747027,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras deeplearning lstm,as marco certliani mentioned in the comments you need to format your input properly for a rnn which as the error you got mentioned is dimensional here is a representation of what the input tensor should look like meaning that your d tensor will have a shape of batchsize timestep inputdimension
63741755,cuda lstm unspecified launch failure error,python tensorflow lstm,how much data do you sent into your model at once it seems to me that you need to adjust your batchsize to me it looks like you feed too much data into your gpu at once causing cuda to crash how big are your sequenced how much is the memory allocation of your gpu however without more information about the data and whether cuda and cudnn is properly installed providing a more clear solution is difficult
63732431,valueerror error when checking input expected timedistributedinput to have dimensions but got array with shape,python tensorflow keras lstm convneuralnetwork,you can put a reshape layer and change the input shape thereafter full working example
63712011,could not load dynamic library cudartdll dlerror,python tensorflow keras lstm,technically its not an error its a warning its mainly just verbose so you can safely ignore it to silence it do this in the correct order you will also need to restart your kernel for now
63549926,tensorflowpythonframeworkerrorsimplalreadyexistserror when training lstm model,python tensorflow keras lstm,so i just tried an experiment and discovered that it was the output shape of the lstm and having a smaller output length and then expanding it with a dense layer removes the error
63455257,usage of lstmgru and flatten throws dimensional incompatibility error,tensorflow machinelearning multidimensionalarray keras lstm,i cannot reproduce your error check if the following code works for you
63329804,keras lstm input and output dimension issue,python keras deeplearning neuralnetwork lstm,you encounter a shape mismatch when handling the temporal dimension the temporal input dim is while you are trying to predict something with a temporal dimension of so you need something in your network that is able to increase from to temporal dimension i used the upsamplingd layer below a full example with input temporal dim output temporal dim you can use lambda or pooling operation if the dimension match below an example with lambda
63254807,l regularizer in keras gives error tensor object is not callable,python keras neuralnetwork lstm,you defined an lstm with name l it overrides the function you imported also named l
63227027,tensorflow keras lstm errors function call stack distributedfunction,python tensorflow keras lstm softmax,it seems that you are trying to feed string data directly into the network hence the error cast string to float is not supported if you are dealing with categorical data you should convert it into numerical first depending on the type of categorical data you are using different techniques should be applied for text consider reading official tensorflow guide on embedding or if your data consists of single tokens like toyota bmw ford check out categoryencoders
63166479,valueerror is only supported for tensors or numpy arrays found keraspreprocessingsequencetimeseriesgenerator object,python tensorflow keras lstm,your first intution is right that you cant use the validationsplit when using dataset generator you will have to understand how the functioninig of dataset generator happens the modelfit api does not know how many records or batch your dataset has in its first epoch as the data is generated or supplied for each batch one at a time to the model for training so there is no way to for the api to know how many records are initially there and then making a validation set out of it due to this reason you cannot use the validationsplit when using dataset generator you can read it in their documentation float between and fraction of the training data to be used as validation data the model will set apart this fraction of the training data will not train on it and will evaluate the loss and any model metrics on this data at the end of each epoch the validation data is selected from the last samples in the x and y data provided before shuffling this argument is not supported when x is a dataset generator or kerasutilssequence instance you need to read the last two lines where they have said that it is not supported for dataset generator what you can instead do is use the following code to split the dataset you can read in detail here i am just writing the important part from the link below i hope my answer helps you
62876780,how to shape test data in keras lstm prediction for multivariate inputs and dependent series problem,python tensorflow keras timeseries lstm,you can always initialize a generator for test predictions complete dummy example
62709313,a problem using lstm network neural networks,tensorflow keras neuralnetwork lstm,if i guessed correctly you want to classify which input in spoke by which speaker in that case your final layer should have a shape batchsize numofclasses or but if you take close look at the summary the output shape for final layer is to get the proper shape remove the argument returnsequencestrue from last lstm layer
62656254,keras lstm error when checking model target expected no data but got data,python keras lstm,the documentation for the model class according to this model class must receive input and output as different parameters and not as a part of the same list thats what the valueerror error when checking model target expected no data but got means the models target variabley is empty so removing the square bracket should help hope this helped may the force be with you
62606345,tensorflow error predictions must be condition x y did not hold elementwise while using bidirectional lstm layer,python tensorflow lstm,you are missing the last layer activation decoderdense layerstimedistributedlayersdensenumberoftags namedecoderdenseencoderbidirectionalrnn you should specify that you want a softmax leaving the activation as default is actually a linear activation meaning that you can have any value therefore the negative ones you should create the last dense layer as follows decoderdense layerstimedistributedlayersdensenumberoftags activationsoftmax namedecoderdenseencoderbidirectionalrnn
62534619,lstm with tfidf as input dimension error,machinelearning keras deeplearning lstm tfidf,this is a complete working example the error occurs because probably u didnt manage your data correctly take care also to define the first layer correctly and returnsequencesfalse because your output is d
62502919,valueerror logits and labels must have the same shape none vs none,python tensorflow machinelearning keras lstm,change the value in nblabels to and set you activation to softmax sigmoid is for binary cases
62453430,typeerror forward missing required positional argument with tensorboard pytorch,python pytorch lstm tensorboard,your model takes two arguments input and batchsize but you only provide one argument for addgraph to call your model with the inputs second argument to addgraph should be a tuple with the input and the batchsize writeraddgraphmodel sampledatatodevice batchsize you dont really need to provide the batch size to the forward method because you can infer it from the input as your lstm uses batchfirsttrue it means that the input is required to have size batchsize seqlen numfeatures therefore the size of the first dimension is the current batch size def forwardself input batchsize inputsize
62317860,valueerror no gradients provided for any variable tensorflow,python tensorflow keras lstm,you get this error when you pass only the training data and missed to pass the labels in modelfit i was able to recreate your error using below code you can download the dataset i am using in the program from here i am using tensorflow version code to recreate the issue output solution pass the training labels in modelfit and your error will be fixed modified to code output hope this answers your question happy learning
62279886,typeerror nonetype object is not callable tensorflow,pythonx tensorflow lstm,problem is that when you stack multiple lstms we should use the argument returnsequences true in lstm layer it is because if returnsequences false default behavior lstm will return the output of the last time step but when we stack lstms we will need the output of the complete sequence rather than just the last time step changing your model to should resolve the error this way you can use bidirectional lstms as well please let me know if you face any other error and i will be happy to help you hope this helps happy learning
62256568,what is the relationship between batch size timestep and error in lstm keras,python keras lstm recurrentneuralnetwork lstmstateful,batch size effect on lstm for batch size the model takes input at each timestep for batch size n model takes n input at each timestep error calculation part mentioned in question it is the error calculation for batch size error for a batch sum up the error for each element of the batch to get the final error batching in stateful lstm my understanding of parallelism was incorrect parallelism is done within batch not across them
62253289,valueerror data cardinality is ambiguous,python tensorflow keras lstm,as the error suggests the first dimension of x and y is different first dimension indicates the batch size and it should be same please ensure that y also has the shape something i could reproduce your error with the code shown below if we observe the print statements this error can be fixed by uncommenting the line y yreshape which makes the first dimension batchsize equal for both x and y now the working code is shown below along with the output the output of above code is hope this helps happy learning
62251598,valueerror error when checking target expected dense to have dimensions but got array with shape,python keras lstm,the returnsequencestrue is the culprit with it true it will return tensor rank maybe timesteps which you can put it to another rnn layers but here you want to go straight to output so change this to false from comment it seems you have more than one lstm the last one need returnsequencesfalse to have rank tensor as output the from error log while others need returnsequencestrue to return tensor as rank for the next rnn layers
62227607,issue with concatenating keras layers,python keras deeplearning timeseries lstm,i found where the issue is the function to concatenate keras layers must be concatenate with capital c since i was using the one with lowercase c it was numpy function and hence it was giving that error a small typo costed me lot of time
62213418,how to fix strange model accuracy graph,python tensorflow keras lstm,it is all flat and then it starts jumping and then stops yes this is weird but you dont have to worry about that what you need is a smooth curve rising up the first thing i would suggest you is to decrease your learning rate or set it i cant see where you have set it up keep it at e the golden learning rate then play around with it if that doesnt work reduce your layers in model and then play with dropout but first try the learning rate
62169725,building cnn lstm in keras for a regression problem what are proper shapes,python tensorflow keras deeplearning lstm,one possible solution is setting the lstm input to be of shape numpixels cnnfeatures in your particular case having a cnn with filters the lstm would receive
62114444,attributeerror list object has no attribute shape,python lstm prediction,your xtrain is not a np array but a list you first have to convert your list to a numpy array
62048059,python keras lstm error when checking input,python tensorflow keras timeseries lstm,feeding a rnn is a bit different than other networks as it works with sequences the problem in your code is the dataset format rnns input should be a d tensor with shape batch timesteps feature as you have a timeseries dataset you should preprocess your input data with a moving window scheme you should check this tutorial about time series forecasting where they implement this window scheme like this here it is an illustration of the moving window procedure taken from this paper your problem is not exactly time series forecasting so you should adapt the code from that tutorial to your problem it should be something like this pasthistory is a hyperparameter that should be tuned hope it helps
61964672,passing embedded sequence to lstm and getting typeerror int object is not subscriptable,python pytorch lstm embedding,the lstm accepts two inputs as described in nnlstm inputs input the input sequences h c a tuple with the initial hidden state h and the initial cell state c but you are passing hiddensize as the second argument which is an int and not a tuple when the tuple is unpacked it fails since hiddensize does not work as integers cannot be indexed the second argument is optional and if you dont provide it the hidden and cell states will default to zero thats usually what you want therefore you can leave it off output hidden lstmsequence
61866244,inverse transform throws error in lstm prediction,python scikitlearn transform lstm valueerror,youre trying to do the inverse transform on your y data when you only applied the transform to the x data the difference in the number of features for x for y is why you get that error edit you need to create separate scalers for your x and y data eg then when you want to make a prediction from your fitted model
61495676,which loss function to choose for sequence classification problem,keras lstm recurrentneuralnetwork,the most standard way is to model the output distribution using softmax the appropriate loss function is categorical crossentropy standard categorial crossentropy expects the targets as onehot vectors if you want to use the indices in y directly use sparse categorical crossentropy see example two in this tutorial it seems to do exactly what you want
61452521,bidirectional lstm model not yet been built error,python keras lstm,the model needs to know what input shape it should expect set input shape for bidirectional layer first layer is bidirectional layer
61417491,getting an error stating i need to specify stepsperepoch,python keras lstm recurrentneuralnetwork,turns out my y was in a symbolic tensor since i was using the tensorflow onehot function i just used the keras to categorical function and was able to get a numpy array which worked
61325958,scalerinversetransform is giving an error while taking lstm nn predictions into real data values,python tensorflow machinelearning keras lstm,it is just what the error is saying when using scikitlearn modules they usually have a fit transform or in case of classifiers also predict methods after making an instance of a class like minmaxscaler in your case you need to fit it on some data just call its fit method and pass your training examples as an argument in your code here you made an instance but it does not know what your data was to update its internal variables which it uses when you call its transform or inversetransform scikitlearn doc is the best documentation you ever see check it out if you still felt stuck
61296736,how to understand and debug the error inside kerasmodelfit,keras lstm keraslayer tfkeras,your output layer has outputs probably classes are given but the target labels in ytrain are given as integers apparently array is flat you need to convert ytrain to a onehot encoded array first otherwise the crossentropy loss cannot be computed input and output tensors must always be compatible with the inputs and outputs of the model given that there are classes in ytrain encoded as you require a conversion etc try it like this
61216136,valueerror error when checking input expected lstminput to have dimensions but got array with shape,python keras deeplearning convneuralnetwork lstm,the input shape of lstm is batchsize x timesteps x inputsize when batch first ie an lstmrecurrent network is unrolled timesteps times for each sample and each unrolling will get an input of inputsize lest see your model architecture the input size of lstm is batchsize x x and the output size is batchsize x x since returnsequence is true so you will have to pass yourtrain samples oflength and each having featuresxyz to the lstm you can reshape your inputs by squeezing out the last dimension code output
61213493,pytorch lstm for multiclass classification typeerror not supported between instances of example and example,python pytorch lstm multiclassclassification,the bucketiterator sorts the data to make batches with examples of similar length to avoid having too much padding for that it needs to know what the sorting criterion is which should be the text length since it is not fixed to a specific data layout you can freely choose which field it should use but that also means you must provide that information to sortkey in your case there are two possible fields text and wagelabel and you want to sort it based on the length of the text trainiterator validiterator testiterator databucketiteratorsplits traindata validdata testtorch batchsize batchsize sortwithinbatch true sortkey lambda x lenxtext device device you might be wondering why it worked in the tutorial but doesnt in your example the reason is that if sortkey is not specified it defers it to the underlying dataset in the tutorial they used the imdb dataset which defines the sortkey to be xtext your custom dataset did not define that so you need to specify it manually
61198125,attributeerror object has no attribute outputtensorcache,python tensorflow machinelearning keras lstm,thats because you are mixing native keras implementation and tensorflow implementation of keras ie tfkeras you should never do that fix the import for model class by using from tensorflowkeras import model
61097579,valueerror error when checking target expected dense to have shape but got array with shape,python tensorflow keras lstm recurrentneuralnetwork,you passed returnsequences true thus lstm returns the output for each time step because you set outputs the shape will be x you have two options set returnsequences false put a flatten layer between lstm and the dense layer sorry i cannot test it right now so maybe there could be other subtleties
61084839,tensorflowkeras error valueerror error when checking input expected lstminput to have dimensions but got array with shape,python pandas tensorflow keras lstm,the problem is that you are trying to feed dimensional array for the model but it is expecting dimensional array instead of reshaping dataframe convert it to array and then reshape according to the modified code below
61023933,keras lstm valueerror error when checking target expected dense to have shape but got array with shape,python tensorflow keras lstm,you need the same units in input layer dense than the output dim of the lstm layer in your case the output of the lstm layer is like you can see in the shape try change by or let the model infer the amount of units of the layer like this
60894506,shap lstm keras tensorflow valueerror shape mismatch objects cannot be broadcast to a single shape,python tensorflow keras lstm shap,from the documentation the first two arguments of shapsummaryplot are shapvalues numpyarray for single output explanations this is a matrix of shap values samples x features for multioutput explanations this is a list of such matrices of shap values features numpyarray or pandasdataframe or list matrix of feature values samples x features or a featurenames list as shorthand in the code that you provided both shapvals and valx have shape therefore you should either flatten the first and second dimensions or select the timepoint you are interested in for example this yields the following plot
60839583,valueerror failed to convert a numpy array to a tensor unsupported object type numpyndarray,tensorflow keras lstm tfidf embedding,i solved the problem by using a sequential model removing line and i only used one input layer and concatenating tfidftrain to featurestrain using npconcatenate instead of a concatenate layer
60785232,error when checking target expected dense to have dimensions but got array with shape,python keras deeplearning neuralnetwork lstm,if you are expecting to get outputs your final dense layer should have neurons and not ie modeladddense activationsoftmax should be modeladddense activationsoftmax also your y should have a shape of numberofexamples in your case it should be instead of if you actually have a y with a shape of then you cannot use a dense layer at the output as dense layer only works with examples that are onedimensional vectors each
60732223,indexerror list index out of range during training in tensorflow,python tensorflow lstm,the format of your call to sequentialfit is incorrect the first two parameters should be x and y rather than a tuple x y ie history simplelstmmodelfit xtrain ytrain epochsepochs validationdataxtest ytest validationsteps
60714477,unimplementederror cast string to float is not supported node cast defined at cusersuserspyderpylstmpy,python string tensorflow keras lstm,you didnt provide the code for modelcheckpoint but im going to assume that it takes a few parameters edit op commented that its from kerascallbackscallbacksmodelcheckpoint the documentaiton says this should fix it note from documentation filepath can contain named formatting options which will be filled with the values of epoch and keys in logs passed in onepochend filepath lstmfinalepochdvalueaccf checkpoint modelcheckpointfilepath monitorvalacc verbose savebestonlytrue modemax
60511334,error when checking target expected dense to have shape but got array with shape,pythonx machinelearning lstm,before i begin a correction in your question each cell in your dataframe is a list of elements this function assumes you have the same number of elements in each cell
60398061,error when checking input expected lstminput to have dimensions but got array with shape,python keras lstm categoricaldata,you need to reshape your input like this and specify the input shape like this so use these lines and it will work lastly your activation function should be sigmoid if you have only two classes
60301760,adding bidirectional in a keras lstm results in a runtimeerroryou must compile your model before using it,python keras lstm,it seems like a bug in your case because i tried the same its working normally please check if model is compiled withinoutside class if problem is solved already please let us know as well
60298928,multicell lstm rnn returns nan training error,python tensorflow lstm recurrentneuralnetwork,the reason why validation error showed a normal value and training error did not was because i was making mini batches that hold nan values apparently and did not make sense totalbatch should have been inttrainsize batchsize it was really hard to find this cause numpy does not return any error when array slicing is out of bounds anyway hope it helps people with similar issues in the future
60227859,how can i do a sequencetosequence model rnn lstm with keras with fixed length data,python tensorflow keras lstm sequencemodeling,if you only want a lstm model that takes an input of shape nbseq with nbseq beeing your number of sequences in your case and outputs the same shape here is a basic model that you can adapt the lstm layer with returnsequencetrue will return a tensor of shape nbseq with beeing the number of neurones of your lstm layer so in order to find the original shape you can either pass this tensor throught a dense layer that will give you a shape of nbseq or you can directly have neurones on your last lstm layer i dont recommand because it will generate many parameters you can modify this as you wish edit after precisions as keras lstm takes only d input you can trick it by passing a timedistributed flatten layer at the begining like this that gives you this summary
60154328,pytorch lstm runtimeerror invalid argument sizes of tensors must match except in dimension got and in dimension,python pytorch lstm,this exception is thrown basically because youre loading batches with different shapes as theyre stored in the same tensor all samples must have the same shape in this case you have an input in dimension with and which is not possible for example you have something like you must have the easiest way to solve this problem is setting batchsize however it may delay your code the best way is setting the data to the same shape in this case you need to assess your problem to check if its possible
60054101,keras lstm error expected to see array,python arrays machinelearning keras lstm,you need to change the inputshape value for lstm layer also xtrain must have the following shape so change to
60053537,how to adjust a data for mlp to lstm expected ndim found ndim error,python keras deeplearning lstm dataprocessing,lstm unlike a perceptron is not a feedforwarding network it needs a history to predict the next point so a proper input tensor to a lstm should be of shape timesteps numfeatures meaning that each sample is a sequence of timesteps observations such that the cell state is initiated in the first observation of the sequence and goes through the entire sequence therefore the input tensor should have the shape numsequences seqlength numfeatures where numsequences number of samples ie how many sequences do you have to train the model seqlength how long these sequences are for variablelength sequences you can supply none numfeatures how many features does have a single observation in a given sequence
59687375,error when checking target expected dense to have dimensions but got array with shape,tensorflow keras classification lstm unsupervisedlearning,a couple of remarks xtrain is given as input to a lstm layer which expects d input the first dimension is the samples second is the time steps and final dimension are the features when you call fit you pass xtrain twice meaning you want the target to be the same as the input data this makes sense if you are trying to do an autoencoder however it cannot work on this architecture the output of the dense layer will be a single value there is no way it can match the shape of the input which is d below i added a ytrain so that your model instead predicts a single value timesteps features xtrain nprandomnormalsize timesteps features ytrain nprandomnormalsize model sequential modeladdlstm inputshapetimesteps features namelstm modeladddense activationrelu modelsummary modelcompilelossbinarycrossentropy optimizeradam metricsaccuracy modelfitxtrain ytrain batchsize epochs verbose best of luck
59679804,array reshape issue while time series prediction with convlstm in python,python numpy keras convneuralnetwork lstm,looks like an issue of pure math youre trying to fit the dimension of size into multiple dimensions of sizes try setting nseq or nsteps this way the data will have the same size when reshaping
59668924,error when checking input expected embeddingembeddinginput to have shape but got array with shape,tensorflow lstm tensorflowjs,the model is expecting input of shape null totalwords null is for the batchsize the input should be at least a d tensor if it is not d tensor it is expanded on the axis the input tensor can be expanded on the axis if you want to predict from a single vector let predictor tftensord sets i predictor totalwords totalwords should be
59203388,valueerror error when checking input expected lstminput to have dimensions but got array with shape,tensorflow lstm tensorflowdatasets,well i found a couple of answers lstms in keras expect a dimensional input by changing xtrain tfvariable dtypetfint to xtrain tfvariable dtypetfint it is close to working but throws the following exception turning the dtypetfint to dtypetffloat works why is it not possible to feed integer values to a lstm furthermore a warning is delivered this is resolved by either changing the tfvariable to tfconstant or using a numpyndarray instead of a tf type i thought that those are represented internally by numpy arrays notice i do nomore deliver the dataset to the fit function i feed the xtrain variables by themselves as input and output data using a dataset still provides the error message this is resolved by batching the dataset eg dsbatch and changing epochs to epochs because tensorflow expects the size of the training dataset to match stepsperepochepochs best regards karsten
59106391,using convd error when checking input expected convdinput to have dimensions but got array with shape,python keras neuralnetwork convneuralnetwork lstm,theres a couple of problems i notice with your code xtrain needs to be a d tensor because anything else convd cannot process so if you have d data you need to add a new dimension to make it d your inputshape needs to be changed to reflect that for example if you added only a single channel it should be nfeatures
59035050,lstm class got error typeerror init got an unexpected keyword argument inputshape,class lstm keyword init,your class is called lstm and you call a function from keras called lstm try renaming your class mylstm or some variant otherwise you wont be able to call your class without overwriting the keras implementation
59015337,fastai lstm forward method issue,lstm fastai,your observation is correct and what happens happens in the collatefunction of your databunch basically when iterating through a dataloader what happens is that x is datacollatefndatatraindsxi for i in listindices where listindices is the list of the indice of the sample youre using for this batch when you look at datatraindsx you get and datatraindsxdata yields the reason why this is yielded is that tabular data in fastai deals with categorical and continuous variable and this is handled by creating a special learner with tabularlearner two solutions are possible here modifying the dataset so that datatraindsxidata is a tensor of shape modifying the collatefn so that it returns a bs inputdim shaped tensor should do the trick as well for example adding and passing collatefncustomcollatefn in the databunch creation will work however you will get an error later because you have items and your model only work if the batch is exaclty of size let me know if you need help on that matter
58902773,im getting an error while running the rnn lstm model with tensorflow,python tensorflow keras lstm recurrentneuralnetwork,i finally found the problemthe problem was my cudnn was less than the recomended version by tensorflow which is when i upgraded ot to the latest version released it was fixed
58803517,keras lstm input shape error pleas help,keras lstm,check tensorflows api model objects must be instantiated using symbolic tensors in your case that is
58660454,error when checking input expected lstminput to have dimensions but got array with shape,python numpy keras lstm recurrentneuralnetwork,as the error message says lstm needs input in dimensions you can reshape like this xtrain xtrainreshapextrainshape xtrainshape basically turning shape a b into a b
58636087,tensorflow valueerror failed to convert a numpy array to a tensor unsupported object type float,python tensorflow keras lstm,tldr several possible errors most fixed with x npasarrayxastypefloat others may be faulty data preprocessing ensure everything is properly formatted categoricals nans strings etc below shows what the model expects printishape idtype for i in modelinputs printoshape odtype for o in modeloutputs printlname linputshape ldtype for l in modellayers the problems rooted in using lists as inputs as opposed to numpy arrays kerastf doesnt support former a simple conversion is xarray npasarrayxlist the next steps to ensure data is fed in expected format for lstm thatd be a d tensor with dimensions batchsize timesteps features or equivalently numsamples timesteps channels lastly as a debug protip print all the shapes for your data code accomplishing all of the above below sequences npasarraysequences targets npasarraytargets showshapes sequences npexpanddimssequences targets npexpanddimstargets showshapes outputs expected numsamples timesteps channels sequences targets expected numsamples timesteps channels sequences targets as a bonus tip i notice youre running via main so your ide probably lacks a jupyterlike cellbased execution i strongly recommend the spyder ide its as simple as adding in and pressing ctrl enter below function used def showshapes can make yours to take inputs thisll use local variable values printexpected numsamples timesteps channels printsequences formatsequencesshape printtargets formattargetsshape
58635521,tensorflow typeerror int object is not iterable,python tensorflow keras lstm recurrentneuralnetwork,keras expects inputshape to always be a tuple for a single value itd look like for lstm however the expected full shape batchshape is numsamples timesteps numchannels or equivalently batchsize timesteps features inputshape is simply batchshape without dimension ie timesteps numchannels if your input data is univariate eg d sequence then numchannels thus modeladdlstm activationrelu inputshape returnsequencestrue lastly for binarycrossentropy a better output layer would be dense activationsigmoid for more info see this answer tip to be sure run printxtrainshape and make sure all the values except first dim match your inputshape id recommend however to always use batchshape over inputshape unless the application involves variable batch sizes it makes debugging much easier for your d example if it returns something like youll need to add a dimension to make it d xtrain npexpanddimsxtrain last axis
58610302,keras cnnlstm error while making ytrain,keras convneuralnetwork lstm recurrentneuralnetwork traintestsplit,i recreated a slightly simplified version of your situation to reproduce the problem basically it appears that the lstm layer is only putting out one result for the entire sequence of time steps thereby reducing the dimension from to in the output if you run my program below ive added the modelsummary which provides details of the architecture i believe youll need to decide if you want to reduce the dimension of the ytrain target data to be consistent with the model or change the model i hope this helps
58573698,how to solve the problem of dimentions after turning the returnsequence off in lstm,python keras lstm,i believe what is happening is when you change returnsequences from true to false you are reducing the output from the lstm layer to be dimensional in particular only having one output at the end of processing all of the timesteps this is now in conflict with what the fit is expecting because the target data trout is d i believe the returnsequencestrue outputs one result per timestep which looks like there are timesteps per sample in your case i think youll need to make the trout dimensions match become d when returnsequencesfalse i hope this helps
58483210,error could not find a version that satisfies the requirement tensorflow from versions none error no matching distribution found for tensorflow,python tensorflow keras lstm,on windows you must use python bits or later version provided it is bits to install tensorflow unfortunately the bit version is not supported by tensorflow and will give you that nasty error could not find a version that satisfies the requirement tensorflow from versions none error no matching distribution found for tensorflow a few important notes install microsoft visual c redistributable packages for x if you have previously installed python for x uninstall it from your system and also delete the directory where the x packages were stored to avoid a conflict with the new x packages that will be placed there in my machine they were stored at cuserskarlphillipappdataroamingpythonpythonsitepackages finally install python x and upgrade pip with python m pip install user upgrade pip now simply install tensorflow python m pip install user tensorflow
58458720,how to fix error when checking input expected input to have dimensions but got array with shape,python tensorflow machinelearning keras lstm,you need to put the sequencr length in the input shape too shape must be the same as xbatchshape like this i suppose you tried to use expanddims to fix this so i dont include that
58414793,python keras running error invalid syntax,python keras lstm,async is reserved keyword just use another name for argument
58357089,keyerror f python for lstm model,python machinelearning keras anaconda lstm,the key to the dictionary wordvecsources can either be fasttext or customfasttext but the key it received was f this means youre somehow assigning vectorsource the value f when it should be none fasttext or customfasttext instead to prevent the keyerror check the shell commandline argument which pertains to vectorsource in your code for eg in yourfilepy say you have vectorsource strsysargv then you should be running something like python yourfilepy fasttext instead of running python yourfilepy f since here the first commandline argument sysargv assigns vectorsource its value at the runtime
58258614,error when creating using a custom keras layer in a model with two outputs,python keras neuralnetwork lstm,my mistake was not using the right tool for matrix multiplication specifically in scoringlayer i should have used instead of using this solves the problem the new architecture used changes the names used in history so i also had to change my report replacing with
58182197,valueerror error when checking input expected input to have dimensions but got array with shape,python tensorflow keras lstm keraslayer,adjusting the code like this solved it for me the problem was that i needed to add a time dimension to my embedding that i was passing through my lstm so basically going from batchsizeinputdimension to batchsizeinputdimensiontimesteps the order of those dimensions might not be correct i still need to check that but this solution should at least get the model to work
58142582,error dimensionmismatchmatrix a has dimensions vector b has length using flux in julia,machinelearning julia lstm fluxjl,introduction first off you should know that from an architectural standpoint you are asking something very difficult from your network softmax renormalizes outputs to be between and weighted like a probability distribution which means that asking your network to output values like to match y will be impossible thats not what is causing the dimension mismatch but its something to be aware of im going to drop the softmax at the end to give the network a fighting chance especially since its not whats causing the problem debugging shape mismatches lets walk through what actually happens inside of fluxtrain the definition is actually surprisingly simple ignoring everything that doesnt matter to us we are left with therefore lets start by pulling the first element out of your data and splatting it into your loss function you didnt specify your loss function or optimizer in the question although softmax usually means you should use crossentropy loss your y values are very much not probabilities and so if we drop the softmax we can just use the deadsimple mse loss for optimizer well default to good old adam now to simulate the first run of fluxtrain we take firstdata and splat that into loss this gives us the error message youve seen before error dimensionmismatchmatrix a has dimensions vector b has length looking at our data we see that yes indeed the first element of our dataset has a length of and so we will change our model to instead expect values instead of and now we rerun huzzah it worked we can run this again the value changes because the rnn holds memory within itself which gets updated each time we run the network otherwise we would expect the network to give the same answer for the same inputs the problem comes however when we try to run the second training instance through our network understanding lstms this is where we run into machine learning problems more than programming problems the issue here is that we have promised to feed that first lstm network a vector of length well now and we are breaking that promise this is a general rule of deep learning you always have to obey the contracts you sign about the shape of the tensors that are flowing through your model now the reasons youre using lstms at all is probably because you want to feed in ragged data chew it up then do something with the result maybe youre processing sentences which are all of variable length and you want to do sentiment analysis or somesuch the beauty of recurrent architectures like lstms is that they are able to carry information from one execution to another and they are therefore able to build up an internal representation of a sequence when applied upon one time point after another when building an lstm layer in flux you are therefore declaring not the length of the sequence you will feed in but rather the dimensionality of each time point imagine if you had an accelerometer reading that was points long and gave you x y z values at each time point to read that in you would create an lstm that takes in a dimensionality of then feed it times writing our own training loop i find it very instructive to write our own training loop and model execution function so that we have full control over everything when dealing with time series its often easy to get confused about how to call lstms and dense layers and whatnot so i offer these simple rules of thumb when mapping from one time series to another eg constantly predict future motion from previous motion you can use a single chain and call it in a loop for every input time point you output another when mapping from a time series to a single output eg reduce sentence to happy sentiment or sad sentiment you must first chomp all the data up and reduce it to a fixed size you feed many things in but at the end only one comes out were going to rearchitect our model into two pieces first the recurrent pacman section where we chomp up a variablelength time sequence into an internal state vector of predetermined length then a feedforward section that takes that internal state vector and reduces it down to a single output the reason we split it up into two pieces like this is because the problem statement wants us to reduce a variablelength input series to a single number were in the second bullet point above so our code naturally must take this into account we will write our lossx y function to instead of calling modelx it will instead do the pacman dance then call the reducer on the output note that we also must reset the rnn state so that the internal state is cleared for each independent training example feeding this into fluxtrain actually trains albeit not very well final observations although your data is all ints its pretty typical to use floating point numbers with everything except embeddings an embedding is a way to take nonnumeric data such as characters or words and assign numbers to them kind of like ascii if youre dealing with text youre almost certainly going to be working with some kind of embedding and that embedding will dictate what the dimensionality of your first lstm is whereupon your inputs will all be onehot encoded softmax is used when you want to predict probabilities its going to ensure that for each input the outputs are all between and moreover that they sum to like a good little probability distribution should this is most useful when doing classification when you want to wrangle your wild network output values of into something where you can say we have certainty that the second class is correct and certainty its the third class when training these networks youre often going to want to batch multiple time series at once through your network for hardware efficiency reasons this is both simple and complex because on one hand it just means that instead of passing a single sx vector through where s is the size of your embedding youre instead going to be passing through an sxn matrix but it also means that the number of timesteps of everything within your batch must match because the sxn must remain the same across all timesteps so if one time series ends before any of the others in your batch you cant just drop it and thereby reduce n halfway through a batch so what most people do is pad their timeseries all to the same length good luck in your ml journey
58119320,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras neuralnetwork lstm,an lstm network expects three dimensional input of this format there are two main ways this can be a problem your input is d you have stacked multiple lstm layers your input is d you need to turn your input to d then specify the right input shape in the first layer you have stacked lstm layers by default lstm layers will not return sequences ie they will return d output this means that the second lstm layer will not have the d input it needs to address this you need to set the returnsequencestrue heres how to reproduce and solve the d input problem heres how to reproduce and solve the stacked lstm layers problem
58095627,how to fix input and hidden tensors are not at the same device in pytorch,python pythonx pytorch gpu lstm,you need to move the model the inputs and the targets to cuda
58072978,model fit and dimension size error for kera lstm network in python,python tensorflow keras lstm dimension,inputshape specifies the shape of one sample in here your train batch size is and shape of one sample is since the first layer is a lstm layer it needs dimensional inputbatchsizenumberoftimestampsofasamplediamentionalityofonetimestampbut we dont mention the batch size in inputshapeso input shape of a lstm layer should be inputshapenumberoftimestampsofasamplediamentionalityofonetimestamp so u should replace inputshape with inputshape add the following line before training trainxtrainxreshape to be more clear please refer to this video uploaded by me
57968421,mean squred error interpretation in lstm model bidirectional or multiparallel,python keras lstm meansquareerror,update the question was updated and hence a new comment again your mse of does not mean of anything it is just a scalar and you cannot take it as a percentage of your ground truth is it a good practical interpretation of the models precision i would ask this question differently what does it mean to me well looking at your target values that range from to you can say that on average your error was now if you need more details it means that when you were predicting you guessed when etc but this is on average hope you got the idea by now lets have a look at the formula of mean squared error mse so it is just the average of the squared difference between the predicted and the actual data points in your case sqaure root of mse rmse is or around rounded does it mean that my model mean error is no you cannot say that your the error is when you get the loss of is just the value of the error and your each individual errors could have been negative or positive making their sum zero but not their squared sum is it good practical interpretation of the model precision depends on your goals you usually measure accuracy depending on some other metrics too like mae r sqaured and others assuming you did not rescale your data to its original magnitude then there is no way to know how good or bad this is on absolute terms so i suggest you measure mse on your rescaled data and then decide if not done so already
57874436,tensorflow data adapter error valueerror failed to find data adapter that can handle input,python tensorflow keras lstm,have you checked whether your trainingtesting data and trainingtesting labels are all numpy arrays it might be that youre mixing numpy arrays with lists
57736680,how to solve nameerror name model is not defined error,python tensorflow neuralnetwork lstm,the issue lies in the magic function time in the latest version of ipython in jupyter running a cell with time magic function as a header runs the cell out of the global context this is also true for timeit practically it means that all the new variables defined in the time cell do not exist in the main context including your model variable which is why you receive the nameerror exception since the interpreter can not find a variable named model removing the time line from your cell will do the trick
57666632,error with numpyndarray object has no attribute append,lstm,i think according to the official documentation the correct may be as follows edit in for loop because the arguments of append is arg array to which new values should be appended and arg valuesarray to append for reference click here
57541947,keras lstm tensorflow error shapes must be equal rank but are and,python tensorflow keras lstm recurrentneuralnetwork,you probably want to repeat the output of first lstm layer as much as the number of timesteps in models output sequence ie y therefore it should be
57410804,valueerror error when checking input expected lstminput to have shape but got array with shape,pythonx keras lstm dimensions valueerror,the model is expecting data in form of batchsize since you are calling fit with batchsizebatchsize
57408579,keras functional api input shape errors,api keras functionalprogramming lstm multipleinput,you should use modelsummary to see the output shapes of your layers and model and then adjust the model andor the targets accordingly the problem is that there is a mismatch between the output of the model and the targets for example if you use an lstm with returnsequencestrue then the output of that lstm is d which is fed into a dense that only operates in the last dimension also outputting a d shape maybe that is not what you want you could just set returnsequencesfalse to the lstm layers that are closer to the output or just flatten if you really need them to output a sequence so the dense layers output d shapes in which case you should reshape the targets as
57245390,error with input shapes for timeseriesgenerator in a lstm stacked rnn,python keras deeplearning timeseries lstm,the length parameter in timeseriesgenerator refers to the number of timesteps to extract from the sequence therefore in your example with lengthnlag as nlag your generator is generating subsequences of length the error is being thrown because you have set and finaldatasetshape so your model expects input sequences of length as for predict the next days you need to decide whether you want to predict days after a selection of subsequences in your current sequence or whether you wish to predict days after the full sequence which would require more labelled time points to train against in both cases you should consider feeding the output of your rnn back in as the input
57172352,lstm invalidargumenterror tensorflow keras when converting to estimator,python tensorflow lstm tensorflowestimator,going to answer this myselfit seems like theres problems with tfkeraslayerslstm as of as seen here i changed the model to the following
57148617,runtimeerror expected hidden size got,python pytorch lstm recurrentneuralnetwork gatedrecurrentunit,this error happens when the number of samples in data set is not a multiple of the size of the batch ignoring the last batch can solve the problem for identifying the last batch check the number of elements in each batch if that was less than batchsize then it is the last batch in data set
57142772,what is the correct procedure to split the data sets for classification problem,python machinelearning lstm traintestsplit,tldr try both i have been in similar situations before where my dataset was imbalanced i used traintestsplit or kfold to get through however once i stumbled upon the problem of handling imbalanced datasets and came across the techniques of overbalancing and underbalancing to do this i would recommend using the library imblearn you will find various techniques there to handle the cases where one of your classes outnumbers the other one i personally have used smote a lot and have had relatively better success in such cases other references
57128200,typeerror call got an unexpected keyword argument inputshape,python keras lstm,this is strange running your code runs fine in my notebook i noticed that inputshape is not an argument of lstm layer as displayed on officially keras maybe it is a versioning issue my versions keras tensorflow to bypass it you can try functional api
57076709,how to fix expected object of scalar type float but got scalar type double for argument mat,pytorch lstm,torchnnlstm does not need any initialization as its initialized to zeros by default see documentation furthermore torchnnmodule already has predefined cuda method so one can move module to gpu simply hence you can safely delete inithiddenself batchsize you have this error because your input is of type torchdouble while modules by default use torchfloat as its accurate enough faster and smaller than torchdouble you can cast your input tensors by calling float in your case it could look like that finally there is no need for hidden argument if its always zeroes you can simply use as hidden is zeroes by default as well
56966085,sequence classification with lstm error when checking input,python tensorflow machinelearning keras lstm,lstm requires input of shape batchsize timestep featuresize you are passing only two dimension features since timesteps you need to add one more dimension to your input if data is a numpy array then data data npnewaxis should do it shape of data now will be batchsize timesteps feature viz
56899973,problem in gridsearching a lstm network batchsize issue,pythonx lstm gridsearchcv batchsize,well i think i got your problem when you are doing cv search a param grid is generated from your param dictionary using most probably a cross product of possible configurations your param dictionary has inputshape of timesteps inputsnumb which is a sequence of two integers actually so your input shape parameter is either timesteps or inputsnumb which then becomes none timessteps or none inputsnumb in the final line of the stack trace this is a tuple int operation so it is not valid instead you want your configuration space to have only one possible inputshape what you should do is to convert this line to this
56890457,how can i fix typeerror unsupported operand types for int and nonetype when process bagging method,keras lstm,there is an error in the modelbaselstm method replace with fix for error when checking input an extra dimension could be added like this this also takes care of scikitlearn dimensions vs keras lstm dimensions problem create a subclass of kerasregressor to handle the dimension mismatch
56704533,getting nan error while using lstm autoencoder,python keras lstm nan autoencoder,this looks like you have exploding gradients which lstm have a tendency to create clipping the gradients can solve this try setting clipnorm to
56418580,runtimeerror input must have dimensions got,python pytorch lstm,finally found solution after hours of browsing
56309793,getting valueerror outputs of truefn and falsefn must have the same type int float while using tfhistogramfixedwidthbins,tensorflow keras deeplearning lstm stat,kintrainphase requires that selfreventropyx selfbetaselfbatch and x must have the same type in this case but tfhistogramfixedwidthbins returns int when your x is float so you need to change type newfwt tfcastnewfwt tffloat
56278153,when trying to feed in variable sequences to keras lstms valueerror error when checking input,numpy keras lstm,just increase the dimensions by npexpanddimsx axis it will become three dimensional
56275128,keras lstm problem how set up correctly a neural network for time series,python keras timeseries lstm recurrentneuralnetwork,first try fixing it by adding an activation function to your output layer however getting nan when training usually means exploding gradient in deep networks or recurrent neural networks error gradients can accumulate during an update and result in very large gradients these in turn result in large updates to the network weights and in turn an unstable network at an extreme the values of weights can become so large as to overflow and result in nan values check this post in machine learning mastery it will give you a very good overview of what the problem is and few potential solutions
56240197,how to fix keras lstm input output dimensions,python keras lstm,i dont think your model is what you want you dont want a lstm layer with neurons and i guess you dont want to predict classes you model expects a dataset where each row is a list of word indices where the max index is vocabsize what do i need to change to get it to match better i think you should take a look at the keras nlp examples it pretty well explained
56194642,error when running lstm model loss nan values,tensorflow machinelearning keras neuralnetwork lstm,solution is to use masking layers available in keras with maskvalue this is because when using empty vectors they are calculated into the loss by using masking as outlined by keras the padding vectors are skipped and not included as per keras documentation if all features for a given sample timestep are equal to maskvalue then the sample timestep will be masked skipped in all downstream layers as long as they support masking
56013688,why not use mean squared error for classification problems,python keras lstm crossentropy meansquareerror,i would like to show it using an example assume a class classification problem assume true probabilities case predicted probabilities case predicted probabilities the mse in the case and case is and respectively although case is correctly predicting class for the instance the loss in case is higher than the loss in case
55811017,training a multivariate multiseries regression problem with stateful lstms in keras,python machinelearning keras lstm recurrentneuralnetwork,usually when results are the same its because your data isnt normalized i suggest you center your data with mean and std with a simple normal transform ie data meanstd try transforming it like so before training and testing differences in how data is normalized between training and testing sets can also cause problems which may be the cause of your discrepancy in train vs test loss always use the same normalization technique for all your data
55805242,typeerror view takes at most arguments given,lstm pytorch torch,it looks like your input is a numpy array not torch tensor you need to convert it first like input torchtensorinput
55782107,how to fix valueerror cannot feed value of shape x for tensor y which has shape z on keras,python keras lstm shapes valueerror,use you are fixing the batch size to via batchinputshape however when you are using predict you are not passing in the batchsize which defaults to so it tries to pass in into and it fails its not failing in fitgenerator because your generator should be returning a batch of size
55577053,time series data input for lstm model throws error,python machinelearning keras lstm,mistake the shape of x should be shape batchsize timesteps inputdim mistake the first argument of lstm is not the batch size but the output size example
55549865,tensorflowpythonframeworkerrorsimplinvalidargumenterror you must feed a value for placeholder tensor,python keras deeplearning lstm autoencoder,thanks to thushv answer i fixed the code as follow
55405272,valueerror input is incompatible with layer repeatvector expected ndim found ndim,python keras deeplearning lstm autoencoder,lstm layer expects dimensional input because it is recurrent layer the expected input is batchsize timesteps inputdim the specification inputdim expects dim input but the expected input is dim so the solution to your error is changing inputdim to inputshape
55360034,error received while building the auto encoder,deeplearning lstm recurrentneuralnetwork autoencoder,given you are using an lstm you need a time dimension so your input shape should be time imagex imagey nbimagechannels i would suggest to get a more indepth understanding of autoencoders lstm and d convolution as all these play together here this is a helpful intro and this also have a look at this example someone implemented an lstm with convd how to reshape channel dataset for input to neural network the timedistributed layer comes in useful here however just to get your error fixed you can add a reshape layer to fake the extra dimension
54700736,keras lstm multiple errors from trying to create model architecture,python tensorflow machinelearning keras lstm,i think that the problem goes around tf version version compatibility between keras and tf is a problem that probably anyone has faced as tf api changes a lot in a small period of time i think that for keras x you need a tf version x try updating it and see if the problem is fixed
54698069,keras lstm typeerror messages,python machinelearning keras neuralnetwork lstm,you should really upgrade to keras in keras x units is not even a valid argument hence your error your case still gives an error in keras albeit a different one omitting the legacy outputdim argument as the message advises we get it to work so i seriously suggest you upgrade to keras i highly doubt that keras x works ok with tensorflow and open a new question if you still have issues
54522426,i was training the lstm network using pytorch and encountered this error,deeplearning lstm pytorch,cast output vector of your network to long you have int as the error says oh and please provide minimal complete and verifiable example next time you ask a question
54312263,valueerror error when checking target expected dense to have dimensions but got array with shape,python tensorflow keras lstm,is the number of units returned by the last layer it is the number of classes for the softmax activation is the number of units returned by the lstm which indicates the size of the sequences returned none is the number of element by batch for the last layer it simply means that the last layer can accept different size for each batches of tensor of shape looking at the data shape there is clearly a mismatch between the batchsize of the features and that of the labels the most left number should be equal between the features shape x and the labels shape y it is the batchsize there is a mismatch here also to solve the issue between the output of the lstm layer and the input of the last layer one can use a layerflatten live code
54242478,valueerror error when checking input expected lstminput to have shape but got array with shape,python tensorflow keras timeseries lstm,youve split your data with respect to timesteps as opposed to the samples you need to decide on what are your samples in the first instance for the sake of the answer i will assume these are along the first axis assuming the data has been framed as a supervised timeseries problem the inputsize in lstm expects the shape of timesteps datadim as explained here and these dimensions must remain the same for each batch in your example samples from training and testing have different dimensions the batch size can differ unless specified with batchsize parameter your data should be split between training and testing along the first axis here is an analogous example from keras tutorials you will notice that timesteps is the same for training and testing data and xtrainshape xvalshape it is the number of samples that differs along the first axis xtrainshape is and xvalshape is
54226792,valueerror input is incompatible with layer lstm expected ndim found ndim multivariate timeseries data,r keras timeseries lstm,keras lstm layer expects the input to be dims as batchsize seqlength inputdims but you have assigned it wrong try this you need to reshape your data to three dims where new dims will represent the sequential data i used toy dataset to show an example here data and labels are of shape initially using the following script output now you can safely feed it to model
54139115,lstm neural network inputoutput dimensions error,python tensorflow machinelearning neuralnetwork lstm,your code is almost fine your ytest and ytrain should be an array with one element or array of shape it doesnt matter your input shape is wrong though first lstm should be notice none as your test and train sequences length are different you cannot specify it and keras accepts first dimension unspecified error was due to lengths of and respectively if you want to use batches with rnns you should perform zeropadding with keraspreprocessingsequencepadsequences no need to specify inputshape with batch rest of the network should be fine and if you are performing regression not classification as is the case probably you should perform two last steps written by ankish bansal eg changing loss to mean squared error and making last layer output value instead of
53955667,i am receiving an error with my resizing input array for a convlstm model,python opencv keras convneuralnetwork lstm,well its exactly what the error says expected convlstmdinput to have dimensions but got array with shape your input data has four dimensions it needs to have five samples time rows columns channels
53914450,problems with lstm model,python pythonx lstm pytorch recurrentneuralnetwork,my fail forgot to scale input features now works fine
53839351,shape error when training a model with keras tensorflow,python tensorflow keras timeseries lstm,as yhenon mentioned in the comments section since your model has some outputs for each timestep you must use returnsequencestrue for the last lstm layer as well however it is not clear what the task is ie classification or regression if it is a classification task you must use categoricalcrossentropy as the loss function instead of sparsecategoricalcrossentropy which you are currently using and use softmax as the activation function of last layer on the other hand if it is a regression task you need to use a regression loss such as mse or mae and set the activation function of last layer properly depending on the output values ie use linear if the range of output values include both negative and positive numbers
53815604,error when checking input expected lstminput to have shape but got array with shape,python keras lstm keraslayer attentionmodel,an lstm layer in keras expects batches of data with shape ntimesteps nfeatures you are constructing your layer with the wrong dimensions first reshape your training data to the shape ndatapoints ntimesteps nfeatures then specify your model with the correct dimensions this will work properly
53796872,attributeerror tensor object has no attribute compile,python machinelearning keras lstm,as mentioned in the comments you need to use keras functional api to create models with branches multiple inputsoutputs however there is no need to do this for all of your code just for the last part
53756339,keras shows shape error at the end of first epoch,python keras lstm autoencoder,you are working with batch size of but at the very end your operand get a tensor with only elements because this amount remains after from basically this is the error message about and that is why the previous steps worked most straightforward solution use fit methods stepsperepoch option stepsperepoch integer or none total number of steps batches of samples before declaring one epoch finished and starting the next epoch when training with input tensors such as tensorflow data tensors the default none is equal to the number of samples in your dataset divided by the batch size or if that cannot be determined in this case basically you drop the last samples
53652359,problems with bidirectional lstm,python tensorflow machinelearning keras lstm,since you are stacking multiple lstm layers on top of each other you need to use returnsequencestrue on the first two layers otherwise their output would have a shape of batchsize nunits and therefore would not be a sequence and cannot be processed by the following lstm layer
53649875,having incompatible issue when build lstm vae model,python keras lstm autoencoder,input shape for the encoder is here is default batchsize and output of decoder is and mse function is complaining about both of the shapes problem should be resolved by replacing x lstm activationrelux by x lstm activationrelureturnsequencestruex ps also you can try running encodersummary and decodersummary to get the shape of each layer edit klloss ksumklloss axis to klloss ksumklloss
53634530,why is import cntk as c not working in google colab,pythonx jupyternotebook lstm googlecolaboratory cntk,in general when asking for help with colab its best to share a minimal notebook that reproduces the issue so its clear to people trying to help what exactly youve done eg i installed opencv version is not as informative as seeing the precise commands youve used and any output they generated and so on at least the first warning is probably not workaroundable because colabs vms run ubuntu but cntk officially doesnt support anything other than note that only ubuntu is officially supported from im not familiar with cntk or its deps but this seems to import successfully albeit with a warning and successfully show the modules interface in a pycpu colab notebook sadly the first line takes almost m to run so be patient
53461561,trying to concatenate two layers in keras with the same shape giving error in shapes matching,python machinelearning keras neuralnetwork lstm,for the second input you specified before the bug that this means final shape is now when the first input passes through lstm since you didnt specify returnsequencestrue it will return a tensor of shape batchsize units viz none which you are concatenating with the above mentioned none your error went away because you changed the input shape for the second input as so the final shape becomes none which matches with output of lstm and hence it concatenated smoothly
53460392,keras functional api gives error expected ndim found ndim,python machinelearning keras neuralnetwork lstm,input shape for embedding should be a d tensor with shape batchsize sequencelength in your code snippet maininput is provided which is a d tensor to fix it change the following lines to it should solve the issue with different dimensions
53334559,error in merging one lstm model output and vgg model output,tensorflow keras lstm keraslayer vggnet,you need to use keras functional api to be able construct a nonsequential model so your code would look like this mult multiplyvggmodeloutputlstmmodeloutput x dropoutmult x dense activationtanhx x dropoutx out dense activationsoftmaxx fcmodel modelvggmodelinput lstmmodelinput out
53309967,valueerror error when checking target expected dense to have dimensions but got array with shape,python machinelearning keras lstm recurrentneuralnetwork,since you want to predict value of a story days in the future you need to set the returnsequences argument of second lstm layer to true to make the output shape of the whole model none further a more general solution to predict the values d days in the future is to use a repeatvector layer right after the first lstm layer this time you need to set returnsequences argument of the first lstm layer to false its as if the first lstm layer encodes the input data and the second lstm layer predicts the future values based on that encoded representation also it is needless to say that the shape of label arrays ie ytrain must also be nsamples d nfeats
53256136,keras lambda layer error did not return a tensor,python tensorflow keras lstm keraslayer,use lambda function to wrap kstack as follow will solve the problem
53158087,how do i get wordvec to load a string problemdict object has no attribute loadspecials,pythonx lstm wordvec,the error is occurring on the line so all the otherlater code youve supplied is irrelevant and superfluous the suffix of your filename lstmdatamodelpkl suggests it may have been created via pythons pickle facility the gensim wordvecload method only expects to load models that were saved by the modules own save routine not any pickled object the gensim native save does make use of pickle for some of its saving but not all and thus wouldnt expect a fullypickled object in the file provided this might be cause of your problem you could try instead a load based entirely on python pickle alternatively if you can reconstruct the model in the file but be sure to save it via the native gensim modelsavefilename that might also resolve the problem
53032586,attributeerror tuple object has no attribute dim when feeding input to pytorch lstm network,python tuples lstm pytorch torch,the pytorch lstm returns a tuple so you get this error as your linear layer selfhiddentag can not handle this tuple so change to this will fix your error by splitting up the tuple so that out is just your output tensor out then stores the hidden states while states is another tuple that contains the last hidden and cell state you can also take a look here you will get another error for the last line as max returns a tuple as well but this should be easy to fix and is yet different error
53018376,mfcc in keras lstm error expected ndim found ndim,python keras lstm mfcc,i worked around your code and here is the working code there were many errors i had to face and warning were also there as you seemed to use old version of keras below code is as per the updated version of keras the error you were facing because you hadnt reshaped the data before inputing in lstm the second error was faced because in updated version of keras there is no term like showaccuracy we just need to define metrics accuracy while compiling model in modelcompile the third error was the way we define input shape in input layer rest you can read and let me know if there is any confusion
52971674,mxnet timeseries example dropout error when running locally,r deeplearning timeseries lstm mxnet,looks like youre running an old version of r package i think following instructions on this page to build a recent rpackage should resolve this issue
52940928,lstm keras value error how to resolve input dimension,python neuralnetwork keras lstm dimensions,lstm inputs have to be in dimensionalsamplestimestepsfeatures and your data seems to be in d you can use the numpys reshape function to convert your data to d for example if you are using timestep then you will have to reshape it as arrayreshape or if you are using timestep then arrayreshape for more information on reshaping input for lstm you can check this site update there are so many problems with your code when you convert your data to you are basically saying that you have just one sampleonly batchsize is but you have only sampleyou have only one sample but you are doing validation splitand since you have only one x value it will have only one y positiveornegativelabelso i have asisgneed just one value ie i have ran your program with the changes few changes with code and data that u have shown in the question i changed na to
52870866,error input is incompatible with layer convd expected ndim found ndim when adding convd layer,python machinelearning keras convneuralnetwork lstm,the problem is that currently the output shape of lstm layer is none however as the error suggests convd layer like lstm layer expects a d input of shape none nsteps nfeatures so one way to resolve this is to pass returnsequencestrue to lstm layer to have the output of each timestep and therefore its output would be d alternatively you can put the convd and maxpoolingd layers before the lstm layer which may be even better than the current architecture since one usage of convd plus pooling layers is to reduce the dimension of lstm layers input and hence reduce the computational complexity
52867472,keras fit model typeerror unhashable type numpyndarray,python numpy machinelearning keras lstm,the problem is that you are directly passing the input and output arrays and not the input and output tensors to model class when constructing your model instead you need to pass the corresponding input and output tensors like this
52698468,error when feeding numpy sequence to bidirectional lstm in keras,python numpy keras lstm,it seems like this error specifically is cause by the following line i was confused about the input for lstm instead to explicitly giving the shape of the input we just need to specify the dimensions of the input in my case this is since the input is a d np array i still have other problems with my code but for this specific error the solution is changing that part with edit when predicting we need to get the mean of the prediction since lstm expects a d input another issue in my code was the shape of xi it needs to be reshaped before yielding it so that it matches the input expected by lstm
52473530,gensim docvec dimensional vector fed into keraslstm model not workingloss is not diminishing,neuralnetwork keras lstm recurrentneuralnetwork,as discussed in the comments the problem lies in the large batch size and maybe also in the optimizer used for the training it is hard to determine an exact reason why your algorithm did not converge with the current settings but it can be argued as such large batch sizes have a slower convergence counterintuitively training with larger batch sizes will actually in some instances slow down your training the reason behind this is purely speculative and depends on the exact nature and distribution of your data generally though having a smaller batch size means having more frequent updates if your calculated gradients all point in a similar direction having these more frequent updates will lead to a faster convergence good practice is to have a batch size that is never larger than in most scenarios is a good rule of thumb and a nice tradeoff between the speed advantage of larger batches and the nice convergence properties of smaller batch sizes note that this only makes sense in cases where you have a lot of training data also note that theoretically the gradients of multiple examples in that large setting can average out meaning the large batch size will have only a very small and indistinct gradient having fewer samples in a minibatch will reduce this chance although it increases the risk of going the wrong direction ie having a gradient that points in the opposite directin sgd is a good starting point but there exist several optimizations one of these smarter variants is the suggested adam method there is a highly cited paper about it which can give you a vague idea of what happens under the hood essentially sgd is a very naive solution that does not have any special assumptions or builtin optimizations from what i know adam for example uses firstorder derivatives there exist many different ones and there is a ton of theoretical articles and practical comparisons of the different implementations it is worth a lot to at least understand partially what the parameters do and know what values make sense to set them to for example you already set the learning rate to a sensible value personally i usually end up with values between and maybe use learning rate decay over time if i have larger learning rates
52411530,getting a valueerror numpy cannot reshape array of size to size while training lstm,python neuralnetwork keras lstm recurrentneuralnetwork,to reshape with the dimensions should be consistent x x should be the number of elements in datax if you want that shape it is not clear what you are trying to accomplish but my guess is that should be
52254466,invalidargumenterror incompatible shapes with keras lstm net,python keras lstm reshape shapes,the problem is that your model input batchinputshape is fixed the length of your test length is and cannot be divisible by it should be changed as follows you should modify test shape before the model evaluate test of course you have to reshape predicted and ytest before inversetransform
51987877,error in lstm embedding layers shape,python machinelearning lstm,your code seems fine change your ytrainlstm to categorical with or change your loss to sparsecategoricalentropy edited based on your github repository the evaluation not going to work because you did not preprocess the xtestlstm try
51941092,tensorflowkeras lstm model is throwing value errors,pythonx tensorflow keras lstm,there are currently some things wrong with your code at first your data generator should not only return one value but all values of the dataset usually you do this with a loop and use yield inside the loop to return single data secondly it might help to not pass python lists but nparrays to keras please try out what happens if you cast the labels before training the model furthermore make sure that your data has the right shape ie the target labels should have the shape batch size and the input data something like batch size sequencesize vocabsize please print the shapes of the trainingvalidation data input and target data and make sure that this is correct lastly if you are doing some classification task the last activation function of your network should be a softmax therefore please replace with
51930096,tesseract command not working and giving file error,c ubuntu ocr tesseract lstm,fixed by following code create training data first then lstm model
51928456,error while changing the sequence length,python keras lstm recurrentneuralnetwork,there are a lot of places where the following line of code is used change that to i think you are assuming that the sequencelength is and hard coded it in the code
51787125,tensorflow value error when chaining content of data cannot feed value of shape for tensor placeholder,tensorflow lstm,what you have here is a classification network it takes inputs or features temperature weight and size and classify them into one of your classes or property field when you modified the original dataset you modified the number of classes from you went to for the code to work you just need to modify the parameters section of your code so it fits your dataset note in this case i find the the term outputs is a bit vague i would have used something like nbclasses
51763983,error when checking target expected dense to have dimensions but got array with shape,python keras output lstm,your second lstm layer also returns sequences and dense layers by default apply the kernel to every timestep also producing a sequence so your output is shape bs to solve the problem you need to set returnsequencesfalse in your second lstm layer which will compress sequence and youll get the desired output note bs is the batch size
51750599,valueerror could not broadcast input array from shape into shape,python timeseries lstm,so your testpredictshape must be the error you are facing because of testpredictplottestpredict this input here shape of testpredictplot becomes and input an array of shorter or different shape than this will throw error i tried to reproduce the error by running the code provided in the link by you but there was no error as lentrainpredict lendata and lentestpredict which goes with the calculation very well of the code by calculation i mean here in this line of code which will become so you need to do the changes in the code such that lengths of input matches you can also try by this change in code solution found the problem you need to do changes here
51684041,stateful lstm input shape error,python tensorflow keras deeplearning lstm,since you have returnsequencestrue in modeladdkeraslayerslstmbatchinputshape activationtanhreturnsequencestruestatefultruerecurrentdropout it returns a dimensional tensor of shape batchsize timestep hiddendim now if you want to use a dense layer for such timeseries data you need to use the timedistributed in keras the following way modeladdtimedistributeddenseactivationsoftmax
51664614,valueerror trying to share variable rnnmultirnncellcelllstmcellkernel,pythonx tensorflow lstm recurrentneuralnetwork,the problem is that you are making a list of the same object repeated twice with cell doesnt merely specify parameters for a multilayer cell the objects are used directly however for your neural network you first cell will map inputs of size to and then your second cell will map to the shapes you see are the sizes of the kernels for each cell at least what they should be lstm kernels can be viewed as size n m x m where n is the input size and m is the state size the factor of comes from the fact that there are gates which require matrix weights the n m comes from stacking the input gate transition on top of the state gate transition for example in your first cell n and m so you see the size which obviously wont work for your second cell which requires a kernel of size because not to fix this simply make two different cell objects
51543875,error tensorflowpythonframeworkerrorsimplinvalidargumenterror indices is not in,python tensorflow keras lstm,i made following changes in the code and it worked i used pythoncatcodes for this the astype converts the data columns to category type hence solving the problem
51537837,keras valueerror input is incompatible with layer convlstmd expected ndim found ndim,python keras deeplearning convneuralnetwork lstm,there are several ways to specify the input shape from the documentation pass an inputshape argument to the first layer this is a shape tuple a tuple of integers or none entries where none indicates that any positive integer may be expected in inputshape the batch dimension is not included therefore the right input shape is after fixing this error you will encounter the next error it is not related to inputshape valueerror input is incompatible with layer convlstmd expected ndim found ndim this error occurs when you try to add the second convlstmd layer this happens because the output of the first convlstmd layer is a d tensor with shape samples outputrow outputcol filters you might want to set returnsequencestrue in which case the output is a d tensor with shape samples time outputrow outputcol filters after fixing this error you will encounter a new error happening in the following lines it does not make sense to have a flatten layer right before an lstm layer this will never work as the lstm requires a d input tensor with shape samples time inputdim to sum up i highly recommend you to take a close look at the keras documentation in particular for the lstm and convlstmd layers it is also important to understand how these layers work to make a good use of them
51474015,keras lstm neural net typeerror lstm missing required positional argument y,python tensorflow neuralnetwork keras lstm,the function lstmxy in which you create your model is shadowing the keras lstm layer so when you call youre indeed calling the function that you defined you need to rename this function to something else
51453506,convlstmpy example uses binarycrossentropy loss for regression why not using meansquarederror instead,python keras convneuralnetwork lstm,well if the outputs are in the range between and its totally ok to use binarycrossentropy its as if you had a classification problem in just one class true or false the function is still continuous though and in the end the point of zero error will be the same depending on the types of activation functions used especially with sigmoid binarycrossentropy would bring you the results way faster than mse due to mathematical details an lstm layer learns from analysing frames or steps of any kind of data recurrently it has what is called an inner state every step it analyses brings an update to this inner state so it works both like a memory of what happened until this point and also as some kind of positioner like where in the movie am i now thus having predicted the previous steps is absolutely necessary for the lstm to give good predictions imagine you never watched star wars before and you start playing it at the scene where dart says luke im your father youd just say what now watch all the movies from the beginning and reach that part will your understanding be different the lstm will agree with you
51440054,shape error when passed custom lstm,machinelearning keras deeplearning artificialintelligence lstm,i presume the error is happening because the shape returned by computeoutputshape when returnsequencestrue is incorrect i would try the following
51223936,tensorflow invalidargumenterror indices while training with keras,python tensorflow keras lstm onehotencoding,embedding layer expects the first argument to be size of vocabulary ie it is the maximum integer index but you are passing the number of features as vocabulary size what you can do is find the maximum number in flowsnd and pass the maximum number plus one to the embedding layer
51056161,keras dense layer output shape error,python keras deeplearning lstm convneuralnetwork,perhaps it is important to note the use of keras framework to provide an one hot encoding function you can see this code code in short i personally think that the use of different functions lead to a different set of categories
51014044,keras lstm multidimensional output error expected timedistributed to have dimensions but got array with shape,python tensorflow keras lstm,you have a mismatch in what the model predicts currently d and what the target is d you have options apply flatten and remove timedistributed which means the model will predict based on the entire sequence remove returnsequencestrue from last lstm to let the lstm compress the sequence and again remove timedistributed this way the model will predict based on the last lstm output not the sequences i would prefer the second option given the size of the sequence and the number of hidden units you have option one will create a very large kernel for the dense layer if you just flatten the sequence ie too many parameters
50848107,error in fitting an rnn lstm model,pythonx keras lstm recurrentneuralnetwork,the last layer of your model has unitsdense assuming you are doing binary classification the last layer should have units with sigmoid acivation
50567049,lstm value error connected to the initializer,python keras lstm,the solution in this case was to restart the kernel thanks to daniel mller
50428040,tflearn valueerror shape must have rank at least,python tensorflow lstm recurrentneuralnetwork tflearn,seems to be a known issue happens with tensorflow versions and above
50340289,error while trying to reuse weights for rnn,python tensorflow deeplearning lstm recurrentneuralnetwork,reusetrue means that the variables have been created previously with reusefalse so each tfgetvariable in your case abstracted behind the lstm interface expects the variable to already exist to have a mode in which variables created if they do not exist yet and reused otherwise you need to set reusetfautoreuse as the error message suggests so replace all occurrences of reusetrue with reusetfautoreuse heres the documentation
50308811,lstm cells input shape error,python keras lstm valueerror,the model code setup seems fine you need to convert your data into a timeseries for the lstm as your first layer inputshape would mean timesteps and features for every timestep you can use timeseriesgenerator documentation to window your data in that manner something along the lines of
50088092,tensorflow lstm valueerror shape must have rank at least,python pythonx tensorflow lstm recurrentneuralnetwork,why are you doing unstack on the input data the input to the rnn should be a tensor of shape batchsize maxtime ninput for timemajor falsedefault and maxtime batchsize ninput for timemajor true just pass the input without the unstack operation should do the trick
49721810,lstm keras value input dimension error,python keras lstm keraslayer,lstms require a dimensional input your input shape should be in the form of samples timesteps features since keras infers the first dimension is samples you should be inputting timesteps features as your input shape since your csv is of dimension i think that the best course of action is to reshape your input to so your lstm can get a d input so inside loaddata and inside createmodel
49680501,lstm dropout wrapper rank error,tensorflow lstm dropout,easier fix than i thought appears that i need to have my default input as a tensor rather than a scalar so is the first step and in my declarations for my testtrain runs i need to use the same approach denoting the keep rate with instead of just sessruntrainingop feeddictx xbatch y ybatch outputkeepprob
49664069,siamese network in keras symmetry issue,python keras lstm,in your example you are creating you are creating to different networks network so they will be independent as yuyang said check the original example first they create the layers only once network definition basenetwork createbasenetworkinputshape then they apply the network to two different inputs you should try using the functional api instead of the sequential to correct your code
49524396,keras convlstmd valueerror on output layer,python tensorflow keras convneuralnetwork lstm,if you want a prediction per frame then you should definitely set returnsequencestrue in your last convlstmd layer for the valueerror on target shape replace the globalaveragepoolingd layer with averagepoolingd plus reshape to make the output shape compatible with your target array
49412725,error when fitting lstm on time series data,r keras timeseries lstm,i actually solved the problem the issue occurred because i missed the validationsteps argument in fit it has to be set to the number of samples batch size used for validation
49166819,keyerror the tensor variable refer to the tensor which does not exists,python tensorflow lstm,for graphgettensorbynameprediction to work you should have named it when you created it this is how you can name it if you have already trained the model and cant rename the tensor you can still get that tensor by its default name as in if reshape is not the actual name of the tensor youll have to look at the names in the graph and figure it out you can inspect that with
49148571,typeerror forward takes positional arguments but were given,python typeerror lstm,i just had the same non descriptive issue and did not call the forward function outside the class as answer before stated after a while i found it was the second value i added to multiple declarations of x variable i had in forward function because i wanted to create a cuda tensor i know this is an old question but it just might help someone who stumbles upon it check your x variables under forward function as they should not take more than one argument
48966371,neural network error rate does not progress,python neuralnetwork lstm recurrentneuralnetwork pybrain,i seem to have fixed the learning problem my network error is now steadily decreasing as i train it i was using the wrong trainer as i am using a recurrent neural network i should not have been using the back propagation trainer i am now using the rpropminustrainer to fix the following line was changed to
48921574,tensorflow error with convlstmcell dimensions of inputs should match,python tensorflow lstm convolution recurrentneuralnetwork,try this note that the first dimension in x is the batchsize equals in the example and the second one is sequencenum equals
48865554,using dynamicrnn with multirnn gives error,python tensorflow lstm recurrentneuralnetwork,i think the problem is roughly addressed here basically to elaborate the answer from this issue using cellnumlayers will create a list of two references to the same cell instance but not two separate instances the correct behavior is that the lstm cell at first layer should have weight shape like dimensionalityoffeature cellstatesize cellstatesize and the lstm cells at consecutive layers should have weight shape like cellstatesize cellstatesize because they no longer take original input but the input from the previous layer but the problem in the shared reference code is that for both layers they will use the same kernel due to they are actually the same cell instance therefore even if you have more than one layers all layers except the first one will have the wrong shape of weight always referring to the weight of the first layer a better code as indicated in this issue is using list comprehension or for loop to create multiple separate instances to avoid sharing references
48855343,lstm keras target size error,python keras lstm,there are a few problemsmisunderstands here you can see that your y is actually dimensional however the last lstm layer you have return sequences as false meaning that the lstm is returning a single long vector and sending that into the dense layer furthermore the use of multiple lstms here seems to lack purpose though it does not necessarily harm anything in order to fit your presumed data you would want the last lstm to have returnsequences as true and have the number of neurons in that lstm not but rather as in the final dimension of your y data you could also not have it at all since you already have two lstms before that and instead make the second lstm only have neurons and have the final lstm layer be removed entirely you would then use a time distributed wrapper on the last dense layer which says to apply the same dense layer to every timestep of the data which is required by the shape of your y data
48792485,value error from tfnndynamicrnn dimensions must be equal,python tensorflow machinelearning lstm recurrentneuralnetwork,change your code from cell tfnnrnncellgrucellselfhiddensizekernelinitializerinitializer rnncells tfnnrnncellmultirnncellcell selfnumlayer to layers tfnnrnncellgrucellselfhiddensizekernelinitializerinitializer for in selfnumlayer rnncells tfnnrnncellmultirnncelllayers and the deeper gru layers will be able to adapt to the output from the earlier layers
48699523,nerual network building running into error,python tensorflow neuralnetwork lstm recurrentneuralnetwork,you should wrap your lstm definition inside a variable scope and then reuse it for validation and testing try the following change your training testing code as below
48407346,typeerror when trying to create a blstm network in keras,pythonx keras lstm recurrentneuralnetwork keraslayer,your problem lies in these three lines as a default lstm is returning only the last element of computations so your data is losing its sequential nature thats why the proceeding layer raises an error change this line to in order to make the input to the second lstm to have sequential nature also aside of this id rather not use inputshape in middle model layer as its automatically inferred
48186569,invalidargumenterror in restore assign requires shapes of both tensors to match,python tensorflow lstm tensorflowserving,looks like youve changed the order of variables in your graph definition at some point is the shape of variable and is the shape of variable in the saved checkpoint the naming indicates that you didnt give explicit names to the variables as a result they got the names variable variable variable the suffix is determined according to the order in which tensorflow sees them so if you swap two variables in code they will get different names after that you can no longer import earlier saved checkpoints because tensorflow sees a different tensor under the same name the best practice is to specify the name of each variable explicitly via name attribute this way the code will be more robust to small perturbations in the model
48121763,python rnn lstm error,python scikitlearn deeplearning keras lstm,theres an error in this line it should be
48084798,rnn python numpy memoryerror,python numpy tensorflow lstm recurrentneuralnetwork,note this question was solved in comments through careful debugging memory errors in my experience come from one of three places really freaking huge datasets shobhlzim dark eater of souls and left socks memory leaks all of these have a solution if you dont mind getting your hands dirty memory leaks these are caused by a section of code that doesnt close properly one example is fork bombs the canonical example to me was in tibasic doing would open up a frame on the stack at the if statement then go straight to the lbl without closing it again via the end statement yes i know thats not quite accurate but its close enough anyways opening files and not closing them can also crash your stack lots of stuff can to fix find it kill it nothing else you can do also maybe some flow rewriting shobhlzim eater of souls youll need a buddhist monk a catholic priest four goats a pentagram a pentacle six candles made of earwax and six pages of the necronomicon doesnt matter which six good now read them and do whatever the mad visions say to do huge datasets these are actually pretty easy to test for first does your dataset look huge is there a number on there thats bigger than yeah youre probably best off here to begin with second if you try a similar but much smaller example dataset does the error go away then youve got too big of a dataset now how do you fix this grab your trusty earwax candle and eww why do you still have that throw it away ok so youll need to take the dataset and break it into a lot of little pieces in the question it was an ai that was being trained so each relatively small piece of training data could be its own file this kind of jigsawificstion can get pretty screwy though so youll want to see if you can rewrite your code to use less memory first in the end memory errors are caused by unsurprisingly not having enough memory optimizations that increase memory at the expense of speed are valuable in this endeavor
47909606,tensorflow valueerror only call with named arguments,python tensorflow neuralnetwork lstm crossentropy,the cause is that the first argument of tfnnsparsesoftmaxcrossentropywithlogits is sentinel sentinel used to prevent positional parameters internal do not use this api encourages you to name your arguments like this so that you dont accidentally pass logits to labels or vice versa
47877858,tensorflow lstm throws valueerror shape must have rank at least,python tensorflow neuralnetwork lstm recurrentneuralnetwork,yes the problem is with inputsseries according to the error its a tensor with shape ie just a number from tfnnstaticrnn documentation inputs a length t list of inputs each a tensor of shape batchsize inputsize or a nested tuple of such elements in most cases you want inputs to be seqlength none inputsize where seqlength is the sequence length or the number of lstm cells none stands for the batch size any inputsize is the number of features per cell so make sure your placeholders and thus inputsseries which is transformed from them has the appropriate shape example x tfplaceholderdtypetffloat shapenone nsteps ninputs xseqs tfunstacktftransposex perm basiccell tfnnrnncellbasiclstmcellnumunitsnneurons outputseqs states tfnnstaticrnnbasiccell xseqs dtypetffloat update this is the wrong way to split the tensor wrong inputsseries tfsplit truncatedbackproplength batchxplaceholder you should do it like this note the order of arguments inputsseries tfsplitbatchxplaceholder truncatedbackproplength axis
47617920,lstm error with date format,python deeplearning lstm forex,this is the code after fixing the error
47545540,attributeerror nonetype object has no attribute update,python machinelearning tensorflow keras lstm,per the docs buildfn should be returning a model it should not be a model buildfn should construct compile and return a keras model which will then be used to fitpredict one of the following three values could be passed to buildfn model instances do not return themselves or new models when you invoke call i believe your intention is to do this
47347098,torchnnlstm runtime error,lstm pytorch,the error message runtimeerror saveforbackward can only save input or output tensors but argument doesnt satisfy this condition generally indicates that you are passing a tensor or something else that cannot store history as an input into a module in your case your problem is that you return tensors in inithidden instead of variable instances therefore when the lstm runs it cannot compute the gradient for the hidden layer as its initial input is not a part of the backprop graph solution it is also likely that a mean of and a variance of is not helpful as initial values for the lstm hidden state ideally you would make the initial state trainable as well eg in this case the network can learn what initial state suits best note that in this case there is no need to wrap hparam in a variable as a parameter is essentially a variable with requiregradtrue
47336169,tensorflow rnnmodel with fixed step output error,machinelearning tensorflow deeplearning lstm recurrentneuralnetwork,during inference you are resetting the state and so youre getting two different values on the same input because the state of the network is different in both cases to keep the state after a prediction you would need to do something like this also to get exactly the training result with the first input of inference you would need to first present all the training examples to ensure that you start inference with the correct state another approach would be to save the state of the network which you then could reuse during prediction
47205248,valueerror when trying to set up a network with basiclstmcell and dynamicrnn in tensorflow,python tensorflow deeplearning lstm,initialize your multiple cells in a loop rather than using the cell n notation otherwise it is basically trying to use the same cell multiple times for which the dimensions do not work out this behavior was changed in i believe the release you used to be able to get away with your original syntax now you have to use this
47062577,error when using batchinputshape for stateful lstm in rnn,keras lstm recurrentneuralnetwork,you need to do following steps reshape c by adjust lstm layer
46811085,dimension error building lstm with keras,keras lstm,add returnsequences to your lstm code
46782596,error when checking target expected timedistributed to have dimensions but got array with shape,tensorflow neuralnetwork keras lstm keraslayer,okay guys think i found a fix according to it says that we are applying dense layer to each timestep in my case i have timesteps so the output of my final layer would be batchsize timesteps dimensions for below will be hence the dimensions mismatch however if i want to convert this to single regression output we will have to flatten the final timedistributed layer so i added the following lines to fix it so now the timedistributed layer flattens to inputslooks like a bias input is included to the final dense layer into a single output okay the code works fine and i am getting proper resultsthe model learns my only doubt is if it is mathematically okay to flatten timedistributed layer to simple dense layer to get my result like stated above
46662683,many to one lstm softmax dimension error on keras,machinelearning neuralnetwork deeplearning keras lstm,actually the softmaxlayer returns none because the size of last layer is probably you want to fix it so in order to fix it you need the size of output layer softmaxlayer should be equal to size of your label array datayshape in other words it must be equal to a number of classes quick fix is
46622363,cntk run time error,python machinelearning deeplearning lstm cntk,just an idea could it be that your for next minibatch loop is never executed i would try to debug it using pdb just import pdbat the top of your jupyter cell and add a pdbsettrace before the for x y loop run the cell you can use step s to go into the methods or use next n to go forward that could maybe help you to analyse the trace and you can use prints in pdb to proof the variables
46549502,indexerror list index out of range when save model in tensorflow,python machinelearning tensorflow lstm,do you have to use context manager with statement on line it seems that the context manager is having a hard time destroying your object this may be a problem in the exit builtin suggest that you submit a bug report to the developers
46117809,valueerror error when checking input expected lstminput to have shape none but got array with shape,machinelearning computervision deeplearning keras lstm,the error message tells you everything you need x should be shaped as number of samples it seems you have one sample only by the shape of x but if you have frames you should definitely change your model for something accepting batch size here batch size seems to be
46015362,valueerror dimensions must be equal but are and,python tensorflow deeplearning lstm recurrentneuralnetwork,the error says that inside the lstm of the decoder decodingdecoderwhilebasicdecoderstepdecodermultirnncellcellcellbasiclstmcellmul there is a dimension mismatch during a multiplication mul my guess is that for your implementation you need twice as many cells for the decoder lstm as for the encoder lstm due to the fact that you are using a bidirectional encoder if you have a bidirectional encoder with a lstm with cells then the result will have units as you concatenate the outputs of the forward and backward lstm currently the decoder seems to expect an input of cells
46011973,attributeerror lstmstatetuple object has no attribute getshape while building a seqseq model using tensorflow,python tensorflow deeplearning lstm recurrentneuralnetwork,i was able to find the answer after some research i found the encoderstates are not in proper format they need to be concatenated for each layer before using them in decoding layer steps concat encoderfwstatec and encoderbwstatec to create total encoderstatec concat encoderfwstateh and encoderbwstateh to create total encoderstateh create another lstmstatetuple using encoderstatec and encoderstateh example below is the full encoder that worked for me
45674690,how can i correct the dimension error i keep getting when i train rnn using keras library,python deeplearning keras lstm recurrentneuralnetwork,the issue is youre building a list of d numpy arrays for xdata when keras expects a single threedimensional array for lstm do this instead
45658906,tensorflow value error with variablescope in lstm,tensorflow machinelearning deeplearning lstm,add a reuse parameter to the basiclstmcell since you are calling the discriminator function twice and calling reusenone both the times it throws the errors when try to create variables with same name in this context you need to reuse the variables from the graph for the second call as you dont need to create new set of variables
45632684,python tensorflow lstm valueerror error when checking model target expected dense to have shape none but got array with shape,python tensorflow deeplearning lstm keras,you have defined dense layer input shape is so you need to make sure your input should always same shape here in your case make xtrain and and ytrain same shape try with
45555376,typeerror expected int got list containing tensors of type message instead in keras,pythonx tensorflow keras lstm,your tensorflow is too old you should at least try tensorflow i believe keras requires at least tensorflow
45354681,issue in lstm input dimensions in keras,machinelearning neuralnetwork keras lstm keraslayer,use data npexpanddimsdata axis before you define the model lstm expects inputs with dimensions batchsize timesteps features so in your case i guessing you have feature time steps and samples you need to add a dimension at the end of your vector this need to be done before you define the model otherwise when you set input inputshapedatashape you are telling keras that your input has timesteps and featuresso it will expect inputs of shape none the non stands for any dimension will be accepted the same holds for input hope this helps
45266707,keras typeerror expected int got list containing tensors of type message instead,python tensorflow deeplearning keras lstm,this error is produced because your tf is too old was released a long time ago you should update it to the latest version specially as you are using keras x
45252051,memory error when making timesteps for lstms python,numpy memorymanagement keras python lstm,instead of preparing all your input to one variable why wont you try to generate batch of reduced size for each call now on your train scripts you can call this generator to feed data to your model in keras this would give something like
45135702,issues saving and restoring tensorflow model lstm,tensorflow restore lstm recurrentneuralnetwork,first note that if you initialize all variables after restoring from a checkpoint you will get their random initial values instead of the trained values second its much easier to get saving restoring right if you use tfestimatorestimator instead of implementing this yourself third i dont understand how youre passing modelmeta to restore but seeing an error about modelmeta i believe though you should pass only model without the meta suffix
45130184,ctcloss error no valid path found,tensorflow deeplearning lstm,it turns out that the ctcloss requires that the label lengths be shorter than the input lengths if the label lengths are too long the loss calculator cannot unroll completely and therefore cannot compute the loss for example the label bifi would require input length of at least while the label biif would require input length of at least due to a blank being inserted between the repeated symbols
45103692,stateful lstm fails to predict due to batchsize issue,tensorflow keras lstm recurrentneuralnetwork,i suspect that the reason for the error is that you did not specify the batch size in modelpredict as you can see in the documentation in the predict section the default parameters are which is why appears in your error message so you need to specify batchsize in modelpredict
44704435,error when checking model input expected lstminput to have dimensions but got array with shape,python keras lstm recurrentneuralnetwork valueerror,setting timesteps since i want one timestep for each instance and reshaping the xtrain and xtest as this worked
44615147,valueerror trying to share variable rnnmultirnncellcellbasiclstmcellkernel,tensorflow deeplearning lstm,i encountered a similar problem when i upgraded to v tensorflowgpu instead of using rnncell i created rnncells stackedrnn by a loop so that they dont share variables and fed multirnncell with stackedrnn and the problem goes away im not sure it is the right way to do it
44583254,valueerror input is incompatible with layer lstm expected ndim found ndim,python keras lstm recurrentneuralnetwork,i solved the problem by making input size and output size and changed the input shape to in the code where model is defined
43957967,tensorflow v multirnn basiclstmcell error reuse parameter python,python tensorflow lstm,have you considered trying what the reuse to trueerror is suggesting if before you were using multirnncellbasiclstmcell numlayers change to multirnncellbasiclstmcell for in rangenumlayers following code snipped works for me already answered here
43676638,valueerror cannot feed value of shape for tensor placeholder which has shape,tensorflow lstm mnist,the problem is batchy is a tensor containing labels numbers from to specifying the correct class while your placeholder expects a vector of length nclasses for each example that specifies the expected probability for each class in this problem only one class is correct at a time so instead of correct labels as nclasseslength vectors you can have y as just labels like and then you also have to change the cost to sparse means that only one class is correct so the argument labels is a tensor of size batchsize with a number representing the correct class for each example
43248122,lstm model error is percent of one output class,tensorflow neuralnetwork lstm,since you have one hot encoding use sparsesoftmaxcrossentropywithlogits instead of tfnnsoftmaxcrossentropywithlogits refer to this stackoverflow answer to understand the difference of two functions
43089598,tensorflowpythonframeworkerrorsinvalidargumenterror,python tensorflow lstm,hey could it be that you defined the vocabsize wrong seems perhaps like a problem like that to say more it is perhaps helpful to see how you execute the model with the parameters
42999102,tensorflow lstm cell in dynamicrnn throws dimension error,tensorflow lstm,i believe you dont need the line you can supply x directly to dynamicrnn since dynamicrnn doesnt take a list of tensors it takes one tensor where the time axis is dimension if timemajor true or dimension if timemajor false actually it seems that x has dimensions only since inputs is list of dimensional tensors as indicated by the error message so you should replace the unstack line with this will add a rd dimension of size that is needed by dynamicrnn
42744903,keras lstm error when checking model input dimension,python neuralnetwork deeplearning keras lstm,this is a really classic problem with lstm in keras lstm input shape should be d with shape sequencelength nboffeatures additional third dimension comes from examples dimension so the table fed to model has shape nbofexamples sequencelength nboffeatures this is where your problem comes from remember that a d sequence should be presented as a d array with shape sequencelength this should be a input shape of your lstm and remember to reshape your input to appropriate format
42553169,cnnlstm in keras dimension error,deeplearning theano keras convneuralnetwork lstm,if we follow the track of the shapes in your model we get to shape none after the dense then you feed that to an lstm which will return the whole sequence of hidden vectors of length so you get a shape none then you take the mean over the axis so you get shape none there is something wrong with the architecture of that network because your targets have the shape none so either change and add a flatten layer after the merge or flatten after the merge and use a dense activationsigmoid to get your predictions it depends on you but right now it cannot work
42513613,tensorflow dynamicrnn regressor valueerror dimension mismatch,python tensorflow neuralnetwork lstm recurrentneuralnetwork,as discussed in the comments the tfnndynamicrnncell inputs function expects a list of threedimensional tensors as its inputs argument where the dimensions are interpreted by default as batchsize x numtimesteps x numfeatures if you pass timemajortrue they are interpreted as numtimesteps x batchsize x numfeatures therefore the preprocessing youve done in the original placeholder is unnecessary and you can pass the oriding x value directly to tfnndynamicrnn technically it can accept complicated nested structures in addition to lists but the leaf elements must be threedimensional tensors investigating this turned up a bug in the implementation of tfnndynamicrnn in principle it should be sufficient for the inputs to have at least two dimensions but the timemajorfalse path assumes that they have exactly three dimensions when it transposes the input into the timemajor form and it was the error message that this bug inadvertently causes that showed up in your program were working on getting that fixed
42499592,resourceexhaustederror oom when allocating tensor with shape,python tensorflow deeplearning lstm wordsensedisambiguation,the problem was caused by this line in the training loop calling the tfnnembeddinglookup function adds nodes to the tensorflow graph andbecause these are never garbage collecteddoing so in a loop causes a memory leak the actual cause of the memory leak is probably the embeddingmatrix numpy array in the argument to tfnnembeddinglookup tensorflow tries to be helpful and convert all numpy arrays in the arguments to a function into tfconstant nodes in the tensorflow graph however in a loop this will end up with multiple separate copies of the embeddingmatrix copied into tensorflow and then onto scarce gpu memory the simplest solution is to move the tfnnembeddinglookup call outside the training loop for example
42466644,dimension error while using lstm after convolutional neural network in keras,machinelearning neuralnetwork keras lstm recurrentneuralnetwork,try exchanging each convolutiond to you need to let your model know that your data is sequential and you want to apply some layer to each element in your sequence this is what timedistributed wrapper is for
42416000,gradients error using tensorarray tensorflow,python whileloop tensorflow lstm,you should add parameter paralleliterations to your while loop call such as this is required because inside body you perform read and write operations on the same tensor array statesta and in case of parallel loop executionparalleliterations some thread may try to read info from tensorarray that was not written to it by another one ive test your code snippet with paralleliterations on tensorflow and it works as expected
42331396,input shape error in secondlayer but not first of keras lstm,python tensorflow keras lstm recurrentneuralnetwork,thanks to patyork for answering this on github the second lstm layer is not getting a d input that it expects with a shape of batchsize timesteps features this is because the first lstm layer has by fortune of default values returnsequencesfalse meaning it only output the last feature set at time t which is of shape batchsize or dimensions that doesnt include time so to offer a code example of how to use a stacked lstm to achieve manytoone returnsequencesfalse sequence classification just make sure to use returnsequencestrue on the intermediate layers like this no errors
42161266,pybrain sequentialdataset using backprop trainer give a slice indices error,python neuralnetwork lstm pybrain,found it the difference is in pybrain this fixed it in my setup try that
42041838,seqseq in cntk run time error function only supports dynamic axis,deeplearning lstm cntk,the last operation strips the dynamic axis since it reduces the input sequence to a single value the thought vector the thought vector should then become the initial state for the second recurrence so it should not be passed as the data argument to the second recurrence in the current version the initialstate argument of recurrence cannot be data dependent this will be soon possible it is already under code review and will be merged to master soon until then there is a more complicated way to pass a datadependent initial state where you manually construct the recurrence without recurrence layer and manually add the initial hidden state in the recurrence it is illustrated in the sequencesequence sample
41495757,error performing lstm using basiclstmcell in v,python tensorflow lstm,since stateistuple is true by default you need to pass in lstmzerostatebatchsize tffloat to the state variable replace with also make sure you pass lstmstatetuple objects to the initialstate argument
40999291,fatal error while installing keras,python git anaconda keras lstm,it seems that the error may be because we have a git repository listed as a dependency not still fully sure though source the solution is to replace pip install gitgitgithubcomtheanotheanogit with pip install git ie replacing the second git with http this works since the repository is public
40956954,tensorflow attributeerror module object has no attribute deprecated,python tensorflow recurrentneuralnetwork lstm,you need to replace all with in ptbwordlmpy details are described here
39898576,wrong number of dimensions error in theano lstm,python numpy deeplearning theano lstm,the function createdataset is returning one numpy array however when you call i o traindataindex you are trying to obtain two values you can for example assigning the value to a temporal variable and then split it as you need edit the variables i and o were not of the same type expected by the function learnrnnfn it was expecting numpy matrices
39298462,keras stateful lstm error,keras lstm,modelfit does the batching as opposed to modeltrainonbatch for example consequently it has a batchsize parameter which defaults to change this to your input batch size and it should work as expected example
38442025,tensorflow grid lstm rnn typeerror,python machinelearning tensorflow neuralnetwork lstm,i was unsure on some of the implementation decisions of the code so i decided to roll my own one thing to keep in mind is that this is an implementation of just the cell it is up to you to build the actual machinery that handles the locations and interactions of the h and m vectors and isnt as simple as passing in your data and expecting it to traverse the dimensions properly so for example if you are working in two dimensions start with the top left block take the incoming x and y vectors concat them together then use your cell to compute the output which includes outgoing vectors for both x and y and it is up to you to store the output for later use in neighboring blocks pass those outputs individually to each corresponding dimension and in each of those neighboring blocks concat the incoming vectors again for each dimension and compute the output for the neighboring blocks to do this youll need two forloops one for each dimension perhaps the version in contrib will work for this but a couple problems i have with it i could be wrong here but as far as i can tell the vectors are handled using concat and slice rather than with tuples this will likely result in slower performance it looks like the input is projected at each step which doesnt sit well with me in the paper they only project into the network for incoming blocks along the edge of the grid and not throughout if you look at the code it is actually very simple perhaps reading the paper and making adjustments to the code as needed or rolling your own are your best bet and remember that the cell is only good for performing the recurrence at each step and not for managing the incoming and outgoing h and m vectors
36941382,tensorflow shared variables error with simple lstm network,python tensorflow neuralnetwork lstm,the call to lstm here will try to create variables with the same name each iteration unless you tell it otherwise you can do this using tfvariablescope the first iteration creates the variables that represent your lstm parameters and every subsequent iteration after the call to reusevariables will just look them up in the scope by name
36669068,keras lstm error,python macos clang keras lstm,it was a clang related error after apple updated the xcode tools my clang became incompatible with nvidias cuda drivers updating the drivers fixed the problem if a new update isnt available one needs to either wait for one or rollback their xcode tools
79203990,error command must be one of from license template system adapter parameter or message when creating a custom model in ollama,yaml artificialintelligence chatbot ollama,ive also had a same issue where i tried to pass a message after keyword system and it was not a single line so i have completely encoded the prompt inside triple quotes complete prompt also i have started my prompt in a same line next to keyword system you can try adding the message like below i have enclosed text in triple quotes or you can try having it in a single line as prompt was small i have used java library and faced same issue this quotes resolved my issue hope it helps
79086955,how to properly add an error message to my chatbot,python windows chatbot,the issue is with how you compare var with the set of all possible answers on the last line you should use the in operator instead in addition you can use a trycatch block to catch errors for example your code would look like this
79045085,issue with login widget in streamlit,python authentication artificialintelligence chatbot streamlit,i think this is a better practice and it works properply
78977010,nextjs edge function module not found error path and internal server error when using openai integration,nextjs path chatbot openaiapi vercelai,i removed this and it worked
78931005,trouble installing chatterbot subprocessexitedwitherror while processing pyyaml,python chatbot chatterbot,information available on pypi shows that chatterbot requires python you may have to downgrade to python to use this package see
78625880,problem exceeding maximum token in azure openai with java,java springboot chatbot azureopenai,as far as i know there is no way to make the llm azure openai in this case remember your context cheaply as you said sending context and a huge chunk of it on each call gets pricy really fast that been said you could change the approach and try other techniques to mimic that the ai has memory like summarizing the previous questions and send that as content instead of a long string with questionsanswers you send a short summary of what the user has been asking for it will keep your prompt short and kind of aware of the conversation there are also conversation buffers keeping the chat history in memory and send it to de llm each time as you did but it gets long pretty fast for that you could configure a buffer window limiting the memory of the conversation to the last questions for example that should help keep the token count manageable there are several ways to manage this but there is no perfect memory as far as i know not one the is worth paying if you could tell us a bit more on how good the bot memory needs to be or the specific use case maybe we can be more precise good luck here are some references that you could use to start generative ai making your llm contextaware using langchain crazy idea put all history in a document and use rag so retrieve the relevant parts of the conversation only
78574282,openai api key gives ratelimit error even though api key not used,python chatbot openaiapi langchain ratelimiting,as trazom pointed out the code works just fine but apparently i just needed to make a new key and link a credit card thanks trazom writing this to make this question appeared as answered
78385449,error during installing chatterbot on mac,pythonx macos chatbot chatterbot,actually what you are facing from your chatterbot not installing is dependency issues of longintreprh files of cypthon you are using python on your macos based on your traceback error call and that file is not installed in your python which chatterbot needs to work to make your installation work kindly downgrade your python from to then run this on your terminal pip install chatterbot however if you insist on keeping your python which you are currently using on your os you can install a thirdparty dependency of chatterbot which works pretty well for version and above of python the developer shonegk has taken good care of this issue our python community face in installing and using chatterbot chatterbot has not been receiving updatesissue resolution for more than years now run this command on your bash and you are good to go pip install git since chatterbot also requires a corpus of text to use as training data you can download the corpus using this pip install chatterbotcorpus now your installation should work perfectly i believe good luck
78171698,issue encountered with the history parameter in the gemini ai integration within a nodejs environment,nodejs artificialintelligence chatbot googlegemini,the current structure of the chat history in google document does not accurately reflect the expected format for multiturn conversations the model appears to be incorrectly handling the conversation turns leading to errors in the conversation flow to fix this issue i propose the following enhancement to the chat history model
77929619,type error when trying to make a llm chatbot using geminipro,python machinelearning artificialintelligence chatbot,the type error is because you are using in on a class that does not store values in only works on sets lists and other data structures that contain values if you want to see whether something has the chatsession attribute you can try hasattrstsessionstate chatsession see also how to check if an object has an attribute pythons in keyword
77882218,lex responsive card buttons not working in facebook messenger,chatbot facebookmessenger amazonlex,problem solved it turns out its not enough to subscribe from the webhook end i need to edit the webhook subscription on my page end after that everything works fine
77597452,not able to understand this rasa train command error,python chatbot rasa,if you look at the stack trace you included in your question it gives the type and reason for the exception you are facing valueerror givecategory is not in list you have a valueerror because givecategory is not in your list a valueerror occurs when an incorrect value is supplied so you can fix this by either including givecategory in your list or modifying your program accordingly
77320311,error in twilio whatsapp when retrieving audio message,python twilio chatbot openaiwhisper,alright it turns out you need to do things you need to top up your account with credit even if you have credit from the trial left you need to use your api key and api secret as credentials in the request heres what i changed after that i was able to download the object into my instance successfully
77131172,openai api key not working in my react app,javascript reactjs chatbot openaiapi,first of all as kenwhite suggested fix the fundamentals use the trycatch statement properly as follows problem note openai nodejs sdk v was released on august and is a complete rewrite of the sdk see the v to v migration guide there are a few problems with the code you posted in your question the solutions to these problems i provide below differ depending on whether you use openai nodejs sdk v or v to check your openai nodejs sdk version run the following command problem passing an invalid parameter to the api endpoint youre trying to pass headers as a parameter to the api endpoint which is not a valid parameter remove it solution you need to set the bearer token as follows if you have the openai nodejs sdk v if you have the openai nodejs sdk v problem using the wrong method name you want to use the gptturbo model ie chat completions api use the proper method name solution if you have the openai nodejs sdk v openaicreatecompletion openaicreatechatcompletion if you have the openai nodejs sdk v openaicompletionscreate openaichatcompletionscreate problem using the prompt parameter you want to use the gptturbo model ie chat completions api the chat completions api uses the messages parameter while the completions api uses the prompt parameter solution use the messages parameter instead of the prompt parameter final solution if you have the openai nodejs sdk v try this if you have the openai nodejs sdk v try this
77105521,i am developing a chat bot with whatsappwebjs but i am having a problem node js,chatbot qrcode whatsapp,whatsapp web was changed modify selector in clientjs line now i used textbox as selector const introimgselector divroletextbox dataiconchat
76661527,openai function calling error openaierrorinvalidrequesterror,artificialintelligence chatbot openaiapi chatgptapi gpt,try modifying the function to something like this ie add properties and required i was facing a similar problem and what worked for me was to start with an official open ai function that i verified to work and then i modified it line by line to verify that no change breaks it i had missed one of the required attributes i built myself also a validation function to check that i pass only valid functions to open ai in the future its not perfect but has already helped me catch a few bugs
76502113,error with fewshot prompting using gpt,chatbot openaiapi gpt chatgptapi,you made a mistake between chat completion and completion see documentation completion chat completion
76479799,attributeerror game object has no attribute triesleft,python chatbot,just change the init function to in your original code you defined a function triesleft that assigns the member but this function has never called
76207665,discord bot in python bug runtimewarning coroutine handleresponse was never awaited,python pythonx discord chatbot,i think the error is in the sendmessage function you need to await handleresponse
75776753,getting error in web demo during integration,dialogflowes chatbot velo,to solve the above issue you can try the below steps this issue commonly happens when the api is in disabled state you need to enable it correctly navigating to api services api library dialogflow api staging and make sure that its enabledif it is already enabled then disable it and enable it again if this does not solve the issue then follow step you may need to make sure the dialogflow service account exists in the iam page and has the necessary permissionsdialogflow api admin dialogflow console agent editor dialogflow service agent if you are using the dialogflow api you need to set up authentication for more details you can follow this link report the issue via issue tracker if none of the above steps solves your issue
75438299,i am trying to make a email chatbot but it spams how could i fix this,python pythonx smtp imap chatbot,this is the usual problem for an email autoresponder if i understand you correctly and rfc offers good advice since answers should be selfcontained i offer a summary add the autosubmitted autoreplied header field on your outgoing messages any value other than no will prevent wellwritten autoresponders from replying to your outgoing messages set the answered flag on the message you reply to immediately before you send the reply change the search key from recent to unanswered not header autosubmitted unanswered means that the search wont match the messages on which you set the answered flag not header autosubmitted means that youll not match messages that contain any autosubmitted header field direct your replies to the address in returnpath or sender not the one in from this is a matter of convention autosubmitted mail will often have a special returnpath that points to an address that never sends any autoreply you may also extend the search key with more details from rfc the one i suggest should work but not header precedence junk will for example prevent your code from replying to a bit of autogenerated mail sendgrid and its friends also add header fields you may want to look for and exclude if the incoming message has headers like this use the view headers function of most mail readers to see it then your reply should have headers like this therell be many more fields in both of course your replys returnpath says that nothing should respond automatically from and to are as expected autosubmitted specifies what sort of response this is subject doesnt matter very much but this ones polite and wellbehaved and finally references links to the original message
74841146,trying to make a discord bot but onmessage or messagecontent not working,python pythonx discordpy bots chatbot,you need to use clientevent for every event clientevent async def onready printonline clientevent this is new async def onmessagemessage
73674966,typeerror clientinit missing required keywordonly argument intents,python errorhandling compilererrors bots chatbot,you shoud pass the argument intent to the client constructor you can try to replace the line client discordclient with intents discordintentsdefault intentsmessagecontent true client discordclientintentsintents
73661991,aws chatbot in slack lambda invoke unmarshal error,amazonwebservices lambda commandlineinterface chatbot slack,if you are trying to invoke the lambda function from a slack message and not from the cli then you can do it in this way note that there is no responsejson parameter as the response wont be saved into a file also note how the payload is declared
72909112,dialogflow messenger custom payload quick respose issue clicked input not understoodmatched in dialogflow es,dialogflowes chatbot,i have also tested the rich response messages using list and button response types on dialogflow messenger using custom payload and im also facing the same problem if i click the buttonlist it triggers a default fallback intent response so it seems only possible with webhook calls that manage the event and retrieve text i was unable to find any direct demonstration of how to call a webhook for an event triggered on a button click and then return a text response already a feature request was raised similar to your issue on the google cloud issue tracker you can follow the updates from there for your requirement you can try using the suggestion chip response type with the following custom payload and test it result once you click the yesno suggestion chip you can see the selected chip as input and response for it in the below image let me know if it helps
72804538,i am new to pytorch why am i getting the attribute error even after adding superclassnameselfinit,machinelearning pytorch artificialintelligence chatbot pytorchdataloader,this is most probably because you havent called superinit in your init function of neuralnet before registering submodules to it see here for additional details the only missing component is a function len on chatdataset other than that the provided code runs fine
71697134,how to code a custom rich presence i tried doing one but gives error,javascript nodejs discord discordjs chatbot,since i do not know where does the client come from i cant give you a proper solution ill show you how i made mine work though client code on code you should put all the client initalizers defining on etc in your indexjs
71400862,nodered error cannot set headers after they are sent to the client multiple devices sending request,chatbot ibmwatson nodered,the last change node implies you are stashing the msgres field somewhere and putting it back im guessing you are using the same context variable to do this so its getting overwritten by the second invocation so both events try to respond on the last msgres and the second one fails because they can only be used once you shouldnt need to do this if all the nodes in the flow behave properly and dont discard msg properties if there is a node discarding properties then you should raise an issue with its author to get it fixed in the meantime you need to make sure each invocation keeps track of its own msgres which may mean storing it under a unique id possibly msgid but if a node is discarding properties this might also change to they all respond to the correct session
71380648,android error aapt error resource drawableicbot aka comexamplechatbotdrawableicbot not found,java android gradle chatbot,make sure you have an icbot in your project directory candroidstudioprojectschatbotappsrcmainresdrawable
71042048,indexerror list index out of range chatbot is not working,python neuralnetwork chatbot,it indeed looks kike predictclass returns an empty list so intentslist does not exist and throws an out of bound on line line is just the call stack aka that is where you call the function that contains the error which you do on that line
70655240,indexerror list index out of range when creating an automated response bot,python csv artificialintelligence chatbot,and then later on so if the anwser dont have id in it you will get index out of range so in your case the lenanswer id is true
69991038,objectsrequirenonnull problem with my dialogflow chatbot app,java android nullpointerexception chatbot,if the messagelist is empty by default and looks like it is you have an empty recycler view and thus there are no nonfatal errors in the logs informing you that the recycler view is missing a layout manager set the layout manager to your chatview recycler view and it should be enough
69878970,in salesforce post chat getting refused to connect error,salesforce chatbot salesforcelightning salesforceservicecloud salesforcecommunities,please check another similar question here yup weve had the same issue the solution is to go to the site page in salesforce setup develop sites and then add a trusted domains for inline frames make sure you verify your clickjack protection level too a value of allow framing of site pages on external domains good protection or lower will work
68768290,error an api error occurred missingscope how to get the userid with nodejs bolt framework for slack,nodejs chatbot slack slackapi slackcommands,whenever a user triggers your slash command slack sends a data payload to your configured request url that payload will include the userid of the user who triggered the command
68620678,inactivitytimer error botframeworkadaptersendactivities unable to send activity as streaming connection is closed microsoft bot framework dls,nodejs botframework bots chatbot speech,cognitive services speech tokens are only valid for mins per their rest api docs you can get a new token at any time however they recommend using the same token for mins to reduce network traffic and latency for clarity exchanging tokens shouldnt affect the performance of the speech service at least it hasnt for me yet
68195299,i am have an issue with post request in the chatbot ap using watson assistant,post chatbot,your postman shows that you added sessionid as url parameters that would be for a get request you need to transfer the assistantid and sessionid as payload of the post request ie as data in the request body then it should work this is not related to ibm watson assistant but how http requests work
68080613,microsoft bot framework returns error from taskfetch,nodejs botframework chatbot microsoftteams adaptivecards,posting the answer for better knowledge copying from siva subramanian comments added a handleteamstaskmodulefetch server side action inside bot section
67876592,python import error undefined symbol aeshwencrypt,python googlecloudplatform dialogflowes chatbot raspberrypi,cryptography lib was old thanks to everyone who helped
67686592,attributeerror module tensorflowpythonkerasenginetraining has no attribute disallowinsidetffunction,python conda chatbot rasanlu rasa,which rasa version are you using it would also be helpful if you could provide the full stacktrace i also had a look at the rasa poetrylock file the latest rasa version should work with tensorflow python keraspreprocessing should be if you want to use rasa with all its dependencies you could try pip install rasafull as stated in the docs please note that if you want to train a model you should run rasa train command instead of rasa init
67657972,i am trying to create a chatbot and i am at a part in which an error is showing similarityscoreslist is not defined let me show u some code,python chatbot,short answer you did not run these code just copy those code to a new cell and click the start button like the video then you run long answer what he did in the video is basically copying the code in the function outside and replace the function input with a manual input hello world maybe you run the function code but that just means you defined the function defining the function will not run the code so you have to copy the code outside and ran again
67623823,valueerror failed to convert a numpy array to a tensor unsupported object type list,python numpy tensorflow chatbot valueerror,the code was creating the bag list incorrectly basically it was empty please try the below code so your train list length will be lenwords and y of length no of classes i hope will make sense
67425837,twilio error invalid autopilot actions json,json googlecloudfunctions twilio chatbot twilioapi,when youre calling a url from twilio autopilot via redirect you need to return a json for twilio autopilot not twiml you need to change the part where youre constructing your return message twilioresponse messagingresponse msg twilioresponsemessage msgbody joinsheetrowvalues joinsheetrowvalues return strtwilioresponse return json instead here from flask import jsonify body actions say joinsheetrowvalues joinsheetrowvalues listen true return jsonifybody for a list of actions twilio autopilot supports see actions overview
66836251,try make discord bot but why clientsend error,python discord bots chatbot,looks like your channel is none according to the docs getchannelid int might return none when channel has not been found double check your discordchannelid is correct and that it is an int not str as in your example
66799322,chatterbot attributeerror module time has no attribute clock,python pythonx chatbot chatterbot,i agree with glycerines answer well if you dont want to downgrade your python version i have another solution for you open the file libsitepackagessqlalchemyutilcompatpy go to line which states if win or jython timefunc timeclock else timefunc timetime change it to if win or jython timefunc timeclock pass else timefunc timetime simply here we are avoiding the use of timerclock method and the timefunc object because i saw that this object is just created for nothing and does nothing it is just created and left untouched i dont know why even this exists when it is of no use but this should work for you hope it helped
66520417,how to deal with none letter keyboard input errors in python guizerotkinter,pythonx tkinter chatbot keyboardevents guizero,adding this at the beginning of keypressed function will eliminate the error when caps lockshift or any key that returns an empty string is pressed if ekey return the following has worked in preventing the tab key from selecting the text if ordekey tab key printtab pressed inputboxappend inputboxdisable inputboxafterinputboxenable basically i have disabled the widget followed by enabling it after millisecond update another approach could be to bind the tab key to the entry widget used internally which can be accessed by using the tk property as stated in the docs i would recommend this approach over the previous also because the append method adds the text at the end there is no built in method to insert the text at the current location so you would ultimately end up using the insert method of tkinterentry with the index as insert def selectnoneevent inputboxtkinsertinsert return break inputboxtkbindselectnone using return break at the end of the function prevents other event handlers from being executed
66397791,error code bad request while trying to send message through telegram api,python chatbot telegrambot,are you sure you are sticking to the signature for sendmessage detailed in your api docs it looks like you are giving several arguments for your text parameter try to wrap it up in a statement like then invoke the sendmessage method providing the following three nonoptional parameters chatid text parsemode so that of course dont forget to edit your chatid which has to be the unique identifier for the target chat or username of the target channel in the format channelusername
66257524,how do i solve a content type error in the microsoft bot framework,python botframework chatbot skype skypebots,as you become more experienced with debugging python code you may find that a line number in a python stack trace refers to the last line of a multiline function call rather than the first this makes sense when you consider how the interpreter reads one line at a time from top to bottom and it cant call the function until it knows what all the arguments are this means that even though the line youre seeing in the stack trace is type applicationjson thats only because its the last interpreted line of the function call so you should be paying attention to the function thats being called uploadattachment your problem has nothing to do with the content type we can see that uploadattachment is throwing a not found error you may recognize that as the message associated with a this means the underlying rest api operation that your connector client is calling does not exist for the given service url which means the operation isnt supported for the skype channel this shouldnt come as too much of a surprise since most operations are channelspecific i tried sending attachments to skype the conventional way and i got a error error code badsyntax message file attachments arent supported for this bot this doesnt leave us with a lot of options if you really want to send a file attachment to a user in skype youll have to upload it to some external server yourself and then give its url to the user if you dont mind uploading the file to a public url then that would be easiest but if you want the file to be private then youll need to upload it to some secure system that both the bot and the user have credentials for that could get complicated you may be disappointed with this answer but i dont imagine youll get a better one skype bots have been deprecated for over a year now which means you shouldnt expect to get any official help with your skype bot problems my recommendation is to switch to another chat platform like teams
65921221,jupyter notebook unable to get user input the second time eoferror eof when reading a line,python jupyternotebook chatbot eoferror jupyterwidget,input was created for terminalconsolecmdexe and maybe this is why it has problem to work correctly in jupyter i would rather use widgetstext to create inputwidget minimal working code edit version which use clearoutput to remove widgets before new question eventually you can use widgetclose to remove only some widgets but they have to be global to access them in other function edit version reduced to two functions askquestion and getanswer
65636961,cant understand why im getting the raise jsondecodeerror expecting value in python spyder ide,json python spyder chatbot,the problem was that my program was calling intentsjson as well as datapickle in a later version but of course the data pickle wasnt initialised yet in this segment of the code so i kept getting a null error
65617064,why my flutter chatbot using ibm watson assistant is not working,flutter chatbot ibmwatson,flutter has dependencies that needs to be added for ibm watson please check if you have added here is the link for more details let me share other option using thirdparty tool like kommunicate after creating watson assistant integrate in kommunicate by providing the api key url and watson assistant id after that bot will be integrated from here you can add it to your website mobile for adding in flutter kommunicate provides appid which needs to be added in launch conversation to launch the conversation you need to create a conversation object this object is passed to the build conversation function and based on the parameters of the object the conversation is createdlaunched below is an example of launch conversation to add the appid to add flutter sdk to your app add the below dependency in your pubspecyaml file kommunicateflutter install the package by running this command as flutter pub get import kommunicateflutter in your dart file to use the methods from kommunicate import packagekommunicateflutterkommunicateflutterdart for ios navigate to your appios directory from the terminal and run the below command pod install please find the attached docs links for reference check out the blog for more details a guide for installing flutter app
65533245,chatscript problem with creating popup chat window,javascript php html chatbot chatscript,if the bot corresponds correctly using the better ui then try to analyze the postvars with a vardump sent to uiphp firstly then follow the same procedure with using it inside the pop up the error tells you thats the uiphp expect an array from your indexphp but it receives nothing to extract because the data are sent through ajax maybe there is a conflict with the original js code and your pop up js code
64956110,using sklearn to create similarity component in chatbot imports not working,python pythonx scikitlearn pycharm chatbot,well the greyed out lines are not really an issue as it simply means that you have not used the packages that you have imported so far and as for the error youre getting it seems the version of numpy package you have doesnt really agree with either the python version or sklearn package version refer how do you fix runtimeerror package fails to pass a sanity check for numpy and pandas i wish you good luck with the chatbot
64530445,problems with the proactive message chatbot,python botframework chatbot,short answer dont worry about it it sounds like youre already doing things correctly by saving conversation references in order to send proactive messages the note in your means the service url is not guaranteed to remain the same forever but the service url is still effectively stable there isnt really an operating procedure for what to expect if a service url changes because its such a rare event but i expect the bot framework team and the teams team would make sure to give bot developers plenty of notice to answer your question if you really want to update your service urls without making your customers send new messages then you can send a message to your bot yourself and see what the new service url is and update all your conversation references accordingly service urls are root paths used for various api calls so they are not scoped to specific users or specific conversations you can expect the service url to be the same for you and your customers teams is a bit of an exception because the channel has multiple service urls to account for different regions so you may need to use some kind of proxy to simulate users in other regions
64525290,problem uncaught in promise typeerror rivescript chatbot in angular,javascript angular typescript chatbot rivescript,instead of a function that defines its own this context use an arrow function as you are using typescript you could also make loadingdone a private function
64428188,deep learning chatbot specific index error list index out of range,github deeplearning pytorch chatbot indexerror,in the end i changed to and now the code seems to be working
64318798,error while installing fastbpe package for translation chatbot,python intellijidea pip translation chatbot,as far as i know it is not really made for windows you could try to install it manually by following the instructions from this post
63827063,microsoft teams returning error when file is uploaded in bot application using teams attachment button,nodejs botframework chatbot microsoftteams azurebotservice,i am having the same issue and found while looking into the teams logs that there is an access denied error when reading back the file the file itself is uploaded teamschatdateientesttxt status response errorcodeunauthorizedaccess errorcode unauthorizedaccess requestid undefined correlationid undefined afdcorrelationid ref a ffcafccfafc ref b amedge ref c tz i already raised a ticket with microsoft but the team responded that the issue is most likely with my bot although i did not change anything
63661115,ms teams microsoft bot framework returns error on taskfetch,nodejs botframework chatbot microsoftteams adaptivecards,eventually it was found that there was a problem with one of the csp headers and microsoft was not happy about it i removed the csp headers and the taskmodule started working thanks to gousiamsft for your help in debugging the issue
63495912,send message to google chat using the rest api google example not working in,googleappsscript googleapi chatbot serviceaccounts googlehangouts,answer i can confirm that the chatbot scope does indeed exist to set up a chat bot with the rest api you must use a service account more information as per the documentation you linked on developing bots with apps script for sending async messages on trigger the only way to achieve this currently is via the external http api see documentation this requires the use of a cloud service account see documentation via the oauth for apps script library this means that you must first set up a service account in the gcp console so that the chatbot scope can be used for these messages the whole process can be quite arduous for the unintitiated so i will provide the steps from start to finish here the process creating a service account navigate to the google cloud console and create a new gcp project hit select a project at the top of the page and click new project you will need to provide a project name the other fields should be filled out for you automatically press create a new popup will appear in the topright of the screen confirming that a new project is being created once loaded you can click view click the icon in the topleft and follow the apis services credentials menu item at the top of this page click create credentials service account give the service account a name and a description and press create followed by continue and finally done your service account has now been created creating service account credentials these will be needed for the code provided in the example from the developing bots with apps script page after creating the service account you will be redirected back to the list of credentials you can use for the gcp project under the service accounts section click you newlycreated service account this will be called serviceaccountnameprojectnamexxxxxxiamgserviceaccountcom click add key create new key keep json selected and press create this will initiate a download of a credentials file which you will need to use to access the api as this service account do not lose or share this file if lost you will need to delete and create new credentials for this account enabling the hangouts chat api going back to apis services and select library search for hangouts chat api and click the only result click enable this will enable the api for your project note do not close this tab yet we will still need to use the gcp console later setting up the apps script project create a new apps script project now you can copy paste the example from the async messages page into the new project open up that credentials file that you downloaded from the gcp console copy the privatekey value the one that starts with begin private key and paste it into value of serviceaccountprivatekey in the apps script project also copy the clientemail value from the credentials file and paste it into the serviceaccountemail in the apps script project in order to use the google apps script oauth library as in the example you will need to add the library to the project using the librarys script id in the apps script project ui follow the resources libraries menu item and copy paste the oauth script id into the add a library box the script id is bfsrkzilrsxxtdgdeuspzlukdsikgutmorstqhhgbzbkmunidf this and the rest of the library can be found on the the oauth for apps script github repository make sure to select the latest stable version of the library at time of writing this is version press save next you will need to link the apps script project to the gcp project you created earlier go back to the gcp console tab and follow the iam admin settings menu item copy the project number defined on this page in your apps script project follow the resources cloud platform project menu item and paste the project number into the enter project number here dialog click set project setting up the project manifest in order to use a chat bot in apps script you must include the chat key in the projects manifest in the apps script ui click view show manifest file after the last keyvalue pair add the following chat addtospacefallbackmessage thank you for adding me your full manifest file will now look something like this save your project final steps youre nearly done now you will need to deply the bot from manifest and set up the configuration in gcp and set up the trigger which will make the actual call deploying the bot in the apps script ui go to publish deploy from manifest and hit create in the newly opened dialog note you can not use the head deployment if you want to use this for your whole domain so a new deployment must be created give the deployment a name and description and press save once this has finished saving press get id next to the deployment you just created and copy the deployment id setting up gcp configuration going back to the cloud console you will need to now navigate to apis services dashboard in the list of enabled apis at the bottom of this page select the hangouts chat api on the left menu select configuration set up your bot configuration you will need to provide a bot name avatar url and description set up the functionality settings so that it works in rooms under connection settings select apps script project and paste in your deployment id from the previous section give your apps script bot the relevant permissions and press save the elusive trigger the only thing you now need to do is set up your trigger this is done like a normal apps script trigger from the edit current projects triggers menu item in apps script to complete the example click the add trigger button in the bottom right and set up the trigger settings as follows choose which function to run ontrigger choose which deployment should run head select event source timedriven select type of time based trigger minutes timer select minute interval every minute and press save and youre done this created bot will now post to all rooms that it is in the current time every minute references service accounts cloud iam documentation understanding service accounts cloud iam documentation developing bots with apps script google chat api google developers botinitiated messages creating new bots google chat api google developers github gsuitedevsappsscriptoauth an oauth library for google apps script google cloud console
63212243,lemmatizer is not working in python spacy librarary,pythonx chatbot,a recommended way of using spacy is to create a document
63202306,ping command not working in discordpy bot,python frameworks discord discordpy chatbot,when you are defining an event you are overriding the default event built in the default event for onmessage has botprocesscommandsmessage in it which allows you to call commands as you might guess you see what im getting at here you need to add an await botprocesscommandsmessage add the end of your onmessage event also please change your token anyone can log into your bot with that i also recommend joining the discordpy discord support server if you need any more help since they are usually pretty active and can answer your question right away heres the invite
62623761,in dialogflow reason for error message permission cloudfunctionsfunctionssetiampolicy denied on resource dialogflowfirebasefulfillment,dialogflowes sendmail chatbot,the issue this is an issue with the proper permissions and the service account permissions whats happening is the dialogflowfirebasefunction needs to have the setiampolict role the fix you can achieve this when you the user have the rolesiamsecurityadmin role check this out for more info other roles may also work but this role will suffice i would suggest also deleting the existing cloud function get the new role and then create the cloud function again
62320002,rasa error invalid story file format and failed to decode parameters as a json object,pythonx chatbot rasanlu rasacore rasa,here is the error you havent ended with a replace this code with
62293476,error in node componentcode is for reference only,javascript nodejs npm nodemodules chatbot,in your code there is no issue to understand more please put the entire code one think i would suggest here there can be some undeclared variable or any sort of compilation issue because of which your chatbot is not able to communicate with server you have use use strict so even undeclared variable will give error proxy server issue is not there if it is working for one component
62002526,botframework composer getting error on callback at the skill,python botframework chatbot botframeworkcomposer,you can see in the documentation that your skill host endpoint needs to end with apiskills and not just api select skills from the composer menu in the skills page if your skill is remote enter apiskills in the skill host endpoint field if your skill is local you should enter localhostportapiskills in the skill host endpoint field the skill host endpoint is used as the service url in activities sent to the skill which means it will be the base uri for any conversations api methods the skill calls if your skill tries to send requests to a nonexistent url then you should expect to get s the composer bot will be routing requests to routes starting with apiskills so thats what you should be putting in your service url
61830421,google apps script googlejsonresponseexception api call to sheetsspreadsheetsvaluesupdate failed with error invalid values,json googleappsscript googlesheetsapi chatbot slackapi,error solved first point it was possible to recover the received json from the webhook by using jsonstringify the incoming json was as expected then it was just a matter of querying the desired field from this to this error solved
61659441,bot dialog action button issue nodejs,dialog botframework chatbot,thank goodness youre just getting started it looks like youre using bot framework v which is severely outdated you should definitely switch to v teams has its own additional trickiness i recommend trying out and combing through each of these samples teamsconversationbot usingadaptivecards then read through the adaptive cards blog post basically a response from an adaptive card comes back in activityvalue so in onturn youll want to use an if statement to watch activityvalue for the value sent in your adaptive card when the user clicks see report then use begindialog or dialogrun as appropriate to start your dialog
61615942,conversational chatbot with twilio and python not working,python twilio chatbot whatsapp,you are almost there tldr your bot works fine on commandline only problem is hooking it to twilio step by step solution create two functions using twilio code send and recv functions ie def sendmsg resp messagingresponse respmessageformatmsg return strresp def recv return requestformgetbody test these functions modify if needed as i dont have account so not tested change print to send and input to send and recv ie input msg inputhi there who i am talking to change it to sendhi there who i am talking to msg recv print print no worries see you later change this to sendno worries see you later this way you will have working chat bot why use send and recv funtions it will help you while you expand or port your bot to other platforms like telegram or twitter or different provider like gupshupio
61463273,getting error message while using tensorflow,python tensorflow machinelearning chatbot tflearn,you need to download and installupdate the microsoft visual c redistributable x from here if you are facing any other issues possible reasons are your cpu does not support avx instructions your cpupython is on bits there is a library that is in a different locationnot installed on your system that cannot be loaded please refer tested build configurations for windows cpu and gpu
61305812,chatbot problem infinite recursion codecademy question,regex pythonx chatbot,the answer is indeed simple lets reduce your code to a minimally reproducible one as you can see you recursively call selfmatchreply without anything to stop it edit there are another things you need to fix lets change matchreply a lets give it the more appropriate name matchalienresponse b lets make it do what it should just match a reply thus we dont need it to get another input from the user c lets make sure it iterates all the keys in alienbabble and doesnt return immediately d we need to use refindall to get all matches in a string all these changes give us the following code inside nomatchintent it should be responses instead of responses
61165835,error while testing the echo chat bot on azure,c azure chatbot azurebotservice,try this using regex output
61015179,getting typeerror convparameters in not a function at appintent with dialogflow fufillment,dialogflowes chatbot,just as the error message says convparameters is not a function it is a javascript object where the object properties are the names of the parameters so your line could be written as note the use of square brackets to reference a property on the object rather than parenthesis to do a function call it could be that this was just difficult to see in the video
60998313,how to solve error in domainyml while training rasa,chatbot rasanlu rasacore rasa,your domainyml is not a valid yaml file so rasa cannot use it and starts instead with an empty file go to and paste the content of your domainyml you can easily validate it and find the errorinconsistency sometimes it is just indentation but it seems you have some duplication reading error trace
60577249,bot framework v dependence injection not working,c dependencyinjection botframework chatbot aspnetcore,remove from your rootdialog it serves no purpose and the container is unaware of how to resolve it for injection into the target class rootdialog will thus refactor to ensure that all explicit dependencies to be injected into the target class has been registered with the service collection and by extension the di container
60436192,azure bot framework v nodejs luis recognizer returns error,nodejs azure botframework chatbot azurelanguageunderstanding,looking at luisrecognizerjs from the botbuilderai module the error is because the recognizer is expecting a turncontext with turnstate property and you are sending a stepcontext turnstate doesnt exist on stepcontext thus the get property is failing and causing your error if you send stepcontextcontext instead that will fix the issue ie let luisanalysis await thisluisrecognizestepcontextcontext
60283642,typeerror must be str not statement,python pythonx machinelearning chatbot chatterbot,what is returnresponse it doesnt look like its defined maybe replace this by this
60213578,getting an error when trying to use microsoft luis and chat bot,azure botframework chatbot azurelanguageunderstanding,problem was that the keys on the blade in azure didnt match the ones in the app settingsjson once we made the keys match it works
60168210,facing problem while integrating voice assistant in rasa chatbot,texttospeech chatbot voicerecognition rasacore rasa,unfortunately i do not know the answer to this question but have you tried asking your question on the rasa forum
59846922,issue with implementing a helpdesk chatbot with dialogflow bigquery ml,googlebigquery dialogflowes chatbot,i have like a same issue i used diagnostic info and copy fulfillment request as curl from your tutorial when i tried to execute this curl i got your client does not have permission to get url if you try to reproduce these steps and you will get the same errors then the problem may be related to your cloud function invoke permissions to fix the permissions problems go to the cloud function tab select your cloud function check box click add members under permissions tab in the right side enter allusers under new memebers select role as cloud functions cloud functions invoker save test your cloud function by just pasting it in the browser from there firebase cloud function your client does not have permission to get url from this server
59740047,try to connect locally to chatbot but get unauthorized error,botframework chatbot ngrok,i got around it by using a vm outside the company network instead its not a solution but it is a work around i highly suspect some port are being blocked by company firewall but not sure what so i am not sure how to get it whitelisted
59703139,encoding problems in python,python pythonx sqlite encode chatbot,you are trying to read a bzcompressed file as textfile but this is not the only problem never format parameters into sqlstatements for example but the function should have a better name like fetchcomment and it should not return false if there is no comment none whould be better is commentid or parentid correct all together this should look like this
59557877,google dialogflow quick replies formatting issues in telegram,dialogflowes telegram chatbot telegrambot facebookmessenger,in dialogflow you can indeed as marc pointed out use a custom payload for telegram here it is an example the quick replies appear a buttons you can click notice the actual response is sent but not displayed within the chat all the best beppe
59421630,azure botframework speech websocket error,azure websocket speechrecognition chatbot,it was no error in my code was an error by cognitve services but is fixed now see for further information
58529504,valueerror the passed savepath is not a valid checkpoint cusersusermodeltflearn,pythonx tensorflow machinelearning chatbot tflearn,change your tensorflow code to i think the problem happens because you are creating and reseting a model and then requesting to load it and then the framework gets lost
58514021,cors policy access issue with chatbot in sdk v c net core,c azure botframework chatbot,after lots of trial and error this was the issue i was attempting to dig into your issue more but when chatting with your bot i get the subscription is in a disabled state did you turn off your azure subscription i just found out that that particular subscription where my dev bot was places has been turned off or whatever my first recommendation is that you need to make sure azure portal your resource group your app service settings cors looks like this second in azure portal your resource group your web app bot channels if you have the directline channel enabled and youre using enhanced authentication options make sure you have the appropriate client hosts listed this is not a common errorat all make sure that you deployed the bot following the deployment docs let me know if you still run into trouble this answer was just too long for a comment but i can edit it if you try these and let me know how it goes update by any chance are you using websockets if so try disabling them im not seeing a single error hit the backend which leads me to believe it may be something on your side are you behind a proxy or firewall that might be preventing access in azure portal your resource group your app service tlsssl settings do you have tls enabled does application insights show any errors if so please update your question with them
58272792,how to fix attributeerror module tensorflow has no attribute resetdefaultgraph,python tensorflow keras artificialintelligence chatbot,i solved the issue from next time ill try to post questions in a cleaner way sorry i fixed the issue by deactivating and reactivating the virtual env and then running the command pip install user tensorflow and also pip install user tensorflow and then that attribute error related to tfresetdefaultgraph session tfinteractivesession was solved
57848494,encountering error running custom actions in rasa,python chatbot rasanlu rasacore rasa,the reason youre getting the exception no registered action found for name actiontestaction error is because of your first exception when running the action server ie because of this try running the action server in debug mode using the debug flag vv like rasa run actions vv to see where the problem in your code is the problem may be because you may not have imported action or sqlite or whatever else hope that helps edit make sure that the name of the action file matches running rasa run actions actions action vv yeilds actionpy
57421478,importerror cannot import name chatbot from chatterbot,python chatbot python chatterbot,for chatterbot i had to do the following
57215941,error sending media files in whatsapp chatbot with dialogflow and twilio,twilio dialogflowes chatbot whatsapp,after many attempts and mails with dialogflow support and twilio support i have discovered that at this moment its not possible to send media files from dialogflow to twilio service directly its about what type of response send dialogflow and witch one are expected form twilio the better solution is to develop it with python
57204197,error when generating access token for facebook messaging app,facebook chatbot facebookmessenger,my apologies i was confused when i saw the login permissions and thought id have to request fb to review my app while it was still in deveopment mode in fact you can follow the instructions below although i have no idea why iy just doesnt fill in the access token as it used to the additional step seems pretty redundant and confusing imho the process is simple however just click on edit permissions select the chatmiester page deselect all others make sure the checkbox is enabled and hit next and complete this should generate the pat having said this i could not replicate this at my end but it should work easily
57124767,code error when run outside of jupyter notebook,python jupyternotebook project chatbot,input returns a string you are comparing against integers try
56975361,getting error file not found and unicode error while running following code for chatbot,pythonx chatbot,your code cant decode one of the files because its not encoded in unicode x is a control character in unicode heres a good resource on this type of error and how to deal with it the file not found error is because one of the files in that directory cant be found the files you are listing are not the full path to those files you need to use in your open call
56931650,python telegram bot error syntaxerror invalid syntax,python python pip telegram chatbot,this args any is pythononly syntax youve installed the latest version of tornado which only supports python for python you need to downgrade is currently the latest version that supports python
56908149,api call from azure app service to api hosted on azure vm error an attempt was made to access a socket in a way forbidden by its access permissions,c azure netcore botframework chatbot,for the socket forbidden error you properly could check the followings the api port is listening on the hosted vm the port is not using by other application service or process on windows vm you could run netstat anbo in cmd as the administrator account to verify this security or firewall issue if you have nsg associated with this vm subnet or vm network interface you need to add an inbound rule to allow port from your web app service also you have to enable this inbound port on the vm firewall if you a firewall inside the vm refer to diagnose a virtual machine network traffic filter problem access restrictions in the azure app service check if any ip rule restriction is to deny the access outside app service hope this could help you
56768032,message template are not working in koreai,chatbot,some templates are channel specific which means not every end channel supports each and every templates please try with different channels such as web or facebook messenger your template will be supported
56474336,how to fix issue of html page for web chat bot developed in c using sdk v template is not opening in ie browser,javascript c html botframework chatbot,you cant use the asyncawait protocol in ie also make sure you are you using the es bundle take a look at this getting started wit es bundle web chat sample hope this helps
56440801,how to fix issue related to error code in web chat channel after clicking submit in adaptive card in chatbot developed using v c,c botframework bots chatbot,below is a summary of the comments on the ops post that lead to the resolution of the issue debugging techniques ensure that the bot is working locally check the log stream or log files via kudu under development tools advanced tools for your app service you can also turn on application logs under monitoring app service logs for your app service then view the log stream via the log stream section of your app service while you test your bot in web chat in another tabwindow check that the app settings entries exist and are correct password app id etc debug the remote channel using your local code as per kyle delaney more instructions are available here basically this entails the following ensure that ngrok is installed the following instructions are roughly based on the guide here open the solution in visual studio start debugging in visual studio note down the port in the localhost address for the web page that is opened this should be navigate into the directory where you extracted ngrok type cmd into the address bar and press enter to open a new command prompt window create a publicly accessible url that tunnels all http traffic on a specified port to your machine ngrok http hostheaderlocalhost copy the forwarding url this should be in the form of keep the command prompt window running ngrok open because once it is closed the url will no longer be accessible stop debugging in the azure portal open the web app bot resource go to bot management settings configuration and replace the first part of the url with the ngrok url the final url should be in the form of click save you have click outside of the text box for the save button to be enabled go to app service settings configuration and note down the value for microsoftappid and microsoftapppassword the value in the bot file is the encrypted value we need the decrypted value for the emulator in visual studio copy the appid and apppassword key value pairs from the production endpoint in the bot file to the development endpoint ensure that the endpoint value for the development endpoint is set to the localhost url save your changes in visual studio start debugging in visual studio open test in web chat in azure test the bot functionality you should hit any breakpoints that youve set in the code clean up steps important restore the messaging endpoint url for the web app bot in azure to its original value and save the change undorevert any changes to the bot file close the command prompt window running ngrok close the bot framework emulator null reference error this has been covered numerous times by multiple authors but in a nutshell one of your objects or a property of one of your objects that you are trying to access is null when you hit your breakpoint step through your code line by line in the debugger until you find the line that breaks once you find the line that breaks you can inspect your variables and their properties by hovering over them channeldata not coming through in the webchat channel i ran into this problem myself and couldnt find any documentation on why this is the case but i managed to solve this issue with the following code inside my onturnasync method this could be further simplified if you like to the reason that the simplified version of code above should work is because as you know adaptive cards are the only time that the value property of the activity should be populated the rest of the time the postback data is in the text property and it will be automatically picked up by the code in your waterfalldialog i would advise testing this simplified code yourself before deciding to go with it as you may have scenarios that i do not where value is populated outside of an adaptive card
56322788,rasa bots response in facebook messenger as button postback but click not working,bots chatbot facebookmessenger rasanlu rasacore,did you subscribed to postback webhook event in your developers account if not you have to subscribe for that to receive the postback events see the reference messagingpostbacks webhook event reference
56286657,how to fix issues related to directlinechannel prompt choice options not displayed in chat bot window developed using v sdk in c,c botframework bots chatbot,there are two versions of embedded web chat at the moment gemini and scorpio embedded web chat is currently in the process of slowly transitioning all clients from scorpio to gemini it appears your embedded web chat is still using scorpio which unfortunately does not support the oauth prompt in the near future you will be able to manually request your client to be switched to gemini in the meantime you can add either add web chat v to your site using a cdn or wait for your client to be migrated to gemini for more details regarding embedded web chat take a look at the documentation hope this is somewhat helpful
56111293,botframework how to fixwelcome message is not getting displayed to the user in c webchatbot developed in v but displayed in emulator,c botframework bots chatbot,this is a common question regarding welcoming users the events thrown by the channels are not the same on every channel one of the main differences between the events in webchat and emulator is that on the emulator conversationupdate events are sent on the beginning of the conversation of the bot added of the user added on the webchat the conversationupdate about the user is only sent after the user has sent message so to bypass this behaviour you can handle on your side an event by using a mechanism called backchannel there is a sample for this usecase on githubs repository here in a few words you have to send an event from the webchat at start handle this event on bot side and process you welcome message
55968963,how to fix next step navigation with oauth prompt in waterfall dialog in sdk v bot created using c without typing anything,c oauth botframework chatbot,begindialogasync or promptasync should always be the last call in a step so youll need to get rid of nextasync oauthpromptbegindialogasync is special because you dont know if its going to end the turn or not if theres already an available token then it will return that token and automatically continue the calling dialog which would be the next step in your case if theres no token available then it will prompt the user to sign in which needs to be the end of the turn clearly in that case it makes no sense to continue to the next step before giving the user a chance to sign in other than that your code should work please update both your bot builder packages and your emulator to the latest versions currently for bot builder and for emulator the emulator should continue to the next step automatically when you sign in but web chat will likely require the user to enter a magic code edit if you are using a version of emulator that does not handle automatic signins correctly you can configure emulator to use a magic code instead in the settings
55845193,how to fix message to appear in new line,javascript jquery newline chatbot,you can append the to the innerhtml property
55821685,when i open url of my app it says aplication error i think i am missing something in the code,nodejs chatbot,replace following appsetport processenvport allows us to process data appuseurlencodedextended false appusejson with appsetport processenvport not an array allows us to process data appusebodyparserurlencodedextended false bodypraser missing appusebodyparserjson bodypraser missing
55629898,why aiml pattern matching is not working,python chatbot aiml,i have replicated and identified your issue you forgot to include tag here is the correct aiml for your case it will work as expected now hope it helps
55005405,quick reply on facebook messenger problem,javascript chatbot facebookmessenger messenger facebookchatbot,quick replies are different from buttons in that their payload do not return as postback messages but instead are delivered as normal messages as if the user typed and sent the message themselves it is literally just a way for the user to return a reply quicker than typing so your bot is treating it like a normal message because it is sent as a normal message so either your code needs to parse and catch normal incoming messages looking for or chocolate or change from using a quick reply to an actual postback button so that you can catch them returned through postback messages instead
54762187,getting error while deploying enterprise bot app,commandlineinterface botframework chatbot,its a known issue with msbot reading the az cli version wrong you have two options a new version of msbot was released that should fix this so you can use npm i g msbot to upgrade to the latest msbot and use the az cli or downgrade to the az cli i tested it just now and can confirm the update fixes this
54622039,problem with chatbothow to count occurrences of some words in a text file in c,c chatbot,return dataindex attempts to return a local variable that is no problem at all since the value of dataindex is returned not its address the main error in getmostvote is that in the while loop the vote line just read is compared to the uninitialized dataiword correct is to compare the current vote to the previously stored vote data until there are no more or the current vote is found among them a second error is in the forint j j rather than going through all allocated vote data of which the posterior may well be uninitialized only the used data shall be regarded
54538980,cant connect to twitch irc server problem with ip address c language,c chatbot twitch,resolved in comments above the port number in struct sockaddrin is in network endianness aka big endian while your computer is likely running as little endian to assign it you must use instead of
54454788,error while creatingimporting skill in watson assistant with lite plan,ibmcloud chatbot watsonconversation,i recommend to reload the tool page in the browser i sometimes see an issue when i have left browser windows open for some time and the authorization token for the web app expired reloading that window would check and the bring up the login page also trying to open the browser in incognito mode could also help the banking workspace bankingjson works in my lite plan of ibm watson assistant
54359715,botium project in eclipse with multiple botiumjson not working,automation chatbot,the convos and the config command line parameters are actually for the botium cli not for mocha you either switch your test scripts to botium cli or you configure botium in a way to use several configuration files and several convo directories my recommendation would be to pack each section in an own subdirectory so you have a botiumdialog and a botiumwatson directory each with its own packagejson botiumjson specconvo folders etc with some configuration changes it is also possible to use your current folder structure add multiple botiumspecjs in spec folder botiumdialogspecjs botiumwatsonspecjs add multiple test scripts to your packagejson packagejson run both of the test scripts for example
54334872,how to solve the parameter subscribedfields is required curl x post command error for creating a facebook chatbot using nodejs and heroku,nodejs facebook heroku chatbot,subscribedfields just is a required parameter so you can not subscribe an app for updates from a page without specifying which fields you want updates for documentation says type is an array of predefined keywords so you should be able to subscribe to multiple fields in one request using subscribedfieldsfieldfield
54274330,filenotfounderror errno no such file or directory englishcomputersyml,python filepath chatbot,you need to include a separating the directory and the file name in order to open the specific file from its appropriate path within the english directory
53651137,facebook messenger button error action unsuccessful there was an error delivering your message,facebook postback chatbot facebookmessenger,the page access token just needed to be updated in the webhook probably to apply the latest permissions that include messagingpostbacks go back to developer app dashboard select messenger settings scroll down to the token generation section select your page from the dropdown and copy the new access token for use in your webhook found a lot of similar questions and no clear answers so i hope this saves someone the days of headache it caused me
53444929,error on sending parameter values on event call,chatbot dialogflowes,for now i am sending parameters as below its working fine
53405004,call qna maker issue with postman,postman chatbot azureqnamaker,the problem is that i choose formdata in postman after i select raw it works fine now
53387160,i cannot sync my facebook pages on chatteron what could be the problem of this,chat messaging chatbot facebookchatbot facebookchat,you may have figured this out already but facebook deprecated the perms field when asking for a users account permissions and replaced it with tasks in your case chatteron probably have an updated version that supports this
53381593,importerror dll load failed the specified module could not be found tensorflow chatbot,python tensorflow dll artificialintelligence chatbot,found out the error something to do with prerequisites i needed i deleted python and reinstalled it then installed tensorflow and it worked
53294681,rasa chatbot framework gives error while training fit got multiple values for keyword argument batchsize,pythonx keras typeerror chatbot rasacore,i had the same error and after several search i did find a solution it is not the best solution but it can help go to rasacorerasacorepolicieskeraspolicypy in lines and delete epochs and batchsize arguemnts from modelfit change this to this then you can pass epochos and batchsize arguments in agenttrain i hope it will help
53147837,amazon lex initializatoin and time zone issue,amazonwebservices timezone awslambda chatbot amazonlex,this is not possible because the initialization and validation lambda function comes after lex processes the input with the timezone setting already used to format date and time values the other thing to note is that requestattributes are only used by lex in the incoming request not in the response here is what is meant when we differentiate between request and response this is the relevant line in the documentation emphasis added a userdefined request attribute is data that you send to your bot in each request you send the information in the amzlexrequestattributes header of a postcontent request or in the requestattributes field of a posttext request so here is where requestattributes need to be added and requestattributes can only be added by postcontent api or posttext api
52760276,problem with loading file to facebook messenger through nodejs,nodejs facebook facebookgraphapi request chatbot,after searching a lot i was able to make the necessary changes to my applications methods to make it possible to transfer files through messeger the concept was almost right what was wrong was the way the data was being sent the correct one is send them through a form here is the solution
51787510,the specified path file name or both are too long c azure chat bot error,c visualstudio azure chatbot,the path appears to be duplicated several times and suggests your folder structure has been changed or otherwise corrupt objreleasepackagepackagetmp objreleasepackagepackagetmp objreleasepackagepackagetmp objreleasepackagepackagetmp objdebugtemporarygeneratedfilecbbdfadcbdcs i suggest examining your projects folders to make sure theyre in the correct place and if needed clean the solution andor manually delete the obj folder and recompile
51752654,microsoft chatbot framework returning http status code internalservererror for facebook messenger,chatbot facebookchatbot,i had a similar problem i quickly implemented nlog basic implementation is simple in my bot and wrote all error logs and some addition events to a text log it was a huge help for me hope this helps
51438496,having problems with global variable in python and line api,python globalvariables chatbot pythondecorators lineapi,python backend apps are typically deployed in multiprocess configuration the front server apache ngnix or whatever runs multiple parallel processes to handle incoming requests and any request can be served by any process global variables are perprocess so which state you will find your global depends on which process served the request conclusion dont use global variables to store application state use a shared database of any kind it doesnt have to be a sql database it just have to be shared amongst all processes
51122311,connection issues with translator text api version microsoft azure qna chat bot,c azure botframework chatbot,i want to connect my azure qna chat bot with the translation layer cognitive system i am using this page as a reference i try to create a sample to achieve your requirement translate user inputs to english and pass translation text to qnamaker dialog the sample works fine both on local and azure you can refer to it in messagescontroller in dialog test result note we can use configurationmanagerappsettingsqnaknowledgebaseid to access qnaknowledgebaseid etc settings from webconfig if run bot application on local for more information please refer to this so thread
51102886,error cs when developing qna bot c,c azure chatbot,handlesystemmessage must be async and return type must be task in order to use await inside also you need to change the code calls for handlesystemmessage too if the calling method is async you need to call like await handlesystemmessage if its not you need to wait for it
50977219,error in jumps in ibm watson,ibmcloud chatbot ibmwatson watsonconversation watsonassistant,did not match the condition of the target node nor any of the conditions of its subsequent siblings this error occurs if no final node is matched if your last node was in a branch where the parent is a node then it will fall back to root to find the answer you get an endless loop which will stop after iterations like this example if the user types in error it jumps to the branch doesnt find a match returns to root to find where to stop and loops if the branch is in a folder then it continues on past the folder to find the match to fix the issue you need to add a final node in the branch that will capture anythingelse like so the other option is to use a folder node it will it allow it to fall through back to the tree where it entered and your final node should capture it
50675764,facebook chatbot responds with action unsuccessful error to persistent menu postback,php facebook chatbot,i had the same problem it was fixed by the following changes you may have forgotten to add the event and fb is not allowed to send any buttons postback payload to the server check thoroughly if the type of the buttons is correctly specified as one of the following weburl or postback in lower case have in mind also that the persistent menu requires pagesmessaging permission according to fb developers doc
50480422,c bot framework resource not found error,c aspnet botframework chatbot,yes the url works only for post request and not a get request because you post a message from user to bot and not a get you can see that in the messagescontroller code being said that if you want to test your bot locally you have to use the emulator you can have a look at bot emulator for the same now if you want to publish the bot to the world so that others can see it and use it so thats where the channel comes in consider channel as a medium by which you enable your bot for others to use with a much better user experience there are multiple channels available for the bot to be published in and yes you can publish the same bot in all the channelswebchat is just one channel and the one which is enabled by default and the way to see it is open your bot in the azure portal and click channels blade click edit for the web chat channel under secret keys click show for the first key copy the secret key and the embed code click done so the embed code is actually an iframe which you can place in your website or share with others who want to use your bot or you can use the src of the iframe too to reach the bot directly again this is just one channel you can take a look at the configure channels documentation for steps to enable the bot in more channels like skype microsoft teams email facebook slack telegram etc
49710301,chatbot error eol while scanning string literal,python chatbot,is the escape character in python if you end your string with it will escape the close quote so the string is no longer terminated properly you should use a raw string by prefixing the open quote with r
49244194,ruby include method not working,ruby chatbot twitch,the problem isnt with its with the line before it admincommands is an array which contains the strings disconnectproject it does not however contain the string project then some substring here so you are never checking if msgincludeproject you should be seeing your log messages about an unrecognized command which means you arent making it into your first if statement what youll want to be doing is something like
48259153,microsoft bot builder chat bot error,nodejs api bots chatbot,in the previous versions of botbuilder microsoft provided a state api for bots the state api managed the state of the bot as you might expect things like the user data the conversation data the dialog data etc they have since deprecated this api and provided a way which you can implement your own storage adapters or indeed us available packages to do so the botbuilder module provides an in memory storage which obviously is fine while the bot is running but will be lost if the bot crashes and isnt suitable if you intend to load balance the bot across multiple machines i tend to use the in memory storage for local development and in production switch it out with a different adapter however there are other storage adapters available the microsoft package botbuilderazure offers table storage cosmosdb storage and sql storage i tend to use the following package botbuilderstorage with the dynamodb adapter it also offers redis and mongodb adapters state management is also documented pretty well here
47805187,facebook chatbot postback not working,php facebook postback chatbot,as i dont see the code which triggers the actual sending the error could be found there if youve copied the basic tutorial it might look like this one i started with a long time ago notice the foundpostback if your send trigger looks like this from one the tutorials it will not send messages as there is no inputentrymessagingmessageattribute in postback messages so if you detect a postback you have to keep that flag however i strongly suggest to build own classes for handling messages postback deliveries echos and so on more about those you can find here facebook messenger docs
47638240,sorry my bot code is having an issue but it does not,c botframework chatbot azurelanguageunderstanding,the error message the bot state api is being deprecated means that your chatbot needs to use your own storage for managing state the url in the message goes to the manage custom state data with azure cosmos db for net page which explains why last july the bot framework team blogged about this saving state data with botbuilderazure in net if you read a couple of the other blog posts after that youll learn how to use other data stores like sql server for state management really you want to use your own state management because the amount of storage they provide is very small improved performance reduced latency and greater control over state setting up state should be a normal task for all of your chatbots
46001873,pass custom debug information to microsoft bot framework emulator,c debugging botframework chatbot,great question yes it is entirely possible you can use the channeldata property of your activity you are responding with the data entered into the channeldata property must be valid json for example in the emulator this will appear as
45953300,ibm watson conversation service error cannot convert from method group to conversationonmessage,c unitygameengine ibmcloud chatbot watsonconversation,according to line in the source code of conversation the delegate was changed to youll have to change your onmessage method to reflect that
43913959,how to troubleshoot this aws lambda error an error has occurred received error response from lambda unhandled,amazonwebservices awslambda amazoniam chatbot amazonlex,the communication between lex and lambda is not straight forward like normal functions amazon lex expects output from lambda in a particular json format and data like slot details etc are also sent to lambda in a similar json you can find the blueprints for them here lambda function input event and response format make sure your c code also return a json in the similar fashion so that lex can understand and do the further processing hope it helps
43476311,issues with claudiajs text responses and alexa,nodejs awslambda chatbot alexaskillskit claudiajs,if you havent already run claudia update configurealexaskill and enter your bots name then use the url it gives in your alexa skill builder setup select instead of lambda arn currently messagetext is being passed as an empty string which means that none of your log blocks are firing and you are returning an empty object at the very bottom of your code test it yourself by replacing that empty object with a string the alexa tester no longer complains so why is messagetext an empty string claudia js populates the text field by concatenating all the slots that have been filled for your intent if your intent has no slots then claudia passes an empty string add slots to your intent make sure the skill is configured and fix the logic statement issues
43114670,facebook chatbot localization of greeting message not working,localization facebookmessenger chatbot facebookchatbot,you need to properly escape nonascii characters textuuuduu userfirstname would be the proper json representation of an array containing one element with the key text and the content userfirstname
41603082,errors calling the microsoft luisai programmatic api,azure botframework chatbot azuremachinelearningservice azurelanguageunderstanding,answering my own question here i have found my luisai programmatic api key it is found by luisai dashboard username upperright settings in dropdown subscription keys tab programmatic api key it was not immediately obvious since its found nowhere else not alongside any of the other key listings in cognitive services or the luis
40394179,facebook messenger chatbot response issue,php facebook laravel chatbot,i finally get it to work here is the code for it
40283554,getting error code while using the http endpoint to the bot on gupshup platform,chatbot gupshup,you must define this function to handle the endpoint in your bot youl encounter the error if you dont register this function read this documentation for more details
40263240,python aiml kernel startup error,python chatbot aiml,check your initpy file i added following code for initpy file which is in the aiml directory
39382479,microsoft bot framework unauthorized error,c artificialintelligence botframework chatbot,microsoft bot framework document says that when youre running in the localhost add your localhost with the correct port and ask you to keep empty in the app id and the app pass but if you have added you your app id and and pass to webconfig file make sure to add them to your emulator as well otherwise you will get unauthorized error
39302446,java chatbot code not working as intended,java chatbot,please read the javadoc of substring returns a new string that is a substring of this string the substring begins at the specified beginindex and extends to the character at index endindex based on your logic you have to modify to no need to use static method in chatbot since you have created the instance of chatbot remember to close the scanner when you dont use it any more see the updated code as below and also the execute class
37439633,python chatbot typeerror list indices must be integers not str,python dictionary chatbot,selfdatabase is a list list items are accessed by specifying their position within the list for example selfdatabase for the first item and selfdatabase for the fifth item youre trying to use text as a list position which makes no sense if you want to store items based on a text key instead of an integer position use a dictionary instead of a list
36030076,error while importing library in android studio,java android chatbot alice,you have to adde the jar files in the lib folder to the library of your android project
21062588,why is this code for a jquery chatbot not working,javascript jquery css chatbot,you are missing a quote in your username function to close the html string errors like this will show up in your browser debug console
20477314,getting code error for a simple chatbot program,java chat chatbot,getresponse is defined for chatbot not bottest
20442547,chatbot return bug,java chatbot,regarding counting the responses just modify your main method if i have the time i will edit my post in a few minutes to check your problem regarding the double appearences of a response you also forgot to close the scanner edit it actually happens because scanner has as a default the delimiter set to be on whitespace so if you input a text with a whitespace the while loop runs twice for one user input just use the nextline command why is this code in your getresponse method its not used at all take a closer look at your methods as they are holding some strange code
72016319,detect languages in a column but ignore ambiguous values why am i getting an error,python languagedetection,you can catch the error and return nan from the function you apply note that you can give any callable that takes one input and returns one output as the argument to apply it doesnt have to be a lambda im not sure why you had if lenx in there that would only return nan when the original string has zero or one characters but i included it in my detectlang function to keep the functionality consistent with your lambda
63684004,how to fix langdetects instable results,python languagedetection,if you use detectorfactory as suggested in the documentation instead of detectorfactory it works result
54624926,how to resolve javalangnoclassdeffounderror in the following code,java noclassdeffounderror languagedetection,please add the jsonicjar and langdetectjar into the build path of your netbeans project you can find both these jars under the lib directory of the github url which you had provided earlier post change you should be able to get the desired output
30591461,language detection not working in typo,typo typoscript typox languagedetection,ive added the following configuration i understood that the functionality is not working only with this configuration we need to select official language iso code in the website langauge alternate language we added in the root now its works as intended
19566633,language detection does not working as expected,java languagedetection,as the faq of the library is stating can langdetect handle short texts this library requires that a detection text has some length almost words over it may return a wrong language for very short text with words you are trying it on oneword or twoword texts this is not the use case this library is build for so youre gonna have wrong results for single words without context you can try to match them with dictionaries of the languages you are targetting
18248386,htaccess language detection returns in endless redirects who finds the bug,htaccess languagedetection,add another rewrite condition that checks if redirection has already taken place
3805144,google language detection api replying error code,java languagedetection googleajax,as a guess whatever is being passed in on str has characters that are invalid in a url as the error code is not acceptable and looks to be returned when there is a content encoding issue after a quick google it looks like you need to run your str through the javaneturlencoder class then append it to the url
79533246,using azure speech to text service where im giving input as memory stream but getting error nomatch speech could not be recognized,azure stream speechtotext,error nomatch speech could not be recognized i got the same error when i tried with a wav file with a sample rate of hz use the command below to check the sample rate of your wav file so to resolve the issue i converted my wav file to hz using the command below and successfully got the speech to text output code using microsoftcognitiveservicesspeech using microsoftcognitiveservicesspeechaudio class program private static string speechkey private static string speechregion static async task mainstring args string filepath try if fileexistsfilepath consolewritelineerror audio file not found return byte audiodata filereadallbytesfilepath using var memorystream new memorystreamaudiodata string resulttext await recognizespeechfromstreamasyncmemorystream consolewritelinerecognition result resulttext catch exception ex consolewritelineexception exmessage public static async task recognizespeechfromstreamasyncstream audiostream try byte channels byte bitspersample uint samplespersecond var audioformat audiostreamformatgetwaveformatpcmsamplespersecond bitspersample channels var contosostream new contosoaudiostreamaudiostream var audioconfig audioconfigfromstreaminputcontosostream audioformat var speechconfig speechconfigfromsubscriptionspeechkey speechregion speechconfigspeechrecognitionlanguage enus using var speechrecognizer new speechrecognizerspeechconfig audioconfig consolewritelinestarting speech recognition from stream var speechrecognitionresult await speechrecognizerrecognizeonceasync if speechrecognitionresultreason resultreasonrecognizedspeech consolewritelinerecognized textspeechrecognitionresulttext return speechrecognitionresulttext else if speechrecognitionresultreason resultreasonnomatch consolewritelinenomatch speech could not be recognized return null else if speechrecognitionresultreason resultreasoncanceled var cancellation cancellationdetailsfromresultspeechrecognitionresult consolewritelinecanceled reasoncancellationreason if cancellationreason cancellationreasonerror consolewritelinecanceled errorcodecancellationerrorcode consolewritelinecanceled errordetailscancellationerrordetails consolewritelinecanceled did you set the speech resource key and region values return null else consolewritelineunexpected result reason speechrecognitionresultreason return null catch exception ex consoleerrorwritelineexception during speech recognition exmessage return null public class contosoaudiostream pullaudioinputstreamcallback private binaryreader reader private int chunksize public contosoaudiostreamstream audiostream int chunksize reader new binaryreaderaudiostream chunksize chunksize public override int readbyte buffer uint size try byte tempbuffer readerreadbytesintmathminsize chunksize tempbuffercopytobuffer return tempbufferlength catch endofstreamexception return catch exception ex consoleerrorwritelineerror reading from stream exmessage return public override void close readerclose consolewritelinecontosoaudiostream closed output
79386909,azure pronunciation assessment could not deserialize speech context error,javascript azure speechrecognition speechtotext azurespeech,try this code block out a few catches audioconfigfromwavfileinput might not supported in node i just used the workaround mentioned in the link it worked the pronunciationassessmentconfig needs to be passed as individual parameter values not a json i used a sample wav from here you can edit to yours
77928050,azure stt is giving error bad request,java azure registration speechtotext,the proxy error occurs due to incorrect configuration or invalid data being sent to the azure speechtotext service to fix the error ensure that the correct endpoint uri for the azure speechtotext service is used along with a valid speech key and properly formatted audio data in a wav file below is the correct endpoint uri the following code has been updated with the correct endpoint uri and speech key enabling it to convert speech to text without any errors code import orgapache import orgapache import orgapache import orgapache import orgapache import orgapache import orgapache import orgapache import javaioioexception import javaneturi import javaneturisyntaxexception import javaniofilefiles import javaniofilepaths import javaiofile import javautilscanner public class speechtotextexample public static void mainstring args uribuilder builder null try builder new uribuilder catch urisyntaxexception e eprintstacktrace return string filepath pathtowav file string sourcelang enus buildersetparameterlanguage sourcelang uri uri null try uri builderbuild catch urisyntaxexception e eprintstacktrace return request new requestsetheadercontenttype audiowav requestsetheaderocpapimsubscriptionkey try file audiofile new filefilepath requestsetentitynew fileentityaudiofile contenttypecreateaudiowav response entity responsegetentity if entity null scanner scanner new scannerentitygetcontent while scannerhasnextline systemoutprintlnscannernextline scannerclose catch ioexception e eprintstacktrace return output it ran successfully and the speech was converted to text as shown below
77155485,bidirectional stream connect not working with gather,stream twilio texttospeech speechtotext twiliotwiml,the behavior you described of the stream working but not the gather is by design of the twiml you are using twilio processes the twiml in order and doesnt proceed until the verb finishes the verbs in the twiml are connect and gather you have the gather twiml after the stream an alternative is to use just the gather twiml and then use the twilio rest api to handle the media streams your application would need to obtain the pathcallsid from the call that was in gather mode then use the twilio rest api to start the media stream for that call one problem with this approach is that gather seems best suited for short segments of the call to address another question you asked do do it completely via streams here i am getting issues in getting streamid connectionid callid at one place check out the status callback parameter when you create the stream the statuscallback attribute takes an absolute or relative url as value whenever a stream is started or stopped twilio will make a request to this url for example the parameters sent to the statuscallback url contain the streamsid callsid
77106671,problem getting text with selenium python,python seleniumwebdriver speechtotext,you have several issues in your code dont use absolute xpath locators you have quite unique locators on your elements ids startbutton and copybutton after you dictated your text there is no direct element in dom which contain it however you have copybutton that can copy text in clipboard you just need to get it for example using pyperclip be aware that there can be google ads modal that can intercept the click to the copy button so if you have it you need to make it invisible or close it import pyperclip from seleniumwebdrivercommonby import by from selenium import webdriver previous code driverfindelementbyid startbuttonclick printlistening while true stop inputtype anything to stop if stop copy driverfindelementbyid copybutton copyclick copiedtext pyperclippaste printcopiedtext driverquit break
76099104,showall false not working with speechrecognition,python speechrecognition speechtotext googlespeechapi,no issue on my side with speechrecognition issue seems present in and related to this pull ticket related to this issue quoting when calling recognizegoogle method of speechrecognition it prints the entire json response investigating the problem is on the and lines of speechrecognitioninitpy with with
75742318,reactnativevoice speech to text not working in android,android reactnative speechtotext reactnativevoice,in my case i do this solution nodemodules reactnativevoicevoice android open androidmanifest and add this block i tested on android device try this
75728911,azure cognitiveservices speech not working as i expected,python azure speechtotext,i tried in my environment and got the below results initially i received the same output as yours for the code below its recognizing hello world but the text output is not sent to transcripttxt file output i wrote another code for getting the transcripttxt file in my environment and it worked perfectly output
74513009,flutter object cant be assinged to type widget error,flutter speechrecognition speechtotext,you have to get words properly using package in here you should get recorgnized words as follows watch video tutorial here after that check the word you want to filter using containsthen navigate it add listner properly as follows other mistake you have done was you assigning navigatorpushnamed to child of container as one of the expected result which is not a widget you can add onspeechresult listner as in example and then navigate to page from that
73726816,multiprocess error while using map function in python with ngram language model,python multiprocessing speechtotext ngram,i finally fixed this error the brokenpipeerror error broken pipe arises from the linux operating system and it will occur when you are doing io tasks specifically when the pipeline of read and write on linux is closed as the other side of communication is still trying to read or write the data this error will occur now the fun part is here i was using workers which was the number of cpu cores in my map function the length of data in the dataset iterable was so the pipeline doing the map function had rows with files in each and row with files i guess it was the cause of the error while the last row with files made some disturbance so i reduced the number of files in the dataset from to and the number of workers to and removed the batchsize this made the length of the data to be divisible by the number of processes and made the error go away here is the link for more information about brokenpipeline error
73567966,azure batch transcription error when downloading the recording uri statuscode conflict fail to download,azure post speechtotext azurecognitiveservices azurespeech,in the end i have opened a ticket with microsoft and after analysis the problem was in part of our infrastructure we are getting the error of the issue because the audio file is not publicly accessible and therefore not downloadable by the speech service the service needs to be able to download the file and therefore you need to generate a sas url for it
73217680,exe file gives error when i try to run it in c window form,c speechtotext,it may be a versioning problem check if you have the necessary versions of microsoft visual c redistributable for visual studio installed also loading packagesconfig sometimes behaves incorrectly in newer versions of visual studio try following the steps from in solution explorer rightclick on packagesconfig and select migrate packagesconfig to packagereference in the opening dialog there should be microsoftcognitiveservicesspeechcore in toplevel dependencies by default and no package compatibility issues select ok to begin the migration after the migration is completed rebuild the solution which should then run without problems hope that helps
72815476,nodejsc addon getting error undefined symbol speechconfigfromsubscription from microsoft speech sdk on ubuntu server,c nodejs gstreamer speechtotext mulaw,i got the solution just coped this libmicrosoftcognitiveservicesspeechcoreso library into usrlib folder and modified bindinggyp file it is working fine targets targetname transcription sources srcstreamingasrcpp cflags wall stdc cflags fnoexceptions cflagscc fnoexceptions includedirs homedelaplextransationspeechsdkincludecxxapi homedelaplextransationspeechsdkincludecapi node e requirenan node e requirestreamingworkersdk libraries usrliblibmicrosoftcognitiveservicesspeechcoreso
71621981,microsoft azure speech translation not working javascript sdk,azure translation speechtotext,thank you for letting us know this should be fixed now
71576655,why is my speech recognition not working in python,python macos speechrecognition speechtotext pyttsx,pyaudio requires portaudio be installed on your system check out the installation section of the pyaudio documentation for mac os the following should do the trick there are probably other acceptable ways to install portaudio too if youre not already using homebrew
70505712,google speech to text not working correctly with very short audio single words,speechrecognition speechtotext googlespeechapi googlespeechtotextapi,the cloud speechtotext api best practices suggest using a lossless codec like flac or linear i verified with linear and it works for single words which are digits so the solution would be to transcode the audio
69296114,error during training in deepspeech internal failed to call thenrnnforward with model config rnnmode rnninputmode rnndirectionmode,python googlecolaboratory speechtotext mozilladeepspeech customtraining,if i try it as below it worked fine basically augment was doing something to break our training in between
69096865,error while making azure speech to text api call via python,python azure azurefunctions speechtotext,your post request body is not supplied in the right format please see the corrected code below thanks sen
68007529,how to resolve the error using watson speech to text api,python ibmwatson speechtotext,i cant test it but error shows stream was bytes and lenfilepath gives probably it needs instead of audiof edit documentation for recognize shows example which uses so it means you may need without read full example from documentation you have to click link recognize and scroll down to see it in documentation
67803905,default is not a function react type error,reactjs typeerror speechtotext,it is because of this line speechrecognitionmic the error states that the default export from your module is not a function which means that speechrecognition is not a function so you cannot call it change your code as looks like you have installed the latest version but trying to use it in old way please take a look at this migration guide
67327588,error when converting speech to text in python,python pythonx audio speechrecognition speechtotext,i dont think renaming the file extension will help you you should use a file converter to make sure the audio data is beeing correctly encoded in another format try using soundconverter
66912855,python speech recognition recognizegoogle with unknownvalueerror for microphone,python pythonx speechrecognition speechtotext,so i managed to solve the problem apparently its a problem with visual studio code i had to run the code from the terminal and now it is working fine im not sure why this is the case but im happy that it is working now
66891203,google speech to text error invalid recognition config bad encoding for an mp file,googlecloudplatform speechrecognition speechtotext googlespeechapi googlecloudspeech,i was able to reproduce the issue seems that the encoding used is the root cause i used the gcloud ml speech recognize command and i got no responses after that i changed the encoding of the file then i tried again and voil please consider that according to this documentation mp encoding is a beta feature and only available in vpbeta so you should consider to convert your file before to send it to the speech to text api
66641177,python speech recognition is not working using speechrecognition library,python pythonx speechrecognition speechtotext,in the speech recognition readme on pypi you can see there is a pyaudio section it means you have to have pyaudio installed on your machine but if you have pyaudio intsalled and you get an error you need to share the error without the try except blocks so we can analyze the error and give a solution to install pyaudio execute pip install pyaudio in the terminal sometimes pip install pyaudio can throw an error similar to this steps to fix this error go to your terminal and execute python version and in the case of optimus prime his version is python find if your python installation is bit or bit which you can see by going to your python terminal download a pyaudio wheel file based on your python version and your python installation bit bit open up a terminal in the directory where you have downloaded the wheel file whl then execute pip install and you are done
66298721,tensorflowpythonframeworkerrorsimplinvalidargumenterror specified a list with shape from a tensor with shape,python tensorflow keras speechrecognition speechtotext,the problem was that the modelpredictdata function wanted my data to have the same shape as the number of the batchsize there are two ways to fix this change the batch size in your model to and this is the best option modelpredictdatabatchsize
65489705,transcribing mp to text python riff id error,python pythonx speechrecognition speechtotext transcription,you need to first convert the mp to wav and then you can transcribe it below is the modified version of your code in above modified code first mp file being converted into wav and then transcribing processes
64677600,problems with google cloud speechtotext api,python api googlecloudplatform speechrecognition speechtotext,looks like youre using an old library version from google async recognizion example this two options seems to be equivalent or btw take a look also at the official google codelab for speech to text they always have uptodate examples
63483644,sfspeechrecognizer not working on real device if playing audio from apple music,ios speechrecognition speechtotext sfspeechrecognizer,i found the answer mentioning it here if somebody needs in future so i was using sfspeechaudiobufferrecognitionrequest instead of sfspeechurlrecognitionrequest if you are selecting media from device you need to take the url of the selected audio file and pass it to sfspeechurlrecognitionrequesturl audiourl this worked for me
63413144,voice assistant using python showing loop error,pythonx speechtotext pyttsx,i think you need to import pyttsx and other modules if you are still facing problems then you can use my source code
63348364,flutter speech to text error the method initialize was called on null,flutter dart speechtotext,this error occurred due to you not instantiating the sttspeechtotext object and then calling the packages initialize function you can instantiate it using the following sttspeechtotext speech sttspeechtotext
62987798,speechrecognition not working in vscodepython,visualstudiocode speechrecognition speechtotext,apparently vscode is not asking for mic permissions for some it seems to work if you start code from the terminal via code i got it to work following this advice from here
62936028,not able to import azurecognitiveservicesspeech as speechsdk in python azure functions error segmentation fault core dumped,python pythonx azure azurefunctions speechtotext,this error is because a package called libasound does not exist in azure ubuntu virtual machines solved by installing using the below commmand
62836586,microsoft azure speech to text java issue connectionfailure when using proxy,java azure speechtotext azurespeech,thanks for dauds comment more details can refer to this link
62578403,flutter speech recognition app platformcallhandler call speechonerror,android flutter dart speechtotext flutterdependencies,maybe there is some issue with the plugin or just check out with the real device i would like to suggest this alternative plugin as the plugin you are using is out of maintenance it might have some issues you can check out their sample code for the abovementioned plugin which works well
62354979,my pc is listening and does not advance does not show me an error or anything,python pythonx speechrecognition speechtotext,i was doing some tests with the same library a long time ago and i tried two computers in the newer one i had problems because the sound card automatically filtered the ambient sound and only recognized the last words while in the other it seemed that it did nothing or took a long time to process the recognition in the end i did it by adding this line according to the documentation in speechrecognition is probably set to a value that is too high to start off with and then being adjusted lower automatically by dynamic energy threshold adjustment before it is at a good level the energy threshold is so high that speech is just considered ambient noise the solution is to decrease this threshold or call recognizerinstanceadjustforambientnoise beforehand which will set the threshold to a good value automatically heres an example speechrecognitionexamplescalibrateenergythresholdpy
62351113,type error record missing required positional argument source,python typeerror speechrecognition speechtotext,i think this line r syrecognizermake an instance of recognizer so you should do that instead r syrecognizer
61492272,defaultcredentialserror raised saying file not found,python googlecloudplatform speechtotext googlespeechtotextapi,i solved that problem setting the googleapplicationcredentials directly in the project folder and removing the quotes of the path just run this on your prompt without any quotes
61067674,how do i fix attributeerror nonetype object has no attribute lower,python pythonx artificialintelligence speechrecognition speechtotext,the error is because the variable query is sometimes none and you are applying lower function on it which only works on str type objects you can control this by putting your code inside a if loop which runs only when theres a string in query variable like this maybe
61059134,uncaught typeerror awstranscribeservice is not a constructor,javascript speechrecognition amazoncognito speechtotext awssdkjs,the easiest way to create your own build of the aws sdk for javascript is to use the sdk builder web application at use the sdk builder to specify services and their api versions to include in your build choose select all services or choose select default services as a starting point from which you can add or remove services choose development for more readable code or choose minified to create a minified build to deploy after you choose the services and versions to include choose build to build and download your custom sdk
61047969,how do i fix this typeerror listen missing required positional argument source,python pythonx artificialintelligence speechrecognition speechtotext,try this instead of hope this helps
60352850,waveerror unknown format arises when trying to convert a wav file into text in python,python wav speechtotext voicerecording,you wrote the file in float format and wave module cant read it to store int format do like this
59378022,googlecloudspeech error enoent no such file or directory open protosjson,nodejs googlecloudplatform speechtotext,i ran into this with googlecloudfirestore both googlecloudfirestore and googlecloudspeech use the same mechanism to to load protosjson so my solution should be relevant here this happened to me because webpack was building the googlecloudfirestore package into my bundle the googlecloudfirestore package uses dirname to find protosjson since the googlecloudfirestore code was in my bundle the dirname variable was set to my bundles directory instead of to the nodemodulesgooglecloudfirestore subdirectory that contains protosjson possible fix set this in your webpack config to tell webpack to set the value of dirname node dirname true possible fix update your webpack config to exclude googlecloudspeech from your bundle one way to do this is to use the webpacknodeexternals package to exclude all dependencies from the nodemodules directory var nodeexternals requirewebpacknodeexternals moduleexports externals nodeexternals target node
59183797,microsoftcognitiveservicesspeechdetailedspeechrecognitionresultcollection error,c speechtotext azurecognitiveservices,its a bug in the data type the extension is using for the offset an int can only track s of audio you can access the raw json that the best method is using from the results property collection through the speechserviceresponsejsonresult property until a fix is available
58854194,microsoft speech to text python sdk spxerrinvalidheader issue,pythonx speechrecognition speechtotext azurecognitiveservices,mpencoded audio is not supported as an input format please use a wavpcm file with bit samples khz sample rate and a single channel mono
57681203,is there an algorithm for speaker error rate for speechtotext diarization,speechtotext transcription,the commonly used approach for this appears to be the diarization error rate der defined by nist in the nistrt projects a newer evaluation metric is the jaccard error rate jer introduced in dihard ii the second dihard speech diarization challenge two projects for measuring these include der is referenced in these papers a comparison of neural network feature transforms for speaker diarization the icsi rt speaker diarization system
57213203,how to fix here is no default audio device configured speechtotext,nodejs windows speechtotext googlespeechapi sox,well i see the question dont get any answer but i apologize because i found a solution few months ago in resumen the version of sox have some issues in my windows os i test in linux version with fedora distribution and works correctly so i found a solution in the nodespeakable repo just downgrading the version to i solved this problem by installing sox rather than here i solved this problem in the same way anyway there is another way in the same post who can solved with the version but i think is a little tricky cause you play with the command and not implement the library or scripts files but is still a option
56348761,error in installing linein package nodejs,python nodejs npm ibmwatson speechtotext,nodegyp requires python python v recommended vxx is not supported there are some windows specific instructions that may help you
56266810,microsoft speech recognition phraselist undefinedwhat is the issue,javascript speechrecognition speechtotext azurecognitiveservices,needed to update my sdk version
56045679,cognitive services azure endpoint not working,c azure unitygameengine speechtotext azurecognitiveservices,i think you might created the wrong service for cognitive services there are many types such as face luis speechservice and so on in this case you need to create a speech service by searching speech when you create a resource on azure portal hope it helps
55240290,watson speech to text invalid credentials error code,pythonx speechrecognition ibmwatson speechtotext,all the information you want is in the api documentation including how to obtain the api key
54908870,error argument error username and password are required unless useunauthenticated is set,ibmwatson speechtotext,the endpoint is specified as url and not iamurl as you have it this works for me
54503875,why am i getting this error in speech to text code,nodejs ibmwatson speechtotext,a number of reason could cause this some are platform or browser specific but to start with is the website up and listening on that port is you firewall configured to allow traffic
54271749,error with enablespeakerdiarization tag in google cloud speech to text,pythonx googlecloudplatform speechtotext googlespeechapi googlecloudspeech,the enablespeakerdiarizationtrue parameter in speechtypesrecognitionconfig is available only in the library speechvpbeta at the moment so you need to import that library in order to use that parameter not the default speech one i did some modifications to your code and works fine for me take into account that you need to use a service account to run this code def transcribegcsgcsuri from googlecloud import speechvpbeta as speech from googlecloudspeechvpbeta import enums from googlecloudspeechvpbeta import types client speechspeechclient audio typesrecognitionaudiouri gcsuri config speechtypesrecognitionconfig languagecode enusenablespeakerdiarizationtrue diarizationspeakercount operation clientlongrunningrecognizeconfig audio printwaiting for operation to complete response operationresulttimeout result responseresults wordsinfo resultalternativeswords tag speaker for wordinfo in wordsinfo if wordinfospeakertagtag speakerspeaker wordinfoword else printspeaker formattagspeaker tagwordinfospeakertag speakerwordinfoword printspeaker formattagspeaker and the result should be like
53261572,ibm watson speech to text transcript the audiio literally with gramatical errors,unitygameengine ibmwatson speechtotext literals transcription,my speech transcribes as what you do want when using the example you can access word alternatives in the response as well
53073521,issue setting up snowboy hotword detection alongside python speechrecognition library,python raspberrypi speechrecognition speechtotext pyaudio,to anyone who might be having the same or similar issues i came across the solution all of the methods i tried ultimately didnt work but i discovered that the functionality i was looking for was already built into the snowboy library however the prebuilt binaries available for direct download are outdated and dont have this functionality yet therefore to get the functionality i described you would have to go through the process of compiling your own custom binary with the instructions in the readme on snowboys github repo making this change immediately fixed the issues i was having and made me able to move on with my project hope this helps anyone else that might be stuck on this
52632457,android speech to text issue,android speechrecognition speechtotext azurecognitiveservices,update the below function you used to detect connection determines if it is connected to network and it does not determine you have internet access on the connected networkhowever that does not appear to be problem in your case experiment with google speech recogniser and see if the same problem occursneeds some more info to provide you a solution web socket connections are designed to disconnect when there is no networkplease make sure you have active internet connection or you probably didnt add to manifest
51065598,bot framework using custom speech service error c,c net speechtotext azurecognitiveservices,in my case the problem was that i had a wav stream audio that doesnt had the file header that cris custom speech service needs the sulution is creating a temporary file wav read the file wav and copy it in a stream to send it as array to cris or copy it in a memorystream and send it as array
51048466,google speech to text extra language set to kn but not working,android localization speechrecognition speechtotext,after hours of research i finally found solution in the below line of code i should send knin instead of kn this made it work this method works for all the languages mentioned above tein mlin
50569428,internal server error updating watsons stt language model,speechtotext ibmwatson,please check your model now i am guessing you were experiencing an intermittent error sending another train request usually helps
47257518,valueerror tensor tensorconst shape dtypefloat may not be fed with tfplaceholder,python tensorflow speechtotext,you defined keepprob as a tfconstant but then trying to feed the value into it replace keepprob tfconstant with keepprob tfplaceholdertffloat or keepprob tfplaceholderwithdefault
46131861,error watsngwerrxc means what,ibmcloud speechtotext watson,it looks like your customization modelaeaeeebeac is in an inconsistent status try deleting the customization and creating a new one deleting a customization
45964054,quality issue with offline voicetotext using sphinx,speechrecognition voicerecognition speechtotext cmusphinx sphinx,this turned out to be a trivial issue thats documented in the faq q what is sample rate and how does it affect accuracy we can not detect sample rate yet so before using decoder you need to make sure that both sample rate of the decoder matches the sample rate of the input audio and the bandwidth of the audio matches the bandwidth that was used to train the model a mismatch results in very bad accuracy the news footage was bbc audio stereo recorded at khz i converted it to mono then downsampled to khz now its working pretty well heres a snippet of transcribed audio from the news article emergency officials said they expect the hall from million people to seek assistance in texas bolton flashy thousand people already being cared for in temporary shelter is on the engine is a big on releasing water from two downs that protect houston city sense of
45139441,speechtotext ibm watson exceptionininitializererror,java android ibmcloud speechtotext ibmwatson,i think your setusernameandpassword is wrong it should be setcredentialsstring username string password unless it is your custom function at the bottom of the link find the function setcredentialsstring username string password also you need to initialize shared instance and use it try following this step provided in the quick start guide
44509361,can someone help me find the bug in my ibm speech to text code,python websocket speechtotext ibmwatson,i took a quick look at your code and saw that there is a missing part you are not signalling the end of the audio stream after pushing all the audio in the onopen method you can signal the end of audio by sending an empty binary message or a text message with the string action stop as described here i believe that is why you do not get any result also please make sure you do not close the websocket until the server replies with the final result thank you for the answer sayuri mizuguchi i actually wrote the code hosted in which is a simple example of interacting with watson stt via websockets that project is being integrated into the watson python sdk here regarding conversion to base you just need to make sure that the audio is sent as a binary message websocket stacks usually come with the ability to send either a text message or a binary message
43165495,how do i fix this error when trying to use the watson sdk for unity,unitygameengine speechtotext watson,this is a very basic singleton pattern implementation you can find more advanced ones online
42176497,streaming audio through websockets ibm not working,python websocket speechrecognition speechtotext ibmwatson,arecord is a linux tool to record audio in alsa framework it is not going to work on linux you need to use something like pyaudio to record sound instead
41797920,importerror no module named speechrecognition in python idle,python module speechrecognition speechtotext,install speech recognition using pip install speechrecognition
40952346,uncaught typeerror cannot read property length of undefined,javascript speechtotext,wov i actually never heard of it but it seems i got it working now here is the fiddle
40917338,cpprestsdk request not working,speechtotext bing bingapi azurecognitiveservices,thanks all i changed the code to following and i got the token
39232150,python speech recognition error converting mp file,python speechrecognition speechtotext,speech recognition supports wav file format here is a sample wav to text program using speechrecognition sample code python output used wav file url
39183759,speechrecognizer stoplistening not working,android speechrecognition voicerecognition speechtotext,assuming google is your default recognition service provider the current release of google now stoplistening is not working correctly please see this thread for more information and other related bugs it has been fixed in the most recent beta release if you want to test that to confirm your issue a few other bugs do still remain
39009779,speech to text result null error,android imageview speechtotext,it looks like you are removing an element from unspokenlist in onactivityresult but not altering the value of randomnum prior to getting the final image add the following logging above the line where the app crashes and youll see the error
37427423,android change language of speech to text to japanese not working,android speechrecognition speechtotext,try like this after that override the onactivityresult method in your activity filewhere you called the fragment
36523705,python pocketsphinx requesterror missing pocketsphinx module ensure that pocketsphinx is set up correctly,python speechrecognition speechtotext pyaudio pocketsphinx,you will need these libraries for compiling pocketsphinx after that its simple to install pocketsphinx sudo pip install pocketsphinx
36235955,duplicate entry error dependancy and project module confliction android,androidgradleplugin buildgradle speechtotext cloudant,i am not sure that it can work the issue happens because you are adding the same class orgapachecommonsiobyteordermarkclass twice if you checking the pom file of the comcloudantcloudantsyncdatastoreandroid library you will find this dependency in your speechandroidwrapper module you can remove the commonsiojar from the libs folder change the dependencies in the buildgradle removing the line compile fileslibscommonsiojar and adding compile commonsiocommonsio
31290921,pocketsphinx acoustic model creating issue with sphinxfe command,ubuntu speechrecognition speechtotext pocketsphinx,do i need to convert my wav files to mfc first and then run sphinxtrain run command i no output and log files can be seen here log says that the file is missing in the path it should be placed at
31173363,applewatch speechtotext functionality not working,swift speechtotext watchos,you can ask for user input and give him suggestion see swift example bellow if suggestion is nil it goes directly to dictation it is not working on the simulator but it is on real watch your approach is correct but something is wrong with your siri try changing the language it should work like these
29312541,error using sphinx jars without maven,java speechrecognition voicerecognition speechtotext sphinx,there are multiple issues with your code and your actions i have create a folder file not needed i have copy the folder enus and the files cmudictenusdict enuslmdmp wav on the folder file not needed you already have models as part of sphinxdata package i dont use maven so i have just include the jar files sphinxcoresnapshotjar and sphinxdatasnapshotjar this is very wrong because you took outdated jars from unauthorized location the right place to download jars is listed in tutorial you took malicious jars from some random website which might have a virus or rootkit in them here this is a wrong link too the correct link is here you use file url scheme which points to files in inappropriate context if you want to create inputstream from file do like this exception in thread main javalangnoclassdeffounderror comgooglecommonbasefunction this error is caused by the fact you took a jar from other place and it said you need additional dependencies when you see classdeffounderror it means you need to add additional jar into your classpath with official sphinx you should not see this error
24570261,android speechrecognizer network error,android speechtotext,according to use of speechrecognizer produces errornetwork value the error was caused by samsungs speech api powered by vlingo changing the speech api in the phone settings to standard google api solved the problems
24436855,what is this error ioerror errno no such file or directory audioflac i am trying to use the google voice recognition api for python,python bit voicerecognition speechtotext ioerror,you miss the sox tool installed which converts recorded wav to flac you can see in line in pygsr sources systemsox s t wav r t flac sflac selffile selffile make sure that sox works for you and it can create flac files
23511015,google speech api v not working,android googleapi speechrecognition speechtotext androidspeechapi,recently google closed v api v api now requires a key and streaming v api is limited to requests per day you can get a key as described here however there are no guarantees on unlimited usage
21693951,google speech recognition not working,c wpf googlechrome speechrecognition speechtotext,parsing json with regular expressions will usually cause you problems consider using a library like jsonnet to parse the string into an object instead
18875882,watsonjs spanish speechtotext not working,nodejs api request credentials speechtotext,problem solved documentation on watsons api was wrong after a couple of emails it got corrected it was something with the xarg variable its updated now att speech api
17778532,raspberrypi pocketsphinx pseye error failed to open audio device,audio speechrecognition raspberrypi speechtotext cmusphinx,took me a while with with some help from a couple sources they will be listed in my answer and some helpful hints from nikolayshmyrev i finally came up with an answer that worked for me key assumptions running these commands as the pi user previously i was running them as root which was incorrect im using continuous recognition and i was only looking for the ability to wakeup my raspberry pi upon waking it up i have other plans on how it should interact my setup canakit raspberrypi hdmi cable to my toshiba tv usb wifi dongle playstation eye for speech recognition moving forward i ran the following commands on my raspberrypi to get pulseaudio pocketsphinx working together w my playstation eye if you see any places for improvement please let me know install pulse audio development packages sudo aptget install gstreamerpulseaudio libao libasoundplugins libgconfmmc libglademmca libpulsedev libpulsemainloopglib libpulsemainloopglibdbg libpulse libpulsedbg libsoxfmtpulse paman paprefs pavucontrol pavumeter pulseaudio pulseaudiodbg pulseaudioesoundcompat pulseaudioesoundcompatdbg pulseaudiomodulebluetooth pulseaudiomodulegconf pulseaudiomodulejack pulseaudiomodulelirc pulseaudiomodulelircdbg pulseaudiomodulex pulseaudiomodulezeroconf pulseaudiomodulezeroconfdbg pulseaudioutils osscompat y setting up alsa per instructions from sudo cp pf etcasoundconf etcasoundconforig echo pcmpulse type pulse ctlpulse type pulse pcmdefault type pulse ctldefault type pulse sudo tee etcasoundconf make sure your camera device loads on boot deviceloadonstartgrep sndbcm etcmodules wc l if deviceloadonstart then sudo cp pf etcmodules etcmodulesorig echo sndbcm tee a etcmodules fi disallow module loading after startup this is a security feature since it disallows additional module loading during runtime and on user request disallowmoduleloadinggrep disallowmoduleloading etcdefaultpulseaudio wc l if disallowmoduleloading then sudo cp pf etcdefaultpulseaudio etcdefaultpulseaudioorig sudo sed i sdisallowmoduleloadingdisallowmoduleloadingg etcdefaultpulseaudio fi set up the pulseaudio daemon for network connections allow other clients on the network to connect to pulseaudio daemon only add authanonymous if you know every machine on your lan this could be a security risk otherwise sudo cp fvp etcpulsesystempa etcpulsesystempaorig echo scarlettpi added this loadmodule modulenativeprotocoltcp authipacl authanonymous loadmodule modulezeroconfpublish sudo tee a etcpulsesystempa echo scarlettpi added this loadmodule modulenativeprotocoltcp loadmodule modulezeroconfpublish loadmodule modulenativeprotocoltcp authipacl authanonymous loadmodule modulezeroconfpublish sudo tee a etcpulsedefaultpa check to make sure it looks okay cat etcpulsedefaultpa change default sound driver from alsa to pulseaudio sudo cp fvp etclibaoconf etclibaoconforig sudo sed i sdefaultdriveralsadefaultdriverpulseg etclibaoconf daemon settings according to pimusicbox sudo cp fvp etcpulsedaemonconf etcpulsedaemonconforig echo scarlettpi added this highpriority yes nicelevel exitidletime resamplemethod srcsincmediumquality defaultsampleformat sle defaultsamplerate defaultsamplechannels sudo tee a etcpulsedaemonconf add pi user to the pulse access group sudo adduser pi pulseaccess shut down the machine to make sure all the settings we just made are loaded correctly sudo shutdown r now make sure to add usrlocallib to library path export ldlibrarypathusrlocallib export pkgconfigpathusrlocallibpkgconfig also add these to your bashrc so they get set once you login echo scarlettpi added this export ldlibrarypathusrlocallib export pkgconfigpathusrlocallibpkgconfig tee a bashrc install base pocketsphinx install python dev packages sudo aptget install pythondev y sphinxbase install required to install pocketsphinx sudo aptget install bison y cd pi wget tar xvf sphinxbasetargz cd sphinxbase configure make sudo make install cd pocketsphinx install set this ldlibrarypathpathtopocketsphinxlibs usrlocalbinpocketsphinxcontinuous wget tar xvf pocketsphinxtargz cd pocketsphinx configure make sudo make install cd install sphinxtrain wget tar xvf sphinxtrain cd sphinxtrain configure make sudo make install cd check if pulse daemon is running ps aux grep pulse if it isnt start it up yourself need to figure out the best way to make this run on bootinitd script maybe usrbinpulseaudio start logtargetsyslog systemfalse finally run sphinx important note you have to be user pi and the pulseaudio server needs to be running assumimg existing corpus file jsgf file dic and lm files using lmtool cd pipocketsphinx pocketsphinxcontinuous lm homepiscarlettpiconfigspeechlmscarlettlm dict homepiscarlettpiconfigspeechdictscarlettdic hmm homepiscarlettpiconfigspeechmodelhmmenushubwsjsck silprob wip e bestpath references advice on how to calibrate pocketsphinx correctly how to get pocketsphinx to recognize new words via a corpus bestsimplest explanation of how java speech grammar format works i plan on adding more details behind why i used certain setting configurations in a blog post im writing on my home automation project but figured id share what ive done thus far incase someone else was stuck like me and would like to move forward with what theyre working on hope this helps someone thanks for the advice guys
15287689,importing pocketsphinx not working python windows importerror no module named pocketsphinx,python windows importerror speechtotext cmusphinx,to compile python module do the following build sphinxbase and pocketsphinx with visual studio as needed copy sphinxbaselib and sphinxbasedll from sphinxbasebinrelease to sphinxbasepython and to pocketsphinxpython copy pocketsphinxlib and pocketsphinxdll from pocketsphinxbinrelease to sphinxbasepython and to pocketsphinxpython open terminal change directory to sphixnbasepython run the command python setupwinpy install change directory to pocketsphinxpython run the command python setupwinpy install run the command to test python python pstestpy
11708812,exceeded maximum number of transactions per day error in dragonmobile sdk for ios,ios speechtotext,you can use up to speech per a day check here for more detail for silver plan
11464701,error in julius asr,speechrecognition speechtotext,error reason julius optput these messages when your word dictionary contains words that are not trained in the acoustic model because the vocaloadhtkdictc tries to match the triphones in dict file with the triphone list in acoustic model so when it does not find it it shows this error and stops the program possible error solutions enable forcedict option or uncomment it jconf file to skip error words in dictionary and force running or map the not found triphone to the most close physical triphone in hmmlist file tiedlist for example beyt veht the first column is the name of triphone generated from your dictionary and the second column is the name of the hmm actually defined in your am but this solution can be done if the not found triphones are little not too many the best solution is to not to include words in your dict file that are not in the am note that the first two solutions are for testing julius only because for production or comercial projects you must train the acoustic model and language model with the same corpus
9860835,flac error input file has an idv tag it doesnt,java ruby mp speechtotext flac,this is now two separate problems the idv issue i have only been able to resolve by sidestepping the use of mp files and using wav instead the java output is still an issue so im shifting that to a new question
7955625,issue distinguishing commands from normal speech with sapi,c speechrecognition sapi speechtotext noise,let me make sure i understand you want a phrase to set apart commands to the system like butler or siri so youll say butler turn on tv you can build this into your grammar here is an example of a simple grammar that requires an opening phrase before it recognizes a command it uses semantic results to help you understand what was said in this case the user must say open or please open or can you open you can then process the speech with something like and check for semantic results like
79395647,how to report an azure texttospeech bug,azure texttospeech azurecognitiveservices,you can try microsoft qa there are official microsoft staff and support there who would be able to reach out to internal team after initial investigation and its free i have been seem fair a few issues there being raised and forwarded in the past their response time is pretty fast too if your question is well elaborated if you have azure support raise a ticket from azure portal is also very effective
79090330,azure text to speech python sdk timeout error,azure texttospeech,below is the complete modified code to fix the error in two ways if the sdk method times out or fails the code automatically falls back to the rest api for speech synthesis ensuring that network or tlsrelated issues with the sdk dont block functionality the tryexcept block catches exceptions early logs the issue and switches to the rest api ensuring smooth execution despite network or sdk issues code import os import logging import azurecognitiveservicesspeech as speechsdk import requests loggingbasicconfiglevelloggingdebug azurespeechkey azureserviceregion tempazureaudiopath azureaudiooutput def texttospeechtext voicenamezhcnyunfengneural if not ospathexiststempazureaudiopath osmakedirstempazureaudiopath outputfile ospathjointempazureaudiopath ftextvoicenamewav try speechconfig speechsdkspeechconfigsubscriptionazurespeechkey regionazureserviceregion speechconfigspeechsynthesisvoicename voicename audioconfig speechsdkaudioaudiooutputconfigfilenameoutputfile synthesizer speechsdkspeechsynthesizerspeechconfigspeechconfig audioconfigaudioconfig result synthesizerspeaktextasynctextget if resultreason speechsdkresultreasonsynthesizingaudiocompleted printspeech synthesis completed if ospathexistsoutputfile printffile outputfile created successfully else printfile not created return none elif resultreason speechsdkresultreasoncanceled printfsynthesis canceled resultcancellationdetailsreason if resultcancellationdetailserrordetails printferror details resultcancellationdetailserrordetails return none return outputfile except exception as e printfexception occurred e printattempting to use rest api fallback return texttospeechresttext voicename def texttospeechresttext voicenamezhcnyunfengneural url f headers ocpapimsubscriptionkey azurespeechkey contenttype applicationssmlxml xmicrosoftoutputformat riffkhzbitmonopcm ssml f text try response requestsposturl headersheaders datassmlencodeutf if responsestatuscode outputfile ospathjointempazureaudiopath ftextvoicenamerestwav with openoutputfile wb as audiofile audiofilewriteresponsecontent printfrest api file outputfile created successfully return outputfile else printfrest api error responsestatuscode responsetext return none except exception as e printfrest api exception e return none if name main text voicename zhcnyunfengneural output texttospeechtext voicename if output printfaudio file generated output else printfailed to generate audio output the following code ran succesfully and got the speech output from text input as shown below
79045780,fluttertts build error nugetexe not found cmake error,flutter windows dart cmake texttospeech,what worked for me download nugetexe from put nugetexe in cwindowssystem restart vscode run flutter clean run flutter pub get try to start a new debug session it should work now
78801301,avatar failed to start error invalidcharactererror failed to execute atob on window the string to be decoded is not correctly encoded,azure webrtc texttospeech avatar,avatar failed to start error invalidcharactererror failed to execute atob on window the string to be decoded is not correctly encoded the credentials you receive from the api are indeed valid base strings first log the response to see what youre getting the current validation function may not be robust enough improve the base validation function check base encoding if isvalidbaseicecredential consoleerrorice credential is not a valid base string icecredential throw new errorinvalid base encoding else if isvalidbaseiceusername consoleerrorice username is not a valid base string iceusername throw new errorinvalid base encoding else consolelogice credentials are valid return iceusername icecredential catch error consoleerrorerror fetching ice credentials error throw error appjsx sometimes intermittent network issues can cause problems maintain a stable network connection result info initializing speech sdk with yourcogsvcsubkey yourcogsvcregion yourvoicename info avatar configuration youravatarcharacter youravatarstyle youravatarbackgroundcolor info fetching ice credentials info ice credentials are valid info webrtc connection created info webrtc status new info webrtc status checking info webrtc status connected info connected to azure avatar service info tz avatar started
78634364,why does pyttsx return a comerror when getting a voice description after i changed the voice registry,python com texttospeech pyttsx,to solve this problem there are two ways use widows windows restore point to back or undo the changes you have been done in the registry file if have a backup for the registry file it will be great if you will restore the backup so the issue will go away before you make any changes in system files dont forget to take a backup
78590244,texttospeech is not working in instant app in android kotlin,android kotlin androidjetpackcompose texttospeech androidinstantapps,the texttospeech class uses the intent androidintentactionttsservice under the hood the android compatibility definition document gives us good indications which restrictions instant apps have regarding managing intents instant apps c instant apps must not interact with installed apps via implicit intents unless one of the following is true the components intent pattern filter is exposed and has categorybrowsable the action is one of actionsend actionsendto actionsendmultiple the target is explicitly exposed with androidvisibletoinstantapps c instant apps must not interact explicitly with installed apps unless the component is exposed via androidvisibletoinstantapps given that information we can now inspect the app which is used by the android os by default for providing tts functionality which is the application comgoogleandroidtts by downloading the apk file end decompiling it we can inspect the manifestxml file in there we find the following declaration mapped to the ttsservice intent you can see that neither of the conditions mentioned in the android cdd is fulfilled the intent is not categorybrowsable the intent is not actionsend there is no androidvisibletoinstantapps attribute on the tag there is a dedicated list which intents are guaranteed to be supported on instant apps which does not contain the tts intent so unfortunately you will not be able to use tts in instant apps by default this would only possible if you installed a tts engine on your device that explicitly sets androidvisibletoinstantappstrue
77849787,modulenotfounderror no module named tts but installed tts,python pythonx pip texttospeech,maybe can help restart your terminal or command prompt active your virtual environment check pip listmake sure the library in the response rerun the command
76776461,speechsynthesis apis getvoices method not working properly returning empty array,javascript texttospeech voice speechsynthesis,i have found the solution to the issue apparently getvoices returned an empty array in the html file because the program was invoking it before the voices loaded so i attached an onvoiceschanged event listener to speechsynthesis which invokes the speech code so that the speech code runs after the voices are loaded this effectively worked and properly changed the voice here is the final code therefore the reason the original code worked in the developers console is because the voices were already loaded there
76594239,data at the root level is invalid line position azure error,php laravel azure texttospeech inertiajs,the solution is do not use http that comes with laraavel write your own curl function writing my own curl function was the solution i guess http adds some other things to the post request
76204125,speech synthesis api not working in chrome,javascript googlechrome texttospeech,you cant generally have a page autoplay audio in chrome and nope that jsfiddle doesnt work out of the box on my windows chrome either adding a button and wrapping the loop in makes it work after one clicks the button
76060592,requestsexceptionsconnecttimeout error in azure cognitive services texttospeech rest api,python azure rest texttospeech azurecognitiveservices,with the below code snippet i tried to convert the files files and then files at a time so far this code snippet worked without any interruptions in the conversion process import os import requests import time import chardet import retrying subscriptionkey region eastus voicename eninneerjaneural outputformat audiokhzkbitratemonomp ttsurl f headers authorization fbearer contenttype applicationssmlxml xmicrosoftoutputformat outputformat useragent snap retryingretrywaitexponentialmultiplier waitexponentialmax stopmaxattemptnumber def postrequesturl headers data response requestsposturl headersheaders datadata timeout responseraiseforstatus return response inputfolder cuserskamalidocumentspythonttsinput outputfolder cuserskamalidocumentspythonttsoutput for filename in oslistdirinputfolder if filenameendswithtxt with openospathjoininputfolder filename rb as f rawdata fread encoding chardetdetectrawdataencoding text rawdatadecodeencoding ssml ftext tokenurl f tokenheaders ocpapimsubscriptionkey subscriptionkey response postrequesttokenurl headerstokenheaders datanone accesstoken responsetext headersauthorization fbearer accesstoken response postrequestttsurl headersheaders datassmlencodeutf with openospathjoinoutputfolder ffilenamemp wb as f fwriteresponsecontent timesleep reference ms doc of texttospeech conversion using python result for the error you are getting it is mentioned in the python examples site that the server should be in up and running state with good configuration such as code snippet for connecting time limit max number of retries etc
75858711,flutter text to speech not working on real device,flutter texttospeech,before setting the language check if the language is available on the device using the islanguageavailable method if you go to your settings and then search for texttospeech output you can see your available languages
73005862,how to fix missing catch or finally after try if i already put catch,javascript discord discordjs bots texttospeech,your catch statement is outside the if block that has the try block this is what i did to fix the error move the catch block closer to the try remove extra that were causing errors fix the indentation heres the code const getaudiourl requiregooglettsapi const langlist en id let language args iflanglistfindc c language iflanguage return messagereplyplease enter the language ifargs return messagereplyplease enter the text const text argsslicejoin iftextlength return messagereplyyou cant input a text with over characters const voicechannel messagemembervoicechannel ifvoicechannel return messagereply please enter a voice channel const audiourl await getaudiourltext lang language slow false host timeout try voicechanneljointhenconnection const dispatcher connectionplayaudiourl dispatcheronfinish voicechannelleave catcherr consolelogerr messagechannelsendthere is an error else return messagereply a tip to avoid this kind of error indent properly if you had indented all of your code it would have been so much easier to read and therefore finding the error would have been really quick and you wouldnt even need to ask stack overflow programs must be written for people to read and only incidentally for machines to execute harold abelson if you make your code easier to read debugging rewriting maintaining understanding etc will become much easier for you and everyone else
72583795,gcp texttospeech api auth issue,authentication googlecloudplatform texttospeech serviceaccounts,this is confusingcomplex but the error is helpful your application has authenticated using end user credentials from the google cloud sdk or google cloud shell which are not supported by the texttospeechgoogleapiscom note you can try this method using googles apis explorer at this link textsynthesize the issue is that gcloud is an oauth application and tokens issued by gcloud either using gcloud auth printaccessidentitytoken and gcloud auth applicationdefault printaccesstoken are issued against a googlemanaged project that google provides for gcloud and importantly not one of your own projects google wants to provide gcloud for its users but does not want to provide arbitrary api access for free to its users hence the not supported part of the error the solution as described is that you should use or create your own google project enable the texttospeech service api in this project create a service account and key gcloud auth activateserviceaccount providing the service account key gcloud auth printaccesstoken to get an access token to invoke the api see the following link for the steps
71260191,getting file read error using python speech recognition after converting mp to wav,python speechrecognition texttospeech mp,i found my problem it wasnt in this function at all i somehow started feeding mps to the function instead of wavs this was a classic case of me assuming that the calling code did not have a problem and then finding that it did yes i should have shared it all correct calling code incorrect calling code tbh i really dont know how i managed to screw that up
71230103,no audio with pyttsx library in python no errors,python texttospeech python pyttsx,you forgot to put the parentheses on enginerunandwait do this enginerunandwait
69684117,text to speech web api results in error on voice setting,javascript jquery texttospeech webapi,you need to use one of the voices returned by getvoices assigning a plain object to voice wont work buttononclick function var utterthis new speechsynthesisutteranceblue sky utterthisrate utterthispitch for const v of speechsynthesisgetvoices if vvoiceuri google us english utterthisvoice v speechsynthesisspeakutterthis click the voices returned by getvoices are verified by the browser operating system to be supported and are clearly tied to an underlying service that translates the text into speech a plain object on the other hand does not which is why a plain object doesnt work
69348868,a simple regex problem in ts tts app im not very knowledgeable with regex or coding so any help would be appreciated,java regex texttospeech,what you need is a socalled positive lookahead your regex should only match numbers if they are directly followed by letters see here in regexcom
68667592,text to speech app not working in emulator in android studio but works in android device,android androidstudio androidemulator texttospeech,the reason why the tts is not working on the emulator is because the texttospeech instance creation should be performed asynchronously not in the main ui thread indeed you have to make sure that the tts initialisation is successful by waiting on the oninit callback verifying that the status is texttospeechsuccess language should be defined as well for instance used asynctask for demonstration only finally if youre targeting android the following code should be added into the manifest
68326453,javascript code for windowspeechsynthesis not working from localhost,javascript googlechromedevtools texttospeech,in chrome getvoices returns empty then theres an event when the voices are ready onvoiceschanged which accepts a callback your commands work in devtools because by the time youve typed allvoices ttsgetvoices the voices have already arrived and are available however other browsers eg safari return voices synchronously and theres no need for a callback therefore the best code seems to be
67006742,tts not working in tensorflow for gesture detection android app,java android tensorflow texttospeech gesturerecognition,solution added the below code in this in which have added a textview and a imageview you can use button or whatever you like to use which gave me the below output then under cameraactivityjava define the imageview then under onclick method i am defined if the particular letter is encountered play that letter sound i have used mp sound file as i was not able to implement tts so i opted to go with mp not the ideal approach but as didnt worked for me used this method after almost searching for a week or week as i am new to android
66802329,having issues with ibm watson tts using curl,curl websphere texttospeech ibmwatson,your very small audio file will actually be a text file containing the error code either cat it or open it with a text editor and check what error it is reporting
66653692,issues with using lexicon on azure cognitive services texttospeach from python,python azure texttospeech lexicon,well i found an issue my xml was missing first line all works now
65523244,text to speech not working on application relating to nfc,android nfc texttospeech,my understanding of texttospeech in the oninit method you just need to setup text to speech engine it does not watch a particular widget and speak everything that is in it and does so every time it changes to get it to speak the text stored on the nfc card in buildtagviews method after the line nfccontentssettextnfc content text add because currently you are never instructing the text to speech engine to speak what is on the nfc card because currently you are only instructing the text to speech engine to speak the words that are in the textview just after you have initialised it from the xml so most likely empty also you could probably remove the code in the oninit below because it is redundant unless you set some text to the edit widget in your layout xml and has additional s
65272775,android text to speech plays without error but does not produce any sound,java android androidstudio texttospeech samsunggalaxy,try removing
64494239,text to speech not working properly in android studio,java android androidstudio texttospeech,you can try by creating an application class and initialize your texttospeech engine in it so that it starts initializing on app launch and than use that in your activity
64405532,why speechsynthesisutterance is not working on chrome,texttospeech,its because in chrome speech synthasis requires user interaction before it speaks eg a button click ive added to your code to put the speak function behind the buttons click event buttonaddeventlistenerclick if speechsynthesis in window const tospeak new speechsynthesisutteranceinputvalue hello world speechsynthesiscancel speechsynthesisspeaktospeak else alertnot supported speak
64306050,no module named appkit if i try to install it in pycharm it keeps saying error command errored out with exit status,python texttospeech appkit googletexttospeech,you dont have appkit installed thats why interpreter is complaining about it you have to install the package theres two possibilities in terminal type pip install appkit in pycharm do preferences project interpreter click type appkit install package et voila it should be working now
64118403,c google cloud texttospeech being wavenet issue,c audio googlecloudplatform cloud texttospeech,according to pricing page you was charged for wavenet voice minus free quota million wavenet sound was set automatically since the name parameter in voiceselectionparams was empty you need to specify a name parameter otherwise the service will choose a voice based on the other parameters such as languagecode and gender voice names you can find here in column voice name
63526060,error trying to freeze file using pyttsx module sapi not found,python texttospeech cxfreeze,fixed it some of the compiled files were in a different directory with the module files not sure why but i copied them int othe right place and it works
63428694,how to fix visual studio attributeerror engine object has no attribute getproperty,python pythonx module texttospeech pyttsx,python identifiers are case sensitive you wrote which is fine and matches the docs exactly the diagnostic you show is for some different code the diagnostic is correct while there is a getproperty attribute the engine lacks getproperty those are two different identifiers spell it correctly and your program will work better
63159317,ibm watson text to speech error the request was aborted could not create ssltls secure channel,c ssl texttospeech ibmwatson,use this line of code in static block like constructor and use this static method
62825132,having problem using pyttsx on ubuntu,python texttospeech python ubuntu pyttsx,as the error points out oserror libespeakso cannot open shared object file no such file or directory that library must be installed using
62820711,xamarinforms issue with text to speech feature for html data,xamarinforms texttospeech xamarinessentials,it will be an expected effect because the content of string is html format as a workaround you could get the content of the html by using regex so you could improve the code like following note in this way the style of css will not work any more you need to set the style font or color in span by yourself
62801379,valueerror mmap closed or invalid,python pythonx pygame texttospeech gtts,this is because youre closing the mmap m as the sound is playing a quick test is that commenting out the mclose makes it work ok you will need to program some other way of handling the loadingplaying do you really need the speed optimisation of mmap another approach might be to use the io module stringio and never write the audio stream back to disk theoretically something like but you still probably cant delete memfile while the audio is playing
62796613,microsoft cognitiveservices text to speech problem,azure texttospeech azurecognitiveservices,the speechrecognitionlanguage parameter is for recognition you can follow this sample to set synthesis language key lines are speechconfig speechsdkspeechconfigsubscriptionspeechkey regionserviceregion sets the synthesis language speechconfigspeechsynthesislanguage heil creates a speech synthesizer for the specified language using the default speaker as audio output speechsynthesizer speechsdkspeechsynthesizerspeechconfigspeechconfig result speechsynthesizerspeaktextasynctextget
62576302,attributeerror module comtypesgenspeechlib has no attribute ispeechvoice,python texttospeech,i decided to reinstall pyttsx with another version and it worked commandpip uninstall pyttsxand pip install pyttsx
61949934,expospeech not working on some ios devices,expo texttospeech,i was facing the same issue on my workouttime app because of silent mode expo solution we can use expoav expo library to play speech on slient mode by playsinsilentmodeios true in expo text to speech is working when other sound is in playing mode so you can attach empty sound download empty sound from here reactnative solution
61869760,problem with pyttsx module not working in visual studio and jupyter notebook,python pythonx module texttospeech pyttsx,this is not how you use pyttsx this program should be using speakwhatever goes here and it should say it but i suggest you look at some tutorials on how to use pyttsx and take some python tutorials as well its quite easy once you look at some examples something else i did with pyttsx is this use def speak for the easier speak function i put this at the top of my program on an unrelated note you can change the gender of the voice by changing the value being male and being female
61763927,creating a simple ibm assistant using their tts and stt i get a bytes and strings error i am using vlc to play audio how can i fix this,python string byte texttospeech vlc,if you pay close attention to the error message you will see that the error is actually being thrown by the vlc code which implies that the output from the tts speech is not what vlc is expecting you need to break up your code and first verify what output you are getting from tts if it is audio then you can work out how the vlc code expects it i suspect it not in the format that the tts is outputting updated answer the output from tts is a data stream of audio content in python this will be a byte array it looks as though vlc is looking for a string this makes no sense if vlc is looking for audio data if however it was looking for a string then that string could be a file destination so i think you need to write the file and give the file destination to vlc imho based on the question you are asking and the code you have cobbled together your coding skills are not up to the challenge and you maybe better off spending a couple of weeks going through some python coding tutorials you may find the investment in training time pays off without you struggling with what are quite fundamental coding issues here
61658740,speechsynthesis not working on mobile safari even though its supported,javascript mobilesafari texttospeech,on my end the issue broke down to proper loading speech synthesis on mobile safari there are some things to check in order are voices loaded are voices even installed on your system is the utterance configured correctly is the speak function called from within a user interaction event the following example summarizes these checks and works on macos desktop browsers plus ios safari let speechsynth let voices const cache retries until there have been voices loaded no stopper flag included in this example note that this function assumes that there are voices installed on the host system function loadvoiceswhenavailable oncomplete speechsynth windowspeechsynthesis const voices speechsynthgetvoices if voiceslength voices voices oncomplete else return settimeoutfunction loadvoiceswhenavailableoncomplete returns the first found voice for a given language code function getvoices locale if speechsynth throw new errorbrowser does not support speech synthesis if cachelocale return cachelocale cachelocale voicesfiltervoice voicelang locale return cachelocale speak a certain text param locale the locale this voice requires param text the text to speak param onend callback if tts is finished function playbytext locale text onend const voices getvoiceslocale todo load preference here eg male female etc todo but for now we just use the first occurrence const utterance new windowspeechsynthesisutterance utterancevoice voices utterancepitch utterancerate utterancevoiceuri native utterancevolume utterancerate utterancepitch utterancetext text utterancelang locale if onend utteranceonend onend speechsynthcancel cancel current speak if any is running speechsynthspeakutterance on document ready loadvoiceswhenavailablefunction consolelogloaded function speak settimeout playbytextenus hello world speak details on the code are added as comments within the snippet
61016951,changing the speechsynthesis voice not working,javascript texttospeech speechsynthesis,ianr i copied the code and it works for me i cut out the pitch and rate controls and made the html simpler but its basically the same if it doesnt work for you are you getting any console errors var synth windowspeechsynthesis var inputform documentqueryselectorform var inputtxt documentqueryselectortxt var voiceselect documentqueryselectorselect var pitch documentqueryselectorpitch var pitchvalue documentqueryselectorpitchvalue var rate documentqueryselectorrate var ratevalue documentqueryselectorratevalue var voices function populatevoicelist voices synthgetvoices for i i voiceslength i var option documentcreateelementoption optiontextcontent voicesiname voicesilang if voicesidefault optiontextcontent default optionsetattributedatalang voicesilang optionsetattributedataname voicesiname voiceselectappendchildoption populatevoicelist if speechsynthesisonvoiceschanged undefined speechsynthesisonvoiceschanged populatevoicelist inputformonsubmit functionevent eventpreventdefault var utterthis new speechsynthesisutteranceinputtxtvalue var selectedoption voiceselectselectedoptionsgetattributedataname for i i voiceslength i if voicesiname selectedoption utterthisvoice voicesi utterthispitch pitchvalue utterthisrate ratevalue synthspeakutterthis inputtxtblur play
60337234,grant permissions to the service account error,service googlecloudplatform permissions texttospeech invalidargument,as per the google documentation you need to provide member and role value your command should look like below also make sure you are using the latest gcloud package
59349720,windowspeechsynthesisspeakmsg not working until button click,javascript googlechrome texttospeech speechsynthesis,this may be due to the browser itself recent updates in some browsers firefox and chrome have policies to prevent audio from being accessed unless some user interaction triggers it like a button click
59151571,azure cognitive services text to speech and audio issue in ie invalid source,azureblobstorage texttospeech htmlaudio azurecognitiveservices,ok i found the issue here as i was checking the texttospeech api docs further i saw there is one output parameter in request header xmicrosoftoutputformat defines the type of audio that will be returned from the api since i was following the api samples the xmicrosoftoutputformat it used was riffkhzbitmonopcmthis should be in alignment with the audio type that we want to save and play back other browsers were able to probably convert it and play back properly but ie used to always give invalid source error in my case changing it to audiokhzkbitratemonomp worked fine and it is saving and playing back the audio properly in ie as well hope this helps someone
58842210,attributeerror enter while trying to take input from microphone,python speechrecognition texttospeech attributeerror microphone,the same kind of problem happened to me and i got it solved by coding it up like this i hope this helps solve your problem if you still are having some problems let me know
58781400,react native tts error texttospeech is undefined,reactnative texttospeech,needed to rebuild the app through gradle reactnative runandroid
57667357,speech synthesis problem with long texts pause midspeaking,javascript texttospeech,ive recently stumbled upon the issue where speech is cut off after a duration of seconds as well this causes for the synthesis to get stuck in speaking mode never marking it as done and thus never actually being able to check when it finishes talking this also makes for all speechsynth options to be blocked until you restart your browser this bug only seems to happen when you use the promise options to fetch the voices and languages if you dont need to option to select and just preset them it seems to be working as should at least for me the solution of frazer didnt work for me until i also added a pause right before the resume to make it stop for a very short moment and then continue this cancels the max second bug with this small tweak added the code from frazer would look like
57381727,speech recognition in python pyaudio needs visual studio error,python pythonx speechrecognition texttospeech pyaudio,this problem was solved a long time agoo why you do not search on google before to ask go to this commend and click on the link what is there and install it manualy the link what you will need to found is something like thispeoplecsailmiteduhubertpyaudiopackages
57158760,azure cognitive services tts an error for other language in sample app,c azure aspnetcore texttospeech,i have reproduce your problem successfully as per my findings there are some key points to language supports like subscription free subscription doesnt allow all language region of support var requestbody thisgeneratessmllang female thisservicenamelang content note language that doesnt support female also can be the issue your case in your code you could try with below code at languagepreference list new selectlistitem value eses text esesspanish also change on below code under servicename i have tested with spanish which works fine valuesaddeses microsoft server speech text to speech voice eses laura apollo point to remember our code base is alright problem is not related to code please be sure your subscriptions and region support translation you are trying can also rise your support ticket on azure portal
56620040,python pyttsx error pickleunpicklingerror invalid load key x,pythonx texttospeech pyttsx,i had this problem as well and fixed it by deleting genpy in my temp directory you can find this folder here
55660915,textspeech error how can i play audio by entering url into browser directly,php html text texttospeech speech,if your post code works you can change post to get as seen below indexphp vphp to prevent spaces from breaking your url use urlencode function in php
55254673,avspeechsynthesizer not working when new strings are added to be spoken xcode swift,swift xcode texttospeech speech avspeechsynthesizer,you can simply use avspeechsynthesizerdelegate for that and you can remove timer from your code and to use avspeechsynthesizerdelegate first you need to confirm your view controller with avspeechsynthesizerdelegate like shown below next thing is you need to add in your viewdidload method and you need to declare outside the methods and inside the class and you can use randomelement property to find random element from string array and your final code will look like edit since you are using only elements in your array there will be possibilities to repeat same sentence many time when you take random string for it so you can add one more logic here which will prevent it update your speaktome function like below and declare var previousstr outside the function
54514798,ios avspeechsynthesizer pause and continuespeaking issue,ios swift swift texttospeech speechsynthesis,i implemented the following code to check your problem testings under mojave ios xcode and swift i noticed another problem with the boundary word that doesnt always pause when triggered but when this constraint is changed to immediate everything is resuming from where it paused however when it rarely pauses with the boundary word it always resumes from where it paused as well i dont know where your problem comes from but with the configuration mentioned above and this code snippet the speech synthesizer resumes from where it paused
54111384,speechsynthesizerselecvoicestring not working,c texttospeech,i managed to solve this with this piece of code synthselectvoicebyhintsvoicegenderfemale voiceageadult cultureinfogetcultureinfoptbr aparently microsoft solution is not working for me well that works ill leave here to help other people with this kind of problem thanks
53009032,avspeechsynthesizer is not working after use one time,ios swift texttospeech,you need to set the avaudiosession for performing such tasks here is my working code hope this helps
52569003,error in hybridsegmentation hmerror when running htk,texttospeech cmusphinx htk,this looks like a case of missing files are you sure youve set the pwd present working directory to what the build guide says it should be and are you sure you have all the files
52555027,python gtts error attributeerror nonetype object has no attribute group,python texttospeech googletexttospeech,there is an official fix now it had to do with an upstream dependency of gtts gttstoken it has been fixed in gttstoken the issue was fixed after i did a fresh install of both gtts and gttstoken now it is working thanks to open source gods and carreycole link
52263407,text to speech for angular apps allowjs is not set issue,javascript angular typescript texttospeech,if you want to import the module at runtime but not check it with typescript try removing the js extension from the import path
51809769,pyttsx not working on freshly installed windows,python windows texttospeech pyttsx,seems like winapipyd which really is dll is not a win application so i think its loading bit drivers to bit runtime in this case i will try to uninstall bit python delete its folder and install a bit version i believe this will solve your problem since you dont really need bit python now the problem is in winpyds so it seems like your pyttsx is bit according to pypiorg you can try pip install pypiwin to fix both win and winapi errors
50654691,hybrid batchvbs tts script not working,batchfile vbscript texttospeech,you can embed the code directly into the batch script without using a temp file this will increase the speed of the script as there will be no redundant io operations you even can use the sp voice objects in a one line
50264987,looper threads issue but not using animator,android xamarin texttospeech,place your ui changes within a runonuithread action ie
49870351,plyer tts not working on windows,python texttospeech,i had the same issue with a notimplementederror it means that the texttospeech enginesoftware that its looking for is not being found on the machine although plyer worked for me with the native tts on mac os x on windows you have to download tts software as i guess plyer isnt set up to find the native one anymore or its not named the same as it used to be if you say it worked in the past fix it by downloading espeak from install it and move the espeakexe file somewhere python can find it for me the easiest thing was to move it to the toplevel folder of the script i was trying to run
49378318,text to speech not working,ios objectivec iphone texttospeech,finally this solved it session category must be avsessioncategoryplayandrecord if tts and stt is continuously working and the function must be called with set options parameter
48727085,ios tts error couldnt set footprint on tts instance,ios texttospeech,i would ignore it its about apples code not yours and your functionality isnt being affected if youre feeling restless you could file a bug with apple chiefly because the framework shouldnt be dumping these messages into your console where they crowd out your own log messages they do this a lot and its really annoying
48377561,texttospeech stop not working,java android texttospeech,stop speak on activity pause
48273711,text to speech not working for other languages except english,android texttospeech googletexttospeech,i think you need to see if you have the french text to speech engine for french on your phone using a method described here google text to speech there is also a device setting for the language described here
46863651,attributeerror module pyttsx has no attribute init,python texttospeech pyttsx,it appears that the module pyttsx is not properly initialised i hope you dont have a file named pyttsxpy anywhere in the module path i found a related issue here
46746513,text to speech for bangla not working,android texttospeech googletexttospeech,its working try to set language work fine for me override public void oninitint i ref for language what is the list of supported languageslocales on android thanks u all
44479883,marytts exception noclassdeffounderror comgooglecommonbas ejoiner,java texttospeech marytts,this is very very strange from the side of marytts team they seem to ignore this error happens solved this by adding guava on the project class path also i see nowhere on the depencities this to be mentioned
43835002,text to speech speech method is not working,java android texttospeech,just check the documentation for the correct form this method was deprecated in api level as of api level replaced by speakcharsequence int bundle string and if that isnt working then the problem lies elsewhere
43727899,error when using jadepug to output audio on a webpage,nodejs texttospeech ibmwatson,add the following line in your appjs appuseaudiofiles expressstaticpathjoindirname audiofiles then shouldnt give error
43580030,debug sapi text to speech engine,c texttospeech sapi nvda,you have to debug nvda not engine you need to figure out which class does it try to access once you figure that out you need to check why you didnt properly register your engine in com registry for more details see error class not registered exception when initializing vcprojectengineobject microsoftvisualstudiovcprojectenginedll
43527859,python subprocess call throws error when writing file,python linux python ubuntu texttospeech,from the documentation providing a sequence of arguments is generally preferred as it allows the module to take care of any required escaping and quoting of arguments eg to permit spaces in file names if passing a single string either shell must be true see below or else the string must simply name the program to be executed without specifying any arguments so you might try with
43184436,the speack function ie articulate js which is not working,javascript html texttospeech articulatestoryline articulate,you should add jquery at the beginning
42712298,error in text to speech code,android texttospeech voicerecognition,change the below liine of code change to reason of problem you are passing the context this in both the parameter of texttospeech therefore this problem occurred second this is the listner of the texttospeech
42604456,festival audio device problems no sound output,texttospeech festival,you simply need to configure pulseaudio add these lines to the end of your festivalrc file or to usrsharefestivalfestivalscm
42536570,android tts speech synthesis error when screen is lock,android texttospeech googletexttospeech,try recreating the tts in the onresume function of your app that is to move the init function of tts to onresume
42523243,bing text to speech not working in android,java android texttospeech bing azurecognitiveservices,i solved the problem by using xmldom class to get the ssml string body xmldomcreatedomdevicelanguage gendername voicename your text here byte xmlbytes bodygetbytes urlconnectionsetrequestpropertycontentlength stringvalueofxmlbyteslength update after using xmldom class to get the ssml i found that the ssml need to specify xmllangyoulanguagehere in voice tag eg
42020544,swift sfspeechrecognizer not working twice,ios swift speechrecognition texttospeech,i think you are missing recognitiontaskcancel before you dealloc task in stoprecording function
41695104,importerror no module named driver in pyttsx,python pythonx texttospeech pyttsx,well the problem seems to be addressed in the following post import pyttsx works in python but not in python can anybody enlighten me if there is any pyttsx library that is written in python x yes please use the following version it is a python port of pyttsx which seems to address the problems you face and targets python x versions for example the error you see importerror no module named drivers is addressed by the following commit which was merged in the aforementioned repository to install the pyttsx python module you can or or or do that inside a virtual environment if you use them you can avoid using sudo to install depends on the environment you use and how you organized the packages locations etc of course use the right python python and pip pip as you have in your environment please remove and clean the previous pyttsx package you had there in the environment in addition you can visit for more details on that
41126494,text to speech not working on head unit and dhu,android texttospeech androidauto,speak method requires four parameters to execute the job i solved this using below code
40937159,issue with watson and,ssl apache texttospeech tomcat ibmwatson,i believe i found the cause of the problem after more research and debugging i realized it had nothing to do with watson apache or tomcat as i indicated in an edit to this question i posted another question more specific to javaxnetssl here intermittent javaxnetssl failure badrecordmac the answer i posted there contains the details of my findings but the bottom line is it appears the cause was a bug in java i fixed it by disabling all diffiehellman cipher suites in the javasecurity file
40773042,html speech not working on safari mac typeerror,javascript html safari texttospeech,okay finally figured it out i had some compatibility code to support browser without html speech this works on chrome and firefox but on safari it seems that any function in any script is evaluated when the script is parsed so the function get declared even though speechsynthesisutterance already exists guess ill need to do this differently
40663130,speech synthesis on ios weird errors on loading and no concurrency,ios swift avfoundation texttospeech speechsynthesis,as for its probably not gonna happen the speech synthesizer is a system shared resource so how the system handles scheduling multiple requests is out of our control as clients of the api note that if you reuse the same synthesizer it queues up extra utterances but if you create multiple synthesizers it fails to speak utterances that are requested while another synthesizer is speaking dunno about sorry looks like diagnostic text not necessarily an error probably worth filing a bug about since they probably dont want to be logging diagnostics when theres no actual problem bonus answer you can use functional programming to make the selection of voice a bit shorter
39360629,watson text to speech not working in chrome,java googlechrome texttospeech ibmwatson,sorry to waste anyones time looking at this question i played around with the speaker settings it was set to surround with speakers and a subwoofer i only have speakers and a subwoofer so i took out the other speakers and the test file now works in chrome as does the watson tts demo weird though that the speaker settings didnt affect firefox thats wasted hours of frustration im never going to get back
38991025,how to resolve attribute error in python,python python object texttospeech,you have to set if first before you use it anyway dont do any advanced logic in constructors its not a good practice make a method instead
38958899,pyttsxinit not working in windows,python python texttospeech,yes its fine it works perfectly
38189386,texttospeech setlanguage not working,java android locale texttospeech googletexttospeech,you need to set the language once the text to speech engine has initialised correctly that should do it
37287261,swift ios text to speech not working with delay in loop,ios swift delay texttospeech,how about something like this notes an array of tuples is passed to speak a tuple pair contains a phrase to speak and a delay to wait before the next phrase is spoken speak takes the first item from the array speaks the phrase and passes the rest of the array if not empty to speak again after waiting for the delay delay was written by matt and comes from here since the last delay does nothing useful you can turn it around and have the first phrase spoken after a delay you would use this one like this
36485170,speechsynthesis not working to speak in portuguese ptbr,texttospeech speechsynthesis webspeechapi googlespeechapi,thanks for your reply bruno i got this situation solved the day next i posted the question but could not post the solution here i solved this situation using this once onvoiceschanged is asynchronous this made everything work fine now even ive already got it solved im very grateful for your reply thanks a lot best regards ulisses
36377342,how to fix speech synthesis utterance in javascript,javascript texttospeech speechsynthesis,your problem seems to be not treating the web speech api asynchronously the following snippet is not needed for now but i want to point out that it looks like you never change the voicename variable anywhere so the if statement looks unnecessary heres one way youll get the voice you want every time notice my changes with comments
35729495,ibm watson texttospeech curl example not working,ibmcloud texttospeech ibmwatson,voice is a url parameter the correct curl command looks like this disclosure i am an evangelist for ibm watson
34130277,speechsynthesisutterance chrome issue,javascript googlechrome texttospeech speechsynthesis,getvoices is asynchronus mentioned in the spec errata and you need to listen to the voiceschanged event the reasoning is a little frustrating but makes sense the voice system is lazily loaded and the first call to getvoices would block the main thread so the first call on android returns results and then fires an onvoiceschanged when the voices become available the linked demo incorrectly sets the voice attribute on the utterance this doesnt exist instead you need to change the lang and optionally the voiceuri to change the default used voice as can be seen below and in the new demo
33338402,texttospeech with gcm not working,android googlecloudmessaging texttospeech,i find the solution not the best but it works run an infinite loop inside onmessagereceived this makes tts not being destroyed when tts finishs its work jump out the loop
33016853,presentmediaplayercontrollerwithurl not working with google tts api,ios texttospeech watchkit applewatch watchos,from watchos documentation place media files that you download from the network or transfer from your ios app in a shared group container a shared group container provides common storage for your watch app and watchkit extension in your extension code create urls for any media files inside the container and use them to configure the media interfaces you need to to enable app groups in capabilities and set up a shared group container then use the container url to place the downloaded speech audio and to play using presentmediaplayercontrollerwithurl example
32200199,aws java nosuchmethoderror using ivona text to speech,java amazonwebservices texttospeech nosuchmethoderror,your error is nosuchmethod i suspect that there is a version mismatch check from ivona which version of aws you need to use according to ivonas pomxml it is
31099157,speechsynthesis api not working,javascript googlechrome texttospeech speechsynthesis,hmmmwell the snippet youve posted here on this page works for me yet the fiddle containing the same code doesnt however if you change the nd dropdown to no wrap in then its just fine this is because jsfiddle wrapped your code into a function that was called when the document loaded like this by doing this code outside of the windowonload handler including the inline js in your html cant see your speak function by changing the dropdown jsfiddle generates different js for the iframe that shows your result like this
31098286,simple webkit speech synthesis test not working,googlechrome webkit undefined texttospeech,apparently the file needed to be tested on a hosted site as opposed to remotely draganddropintothebrowser style so the api could access the internet this fixed the speechsynthesisutterance objects text attribute from being undefined testing on a hosted site revealed strange output the female voice read aloud undefined does this work does this work does this work it works after all it works after all it works after all in succession in other words the sample utterances i had forcibly assigned during testing one by one remotely very odd that that was somehow cached
29953537,c speech synthesis issue,c wpf texttospeech speechsynthesis,stewartr psudonym and ebrown all helped answer this question debug settings needed to be changed to reflect how i wanted certain exceptions to be handled and the try catch method was the better method to use instead of the if else statements i had to make sure i caught the specific exception i wanted in this case it was argumentexception thank you guys for your help i really appreciate it
29611993,my tts is not working in android lollipop,android androidlollipop texttospeech flite,after hours of struggling i finally found that using underscore in locale names like fasirnmaleodj which was used in voice file names confuses android and it cannot parse language country and variant parts out of it it seems android treats and both as separators in locale names and since it expects at most parts for a locale definition language country and variant falls in trouble with something that has parts
29198999,speechkit not working on iphone bit,ios xcode texttospeech iphonebit,to support speech kit you need os and above then and then only you can able to run speechkit on iphone
28806399,speechsynthesisutterance not working in mobile broswer,mobile browser texttospeech speechsynthesis webspeechapi,i have seen several mentions like this for example that speak only works when called from a user interaction such as a click from what i have seen that seems to be the case for safari on ios and that appears to be what you are describing in your question i have not actually found any kind of documentation that confirms this behavior and would appreciate any references anyone can provide
26079050,ivona request signing issue signature does not match aws signature version,c rest amazonwebservices texttospeech,solved actually there is an issue in documentation example so the code works fine
25262733,google text to speech for hindi language not working now,android texttospeech hindi googletexttospeech,google text to speech engine got updated today and hindi is now supported for online synthesis as well as offline synthesis
25186270,audible errors custom nonblocking texttospeech handler for pythons logging module,python logging raspberrypi texttospeech,i know this was asked ages ago i missed it when it was posted and stumbled on it by chance the following works for me when i run it i hear hello and then goodbye spoken
24560210,ios text to speech api not working,ios iphone objectivec ios texttospeech,you are creating your synthesizer and then your method ends and the synthesizer vanishes in a puff of smoke before it can do anything if you want it to speak retain it eg in a property so that it lives song enough to do so
23645404,simple android text to speech proof of concept not working,android audio texttospeech,i have had bad luck using ttssetlanguagelocaleus heres the code i use instead when this code runs on my samsung s phone it uses the default locale instead of localeuk or localeus try removing the ttssetlanguagelocaleus and the tests for result another possibility is that ttsspeak must be called from the ui thread im just guessing that i dont know for sure ive noticed many android apis dont specify what calls must be made from the ui thread you might try the following code instead of the call to speakout
23483990,speechsynthesis api onend callback not working,javascript googlechrome texttospeech speechsynthesis googletexttospeech,according to this comment on the bug mentioned in the answer from kevin hakanson it might be a problem with garbage collection storing the utterance in a variable before calling speak seems to do the trick
22702987,android texttospeechspeak not working in onactivityresult,android texttospeech onactivityresult,you shutdown the texttospeech in onpause thus mtts is not binded to the text to speech engine anymore you need to move your code in onpause to onstop if what you do is to only to show the recognizer dialog when the speech recognizer dialog is shown your activity onpause is called but not onstop unless your activity is not visible anymore you should also instanstiate the text to speech again in onstart
22488558,texttospeech function not working,java android texttospeech,there are several problems with your code you should check for error returned by setlanguage you cannot call speakout until oninit has been called one way you can insure this is to disable the talk button in the xml layout file and enable it in oninit
22367203,phonegap tts plugin android not working,cordova texttospeech senchatouch,after some struggle i have the tts working but there is still one issue i had to manually fix following are the steps to get the tts working install the plugin like below once installed and built add this plugin to the phonegap configxml file if you are building the app using sencha touch the configxml will be in the root folder this will add the plugin to the final build now to start the tts service and speak some text use the following snippet the issue i had was the ttsstarted in the startupwin is not defined in the plugin i just used the constants value and the plugin works perfectly
21840688,utteranceprogresslistener not working for android tts,java android texttospeech,ok i solved it the problem was that i failed to pass the keyparamutteranceid i was passing the parameter parameter as null
21321844,text to speech producing error,java android texttospeech,you created the speakactivity activity but in the code you have posted you never started the activity because the activity was never started oncreate was never called and therefore tts is not initialized that is why you get a nullpointerexception when you call in speakout see this stackoverflow question for more information about when oncreate is called is oncreate called when an activity object is created
20878449,synthesistofile not working,android texttospeech,please declare this permission in your manifest
20035628,stdafxh and sapih not working correctly in c using codeblocks,c texttospeech voicerecognition,microsoft tutorials assume you are using visual studio stdafxh is created by the visual studio wizard as part of the precompiled header configuration you can probably replace with plus the other stuff the tutorial said to put into stdafxh then youll have to make sure the windows sdk header directory is listed in your include path you did install the sdk right
19635879,jquery mobile function not working page refresh,javascript jquery jquerymobile refresh texttospeech,youre not preventing the submit button from submitting the page and as youve most likely set no destination its submitting to the same location causing a page refresh preventing the click event can be done in jquery by returning false at the end of the function demo to play with
19376601,weird jquery quotes issue,javascript jquery twitterbootstrap texttospeech bootstraptypeahead,you need to escape the quotes in your onclick or else use apostrophes also if youre using jquery you dont need an onclick handler at all
19221650,error in building speech tools for festival under gcc in mac osx,macos gcc makefile texttospeech,just had to update gcc and everything was okay the gcc in apple was or something before that and i installed macports and installed gcc gcc version experimental macports gcc
18951632,google text to speech api not working on php,php googleapi texttospeech fopen fclose,you need to add the user agent string so you arent told to be coming from curl your code in the loop should be like this
17672539,text to speech function is not working in my listview,java android texttospeech,you are getting npe you have this below and you have this your btnaudioprayer is not initialized until list item click when you click button in each row of listview it causes npe cause your actually clicking button and not listview row items use a custom simplecursoradapter in your filldata method speakout your class implements texttospeechoninitlistener
17410569,texttospeechstop is not working,java android texttospeech,you should set a class member flag and then check the flag in onutterancecompleted when the event finished just set mshouldspeak to false
17123410,android texttospeech addspeech is not working,android texttospeech,ok i found my problem very silly situation which wasted several hours of mine i hope it will help if someone makes my mistake we should postpone this mapping of texts to the point tts is successfully initialized for example in oninit function
16850966,text to speech not working in android,android texttospeech,you call stopself before speak method has a chance to speak you should implement onutterancecompletedlistener and call stopself inside onutterancecompleted also speakouttmptaskbrief should not be called inside shownotification method since speak method only works after oninit is called
16353475,errors are generated when sapi is run in qt,c windows qt texttospeech sapi,should be theres an extra semicolon in your version
15691031,android text to speech not working w japanese,java android texttospeech,most android devices do not come with japanese language built in you will have to install a third party language pack such as svox after installing it the next time you open your app it should prompt you if you want to use the android system or the new svox text to speeh if it does not you will have to change the default settings in app language and input texttospeech once this is done it should be able to speak japanese you will not have to modify your code
12240855,texttospeech setspeechrate not workingchanging,android texttospeech,i discovered that the thunderbolt has a global setting to always use my settings which overrides any changes to texttospeech by apps it is in the devices settings then in voice input output and then texttospeech settings you can check this by calling texttospeecharedefaultsenforced the reason this worked fine in ics was because as of the ice cream sandwich release user settings never forcibly override the apps settings
12121833,lexical or preprocessor issue while using fliteiphone library for tts,objectivec ios texttospeech,the problem is that the compiler cannot find fliteh looking at sfosteriphone tts it seems the file is supplied in the fliteiphoneinclude directory to make the compiler find the file add the directory where fliteh is in eg srcrootfliteiphoneinclude to the projects header search paths see this so qa for details on how to modify the path
8520662,a possible bug in nsspeechsynthesizer,objectivec macos cocoa texttospeech,ok i have found the source of the problem and a workaround it has nothing to do with the language as such but the fact that most nonenglish voices in os x lion are nuance voices made by nuance communications i have confirmed this by testing with english nuance voices and they indeed have the same problem it looks like there is something wrong in the api for voices provided by nuance i have created a workaround for the problem by instantiating a new nsspeechsynthesizer object after the reading has been stopped its not pretty but it works
8496153,text to speech doesnt seem to be working and there is no error message in logcat,android texttospeech,your code appears to be very similar to mine the only difference i can see is i call speakspeaktest texttospeechqueueflush null you call speakspeaktest texttospeechqueueadd null
7674482,speechsynthesizer in aspnet async error,net aspnet asynchronous texttospeech speechsynthesizer,you should probably move the code above into the pageload method theres no real reason for doing what youre doing in prerender if you make the page async then you need to change your programming style see if this helps example of asynchronous page processing in aspnet webforms net
7647294,issue with tts in android service isnt started,android texttospeech,ok changed class name problem was solved and
7425272,freetts portability issue,java texttospeech freetts,i had the same problem and i found this page i did the easy way which was to add the freetts installation to my dist folder change the manifest of my jar on where to find the jsapijar freettsjar etc and it is worked i am just distributing freetts inside my folder the hard way is on the link above
7185282,texttospeech stop issue,android texttospeech,it is hard to tell without looking at your code what you are doing wrong but you should be able to call texttospeechstop or texttospeechshutdown in your onpause and make that work it is possible the stop fails for any number of reasons and if it does then youre just out of luck this works for me consistently on different models of android device mtts is my texttospeech instance
7025616,error when trying to call texttospeechactivity,android androidactivity texttospeech,have you declared your activity in the androidmanifestxml
6261379,text to speech problem with incoming call in android,android texttospeech,you can see the incoming call notification from link and stop the tts as below where mts is
6068139,google translate tts problem,html audio texttospeech googletranslate,the reason this isnt working is most likely because translategooglecom restricts certain types of requests to prevent the service from being overloaded for instance if you use wget without the u mozilla user agent option you will get an http because the service restricts responses from wgets default user agent string in your case it looks like what is going on is that translategooglecom is returning a http if a http referrer is included in the request when you run wget from command line there is no referrer when you use the audio tag from within a webpage an http referrer is provided when requesting the translation i just tried the following and got a however if you take the referer option out it works
5949451,soundeffect raised an error while assigning buffer from speechlib,c silverlight windowsphone texttospeech,after some test it works by setting channel mode it works by setting channel mode to mono view the difference between these at mono vs stereo
5680248,android tts not working in device,android texttospeech,on my captivate i had to manually install prompted by google navigation the tts data files to enable speech directions this may have been because the rom didnt include them i cant recall if the stock rom had the speech files or not see this link for how to detect the presence of the right files basically and check the result should be checkvoicedatapass
5221575,synchronization problem for sapi or text to speech c,c texttospeech sapi,you need to do the next line after the previous line finishes using the ttss completion event
5101903,problem building festival text to speech on ubuntu,c ubuntu texttospeech gnumake festival,you probably need to install the development packages for libncurses try
4809291,text to speech is not working in countdowntimer,android eclipse texttospeech,you need to initialize tts within your activities oncreate or onresume method see also how to wait for texttospeech initialization on android
3100095,festival cc api compiling an example linking libraries error,c api texttospeech festival,replace lib with l for instance libfestival wont work do
3091100,make test error festival windows xp cygwin,windowsxp cygwin texttospeech festival,one possible solution is move the folder to c i think that this is not the best but it worked
2683924,texttospeech setonutterancecompletedlistener always returns error,android texttospeech,naturally after being stumped for more than a day i stumbled onto the answer mins after i asked the question on here the answer the onutterancecompletedlistener can only be assigned to the texttospeech object after the tts oninit fires i was trying to set the listener immediately after creating the tts instance i moved setonutterancecompletedlistenermylistener to my oninit code and now it returns result code success imo the texttospeech setonutterancecompletedlistener documentation lacks this detail and should be updated
1992452,debugging a scheme program,scheme texttospeech festival,look at how parameter is internally represented if it is an association list you can just print it you can also try these free scheme debuggers schemeide psd
1691306,strange problem with systemspeech speech synthesizer,net windows texttospeech,can you compare with windiff both csproj file and look if they use the same stuff for the references maybe one use a local filein his own sub directory while the other use a system file
552436,why am i getting a systemsecuritypermissionssecuritypermission error in my net application,c net exception net texttospeech,the system is saying that there has been an attempt to do something that is not allowed according to the current policy more information is needed to give good advice here what is the full text of the exception message there are a dozen or so situations that can cause a securitypermission so this is important specifically there is a flag field that indicates the exact nature of the security violation what is the environment are the files stored locally or on a network share apps run from a network share have always been partialtrust prior to sp do you have net sp installed
254930,question speechsynthesizersetoutputtoaudiostream audio format problem,c audio speechrecognition texttospeech,its entirely possible that the lh michael and lh michelle voices simply dont support hz sample rates because they inherently generate samples hz sapi allows engines to reject unsupported rates
79526058,error when trying to install gensim via pip on windows,windows pip gensim mesonbuild,from your error messages you seem to be using python but the stable release of gensim ie currently has python wheels for all major platforms theres no support yet for python you can downgrade to python to install this package
79241502,python error trying to install gensim on macos,python macos scipy gensim openblas,gensim has precompiled wheels up to python your easiest resolution is to downgrade python from to so you can use one of those precompiled wheels gensimcpcpmacosxarmwhl thats a detail you dont need to care about pip install will choose that wheel when compatible so you dont need to have an environment in which you can compile gensim from source you might also want to raise an issue on gensims github so thered be wheels for in the future
78279136,importerror cannot import name triu from scipylinalg when importing gensim,python scipy gensim,i found the issue the scipylinalg functions tri triu tril are deprecated and will be removed in scipy scipy release notes deprecated features so i installed scipy v instead of the latest version and it was working well pip install scipy
75490275,gensim pickle error enable to load the saved topic model,pickle gensim lda,another report of an error about randomstatector at suggests the problem may be related to numpy object pickling is there a chance that the configuration where your load is failing is using a later version of numpy than when the save occurred could you try at least temporarily rolling back to some older numpy thats still sufficient for whatever gensim youre using to see if it helps if you find any load that works even in a suboptimal config you might be able to nullout whatever randomrelated objects are causing the problem and resave then having a saved version that loads better in your trulydesired configuration then if the randomrelated objects truly needed after reload it may be possible to manually reconstitute them i havent looked into this yet but if you find any workaround allowing a load but then arent sure what to manually nullrebuild i could take a closer look
74392926,problem with training wordvec after opening csv,python pandas machinelearning gensim wordvec,first itd help to name the variable holding data thats come from a different place different from the original data for clarity of referencecomparison for example instead of loading your saved data as give it a distinctive name instead then check whether your original series data later datafromcsv actually look the same as the exact same type of objects in each item of the iterable corpus to wordvec for example to look at the st item in each iterable object look at and also for comparison if these arent identical then your second datafromcsv case isnt showing wordvec the same type of corpus in particular each individual text item in the wordvec training corpus should be a python list of individual string tokens if you instead pass it only strings it will instead see each individual item in that string single characters as if they were string tokens resulting the problem youve described all the models known words are singlecharacters are you sure you didnt see a warning in your logsconsoleoutput to that effect on the nd run the buildvocab step checks if the st item in the corpus is a plainstring rather than the proper list and prints a warning when it is make sure to do one or the other of write the csv as spacedelimited texts not python list literals then resplit into a list after csvreading the raw strings or write the csv as python list literals eg with brackets and stringquoting but then also after loading those literals as raw strings interpret them as python objects as if using an eval to turn them back into listsoftokens then the corpus youre feeding to wordvec will be of the right format to get the intended individual words option above is usually the preferred approach spacedelimited strings are a simplersaferfaster format to later reparsesplit with no risk that and eval could potentially run unintended code as a totally separate issue using odd nondefault values like alpha minalpha usually indicates youre following a bad tutorial thats changed these for no good reason such a change is unlikely to either hurt or help your results much its just an odd unnecessary choice hinting at random guesswork unthinking copying rather than true understanding
72519612,gensim keyerror word not in vocabulary,python gensim wordvec,if you get a keyerror telling you a word isnt in your model then the word genuinely isnt in the model if youve trained the model yourself and expected the word to be in the resulting model but it isnt something went wrong with training you should look at the corpus purchasestrain in your code to make sure each item is of the form the model expects a list of words you should enable logging during training and watch the output to confirm the expected amount of worddiscovery and training is happening you can also look at the exact listofwords knowntothemodel in modelwvkeytoindex to make sure it has all the words you expect one common gotcha is that by default for the best operation of the wordvec algorithm the wordvec class uses a default mincount wordvec only works well with multiple varied examples of a words usage a word appearing just once or just a few times usually wont get a good vector and further might make other surrounding words vectors worse so the usual best practice is to discard veryrare words is the pseudoword in your corpus less than times if so the model will ignore it as a word toorare to get a good vector or have any positive influence on other wordvectors separately the site youre using example code from may not be a quality source of example code its changed a bunch of default values for no good reason such as changing the alpha and minalpha values to peculiar nonstandard values with no comment why this is usually a signal that someone who doesnt know what theyre doing is copying someone else who didnt know what they were doings odd choices
72502665,googlenewsvectorsnegativebin cannot be loaded in gensim models memoryerror,python gensim,you are getting a memoryerror because your system lacks enough ram to finish the operation the googlenews vectors are over gb on disk and require more ram than that to load into the python object heap even if you were doing nothing else on the same machine its doubtful you could do much with them on a system with gb of ram youd need gb or more depending on what else is using memory on the machine and in your python processes if the same step was succeeding a few days ago it is certain that the system you were using then even if the same system as now had more free memory at the time you attempted the load then compared to now your options are move to a system with more ram perhaps by upgrading the one yo are using load a smaller set of vectors the gensim keyedvectorsloadwordvecformat method takes an optional limit parameter which only reads exactly that many words from the front of the supplied file as the googlenews model includes million words using something like limit loads jut th of the words and thus uses about th of the ram thats still a ton of words and as such models typically list the mostfrequentlyused words first a limit only discards lessfrequentlyused words sometimes with naturallanguageprocessing discarding more of the rare words can even improve results on common tasks rarer word sensesofmeaning can vary more their vectors are often lowerquality as theyve been trained on fewer examples and yet altogether they are quite numerous overall sometimes making their inclusion cost more in model size and processing time than any incremental meaning they deliver separately and unlikely to be a major factor in your issue it appears youre using a yearsold version of gensim generally efficiency with regard to memoryusage and taskruntimes will improve with later versions so no matter how you get around this particular memoryerror you should generally prefer to use a current version of gensim such as as of this writing in june
72458031,i get an attributeerror the vocab attribute was removed from keyedvector in gensim when i try to load google news vector embeddings,python flask gensim,the load succeeded the failure was in your line of code that tried to report lenselfwordvecvocab let me quote the error message for the reason that your code couldnt access a vocab property the vocab attribute was removed from keyedvector in gensim use keyedvectors keytoindex dict indextokey list and methods getvecattrkey attr and setvecattrkey attr newval instead see so you cant use vocab anymore but there are several new properties listed there like keytoindex a dict like vocab was or indextokey a list of all lookup keys words in the setofvectors have you tried using any of those specific properties recommended in the error message you received instead of vocab or visiting the recommended url which makes specific suggestions with before and after code examples how to replace references to the nolongeravailable vocab attribute here are the relevant lines of things not to do and to do instead for your case vocablen lenmodelwvvocab vocablen lenmodelwv
71295840,python pip error legacyinstallfailure,python pip gensim,if you fail to install plugins you can download it from other repositories like this one repository depends on the version of python and the system for example for windows x and python you should take this file gensimcpcpwinamdwhl
68577177,lda mallet gensim calledprocesserror,python gensim,as silly as this sounds i resolved this by changing the path to so if you have the mallet directory in the same one as where your code is this will work
68350237,gensim wv unicodedecodeerror utf codec cant decode byte x in position invalid start byte,pythonx utf gensim wordvec,in utf multi byte sequences start with a byte having bits xxxxxx and having continuation bytes xxxxxx the error states that a continuation byte x was encountered at a start position this can happen on nonutf textbinary data or when carelessly text is read buffered and a buffer begin or end splits a multi byte sequence the limit could break the decoding but i would consider checking the data first you could try iso to check what happens when the decoding passes silently x at position reeks of wrong datawrong algorithm
68048018,dutch pretrained model not working in gensim,gensim fasttext,are you sure youre using the latest version of gensim with many improvements to the fasttext implementation and there you will definitely want to use loadfacebookmodel to load a full bin facebookformat model but also note the posttraining expansion of the vocabulary is best considered an advanced experimental function it may not offer any improvement on typical tasks indeed without careful consideration of tradeoffs balancing influence of later traiing against earlier it can make things worse a fasttext model trained on a large diverse corpus may already be able to synthesize betterthannothing guess vectors for outofvocabulary words via its subword vectors if theres some data with verydifferent words wordsenses you need to integrate it will often be better to retrain from scratch using an equal combination of all desired text influences then youll be doing things in a standard and balanced way without hardertotune and hardertoevaluate improvised changes to usual practice
67922777,ploting function wordvec error wordvec object is not subscriptable,python gensim wordvec,in gensim the wordvec object itself is no longer directlysubscriptable to access each word instead you should access words via its subsidiary wv attribute which holds an object of type keyedvectors so replace modelword with modelwvword and you should be good to go
67687962,typeerror wordvec object is not subscriptable,pythonx jupyternotebook gensim wordvec,as of gensim higher the wordvec model doesnt support subscriptedindexed access the to individual words previous versions would display a deprecation warning method will be removed in use selfwvgetitem instead for such uses so when you want to access a specific word do it via the wordvec models wv property which holds just the wordvectors instead so your unshown wordvector function should have its line highlighted in the error stack changed to
67426039,gensim update to wvecattributeerror int object has no attribute index,python gensim,try using wordvecwvvectors instead of wordvecwvsyn thats the array holding the raw vectors keyedvectors hasnt had a true syn for a while but if it might have had a backwardcompatibility alias at the time your code was st crafted
67261993,i had a problem using wordvec maybe its a version problem but i dont know how to solve it,python gensim wordvec,the wordvec model object itself wv in your code no longer supports direct access to individual vectors by lookup word key in gensim and above instead you should use the subsidiary object in its wv property an object of type keyedvectors which can be used to work with the set of wordvectors separately separating functionality like this helps in cases where you only want the wordvectors or only have the wordvectors from someone else but not the full models overhead so everywhere you might use wvword try wvwvword instead or perhaps name things more like the following and hold a different variable reference to the wordvectors wvmodel wordvec wordvectors wvmodelwv printwordvectorsword for other tips in adapting your own older code or examples online to gensim the following project wiki page may be helpful migrating from gensim x to
67095698,genism module attribute error for wrappers,python gensim lda topicmodeling,the latest major gensim release removed the wrappers of other library algorithms per the migrating from gensim x to wiki page removed third party wrappers these wrappers of rd party libraries required too much effort there were no volunteers to maintain and support them properly in gensim if your work depends on any of the modules below feel free to copy it out of gensim the last release where they appear and extend maintain the wrapper yourself the removed submodules are if you desperately needed the old support you could also consider installing using the older gensim for example via pip pip install gensim but in general the latest version will be bestsupported
66884353,modulenotfounderror no module named gensimmodelswrappers,python gensim lda mallet modulenotfounderror,if youve installed the latest gensim as of late march the ldamallet model has been removed along with a number of other tools which simply wrapped external toolsapis you can see the note in the gensim migration guide at if the use of that tool is essential to your project you may be able to install an older version of gensim such as though of course youd then be missing the latest fixes optimizations on any other gensim models youre using extract the ldamalletpy source code from that older version updatemove it to your own code for private use dealing with whatever issues arise
66448514,memory error in python using gensimutilssimplepreprocess,python memory gensim wordvec,it appears that attempting to hold all the documents in a list in memory requires more ram than your system has perhaps also one of the files is gigantic whats the largest single file its not necessary to hold all docs in memory gensims wordvec other algorithms can almost always accept any python iterable object one that can iterate over its contents onebyone repeatedly even if theyre coming from some other backend this typically uses far less ram the leader of the gensim project has a useful post about iterables that could help you adapt your readinput function into a wrapper class that can reiterate over the files repeatedly two other notes simplepreprocess isnt especially sophisticated and you may want to do your own tokenization instead but if you retokenize on every iteration especially if your tokenization does anything sophisticated or uses regularexpressions youre doing a lot of redundant retokenizing of the same texts which is likely to be a bottleneck in your training so in fact you might want to just use your readinput not to stuff all rokenized docs into a list but write them to a new file posttokenization onedocument to a line and all tokens separated by single spaces then a utility class like gensims linesentence can provide the iterablewrapper for feeding that almostfullyready file to wordvec
66031545,gensim returns valueerror input must have more than one sentence in for loop through list of paragraphs,python pythonx selenium forloop gensim,note that the summarization module will be removed from the next gensim release it was quite idiosyncratic in its approach hardtogeneralize and without any active maintenance that said if youre getting the error input must have more than one sentence youre probably feeding it an input of just one sentence or at least something that looks like a single sentence to its verycrude sentencesplitter have you tried printing the text values that specifically trigger this error to verify that they have more than one sentence
65876755,gensim installation error on macs with,python gensim,according to of of the main gensim pages it might not be ready for python yet gensim is being continuously tested under python and support for python was dropped in gensim install gensim if you must use python you can downgrade your python version to or and it should work
64540488,unicodedecodeerror utf codec cant decode byte x in position invalid start byte while reading a text file,pythonx gensim wordvec,it looks like one of your files doesnt have proper utfencoded text your wordvecrelated code probably isnt necessary for hitting the error at all you could probably trigger the same error with just sentenceslist listmycorpus to find which file two different possibilities might be change your mycorpus class so that it prints the path of each file before it tries to read it add a python try except unicodedecodeerror statement around the read and when the exception is caught print the offending filename once you know the file involved you may want to fix the file or change the code to be able to handle the files you have maybe theyre not really in utf encoding in which case youd specify a different encoding maybe just one or a few have problems and its be ok to just print their names for later investigation and skip them you could use the exceptionhandling approach above to do that maybe those that arent utf are always in some other platformspecific encoding so when utf fails you could try a nd encoding separately when you solve the encoding issue your iterable mycorpus is not yet returning whet the wordvec class expects it doesnt want full text plain strings it needs those texts to already be broken up into individual wordtokens often simply performing a split on a string is closeenoughtorealtokenization to try as a starting point but usually projects use some moresophisticated punctuationaware tokenization
64125039,memoryerror in pddataframe,pandas gensim lda topicmodeling,its quite possibly the datacvtoarray thats mostresponsible for the memory expansion by turning an efficient sparse representation into a fulldense array try doing that step on a separate preceding line into a temporary variable to check but if your endgoal is doing a gensim lda analysis those steps work well with tokenized text as input so other actions involving countvectorizer and storing interim results or giant termdocument arrays in pandas data structures may be superfluous for example ignoring any stopword filtering this still creates two giant copies of the dftext column in memory a list of tokenizedtexts and a list of textsbows so it uses more memory than optimal the best practice for verylarge corpora is to leave them on disk and stream them itembyitem into the processing steps but it may use less than the giant dense toarray step and never even creates notstrictlynecessary countvectorizer or interim array and dataframe objects
63567713,indexerror index is out of bounds wordvec,pythonx numpy gensim wordvec indexerror,the st time you do a mostsimilar a keyedvectors instance in gensim versions through will create a cache of unitnormalized vectors to assist in all subsequent bulksimilarity operations and place it in vectorsnorm it looks like your addition of a new vector didnt flushrecalculateexpand that cached vectorsnorm originally the keyedvectors class and mostsimilar operation were not designed with constantlygrowing or constantlychanging setsofvectors in mind but rather as utilities for a posttraining frozen set of vectors so thats the cause of your indexerror you should be able to workaround this by explicitly clearing the vectorsnorm any time you perform modificationsadditions to the keyedvectors eg wordvectorsvectorsnorm none this shouldnt be necessary in the next release of gensim but ill doublecheck theres not a similar problem there separately your wordvector is not defined error is simply because you seem to have left the s off your chosen variable name wordvectors you probably dont need to be using the gensimtestingutilitymethod gettmpfile just use your own explicit intentional filesystem paths for saving and loading whether its proper to use keyedvectorsload depends on what was saved if you are in fact saving a full wordvec class instance more than just the vectors using wordvecload would be more appropriate
63419318,array reshape error when loading wordvec model,python amazonec gensim wordvec,i checked what gojomo referred to in the comments and he was correct my file sizes were wrong something must have happened during upload for large models wordvec saves the model in files assuming your model name is model you will have model modeltrainablessynnegnpy modelwvvectorsnpy my wvvectorsnpy was a few kilo bytes too small than the version in my other machine
63385272,how to fix unpickling key error when loading wordvec gensim,python gensim wordvec,per your link these are to be loaded using that librarys wikipediavecload method gensims load methods should only be used with files saved directly from gensim model objects the wikipediavec project does say that their txt file formats would load with loadwordvecformat so you could also try that but with one of their txt format files their full model pkl files are only going to work with their classs own loading function
63345527,error when building requirementtxt in docker,python docker gensim,in the post you mension thye install libcdev to compile packs you dont i have problems trying to use alpine with python so we choose slimbuster as docker python so if you can i would try slimbuster if you can try a numpy ready docker install your python packages
62348981,wordvec recommendation system keyerror word not in vocabulary,gensim wordvec,if you get a keyerror saying a word is not in the vocabulary thats a reliable indicator that the word youre lookingup was not in the training data fed to wordvec or did not appear enough default mincount times so your error indicates the wordtoken did not appear at least times in the texts purchasestrain supplied to wordvec you should do either or both of ensure all words youre going to lookup appear enough times either with more training data or a lower mincount however words with only one or a few occurrences tend not to get good vectors instead just drag the quaality of surroundingwords vectors down so keeping this value above or even raising it above the default of to discard more rare words is a better path whenever you have sufficient data if your later code will be looking up words that might not be present either check for their presence first word in modelwvvocab or set up a try except to catch handle the case where theyre not present
61873864,gensim saving word vectors in txt format error,python gensim wordvec,i fixed it now apparently i was trying to use a ndarray but of strings as coefficients and gensim uses an ndarrayfloats that was the problem where my own vectors when switching to the wv were of type str so it ended up being empty the switching of vectors now is done like thanks for your comments they helped me figure it out
61572397,build the corpus by wikipedia modulenotfounderror no module named gensim,python gensim,you do not have the gensim module installed in your system or download it from gensim depends on scipy and numpy you must have them installed prior to installing gensim there is a bug in pip either upgrade to using or downgrade to
61310229,is this a bug on gensim hdp model for python,pythonx time gensim python,you are coming across an issue caused by deprecation of clock function of time module it has been deprecated since python v and removed in v to resolve it you have options try to upgrade gensim if you do not have the newest version try to downgrade python
59813664,error while implementing wordvec model with embeddingvector,python machinelearning keras gensim wordvec,yes gensims keyedvectors abstraction does not offer a get method what docs or example are you following that suggests it does you can use standard python indexing eg though there isnt really a reason for your loop copying each vector into your own embeddingmatrix the keyedvectors instance already has a raw array with each vector in a row in the order of the keyedvectors indexentity list in its vectors property
59050644,memoryerror unable to allocate array with shape and data type float while using wordvec in python,python multiprocessing pythonmultiprocessing gensim wordvec,ideally you should paste the text of your error into your question rather than a screenshot however i see the two key lines after making one pass over your corpus the model has learned how many unique words will survive which reports how large of a model must be allocated one taking about bytes about gb but when trying to allocate the required vector array youre getting a memoryerror which indicates not enough computer addressablememory ram is available you can either run where theres more memory perhaps by adding ram to your existing system or reduce the amount of memory required chiefly by reducing either the number of unique wordvectors youd like to train or their dimensional size you could reduce the number of words by increasing the default mincount parameter to something like mincount or mincount or mincount you probably dont need over million wordvectors many interesting results are possible with just a vocabulary of a few tensofthousands of words you could also set a maxfinalvocab value to specify an exact number of unique words to keep for example maxfinalvocab would keep just the mostfrequent words ignoring the rest reducing the size will also save memory a setting of size is popular for wordvectors and would reduce the memory requirements by a quarter together using size maxfinalvocab should trim the required memory to under gb
58816895,type error when trying to create a docvec model in gensim,python gensim docvec,enumerate returns an integer counter and the value in the list so in your third line of code i is an integer however the second parameter of taggeddocument function should be an iterable
58182293,aws lambda boto gensim model module initialization error exit,python amazonwebservices awslambda gensim wordvec,instead of connecting using boto simply worked but of course unfortunately it doesnt answer the question on why the mysterious exit error came up and how to get more info
57148357,attributeerror module gensimutils has no attribute smartopen,python gensim databricks docvec,i believe this is because you installed a new gensim version then you will get this error you can either update the call as this following suggestion pythonsitepackagessmartopensmartopenlibpy userwarning this function is deprecated use smartopenopen instead see the migration notes for details or pip install gensim hope this helps
56667348,gensim import error importerror dll load failed is not a valid win application,python winapi dll pycharm gensim,okay so my problem resolved when i tried to import gensim outside pycharm i imported directly in python console and it worked thanks james
56148576,calledprocesserror returned nonzero exit status,python gensim lda mallet,make sure you have java developers kit downloaded jdk mallet unzipped mallet and have your env in the correct folder otherwise update it eg import os osenvironupdatemalletpathrpythonmalletmalletbin
55612440,fasttext error typeerror supervised got an unexpected keyword argument pretrainedvectors,python gensim fasttext,according to the documentation the named parameter to the function is called pretrainedvectors not pretrainedvectors this naming convention is in line with pep style and so is normal for a python api
55487124,gensim attribute error when trying to use prescan on a docvec object,python gensim,thats a known problem after a refactoring of the docvec code you can just skip that cell to proceed with the rest of that demo notebook if you really needed to adjust the mincount using the info from a fullscan you might be able to call some internal classesmethods mentioned in the above issue
55288724,gensim mallet calledprocesserror returned nonzero exit status,python windows jupyternotebook gensim mallet,update the path to and edit the notepad malletbat within the mallet folder to in command line these were helpful commands to figure out what was going on the problem is with java not being installed correctly or with the path not including java and the mallet classpath not being defined correctly more info here this solved my error hopefully it helps someone else
55095368,gensim keyerror word good not in vocabulary,pythonx gensim,the first parameter should be iterable since data is just iterable of sentences it takes every character but data takes every word from the docs your solution now if you do this you will get the answer
55016629,facebook fasttext bin model unicodedecodeerror,python facebook utf gensim fasttext,it is better to load the fasttext word embeddings using the fasttext package rather than gensim you need to first install the fasttext module for python using pip install fasttext then follow the python code chunk from below source of the code
54917218,python gensim attributeerror list object has no attribute,python gensim,are you using like that it is fine with this or change you code by this
53989210,how to fix relative import error in python gensimsummarization,python gensim summarization,happened to me also i reinstalled the gensim pypi module i believe reinstalling the above module will fix it
53417171,fixedsize topics vector in gensim lda topic modelling for finding similar texts,python gensim lda topicmodeling cosinesimilarity,i have used gensim for topic modeling before and i had not faced this issue ideally if you pass numtopics then it returns top topics with the highest probability for each document and then you should be able to generate the cosine similarity matrix by doing something like this but for some reason if you are getting unequal number of topics you can assume a zero probability value for the remaining topics and include them in your vector when you calculate similarity ps if you could provide a sample of your input documents it would be easier to reproduce your output and look into it
52220514,typeerror ufunc add did not contain a loop with signature matching types dtype,python gensim wordvec,gensims wordvec requires a corpus of texts such as in its intializers st argument thats an iterable sequence object of listsofstringtokens it doesnt take a raw numpy array further if you do supply a corpus at instantiation as in your line of code then it will automatically do its vocabularybuilding and training steps you dont need to then call train explicitly and while its possible to call train again very few users doing very advanced things will need to do so the usual safe approach is a single training session on a complete corpus after which the model is done finally train also expects any corpus as an iterable sequence object of listsofstringtokens if you supply the right kind of corpus its doubtful youll receive an error like youre getting
51680023,gensim calling docvecsmostsimilar yields error,python numpy gensim,rtfd deletetemporarytrainingdatakeepdoctagsvectorstrue keepinferencetrue discard parameters that are used in training and score use if youre sure youre done training a model parameters keepdoctagsvectors bool optional set to false if you dont want to save doctags vectors in this case you will not be able to use mostsimilar similarity etc methods keepinference bool optional set to false if you dont want to store parameters that are used for infervector method
51449841,how to install gensim without pip firewall issues,python installation firewall gensim,tedious and only solution then is to download all the dependencies one by one and install them along with gensim so go to pypi and download gensim first and install it then it might raise errors saying some package is missing or trying to download itthen download that specific package and install it via pip by giving path to the downloaded whl or source file
51388707,intel mkl fatal error while trying to import gensim package,python tensorflow anaconda seaborn gensim,here is my theory on your question is there any dependency between gensim tensoflow seaborn and such packages when you try to install these packages one by one using conda you might have already seen conda prompting that some of the dependencies will be downgradedupdatedinstalled hence there is dependency between the dependencies of these packages why import error is thrown only on certain cases looks like a dependency issue when you try to import gensim it tries to load certain lib files which its not able to find however when tensorflow or seaborn is imported the mentioned lib files might have already loaded hence importing gensim did not show an error why installing few packages and uninstalling few help to solve the problem this might help to have the correct dependencies for the packages to work properly having said that i tried to recreate the error that you got however gensim is importing fine for me if you could give the result of conda list will try to recreate the problem and would be able to give a better insight
50573054,unicodedecodeerror error when loading wordvec,python wordvec gensim pythonunicode polyglot,for kyubyongs pretrained wordvector bin file it may have been saved using gensims save function load the model with load not loadwordvecformat thats for the ctool compatibility ie model wordvecloadfname let me know if that works reference gensim mailing list
50214899,indexerror when trying to update gensims ldamodel,pythonx gensim lda topicmodeling indexerror,the solution is simply to initialize the ldamodel with the argument idword dictionary if you dont do that it assumes that your vocabulary size is the vocabulary size of the first set of documents you train it on and cant update it in fact it sets its numterms value to the length of idword once there and never updates it afterwards you can verify in the update function
49800622,python importerror cannot import name config when trying to import gensim,python jupyter boto gensim,i have recently bumped into a similar problem as following as well based on my experience it is rooted in the conflict of dependencies for packages in your conda environment so the way i resolved it is as following remove anaconda completely see here reinstall anaconda from scratch see here install gensim library with conda install c anaconda gensim command it is needful to say that to avoid future issues similar to this you should try creating different environment variables for conda as it keeps packages separate from each other hence no conflict of packages you can see this post that explains clearly how to create such an environment before starting your different projects i hope that helps
49676060,unpicklingerror invalid load key,pythonx wordvec gensim,if it is a binary file you need to mention it like this
49514111,typeerror not supported between instances of float and nonetype,python gensim lda,cleaning and reinstallation in conda fixed it i am guessing a corrupt version was installed and the clean command removed it before reinstallation
48917449,typeerror a byteslike object is required not str when converting gensim to tensorboard,python pythonx typeerror gensim,you can just convert your string back to bytes however your code already reads the file as bytes and then you explicitly convert to str with this i guess tjoinstrx for x in modelword so you might want to clean up and use bytes everywhere instead of going back and forth
48623214,elki kmeans clustering task failed error for high dimensional data,clusteranalysis kmeans gensim docvec elki,the error which took me a bit to understand when i saw it the first time says that your data has the shape ie some lines have only columns some have this may be a file format issue for example due to nan missing values or similar bad characters you get that error if you try to run an algorithm like kmeans that assumes the data comes from a rd vectorspace that is the numbervectorfield requirement because the input data is not meeting this requirement
48044670,docvecgensim issue with shuffling sentences in the epochs,python wordvec gensim docvec,those arent great tutorials for the latest versions of gensim in particular its a bad idea to be calling train multiple times in a loop with your own manual management of alphaminalpha its easy to mess up the wrong things will happen in your code for example and offers no benefit for most users dont change minalpha from the default and call train exactly once itll then do exactly epochs iterations decaying the learningrate alpha from its max to min values properly your specific error is because your taggedlinesentence class doesnt have a sentences property at least not until after toarray is called and yet the code is trying to access that nonexistent property the whole toarraysentencesperm approach is a bit broken the reason for using such an iterable class is typically to keep a large dataset out of mainmemory streaming it from disk but toarray then just loads everything caching it inside the class eliminating the iterable benefit if you can afford that because the full dataset easily fits in memory you can just do to iteratefromdisk once then keep the corpus in an inmemory list and shuffling repeatedly during training isnt usually needed only if the training data has some existing clumping like all the examples with certain wordstopics are stuck together at the top or bottom of the ordering is the native ordering likely to cause training problems and in that case a single shuffle before any training should be enough to remove the clumping so again assuming your data fits in memory you can just do once then youve got a sentences thats fine to pass to docvec in both buildvocab and train once below
47332205,issues in docvec tags in gensim,python gensim docvec,try to change this line to
47235153,suffixes being added to extra model files during save,python gensim,for larger models a single save can result in multiple files being written with extra suffixes see why are multiple model files created in gensim wordvec for more details
46197493,using gensim docvec with keras convd valueerror,python machinelearning keras gensim,the problem is with the input shape instead of none you can try with none you can tell that by looking at the exception the kernel is bigger than the second dimension
45458493,gensim word vectors encoding problems,python gensim,if you save vectors using gensims native save method you should load them with the native load method if you want to load vectors using loadwordvecformat youll need to save them with savewordvecformat youll lose some information this way such as the exact occurrence counts that would otherwise be inside the keyedvectorsvocab dictionary items
45404027,attributeerror module pyro has no attribute expose while running gensim distributed lsi,python gensim latentsemanticindexing pyro,pyroexpose was added in pyro version from august it looks to me that you have a very old pyro version installed from before this date and that your gensim requires a more recent one check using you should probably upgrade your pyro library pay attention though i believe gensim doesnt support the most recent versions of pyro so you should probably check its manual for the correct version that you need you can always try to install the latest right now and see how it goes edit i suppose you could also try to find gensim specific support
45352522,issues installing gensim on ubuntu,python python gensim,had to upgrade scipyfollowed the solution given in the answer by josteinb in the following thread cant upgrade scipy was able to upgrade scipy as follows easyinstall of gensim worked smoothly after this
45193550,gensim errors after updating python version with conda,pythonx conda gensim,more context for the errors including reported stacks would be necessary to know where the errors are happening gensim is supported and releasetested for python so the issue is likely something specific to your system you may want to try uninstalling reinstalling gensim or whatever library error stacks also implicates or starting a fresh conda environment thats python from the beginning
44449132,getting error while using gensim model in python,python gensim wordvec,the error means quite literally that no docvector with the key tag testpos is part of the model there mustnt have been any documents with that tag presented during training you can see a list of all known doctags in the model in modeldocvecsoffsetdoctag if testpos isnt there you cant access a docvector via modeldocvecstestpos if that list is empty then the docvectors were trained to be accessed by plain int keys and modeldocvecs would be a more appropriate way to access a docvector separately docvec wont work well with tiny corpuses of a few hundred documents and the warning in your screenshot slow version of gensimmodelsdocvec is being used means that gensims optimized ccompiled routines werent part of the installation and training will be x or more slower
44233296,problems accessing docvectors with gensim,gensim docvec,the gensim docvec class uses exactly the document tags youve passed it during training as keys to the docvectors and yes that labeledlinesentence class is adding n to the documenttags specifically those appear to be the linenumbers from the associated files so youll have to request vectors using those same keys that were provided during training with the n if what you really want is a vectorperline if you instead want each file to be its own document youll need to change the corpus class to use the whole file as a document looking at the tutorial you reference it appears they have a second labeledlinesentence class that isnt lineoriented but still is named that way but youre not using that variant separately you dont need to loop and call train multiple times and manually adjust the alpha thats almost certainly not doing what you intend in any recent version of gensim where train already iterates over the corpus multiple times in the most recent versions of gensim there will even be an error if you call it that way since many outdated examples on the web encourage this mistake just call train once it will iterate over your corpus the number of times specified when the model was constructed thats a default of but controllable with the iter initialization parameter and or more is common with docvec corpuses
44143441,code for gensim wordvec as an http service keyedvectors attribute error,python gensim wordvec,fyi that demo code was baed on gensim from as listed in its requirementstxt and would need updating to work with the latest gensim it might be sufficient to add a line to wvserverpy at line just after the loadwordvecformat to force the creation of the needed synnorm property which in older gensims was autocreated on load before deleting the raw syn values specifically you would leave out the replacetrue if you were going to be doing operations other than mostsimilar that might require raw vectors if this works to fix the problem for you a pullrequest to the wvservergooglenews repo would be favorably received
44022180,unpickling error while using wordvecload,python gensim wordvec,this would normally work if the file was created by gensims native save are you sure the file ammendmentvectorsmodelbin is complete and uncorrupted was it created using the same pythongensim versions as in use where youre trying to load it can you try recreating the file
43942790,gensim file not found error,python pythonx gensim,the code requires an absolute path here relative path should be used when entire operation is carried out in the same directory location but in this case the file name is passed as argument to some other function which is located at different location one way to handle this situation is using abspath
43855348,runtimeerror release unlocked lock while training docvec,gensim docvec,most likely a numpy issue see discussion in gensim bug tracker
43146420,gensim error while loading pretrained docvec model,python gensim docvec,gensim will generally try to support loading of models saved from older versions into newer versions but the reverse is a much harder problem and will only work sometimes so upgrade the environment where you want to load the model to to match where it was trained or try the mostrecent version but dont try to move models backwards
43146077,indexword in gensims docvec raises an attribute error,python gensim,the indexword list of wordvectors has moved to the wv property of the model in recent gensim versions so where you would say modelindexword you must now use modelwvindexword note that this is still just wordvectors which are only trained by the dm dm docvec modes docvectors are in the modeldocvecs object and you can see a list of the string tags to which docvectors may be associated in modeldocvecsoffsetdoctag
42836992,gensim on python typeerror object of type map has no len,python pythonx anaconda gensim,gensim supports python of course it is your or nodevecs responsibility to supply wordvec with an iterable of your sentences in this case you have to pass it an iterable that contains walks where each walk is a list of vertices
42539384,got eoferror during loading docvec model,python pickle gensim docvec,i think your model causes the problem are you check with same model i mean build in a same way please see this page
41829323,attributeerror list object has no attribute lower gensim,python string split gensim,try you were trying to apply lower to data which is a list lower can only be applied to strings
41628856,gensim getting started error no such file or directory text,python pythonx errorhandling gensim wordvec,it seems youre missing the file used here specifically it is trying to open text and cant find it hence the filenotfounderror you could download the file itself from here as is stated in the documentation for textcorpus and make it available extract it and then supply it as an argument to textcorpus
41430565,encoding issue in python while using wv,python gensim wordvec,from the gensim faq you can that option about setting unicodeerrors as ignore or replace which seems to work in some occasions but not all but if you look at the specific help of the function there is also this this is beause the wordvec model is saved as binary and not as any encoded string therefore just setting binary true should work in all these cases for example if you are trying to use the google pretrained model from here this should work hope this helps
40840731,valueerror cannot compute lda over an empty collection no terms,python gensim lda topicmodeling,finally figured it out the issue with small documents is that if you try to filter the extremes from dictionary you might end up with empty lists in corpuscorpus dictionarydocbowtext so the values of parameters in dictionaryfilterextremesnobelow noabove needs to be selected accordingly and carefully before corpus dictionarydocbowtext i just removed the filter extremes and lda model runs fine now though i will change the parameter values in filter extreme and use it later
40671057,use aall or aany error while trying to use gensim wordvec,python gensim wordvec,i ran into this problem as well for me the error apparently had another background usually i collect textdata into numpyarrays of dtypenpstr when i loaded the numpytextarray into the gensimmodel i received the error you describe actually storing the array in a normal pythonlist instead did the trick maybe this helps out somebody else running into this issue
40643082,python gensim typeerror coercing to unicode need string or buffer list found,python python typeerror iterable gensim,the line below is expecting an iterable object where the readcorpus function was supposed to be a generator using the keyword yield however the readcorpus function was not behaving the way a generator is supposed to be because of a poor implementation of the yield keyword the current implementation yield an array of items every loop iterations while the correct way is yield item by item hence the readcorpus needs to be modified as following
39615436,fails to fix the seed value in lda model in gensim,python numpy gensim,the dictionary generated by corporadictionary may be different to the same corpussuch as same words but different orderso one should fix the dictionary as well as seed to get tht same topic each timethe code below may help to fix the dictionary
38556496,gensim can not be imported because importerror no module named queue,python queue gensim,according to the website genesis should work with python however i still think you can simply solve your issue by using it with python instead
36509957,why gensim docvec give attributeerror list object has no attribute words,pythonx gensim wordvec,input to gensimmodelsdocvec should be an iterator over the labeledsentence say a list object try i have reduced the window size and mincount so that they make sense for the given input also go through this nice tutorial on docvec if you havent already
36328261,importerror cannot import name bytesio on eclipse,python lda gensim topicmodeling,i had a similar error message with this version of boto after solving the issue removing the old version that was hiding the new one i managed to load gensim i suggest you try and update boto to a more recent version
36223864,keyerror in docvec model even when mincount set to during training,python gensim wordvec,from gensims docvec documentation input to docvec should be an iterator of labeledsentence objects your corpus variable needs to be constructed as follows followed by
34309428,cannot run pyldavis getting error importerror cannot import name pcoa,python scikitlearn gensim skbio,the name of this function changed from the x alpha to x beta branch of scikitbio you can either use the function with the old name by installing scikitbio or modify your code to use the new function name i recommend the latter as this interface is stabilizing so making the change now will allow you to continue to get access to the latest features there are two pieces involved in updating your pcoa call for youll need to adapt your import and function calls to use the new name pcoa all lowercase now see changelog note on this here and then change how you interact with the results as the ordinationresults object has been improved between these releases first your import should now look like then you can review the changelog description of whats changed with the ordinationresults object here if you do just want to stick with either of the following should work for installation on a related note see our api stability docs to see how you can learn about what features are stableexperimentaldeprecated in scikitbio
34057374,how to resolve error when installing gensim,python gensim,numpy and scipy can be pretty difficult to install because they rely on some classic implementations of algorithms in older programming languages like fortran which have complicated dependency chains would you be amenable to installing anaconda a fantastic python distribution that includes numpy and scipy if you install anacondas python installation you should find it easy to use pip install gensim because youll already have the most difficulttoinstall dependencies installed there are python and versions here if youd like to give it a try as someone who regularly uses numpy and scipy i regularly install anaconda on new machines and it saves lots of heartache
33989826,python gensim runtimeerror you must first build vocabulary before training the model,python gensim wordvec,default mincount in gensims wordvec is set to if there is no word in your vocab with frequency greater than your vocab will be empty and hence the error try
33229360,gensim typeerror docbow expects an array of unicode tokens on input not a single string,python gensim,in dictionarypy the initialize function is function adddocuments build dictionary from a collection of documents each document is a list of tokens so if you initialize dictionary in this way you must pass documents but not a single document for example is ok
32101795,error while loading wordvec model in gensim,python gensim wordvec,fixed the problem with
31512853,gensim valueerror failed to create intentcachehideoptional array must have defined dimensions but got,python gensim latentsemanticindexing,i think because of your data in documents have blank try add
31384947,getting unicodedecodeerror when installing gensim on ubuntu,ubuntu pip gensim,it turned out that i can manually install the gensim lib by downloading and unzipping the targz source for gensim then run
31286574,python wordvecwordvec overflowerror,pythonx windowsx integeroverflow gensim wordvec,i get this as well it looks like gensim has a potential workaround in the dev branch this doesnt solve the core issue of navigating between different hardware and install int sizes but i think it should alleviate issues with this particular line the necessary change involves switching out modelvocabwsampleint modelrandomrandint for modelvocabwsampleint modelrandomrand this avoids the bit bit int issue created in randint update i manually incorporated that change into my gensim install and it prevents the error
30973503,attributeerror numpyndarray object has no attribute a,python numpy matrix gensim,selfa is either an npmatrix or sparse matrix for both a means return a copy that is a npndarray in other words it converts the d matrix to a regular numpy array if self is already an array it would produce your error it looks like you have corrected that with your own version of tfidf except that uses variables m and amatrix instead of selfa i think you need to look more at the error message and stack to identify where that a is also make sure you understand where the code expects a matrix especially a sparse one and whether your own code differs in that regard i recall from other so questions that one of the learning packages had switched to using sparse matrices and that required adding todense to some of their code which expected dense ones
30488695,docvec memoryerror,python memory gensim wordvec,m vectors dimensions bytesfloat gb for the models syn array trained vectors the syn array hidden weights will also be gb even though syn doesnt really need entries for docvectors which are never targetpredictions during training the vocabulary structures vocab dict and indexword table will likely add another gb or more so thats all your gb ram the synnorm array used for similarity calculations will need another gb for a total usage of around gb its the synnorm creation where youre getting the error but even if synnorm creation succeeded being that deep into virtual memory would likely ruin performance some steps that might help use a mincount of at least words appearing once are unlikely to contribute much but likely use a lot of memory but since words are a tiny portion of your syn this will only save a little after training but before triggering initsims discard the the syn array you wont be able to train more but your existing worddoc vectors remain accessible after training but before calling mostsimilar call initsims yourself with a replacetrue parameter to discard the nonnormalized syn and replace it with the synnorm again you wont be able to train more but youll save the syn memory inprogress work separating out the doc and word vectors which will appear in gensim past verstion should also eventually offer some relief itll shrink the syn to only include word entries and allow docvectors to come from a filebacked memmapd array
29369317,tweet analysis python error when making dictionary for lda,python dictionary lda gensim,your error handling is in the wrong place you cant open a file while asking to ignore decoding errors that happens further down the line what you should do is open the file in binary mode rb modifier and read the lines they will become bytes objects then you can decode them and ignore errors
27477084,gensim error in canopy express,python canopy gensim,canopy python itself is regular python however the python pane in the canopy gui is an ipython qtconsole which adds a layer of functionality mostly for better but on rare occasion for worse by default it starts in pylab mode which can be confusing to beginners see you dont describe what you are doing with any precision but from the symptom that you describe it sounds as if you are running your commands onebyone at the ipython prompt either by copypaste or by selecting your commands in the text editor and doing run selection in the ipython prompt because pylab does an implicit from numpy import the sum function refers to numpys sum rather than the builtin python sum which would account for the error message that you report three different solutions out of many if you simply run your script rather than run selection or copypaste commands it should act as expected this is the most robust flexible solution disable pylab mode in canopy preferences then you can run your commands either way not a great solution but instructive do del sum at the ipython prompt this will delete the numpy sum from the ipython namespace uncovering the original builtin sum and allowing your code to run either way
26812617,index error when running lda in gensim,python lda topicmodeling gensim,it looks like you created a corpus with a dictionary then modified the dictionary so the indices dont align try removing stopwords first then build the corpus and finally apply the lda model
26286206,python ioerror errno no such file or directory modelsdictionarydict,python gensim,when no such file or directory occurs during a save operation it usually means the directory path that you have specified as the container for the output file does not exist in this case you have clearly given it selfdictionarypath modelsdictionarydict which is a relative path an error saving to this path presumably means a file cannot be saved inside models because the directory models does not exist relative to the current working directory to find out the current working directory you can use osgetcwd to test whether a directory exists you can use ospathisdir to create a directory you can use osmkdir
26145937,bleicorpus and associated press dataset in gensim io error,python enthought lda topicmodeling gensim,you dont have wget installed on your computer are you using windows with cygwin download the file from unpack it and place it in the correct folder
23853828,python indexerror using gensim for lda topic modeling,python lda topicmodeling gensim,this is caused by using a corpus and dictionary that dont have the same idtoword mapping it can happen if you prune your dictionary and call dictionarycompactify at the wrong time a simple example will make it clear lets make a dictionary this dictionary now has entries for these words and maps them to integer ids its useful to turn documents into vectors of id count tuples which wed want to do before passing them into a model sometimes youll want to alter your dictionary for example you might want to remove very rare or very common words removing words creates gaps in the dictionary but calling dictionarycompactify reassigns ids to fill in the gaps but that means our vectorizedcorpus from above doesnt use the same ids as the dictionary any more and if we pass them into a model well get an indexerror solution make your vector representation using the dictionary after making changes and calling dictionarycompactify
18867516,how to resolve the unpicklingerror in loading gensim corpus python,python lda topicmodeling gensim,see the documentation at what youre trying to do is store the corpus in matrixmarket format a text format and then load it using the saveload binary interface to load a serialized matrixmarket corpus simply corpus corporammcorpusfoobarmm
16553252,gensim importerror in pycharm no module named scipysparse,python scipy pycharm lda gensim,id suggest using the pythonorg version of python not the one that came with osx as there are some issues that are most easily overcome by installing the latest version in the case of the x branch dont worry about breaking anything both versions will happily coexist together once you have that you can install the latest numpy and scipy binaries get the dmg files numpy is required for scipy to work make sure you set up pycharm to work with the new version of python and doublecheck that your modules are installed in the right sitepackages directory it should be libraryframeworkspythonframeworkversionslibpythonsitepackages you can always copy all of the files in your librarypythonsitepackages directory to the one i just mentioned as the majorminor version of python is still the same then you should be good to go you will likely want to symlink usrlocalbinpython to libraryframeworkspythonframeworkversionsbinpython it may be already to make an easier shebang line and dont forget to put usrlocalbin in front of usrbin in your path for when you do commandline work and for usrbinenv python shebangs good luck
15260864,gensim topic printing errorsissues,python topicmodeling gensim,to answer why your lsi topics are tuples instead of words check your input corpus is it created from a list of documents that is converted into corpus through corpus dictionarydocbowtext for text in texts because if it isnt and you just read it from serialized corpus without reading a dictionary then you wont get the words in the topic outputs below my code works and prints out the topics with weighted words the above outputs
6615569,eclipse pydev importerror,python eclipse import pydev gensim,this is independent of eclipsepydev youll get the same error running the code in any other way your module imports gensim the first entry on the pythonpath is the current directory and your module is called gensimpy so your module attempts to import iteself because imports are cached you dont get into infinite recursion but get a reference to a module containing nothing especially not the things you expected from the real gensim module the error message should mention this possibility its incredibly common the solution is to rename your file
79448910,why facing cuda error deviceside assert triggered while training lstm model,python machinelearning deeplearning pytorch,so couple things first you should understand the error that error is triggered either when you are getting nan or inf values which are obviously a problem or you are giving something invalid target values eg passing in something out of bounds or with incorrect shapes some potential issues i see in no particular order check for correct shape and type of labels in training fn if your labels are of type torchfloat convert them using labels labelslong and if your shapes mismatch then check shapes of outputs going into ce loss crossentropyloss expects a shape of batchsize numclasses make sure outputs is the correct shape ensure no nan or inf values in forward pass if this print is triggered try playing with hyper params eg lower learning rate clipping gradients etc put a ton of print statements everywhere and ensure the shapes of all tensors are what you expect them to be honestly the hardest part of ml for me is keeping track of tensor shapes since they cause most of my problems lol put comments next to every line with tensors of what the input and output shape of those lines should be and put print statements to verify failing all that try running it on cpu you may get more insightful error messages
79360262,use matplotlibinline and torchdl show error notimplementederror implement enablegui in a subclass,matplotlib deeplearning pytorch dl,after times trying i found the solution change the py run the command ipython py then it worked successfully and showed the figure hope it can help someone
79338114,runtimeerror numpy is not available when using inversetransform,numpy deeplearning convneuralnetwork dataanalysis,i have faced this error after the release of numpy not knowing which version of pytorch or numpy you are using try in your python package environment
79280773,runtimeerror trying to backward through the graph a second time on loss tensor,python deeplearning pytorch tensor autograd,from what i understand the xtraintensor is output from the autoencoder when you do not run torchnograd during the encoding step a computational graph is created for the outputs of the autoencoder which links the autoencoders operations and weights to the encoded tensors in your code since the models output uses the xtraintensor the models loss is connected to the autoencoders computational graph when you call lossbackward the first time pytorch traverses the entire computational graph including the autoencoder to compute gradients and then clears the graph when you call lossbackward in the second iteration of the loop you are attempting to traverse the cleared autoencoders computational graph torchnograd prevents pytorch from creating the autoencoder computational graph or linking the resulting loss to the autoencoder
79264683,error loading pytorch model checkpoint pickleunpicklingerror invalid load key xf,python deeplearning pytorch pickle torch,the error is typical when trying to open a gzip file as if it was a pickle or pytorch file because gzips start with a f byte but this is not a valid gzip it looks like a corrupted pytorch file indeed looking at hexdump c filept head shown below most of it looks like a pytorch file which should be a zip archive not gzip containing a python pickle file named datapkl but the first few bytes are wrong instead of starting like a zip file as it should bytes b or ascii pk it starts like a gzip file f b in fact its exactly as if the first bytes were replaced with a valid empty gzip file with a timestamp ff pointing to november pm gmt your file inspecting the pickle data we can see a dictionary bestacc statedict with the typical contents of a checkpoint of a pytorch model a valid zipped pickle produced by torchsavebestacc nparray statedict cdfpth a gzip containing an empty file with the same name and timestamp with gzip best has bytes the same as your files prefix except for the two operating system bytes edit heres a script that might fix such files in general usrbinenv python import os import sys from pathlib import path from shutil import copy from tempfile import temporarydirectory import numpy as np import torch chunksize def mainorigpath path none fixedpath origpathwithsuffixfixedpth copyorigpath fixedpath with temporarydirectory as tempdir temppath pathtempdir origpathname torchsavebestacc nparray statedict temppath with opentemppath rb as ftemp with openfixedpath rb as ffixed while true content ffixedreadchunksize replacement ftempreadchunksize if content replacement break printfreplacing contentr with replacementr ffixedseekchunksize osseekcur ffixedwritereplacement if name main assert lensysargv expected exactly one argument the path to the broken pth file mainpathsysargv
79240688,notimplementederror could not run atenaddout with arguments from the quantizedcpu backend while implementing qat on resnet using pytorch,pythonx deeplearning pytorch resnet quantizationawaretraining,this tutorial tells that for torch this feature is beta and you need to adjust original model with at least one change for residual addition replacing addition with nnquantizedfloatfunctional you can see in your error trace that this line of code throws the error so we need to reimplement basicblock by replacing operator with skipadd inject basicblock to the resnet constructor step step inject this by creating new constructor method for quantized model step go back to the cell with and change it with this see the first line is changed then execute all cells below again
79140091,inference error after training an ipadapter plus model,machinelearning deeplearning pytorch transformermodel stablediffusion,the model can be trained and inferenced successfully now set safeserialization to false in model training file tutorialtrainpluspy it will generate pytorchmodelbin instead of modelsafetensors during training once training is complete modify the model conversion code as below based on the original instructions in readme model file ipadapterbin will be generated for inference
79061201,while inferencing through saved rtdetr model weights keyerror,deeplearning pytorch yolo ultralytics,use ultralytics rtdetr module instead of yolo to operate with the rtdetr model from ultralytics import rtdetr load a model model rtdetrrcontentdrivemydrivelnbestpt display model information optional modelinfo imagepath rcontentdrivemydrivelnjpgrfbcfebcaeecfcedjpg run inference results modelimagepath
78995266,ho predict error testvalidation dataset has no columns in common with the training set,r deeplearning ho,please check and make sure the test data frame xdfnew has the same predictor column names as your xdf you can find the names of a ho frame by calling namesxdf
78964885,i need to write complexvalue neural network in tensorflow but i get an error,tensorflow deeplearning neuralnetwork,def tensorflowmodel import numpy as np import tensorflow as tf from cvnnlayers import complexdense complexinput data nprandomrand j nprandomrand labels npabsdatasumaxis astypeint def getcomplexmodelinputshape model tfkerasmodelssequential modeladdcomplexinputinputshapeinputshape modeladdcomplexdense activationcartrelu modeladdcomplexdense activationconverttorealwithabs return model model getcomplexmodel modelcompileoptimizeradam lossbinarycrossentropy metricsaccuracy modelfitdata labels epochs batchsize validationsplit testdata nprandomrand j nprandomrand testlabels npabstestdatasumaxis astypeint testloss testacc modelevaluatetestdata testlabels printftest accuracy testacc if name main tensorflowmodel output test accuracy environment python cvnn tensorflow tfkeras tensorflowprobabilitytf dmtree please note that cvnn is experimental and not maintained so it might not work with newer versions of tensorflow see invalid dtype complex with tf
78879312,nifty data science project in python error occuring keyerror date,python database deeplearning datascience project,too long to comment all of the field names have a trailing space excepting the last hope this helps
78870533,issues trying to load saved keras unet model from h file,python tensorflow keras deeplearning unetneuralnetwork,as someone suggested in the comments try expanding the dimension of the input tensor the model expect a batch of images as input and for one image the input shape should be x h x w x
78862667,pytorch runtimeerror expected all tensors to be on the same device but found at least two devices cpu and cuda,python deeplearning pytorch densenet,you need to move the x tensor to the same device change x torchrandn to x torchrandntodevice
78811507,tensorflow object detection api installation issues,python tensorflow deeplearning objectdetection objectdetectionapi,downgrade numpy to a version lower than in their new release the types changed
78780556,tensorflow valueerror argument must have rank ndim,python tensorflow deeplearning typeerror valueerror,that error is due to some images of the dataset being in only dimensions and not grayscale for example you need to modify your generator to check for such a case you can handle it like this
78696299,issue with training keras model using modelcheckpoint in kaggle notebook unexpected result of empty logs,python tensorflow keras deeplearning,check that your metrics and optimizers are contained within a list as keras wants them as such plus it wants a roc metric as far as documentation goes
78631488,attributeerror nonetype object has no attribute items when training dl dataset made using imagedatagenerator,pythonx deeplearning resnet imageclassification imagedatagenerator,the issue occurs in these lines because the imagedatagenerator is deprecated in the latest version of tensorflow as seen here i ran into the same problem and the solution i found was to downgrade tensorflow to a previous version through this command in jupyter
78507359,deep learning model training issue,python tensorflow deeplearning,you got mix up an order of variables here it should be
78505569,why am i keep running into the nan problem when training cibhash model,python deeplearning hash,it turns out that this problem is caused by lack of gpu memory and some kind of unknown bug in some previous cuda versions i have tried setting learning rate to did not solve the issue switching to cpu execution loss explosion issue did not occur on cpu but it was tooooo slow modifying forward function code followed your code modification but problem persisted upgrading pytorch and related libraries tried upgrading to versions with cuda suffix from torch to torchcuda but loss explosion occurred earlier before eval in the first epoch downgrading cuda version attempted downgrade to cuda encountered new errors since my gpu gs compute capability is but cuda only support it in the range changing data type precision to float consumed excessive memory necessitating reduction of batch size to decreasing batch size set batch size to mitigated the problem but it hardly improve the model after evaluating a batch size of leads to the loss nan problem migrating to alternative environment shifted to a colab environment indicating a possible issue with gpu memory constraints upgrading cuda version to upgraded to a newer cuda version encountered memoryerror heres some information of difference performance on different gpus that i have gathered or tried my machine gpu nvidia geforce rtx g cuda version known nan issues occurred with cuda compute capability compatibility problem in cuda memoryerror problem in cuda ti machine gpu nvidia geforce rtx ti gb perhaps cuda version driver version success no nan problem colab environment gpu nvidia t tesla t vram gb cuda version the graph in colab shows that the model occupied gb of all gpu ram success no nan problem a gpu environment gpu nvidia a vram not specified cuda version not specified but known to work fine in this environment success no nan problem in conclusion i suspect that the memoryerror issue was not properly handled in older versions of cuda as reported in cuda errors like out of memory may also lead to nan results my suspicion is that older versions of cuda may lack a proper error handling mechanism for nan caused by insufficient memory however i have too few evidence of this problem if anyone knows more specific detail of this problem please contact me
78486965,pytorch runtimeerror device device numgpus internal assert failed,deeplearning pytorch yolo torchvision,this was a bug in pytorch to solve find and go to pythonxxsitepackagestorchcudainitpy and modify this function remove or comment out this old function and replace it with this new code
78475863,optree is actually installed but when running from kerasmodels import sequential it still throws importerror asking me to install optree why,pythonx visualstudiocode keras deeplearning importerror,solved it has everything to do with typingextensions dependency which version was too old i realized about it this way i ran the following script in a jupyter notebook cell which threw searching all over stackoverflow i found this answer to a similar problem so i followed the advice anr ran such command pip install typingextensions upgrade on vs code terminal then i ran pip install tensorflow and then ran pip install numpy to finally reboot python kernel to be able to import such dependencies with issues and warnings
78467829,valueerror data cardinality is ambiguous make sure all arrays contain the same number of samplesx sizes y sizes,python deeplearning neuralnetwork convneuralnetwork artificialintelligence,the example of sklearnmodelselectiontraintestsplit states xtrain xtest ytrain ytest traintestsplitx y testsize randomstate since the code you provided is assigning the returning splittings in the wrong order i am assuming you are providing the fit function of your model with the input test data instead of the desired output data of your train splitting try the following
78460997,mnist problem with mnisttrainimages forbidden,machinelearning deeplearning neuralnetwork classification mnist,it seems indeed that the web server is misconfigured as this dataset is builtin in many standard libraries like keras see this tutorial it is not so frequently downloaded from the lecun url i think in the source mnistinitpy there is a comment and can be set by the user using mnistdatasetsurl mnisttemporarydir lambda tmpmnist datasetsurl temporarydir tempfilegettempdir so theoretically you could set the mnistdatasetsurl variable for a mirror and it should work the only mirror i found with the original format is this but this is and it did not work for me so instead you can manually download the data from the github mirror into the temp directory shown by this code import temp file tempfilegettempdir and then mnisttrainimages should work
78436209,yolov plotting labels issue satellite imagery data,deeplearning imagesegmentation yolo yolov,for the object segmentation task the required labeling format for yolov is the following you need to keep normalized segment coordinates labelclsspolygoncoord and get rid of the redundant bbox part xcenterycenterbboxwidthbboxheight as it is needed only for the object detection task in the case of object segmentation the yolov program code will easily calculate bbox coordinates out of the provided segment coordinates so in your case i suspect the model accepts the bbox coordinates in a label as a part of the segment and plots this distorted result
78373468,input shape error when updating pretrained cnn from binary classification to multiclassification,python tensorflow keras deeplearning convneuralnetwork,your label data is not categorical modify getfeaturesandlabels return output to
78362630,how to fix a typeerror in this tensorflow code,python tensorflow keras deeplearning artificialintelligence,add accuracy metric in compile you will get only loss value without it
78355619,importerror cannot import name imagedatagenerator from tensorflowkeraspreprocessing,python pythonx tensorflow keras deeplearning,try this or this is depcrecated as well deprecated tfkeraspreprocessingimageimagedatagenerator is not recommended for new code prefer loading images with tfkerasutilsimagedatasetfromdirectory and transforming the output
78344194,facing an issue while training unet for,python keras deeplearning semanticsegmentation unetneuralnetwork,youre likely running into the issue that your input must be divisible by n where n is the number of filter layers in your model each layer on in the contracting path or left side of the unet divides the number of pixels in each dimension by two while allowing the number of filters to increase on the expansion path each layer doubles the number of pixels however the number of pixels is always required to be an integer if you have five pixels and you divide it by two and round down you get two pixels double that again and you get four pixels which doesnt match the five pixel layer anymore comparing this to your example you have five layers so all inputs shapes must be divisible by but is not divisible by this is a problem because of the concatenation step when combining the input from expansion path with the input from the contracting path the dimensions must be compatible except for the axis youre concatenating on so if you had shapes of none and none that would work however you have a mismatch in dimension there are various ways to handle this pad crop or resize the original a multiple of n this is likely the simplest it doesnt require modifying this library within the unet crop one input the size of the smaller concatenation within the unet add up to row or column of padding at each layer if needed
78314572,how to solve outofmemoryerror cuda out of memory in pytorch,python deeplearning memorymanagement pytorch,your model is too big or your input is too big you do not have much choice use a smaller model or use smaller inputs xx is usually a very big array for most networks if you work with images they often take images like x or x as input so you need to resize or do tiling
78308901,runtime error coccures when using torchsummary,deeplearning pytorch cupy,torchsummary works by running your model on a sample input and observing the shapes of intermediate results torchsummary does this by creating a random tensor of yourshape that is it takes the shape you suggested and prepends a dimension of size so an input shape of becomes also note that while your model expects a cupyndarray as an input torchsummary will pass a torchtensor this is perfectly fine because by convention classes that inherit from nnmodule should accept torchtensors rather than other types finally elevationview elevationshape elevationshape assumes that elevationshape and elevationshape will be your inputsize so this is a very strong assumption and it is better to assume that the last two dimensions will be heres a working version of your code but modified to take into account what i just described
78282837,unusual error when using nnsequential in model class in pytorch,python deeplearning pytorch,you are passing your input to the model constructor you need to instantiate the model before calling the forward method model net out modeltorchrandn
78265092,error when plotting confidence intervals with neuralprophet,python deeplearning neuralnetwork timeseries,if i understand correctly the error is stating you are missing a step in your code when you are initiating m neuralprophet yearlyseasonalitytrue weeklyseasonalitytrue dailyseasonalityfalse quantilesquantilelist nlags epochs nforecasts you are setting nforecasts doing so triggers the following if statement if selfhighlightforecaststepn is none and selfnforecasts or selfnlags you failed to set highlightforecaststepn and have set nforecasts thus the if statement is true because highlightforecaststepn equals none and nforecasts is higher then t to fix this you need to set the highlightforecaststepn attribute of your model this can be done by using the class function highlightnthstepaheadofeachforecast something like this will work mhighlightnthstepaheadofeachforecaststepnumber reference highlightnthstepaheadofeachforecast
78257348,whats causing the valueerror could not interpret identifier loss,python deeplearning neuralnetwork valueerror loss,try this use keras wrappers instead here is a helpful example for your reference
78253997,vision transformers runtimeerror mat and mat shapes cannot be multiplied x and x,python machinelearning deeplearning pytorch transformermodel,if you look into the source code of visiontransformer you will notice in this section that selfheads is a sequential layer not a linear layer by default it only contains a single layer head corresponding to the final classification layer to overwrite this layer you can do
78228902,how can i correct an inputshape error produced when following the overfit and underfit tensorflow tutorial,tensorflow keras deeplearning tfkeras,going through the tutorial it seems you maybe forgot to include the lines validateds validatedsbatchbatchsize trainds traindsshufflebuffersizerepeatbatchbatchsize which are just before this part scroll a bit up the important part for your error message is the batch call which divides the training and validation dataset into batches the confusing bit in the beginning is that with inputshapefeatures one sets the shape for one sample for the network in this step you ignore the batchsize but when you call modelfit it expects batches of data thats the none features shape it is none features and not batchsize features because tensorflow handles the batch size as a variable size none stands for variable size in this context this is mostly done just for the last batch which is in most cases smaller than the real batch size for example if you have samples and batch size you have batches of data points because in the last batch is not enough data to fill it completely to the error you got indicates unbatched data so the network gets one sample at a time but the network in tensorflow always needs a batch size even if it is just for one sample edit to solve the error valueerror arguments target and output must have the same rank ndim the problem here is that the targets has to be in shape batchsize labels if the targets have only one value as in this case the list often gets flattened and the second labels dim is not there to solve this do use one of the two solutions if your data is in numpy arraystensorflow tensors etc you can use labels labelsreshape or similar functions if you already have a dataset use ds dsmaplambda xy x tfexpanddimsy this maps the dimension expansion to every element of the dataset
78215817,valueerror dimensions must be equal resnet transfer learning tf,tensorflow keras deeplearning transferlearning,as you are using sparsecategoricalcrossentropy as loss i assume you have labels as a one dimensional scaler array but then you should also change you accuracy metric to work on these kind of predictions namely you should change your code as
78213696,after loading a pretrained pytorch pt model file modulenotfounderror no module named models,python deeplearning pytorch yolo yolov,in the repository you linked there is a zip file called source code which contains models and some other helper modules i was able to load the model in colab by downloading the zip expanding it to a directory in my google drive called yolov moving yolovtinypt to this directory and then running the following from googlecolab import drive drivemountcontentdrive import torch import onnx import sys syspathappendcontentdrivemy driveyolov model torchloadcontentdrivemy driveyolovyolovtinypt
78196998,pytorch matrix multiplication shape error runtimeerror mat and mat shapes cannot be multiplied,machinelearning deeplearning pytorch neuralnetwork linearregression,i reshaped the encoded word vector from to if input size is and batchsize
78196651,tensorflow error loading model attributeerror exception encountered when calling flattencall list object has no attribute shape,python tensorflow keras deeplearning,flatten works for a single tensor and it seems youre handing it a list that is why it says it cannot find shape on a list in order to fix this inspect your models shape you have this but add those two last lines look at the shape and ensure the layeroutput fits your models input
78186908,runtimeerror output with shape doesnt match the broadcast shape,deeplearning pytorch computervision dataaugmentation imageaugmentation,assuming that xi has the same shape as xaug and maski has the same shape as maskaug no broadcasting is required for those however you said ws i was d of shape that means you need to unsqueeze two dimensions no more a double indexing with none should work
78185614,typeerror while unpacking train data loader in pytorch,machinelearning deeplearning pytorch,tqdm is both the name of the library and the name of the main class contained inside you should simply change at the start of your file to this can be confusing for beginners but is often the case in python for packages with one main class you should always try to reduce your buggy code to a minimal reproducible example before posting
78179722,docker container importerror libglso cannot open shared object file no such file or directory,python docker opencv flask deeplearning,the solution is i added this package to requirementstxt file opencvpythonheadless but now i have a camera error
78170750,python tensorflow keras error when load a json model could not locate class sequential,python tensorflow machinelearning keras deeplearning,after checking the version because of kartoos comments yes my kaggles notebook use tensorflow version meanwhile im trying to load the model to tensorflow version i tried to install but i cant find a way so im build and retrain my model using version tensorflow and it works no more error and the model can works fine when get saved and loaded again saved the model as keras instead of json modelsavemodelkeras and load the model again loadedmodel kerasmodelsloadmodelmodelmodelkeras thanks
78089650,tensorflow keras error attributeerror tuple object has no attribute lower,python pythonx tensorflow keras deeplearning,the error youre encountering occurs because the tokenizer class from keraspreprocessingtext expects a list of strings ie text samples but is receiving a list of tuples instead understanding the error the error message attributeerror tuple object has no attribute lower happens because the tokenizerfitontexts method is trying to call the lower method on each text sample but it encounters a tuple instead of a string solution you need to adjust your code so that you pass a list of text strings rather than tuples for your chatbot task you should separate the input and output text into two different lists then combine them when needed
78088735,using batches results in errors d or d tensor expected for input expected input to have channels but got channels,deeplearning pytorch generator,the issue is that you are not using torchdatautilsdataset please read the documentation page for more information you dont have to worry about assembling the batch yourself the point is for your datasets getitem to return a single element at a time its the job of torchdatautilsdataloader to collate the data properly depending on a batch size here is a demonstration following your example first define dummy data make sure the number of elements is larger than of course then initialize the dataset and wrap it with a data loader now you can iterate using dataloader which provides a sampler to navigate through the dataset
78058590,how to fix cppexception forward expected a value of type tensor in pytorch android but the same model works fine in python,python android deeplearning pytorch tensor,solved by changed android invoke to
78048435,trying to understand pytorch runtimeerror trying to backward through the graph a second time,python deeplearning pytorch recurrentneuralnetwork,adding totalloss totallossdetach after modeloptstep is indeed the solution as clarified by c p to properly update the optimizer from the averaged loss after each batch
78034587,textvectorization issue,tensorflow keras deeplearning,following the tensorflow documentation on text classification i modified your code and applied textvectorization as a preprocessing step where the vectorized text is passed to the model together with the label below is a fully working code snippet using a subset of the data you provided in your comment
77942574,multioutput resnet model in keras issue with loss dictionary and training,tensorflow machinelearning keras deeplearning multilabelclassification,if you use sequential you have in general only one output use the functional approach to have multiple output layers
77939328,error in google colabe valueerror no loss found you may have forgotten to provide a argument in the method,tensorflow opencv keras deeplearning convneuralnetwork,i think the problem is that the target data paddedcategorysequences and the input data paddedtodistributesequences have different numbers of samples which causes a valueerror add this after the data processing i am assuming that paddedcategorysequences is your target data
77932233,training xception model keras batch size gives error but it works for batch size,tensorflow keras deeplearning,nothing to do but reduce the batch size unless you want to decrease the size of the use a smaller architecture batch size must not necesarily be a power of you can try with batchsize gradually increase from until you reach the limitfully utilize the gpu memory
77824012,pytorch layernorms mean and std div are not fixed while inferencing,deeplearning pytorch normalization transformermodel inference,from the pytorch documentation this layer uses statistics computed from input data in both training and evaluation modes the ex and varx are calculated on every input tensor only the and are fixed at training time thus what you are observing is the correct and expected behavior
77782475,valueerror input of layer fullmodel is incompatible with the layer when creating a multiview variational autoencoder model using keras,machinelearning keras deeplearning tfkeras keraslayer,this crashes because you are giving the encoder inputs to the decoder which doesnt make sense there is no need to have any decoderinputs at all if you are not planning to use the decoder separately you can simply chain the encoder and decoder in your createvae function and then remove decoderinputs completely
77735814,mljflux crashes only on gpu with methoderror no method matching parent,deeplearning julia,updating julia packages has resolved the issue the code is working under julia with flux cuda mljflux and mlj
77714222,http error when calling the duckduckgo api,python deeplearning artificialintelligence,on the right side of the webpage under notebook options find a drop down menu called environment change this to always use latest environment rerun all the cells including the cell with the import statement and see if the error persists
77698907,pytorch problem with shape of model output,python deeplearning pytorch neuralnetwork tensor,since the output has the shape of batch output length channel this means that in the output we can see the contribute to the final prediction of each featurechannel this means we need to sum all the channels to get the final prediction this can be achieved by doing
77673041,kerasmodelsloadmodel gives error on hfopen,python machinelearning keras deeplearning,model can means few different elements python code in variable model models weights saved in file h and you are confusing these concepts if you want to load code from dnnmodelpy then use standard import but this gives fresh model without pretrained weights in model and it needs long time to train it so we use file h to keep pretrained weights from model and later we load it again to create model with pretrained weights and we dont have to waste time to train it again
77672156,how to solve a runtimeerror expected input to have channels but got channels instead in pytorch,python deeplearning pytorch,you need to change selfconv nnconvd to selfconv nnconvd
77631145,can not solve this error in pytorch runtimeerror one of the variables needed for gradient computation has been modified by an inplace operation,python deeplearning pytorch generativeadversarialnetwork,when training a gan you need to separate the loss computations for the generator and the discriminator in particular you dont want your discriminator loss to backprop into the generator you want to do something like this with tqdmtotalintitermax as pbar for idx x y in enumeratetrainloader xreal yreal buildinputx y device xfake yfake generator gloss glossfunctiondescriminatorxfake yreal goptimizerzerograd glossbackward goptimizerstep dloss dlossfunctiondescriminatorxfakedetach yfake doptimizerzerograd dlossbackward doptimizerstep using xfakedetach is the important part that prevents the discriminator loss from backproping into the generator it should also clear up the need to retain the graph
77616147,both pytorch model and tensor on gpu but getting all tensors not on same device error,deeplearning pytorch,in cnnmlp the fclayers member needs to be an nnmodule because it contains other modules when you call to on your model to will be called recursively on all the other members that inhereit nnmodule fclayers is a python list not an nnmodule so the parameters of these modules are not being transferred to the gpu the simplest fix would be to replace selffclayers with selffclayers torchmodulelist see torchmodulelist for more information a better solution would be to make fclayers an nnsequential and then call it like a normal nnmodule during forward
77611685,problem building cnn only using python numpy when gradient descent and batching,python deeplearning convneuralnetwork gradientdescent,is just a constant that is removed it effectively corresponds to a learning rate if you remove it you get this phenomenon where your learning rate is too high if you change all weight updates from to then it is no longer finding the minimum loss it is now finding the maximum of the loss ie the worst possible model however in your case you only changed it for one layers weights so what probably happens is that your other parameters compensate for it
77584021,error with pytorch runtimeerror mat and mat shapes cannot be multiplied x and x,deeplearning pytorch dataset objectdetection tensor,in your model you have nnflatten which flattens the images to a dimensional vector the comes from having colour channels if you want to use a fully connected neural network for these images you would need to set the size of the first layer to
77542331,inconsistent batch size issue for multilabel classification input output,python deeplearning pytorch huggingface imageclassification,in your model architecture youve defined types of d convolutional layers selfconv selfconv selfconv and one maxpooling layer selfpool for the input tensor size after passing through the specified layers based on your input tensor size after the through the size of x would become now youd like to utilize the fullyconnected network in the subsequent module to do this you should replace the line with this modification flattens the tensor to the shape consequently you should adjust to the revised wikiartmodel class is as follows
77537989,docker error oci runtime create failed runc create failed unable to start container process exec executable file not found in path unknown,python docker flask deeplearning computervision,to run the command specified in the dockerfile ie python applicationpy to run the command specified in the dockerfile and see its output as it runs to run an interactive shell inside the docker container eg for debugging the command that you provided tries to run a program called which does not exist the t flag is used to allocate a terminal to the container while the i flag makes it interactive
77516384,deep learning cnn valueerror aslist is not defined on an unknown tensorshape,python tensorflow keras deeplearning convneuralnetwork,i think that you did some unnecessary steps with asnumpyiterator your imagedatasetfromdirectory is a tfdatadataset which you can manipulate directly without turning it into a numpy iterator you will simply need to use tfonehot instead of tfkerasutilstocategorical heres a full example that works using a local version of mnist youll have to change the path and number of categories
77430984,typeerror expected tensor as element in argument but got list,python deeplearning computervision,replace with and also replace with you would find the generated images in the results folder
77402876,runtimeerror given input size xx calculated output size xx output size is too small,python machinelearning deeplearning maxpooling,the primary issue lies in your input size if you examine the specrnet architecture youll notice that it includes some maxpoold modules lets consider an example where we input a tensor with the size here are the outputs of each layer within the specrnet we observe that the shape is halved after passing through block block block and undergoing maxpoold operations since specrnet utilizes block block block and applies maxpoold times your input size should ideally be which equals on the other hand because you define your model architecture in configpy as it means that your input channel is in summation your input size should be batchsize
77297506,add new layer error message the added layer must be an instance of class layer,python tensorflow keras deeplearning neuralnetwork,referring to the tensorflow guide
77287416,how to remove runtimeerror expected d unbatched or d batched input to convd but got input of size,python machinelearning deeplearning pytorch convneuralnetwork,you just reshape the tensor bs c h w imagetensorshape flattenedtensor imagetensorreshapebs c hw also doing a d conv on a channel input is kinda pointless this is why most mnist models just drop the channel and send the flattened input to a mlp
77255420,keras training valueerror setting an array element with a sequence the requested array has an inhomogeneous shape after dimensions,python numpy tensorflow keras deeplearning,the problem was that apparently there are some grayscale images channel in my dataset that caused errors so i added this line in loaddata to make sure only channel images are added and it fixed the problem
77117873,how to fix tuple error index out of range,pythonx deeplearning,traintestsplit function returns four separate variables you are placing the variables is wrong order which causes a mismatch when you try to access featurestest you need to assign the returned values correctly rearrange the order of variables
77110863,keras reshape layer error total size of new array must be unchanged,python tensorflow machinelearning deeplearning artificialintelligence,after your last maxpooling layer you have a data shape of the flatten layer turns that to is not divisible by without remainder so it cant get reshaped into that find another divisor instead of that divides without remainder or find another input shape that is divisible by after the last maxpooling layer
77102998,loading npy into torch throws a missing key error,python numpy deeplearning pytorch,the solution is to combine both of your ideas as they address two different issues one is renaming keys and the other is converting numpy arrays to torch tensors this will probably work
77079598,yolo v training on nano dataset not working,python deeplearning computervision yolo yolov,your error seems to be related to incorrect setup with clearml if you dont need it just uninstall like this you can set it up again later but you probably dont even need it
77077474,typeerrorunsupported format string passed to function format,python machinelearning scikitlearn deeplearning,you are trying to print functions instead of the values they return for instance you assign but then try to format the function not the result instead you should use the calculated value but youve also created a dictionary with rounded values you could use that dictionary with an fstring or use the format method instead
77073535,runtimeerror sizes of tensors must match except in dimension expected size but got size i always get mismatch error always factor of,deeplearning pytorch artificialintelligence imagesegmentation unetneuralnetwork,it is the dimension problem of the input data inputs and labels shoud not have three dimension the dimesion should be torchsize torchsize if the batch size is or torchsize torchsize if the batch size is you forget the batch dimension in the first dimension the codes below works fine
77022231,softmax is not working i have syntax error somewhere,python deeplearning max,solution you can use the scipy package to calculate softmax as follows
77010461,how to fix typeerror list object is not callable when loading data for training,python deeplearning pytorch dataset loading,in python functions and variables can override each other therefore when you define the function train you are overwriting the train dictionary of data python function and variable with the same name i would suggest just renaming the train dictionary to traindata i would also watch out for naming variables input or list as this will cause issues since you are overwriting pythons default classes
77006924,debugging autoencoder training loss is low but reconstructed all black,python tensorflow keras deeplearning autoencoder,i ran on my implementation with no problems i ran on a copy of the vae tutorial and got the following results after epoch i didnt have your data so i could not check if your data loader is doing any mistakes you probably want to check your call functions on your autoencoder and decoder but following the tutorial code i did not find your issue i would suggest a comprehensive debug with your data check data inputs structures shapes etc sanity tests are good try removing the test validation ensure model works on known dataset ie mnist try calling your vae on the mnist data set and if it doesnt work then something is wrong here unless i have some samples of your input data there really is no way to properly debug your code i highly suspect the data inputs because the model construction code looks fine
76966794,how to fix modulenotfounderror no module named quaternion problem,python numpy deeplearning package quaternions,install quaternion with conda instead of pip it will solve your problem i use quaternion too and only conda seemed to install it properly with all the dependencies heres how you do it with conda add condaforge to your channels install quaternion by conda in case you dont have conda in your system heres how you install it by following the conda documentation condainstallation
76935321,sb attributeerror dummyvecenv object has no attribute getactionmeanings,deeplearning reinforcementlearning stablebaselines,i managed to solve using envs is a list of environments
76900186,keras variational autoencoder with imagedatagenerator returns invalidargumenterror graph execution error,tensorflow keras deeplearning autoencoder imagegeneration,means that for this operator the input shapes are not broadcastable eg and are not broadcastable take a look at numpy docs for broadcasting rules to facilitate debugging you can use eager mode during training that can be enabled by replacing vaecompileoptimizeradam with vaecompileoptimizeradam runeagerlytrue hopefully this will give more hint in terms of the mismatched shape
76865316,real time object detection with yolo model not working,python deeplearning pytorch convneuralnetwork yolo,yes you cannot see anything due to the none type of object return in the resultsrenders you can change the code script like this and write the frames in the video the full code should look like this references
76832175,typeerror float object is not subscriptable while printing,python deeplearning neuralnetwork runtimeerror trading,balances is a dictionary so if symbol is inside this dictionary balanceinfo is a value of the balance under this symbol and thus in your current code a float in the next line you treat balanceinfo as a dictionary which causes an error you probably meant to do inside your get current balance function so that under symbol key you actually still have a dictionary rather than a float
76822814,tensorflow inceptionv importerror cannot import name tensor from tensorflowpythonframework,python tensorflow keras deeplearning,so when tfmodelsofficial or slim is installed the tfslim package is installed as well here the tfslimlayersutils module was updated to import tensor from tensorflowpythonframework however the tensor module is only present starting from tensorflow you can see this by comparing the different branches here i made it work by installing tensorflow pip install tensorflow tfmodelsofficial possibly not necessary cd pathtoclonerepo git clone as described here in my python script
76802326,assertionerror size mismatch between tensors when modelling timeseries data,python deeplearning pytorch,try squeezing your tensors using squeeze method before applying tensordataset use xtraintensors torchfloattensorxtraintensors ytraintensors torchfloattensorytraintensors ytraintensors ytraintensorssqueeze traindataset tensordatasetxtraintensors ytraintensors
76786725,this variational autoencoder fails with invalidargumenterror,python tensorflow keras deeplearning,the following modification seems to fix the issue reported earlier while the code training gives diminishing losses across epochs change the vaeloss function here are the modified python instructions
76779213,attributeerror cutoff time dataframe must contain a column with either the same name as the target dataframe index or a column named instanceid,python deeplearning featureengineering featuretools,the error message here is really pointing you in the right direction it says that the cutoff time dataframe must have a column with the same name as the index column of the target dataframe in your example you set the target dataframe to rul data but the index of that dataframe is set to the column named index since there is no column in the cutoff time dataframe named index you are getting this error based on the example notebook you have linked i think you really want to have targetdataframename set to the normalrul dataframe which has its index set as engineno
76748038,detection object with custom yolov model by using sahi attributeerror module yolov has no attribute load,python deeplearning pytorch objectdetection yolo,you need to add two thing in sahi library in your environment yolovcustompy class with your model and add your model to dictionary in automodelpy below i place code in yolovcustompy and add your new model to dict in automodelpy
76742397,valueerror inputs have incompatible shapes received shapes and,python tensorflow keras deeplearning resnet,the problem is that you reduce the input dimension with the stride and padding doesnt counter that so both your mergeinput convd layer as well as your data convd layer reduces your x input to x as you can see as the second shape in your error message but then the convd layer further reduces the x output from bn to x with stride x to solve that you could either remove the stride from either the first convolution mergeinputor the second convolution convd or reduce data to x looking into resnet building blocks like this one you can see that it uses only one stride on both the normal connection and the skip connection edit for your new error looks like your target data is only one number most likely label encoded look here it should work when you switch to sparse categorical crossentropy or onehot encode your target data but without knowing how your data looks it is just a guess
76644106,how to resolve runtime error in cnn model related to mismatch input size,machinelearning deeplearning pytorch neuralnetwork convneuralnetwork,when you do replace with the following to add the channel dimension this is similar to unsqueeze if you have numpy loaded you can replace none with npnewaxis which is more informative but the same thing
76577467,pytorch is not working with trained modelpretrained model intel open vino,python opencv deeplearning pytorch openvino,i tested and compared your model with openvino pretrained model pedestriandetectionadas since you mentioned that this is the reference to your dl model development i inferred both model with object detection python demo i get this from omz repo my finding is that your model does not have the correct wrapper as the ov modelthe pedestriandetectionadas uses network based on ssd framework with tuned mobilenet v as a feature extractor perhaps this is the part that you need to cater to your custom model
76511182,tensorflow custom learning rate scheduler gives unexpected eagertensor type error,python tensorflow machinelearning deeplearning transformermodel,i ran across this just yesterday its a type coercion issue since the value of step being passed into call is int so the math is converting everything to int for your specific case this should probably fix it class lrscheduletensorflowkerasoptimizersscheduleslearningrateschedule def initself dimembed warmupsteps selfdimembed tensorflowcastdimembed dtypetensorflowfloat selfwarmupsteps tensorflowcastwarmupsteps dtypetensorflowfloat def callself step step tensorflowcaststep dtypetensorflowfloat return selfdimembed minstep step selfwarmupsteps
76510708,simple gan runtimeerror given groups weight of size expected input to have channels but got channels instead,python machinelearning deeplearning pytorch generativeadversarialnetwork,according to pytorchs documentation for convd the input has to be batchsize channels sequencelength also note that the data is eeg voltage values the channel should be ie rgb should be so for samples of x eeg voltage values the input should be reshaped into
76482724,filenotfounderror when deploying a python streamlit application on the streamlitio platform,python deeplearning streamlit filenotfounderror,the path is relative to the current working directory which is not the same as files location when you run your code locally youre probably in that directory and just doing python myfilepy but thats not what the platform is doing so the current path is different the current python files path can be accessed using file then we could use remove filename from that to get the directory i like using pathlib for that import pathlib codedir pathlibpathfileparentresolve resolve converts the path to absolute for safety pathlib allows do what ospathjoin did using operator and strings fileslocation codedir data s fileslocation fileslocationresolve because we used in path its safer to resolve so we get absolute path alternative forms of the fileslocation codedir data s line fileslocation codedir datas fileslocation codedirparent data s fileslocation codedirparent datas use whichever seems the most intuitive for your usecase listing the dir oslistdir accepts pathlike object and pathlibpath is pathlike so we can just do oslistdirfileslocation other lines to convert instead of filepath ospathjoin data s selectedvideo we can do filepath fileslocation selectedvideo and outputpath ospathjointestvideomp becomes outputpath codedir testvideompto dump it in the same location as the code file is what you had locally
76472778,im getting an error while running this code snippit i seems like the code is unable to create the numpy arrayi think its running an infinite loop,python numpy tensorflow machinelearning deeplearning,i think you need to create the full path where you are saving your numpy array more specific you need to make all the subfolders in the defined path so you can call osmakedirsnpypath exitoktrue before calling npsave the parameter exitoktrue will not create the folders if they were already created
76434294,pytorchscarf package runtimeerror expected all tensors to be on the same device but found at least two devices cuda and cpu,python deeplearning pytorch runtimeerror tensor,the problem is in scarflosspy file you should replace the line with the author forgot to move mask tensor to zidevice
76428833,time issues with training a ho autoencoder,pandas deeplearning ho autoencoder onehotencoding,the problem here is the number of columns while the number of rows control the overall training time the number of columns control the training time per row having is quite a lot if you can do some data munging and reduce the number of predictors you use it will definitely speed up training you can also try the following set stoppingtolerance to a higher number or higher this will enable early stopping to stop training if the average improvement in some metrics does not improve by compared to the last one set maxruntimesecs if you want to stop the model building after seconds reduce scoretrainingsamples from default of to say this will perform scoring on a smaller number of samples and hence can reduce training time note that stopping the model early as in may reduce the model training time but will get you a model that may not be a good fit for your data
76413619,backward propagation method not working in first convolutional layer broadcast error,python deeplearning neuralnetwork computervision convneuralnetwork,the problem was that my backward method wasnt designed to deal with when a stride greater than was applied in the forward method to resolve the issue dedy had to be dilated before performing the correlated operation the updated backward method is as follows some of the code within if selfinputlayer true is hardcoded for that first layer so it wont generalize to layers with different strides for further information regarding the dedy dilation id recommend this article he also has an article addressing the calculation of dedx with strides but as i was just dealing with the input layer i was only interested in the derivatives of the weights
76395485,npconcatenate dimension problem of channel,python numpy imageprocessing deeplearning,i think the problem is that there are masks of different sizes but npconcatenate requires nparays in the same dimension for example this case will work well but this one will show the same error that you have i would recommend to print and deep dive into function adjustdata for those samples where mask have different shape
76299386,how to fix error module is not callable,python deeplearning neuralnetwork,ucf is a module i think you want to use ucf class you can directly use it from pytorchvideodata or you can import it from pytorchvideodataucf so your final code will look like this testdataset pytorchvideodataucf datapath ospathjoindatasetrootpath test clipsampler pytorchvideodatamakeclipsampler uniform selfclipduration decodeaudio false transform valtransform
76298066,how to solve broadcast issue in deep learning,python tensorflow scikitlearn deeplearning,the root cause of the error is shapemismatch in those tensors see step colstoscale feature feature traindatascaled scalerfittransformtraindatacolstoscale traindatascaled has the shape of note that the traindata has features step trainpredictions modelpredictxtrain trainpredictions has the shape of trainpredictions scalerinversetransformtrainpredictionsreshape flatten note that the output data predictions of the model has only one feature therefore it throws an exception because scalerinversetransform expects the input argument to have the same number of features as that of traindatascaled which was used to fit it solution i assume that the output feature and input features of the neural net model have the same meaning because of the way you prepare them in the createsequences you can trick inversetransform by padding the output tensor trainpredictions with arbitrary values so that trainpredictions have features as expected by scaler import numpy as np def paddingoutputy padding y so that newyshape yshape return nppady edge trainpredictions paddingoutput trainpredictionsreshape trainpredictions scalerinversetransformtrainpredictions
76269310,getting error while training yolact model with resnet backbone,python deeplearning pytorch model nvidia,i had a similar problem previously could not find a reason but some solutions that worked for me you can try the following fixes to solve it change the value of numworkers to source change the version of pytorch from to source
76266905,how to fix indexerror invalid index to scalar variable,python machinelearning deeplearning,i see the error is occurring when you are trying to getlayernames from your defined network basically you are trying to access an element of a scalar variable as if it were an array or list but in fact getunconnectedoutlayers outputs the position of the layers as an ndarray of d shape so to make it short even though it is a tuple treat it as int for cpu wcuda replace this line that you have simply to this instead hope this resolves your issue if so please mark this response as the answer
76124928,missing keys in statedict error when loading model,python deeplearning pytorch,as pointed out by tim roberts this is likely to be a version mismatch between the code used to produce mymodelpt and the one used to create your alexnetddropoutregression model a state dict is just a dictionary with layer names as keys and layer parameters as values thus the issue here is that layer names are different in the saved statedict and the model you created it is often possible given that it is only a naming problem without any changes to the architecture to convert the state dict for example by creating a new dictionary with corrected keys and values coming from the previous statedict in your case the new layer name would be the old one without the module prefix but be careful it is possible that some layers in both the model and the state dict have this module prefix and in this case you should not change the name of such layers i also did not verify whether all your missing keys have a corresponding unexpected key in the list
76083560,runtimeerror stack expects each tensor to be equal size but got at entry and at entry,python deeplearning pytorch convneuralnetwork,seems like the data in your train folder and the ones in your test folder dont have the same size one way to solve it is to change the transformsresize to
76045412,tensorflow cannot train neural network due to the valueerror slice index of dimension out of bounds error,keras deeplearning tensorflow tfkeras tensorflowdatasets,you forgot to batch the dataset you are supposed to pass tensor of size batchsize to the modelfit method
76041406,yolov despite having enough vram i get runtimeerror cuda error out of memory error even though there is not enough vram,python deeplearning pytorch nvidia yolov,it may be related to wsl both not letting you use most of your systems ram and also constraining the memory available for one application as that is one of the known limitations of wsl references to the nvidia wsl guide regarding its limitations etc pinned system memory example system memory that an application makes resident for gpu accesses availability for applications is limited for example some deep learning training workloads depending on the framework model and dataset size used can exceed this limit and may not work regarding how to fix this problem the following thread provides some advice on it setting a higher limit for your systems ram on wsl and updating the distribution may help getting a higher use rate of your hardware resources modify the wslconfig file to set a higher amount of system memory and also call wsl update to update your linux distribution within windows
76021698,runtimeerror mat and mat shapes cannot be multiplied only when testing single images with the same input size pytorch,python deeplearning pytorch,just unsqueeze the the get in pytorch you can do that with frameunsqueeze
75998650,dqn runtimeerror mat and mat shapes cannot be multiplied x and x,matrix deeplearning pytorch neuralnetwork runtimeerror,your model uses linear relu linear architecture where nnlinear is just matrix multiplication layer the first linear matrix is of size x and second linear matrix is of size x envactionspacen given matrices a and b of size mxn and pxq respectively for matrix multiplication between a and b n should be equal to p the issue is in dimension of input when passed for forward inference to the first layer for matrix multiplication your input tensor x of size bxxx should be changed to bx to match first linear matrix dimensions x where b is batch size solution use resize more generic or flatten to fix the tensor size or
75989228,i am trying to build a variational autoencoder i am getting an error while running modelfit which i dont understand,python tensorflow keras deeplearning autoencoder,i remember this happened to me as well it seems that tensorflow doesnt support a vaeloss function like this anymore i have solutions to this i will paste here the short and simple one instead of creating a vaeloss function you need to add the loss like this i also have another solution to this problem that is using a custom model you can find this solution here
75975877,error reshaping a dense layer in keras functional api,python tensorflow keras deeplearning,the mistake is you added the additional commas just delete them like this
75967951,pytorch weight initialization problem for dcgan,pythonx oop deeplearning pytorch dcgan,three problems use modelapply to do module level operations like init weight use isinstance to find out what layer it is do not use data it has been deprecated for a long time and should always be avoided whenever possible to initialize the weight do the following edit added convtranspose in condition
75953462,vgg transfer learning unkown metric function fscore error,python tensorflow keras deeplearning jupyternotebook,because you used a custom metric so now you need to do this whenever you have a custom object like a custom model layer metrics you need to do this if you save your model in h
75934781,i am creating an there is a problem when training a neural network,deeplearning neuralnetwork googlecolaboratory imageclassification,i guess it is because of the spelling of this loss function you are getting this issue it must be categoricalcrossentropy instead of catigoricalcrossentropy this must fix your issue
75918443,federated learning implementation code shows a runtimeerror all elements of input should be between and,deeplearning pytorch runtimeerror pytorchlightning federatedlearning,seems like your dataloader is not returning labels that fall in the desired range in all cases as it is safe to assume that the outputs of the sigmoid activation function do fall in this range though of course you could doublecheck that as well i recommend checking conformance with an assertion
75848677,how do i solve this error thattensorflow graph execution error,deeplearning neuralnetwork computervision convneuralnetwork tensorflow,just got a similar error while using the imagedatagenerator you must check this part of the error its throwing your code logits and labels must be broadcastable logitssize labelssize so i think what is going wrong is that you defined your output model to be outputs in the final dense layer but acctually it seems that the lable size is
75776364,valueerror layer model expects inputs but it received input tensors,tensorflow keras deeplearning reinforcementlearning dqn,i guess the issue is when you append the inputs to nstate and cstate in trainposnn because that as the error suggests yields tensors meaning your nstate is a list of lists likewise cstate instead of a list of numpy arrays each of size minibatchsize try this from collections import ordereddict def trainposnnself printin training start training only if certain number of samples is already saved if lenselfreplaymemoryposnn minreplaymemorysize printexiting training replay memory not full enough return get a minibatch of random samples from memory replay table listmemory listselfreplaymemoryposnn randomshufflelistmemory draw a sample samples randomsamplelistmemory minibatchsize starttime timetime prepare the batch state action reward newstate done zipsamples nstate ordereddictpos reqs numsatisfaction cstate ordereddictpos reqs numsatisfaction starttime timetime for nstate in newstate posnext nparraynstate reqsnext nparraynstate numbersatisfactionnext nparraynstate nstateappendposnextreqsnextnumbersatisfactionnext add elements for k v in zipnstatekeys posnext reqsnext numbersatisfactionnext nstatekappendv for currstate in state pos nparraycurrstate reqs nparraycurrstate numbersatisfaction nparraycurrstate cstateappendposreqsnumbersatisfaction add elements for k v in zipcstatekeys pos reqs numbersatisfaction cstatekappendv now concat each list of values in nstate and cstate to get a list with arrays each of minibatch size nstate npconcatenatev axis for v in nstatevalues cstate npconcatenatev axis for v in cstatevalues endtime timetime printtime endtime starttime printnextstate nstate len lennstatenpasarraynstateshape npshapenstate done nparraydonenone reward nparrayrewardnone qfuture selftargetmodeluavpospredictnstate targets reward selfgammanpmaxqfuture axis keepdimstrue fit the model selfmodelfitcstate targets epochs verbose endtime timetime printtime endtime starttime selftargettrain i havent tested it so it may not run at the first try if so check the shape of each element of nstate and cstate adjust then try again hope it helps
75649998,attributeerror module utils has no attribute getroccurve,python function deeplearning jupyternotebook,you need to define the getroccurve in the utilspy file currently there is no function named getroccurve in the file thats why youre getting the error
75649848,predicting data bug with keras,python keras deeplearning,from what i can see it trains fine and fails at the predict step see if it runs fine when you set xtest xtrain and not just the first slice or reshape xtrain
75649344,typeerror max received an invalid combination of arguments when trying to use beam search decoding,deeplearning pytorch huggingfacetokenizers huggingface ctc,according to this issue on github this error should be resolved by converting your logits to a numpy array
75605946,extract features by a cnn with pytorch not working,python deeplearning pytorch computervision convneuralnetwork,since you register a hook for avgpool which assign the output of avgpool to featuresfeats you should get the d vectors through featuresfeats or you can also remove the last layerfc of model so that the output of model would be the output of avgpool layer
75565097,runtimeerror indices should be either on cpu or on the same device as the indexed tensor cpu,deeplearning pytorch gpu pytorchlightning,so i solved it by this selfdatasetdatax should be in the same device as the indices inx selfdatasetdataxtoniddevicenidtoselfdevice hope it helps someone
75525013,opencv vector subscript out of range error in java,java tensorflow opencv deeplearning caffe,for what its worth i ran into this with version going back to fixed it for me
75492321,filenotfounderror errno no such file or directory for onehot encoded,python keras imageprocessing deeplearning onehotencoding,you forgot to join contentimdadulhaque with classname and imagefile for your variable imagepath do something like imagepath ospathjoincontentimdadulhaque classname imagefile
75482612,getting wrong input error in pytorch model,python deeplearning pytorch semanticsegmentation,the kernel group is of size filters or kernels in this layer channels per kernel should match channels per image kernel size height and width of and respectively and in your image as state in comments batch size channels height width to address this issue you can either remove a channel from your image or else add an additional channel to dimension of the kernel something like this
75440700,typeerror input y of mul op has type float that does not match type int of argument x in computer vision,python machinelearning keras deeplearning artificialintelligence,as delirium mentioned i used tfcastvariable tffloat
75396610,bad zip file error while loading the trained deep learning model,python tensorflow machinelearning deeplearning,i just needed to save the model with not torch because the model was built in tensorflow keras
75325073,pytorch tensor error valueerror expected input batchsize to match target batchsize,python machinelearning deeplearning pytorch convneuralnetwork,i dont think i could guess why your batch sizes arent matching up without seeing what your training loop looks like but if you started getting this error after implementing your shuffling code you could try replacing it with a pytorch dataloader which is the standard way of shuffling and batching training data in pytorch for example if i have an ordered dataset and i want to load batches of random examples at a time from torchutilsdata import dataloader dataset input input target rock input input target paper input input target scissors input input target rock input input target paper input input target scissors input input target rock input input target paper input input target scissors dataloader dataloaderdataset batchsize shuffletrue for batch in dataloader printbatch output
75317985,problem in lists content in dnn work by python,python list deeplearning artificialintelligence,in python when you assign a value from parameter a to another parameter b you need some attention whether the parameter b will change later that change could affect parameter a which is happening in your code you need to check the difference between a shallow copy and a deep copy in python a deep copy will avoid this issue in your case you can change two of your lines in the following way
75288567,multiprocessing freezesupport error in python,python numpy deeplearning pytorch pycharm,multiprocessing standard library module does not work correctly if your application does not have main function but executes it code during import time the solution is to move the code to main function like
75264456,valueerror inputs have incompatible shapes,python tensorflow deeplearning imagesegmentation,your model implementation looks correct the problem lies with the input shape the input shape must be divisible by like or you can use nonsquare inputs as long as the width and height are both divisible by for example would work in the code that errors you are using the shape but is not divisible by alternatively you can zeropad the inputs to have a shape that is divisible by your code includes a line to do that but as another answer notes you do not use the outputs of the zeropadding another thing instead of zero padding to i suggest zeropadding to the next highest multiple of so if your input width is you would zeropad to if your input width were you would zeropad to explanation the downsampling path of the fcn uses convolutional blocks to downsample the input because the model uses such blocks the input must be divisible by related to working examples testconvs testimginput fcninputheight inputwidth testfcndecoder fcndecoderconvstestconvs nclasses testconvs testimginput fcninputheight inputwidth testfcndecoder fcndecoderconvstestconvs nclasses testconvs testimginput fcninputheight inputwidth testfcndecoder fcndecoderconvstestconvs nclasses
75253989,typeerror where received an invalid combination of arguments,python deeplearning classification,the first tensor is an impostor you are mixing tensorflow and pytorch you used a tensorflowpythonkeraslayersdense to create the dense layer only pytorch tensors transform into tensorcondition when used in a condition so here you obtained a tensorflowpythonframeworkopstensor instead indicatorprobability dit did not turn into a tensorcondition where expected tensorcondition tensor tensor but got instead a tensorflow tensor pytroch tensor pytorch tensor which he proceeded to display as tensor tensor tensor hiding the datatype of the first attribute
75250628,while coding a gan and i encountered an error saying please explain this error and some possible solutions,deeplearning pytorch tensor generativeadversarialnetwork pytorchdataloader,please check your getgenblock function looks like you missed else branch or messed up the indentation and when finalblock false it returns none instead of return nnsequential nnconvtransposedinchannels outchannels kernelsize stride nnbatchnormdoutchannels nnrelu if cond return module return module always returns module when condition is met otherwise none i think you wanted this if cond return module return module when condition is met return module otherwise module and now compare the indentation
75236815,custom load datset error with mscoco data,python deeplearning dataset coco mscoco,the features should be like that pay attention on the type of the image image datasetsimagedecodetrue
75117132,typeerror no loop matching the specified signature and casting was found for ufunc greater,python numpy deeplearning pytorch yolov,try with
75099305,strange error typeerror forward takes positional arguments but were given,python machinelearning graph deeplearning autoencoder,def encoderforwardself x edgeindex x selfencoderx edgeindex inputs self x edgeindex encoder is just a nnsequential for sequentials the forward is defined as follows which can only take two arguments and should be the root of the error while a bit of a hassle to solve your problem write your forward method for encoder and decoder layer wise i dont know about the special layers you use and what output they produce so at best you can do something like for module in selfencoder x modulex edgeindex probably you need an extra if statement if you hit the relu sureway would be to do the forward pass manually def initself indim hiddendim latentdim selfencconv gnngcnconvindim hiddendim selfencrelu nnrelu selfencconv gnngcnconvhiddendim hiddendim selfencrelu nnrelu def encoderforwardself x edgeindex x selfencconvx edgeindex i dont know how these layer work and what output they produce x selfencrelux logvar selffclogvarx return mean logvar
75079576,indexerror target is out of bounds while training pytorch model,python machinelearning deeplearning pytorch,tldr you need to map your labels from to your model does not know that the five classes it predicts have names eg the first class is the third is and so on it only outputs a probability over these five buckets it is up to you to map the names into indices into the predicted classprobability vectors these indices should be valid in the range
75049828,tensorflow text classification shapes none and none are incompatible error,python tensorflow deeplearning,i resolved this according to the comments as follows for text classification use dense layer with number of unique labels in the end of the model convert string category labels to indexes and use sparsecategoricalcrossentropy and sparsecategoricalaccuracy in the model when converting results to string labels get the max valued output and get index of it in the labels list
75045055,error when run model sequential model was constructed with shape none for input kerastensor,python tensorflow machinelearning deeplearning imageclassification,as the error message suggests the problem is in the way that you load your data you are expected to pass an input of size none for this i would change the way you have programmed loaddataset check if the data is loaded correctly since you are working with also have a look at tfkerasutilsimagedatasetfromdirectory from tensorflow
75039459,for categorical class runtimeerror d or d target tensor expected multitarget not supported,python deeplearning pytorch neuralnetwork,the ytrainbatch in criterionytrainpred ytrainbatch where criterion is nnnllloss should be with the shape batchsize containig indices in the range nbclasses however according to your explanation ytrainbatch is with shape of batchsize therefore in order to solve your problem you should modify the line trainloss criterionytrainpredytrainbatch in your code with or with or with
75025366,valueerror a target array with shape was passed for an output of shape none while using as loss,python keras deeplearning,the model output is denseunits activationsigmoid layer it produces a number between and however your ytrainshape is which is a xx tensor for each input item for binarycrossentropy you should provide or values as ytrain ie the expected ytrainshape is and ytrainshape should be an array with s and s ground truth labels in your case it looks like ytrain is an image just as an input
74968093,most efficient way to use a large with google colab getting drive timeout memory errors,deeplearning pytorch computervision googlecolaboratory,i have suggestions subfolder strategy simply divide the data folder into subfolder with certain naming convention and adapt your dataset based on this convention you can see this relevant link google suggestion gcp object storage strategy you can use use google cloud storage bucket without changing data format upload your data to google cloud bucket give authorization to your colab environment and use gcp sdk to access your data in gcp i suggest you use bucket since object storage is ideal for data with large number of files this strategy might cause some overhead but it might not be that slow since you would use gcp both operated by google note there is also option where you can mount gcp to your colab i did not use this before update small note also found in the link below you will probably need to install some system packages for vm of colab relevant link for gcp and colab
74953148,stacking classifiers sklearn and keras models via stackingcvclassifier problem,machinelearning keras scikitlearn deeplearning mlxtend,the error is happening because you are combining prediction from traditional ml models and dl model ml models are giving predictions in the shape like this whereas dl model is predicting in shape like this so there is mismatch while trying to append all the predictions common workaround for this is to strip the extra dimension of predictions given by dl method to make it instead of so open the py file located inside anacondalibsitepackagesmlxtendclassifierstackingcvclassificationpy in the line and outside of if block add this so it will look something like this
74917051,tensorflow error on macbook m pro notfounderror graph execution error,python macos tensorflow deeplearning metal,after extensive searching this is due to the dependencies with anaconda compared to the tensorflow version installed via pip the version of tensorflow i have installed does not match the tensorflow dependencies hence the error the solution was to downgrade to the same version as the dependencies as well as downgrade tensorflowmetal consulting the tensorflowmetal plugin documentation tensorflowmacos is the last known working version to successfully interface with tensorflowmetal and the documentation here lists that version is the highest that is supported this resulted in downgrading both tensorflowmacos to and tensorflowmetal to once i did this and ran the sample code the training was successful
74903186,nonokstatus gpulaunchkernel error when predicting but training runs smoothly,python tensorflow keras multidimensionalarray deeplearning,i had the exact same problem and its now gone i changed a few things and at some point the error message changed to split on gpu requires input size change dtype of input and labels to bool unintentionally used float before i use batch size anyway i decreased the number of filters in my convlayers i use tf cudnn cudatoolkit generally i couldnt and still cant make sense of the error message invalid configuration argument but think its probably a memory problem my model is even smaller than yours but our arrays are huge my input is xx and labels xx hope that helps at least a bit
74884278,why is my error value increasing for this simple implementation of a neural network,python numpy deeplearning neuralnetwork,the main reason why your loss is increasing is that you calculate the opposite of the symmetric difference quotient in your code you have paramswij h bef losss paramswij h aft losss paramswij h deriv aft befh paramswij deriv if you switch the second and fourth lines and do the same for other hyperparameters the loss starts to decrease some other problems include not actually calculating the relu activation function youre appending row instead of row to the new array and im not sure if changing the value of the parameter while calculating its gradient is correct besides that the code looks ready for some hyperparameter tuning heres my final version coding utf import numpy as np import matplotlibpyplot as plt params paramsw nprandomrand paramsw nprandomrand paramsb npzeros paramsb npzeros creating training data values s nparray creating actual classification values for the training data simplearray nparray ttrain npzerossimplearraysize simplearraymax dtypeint ttrainnparangesimplearraysize simplearray def predicts find a a npdots paramsw paramsb calculate z relua z for row in a row for element in row rowappendmax element zappendrow z nparrayz assert npallz calculate a b zw a npdotz paramsw paramsb calculate z softmaxa z for i in rangelena row ai npmaxai summation npsumnpexprow row npexprow summation zappendrow z nparrayz assert npallclosenpsumz axis return z def losss predictions predicts array error npsumttrain nplogpredictions e return error losslist losslistappendlosss finding numerical derivative and updating parameters h alpha for m in range w paramswcopy for i in rangeintparamswshape for j in rangeintparamswshape paramswij h aft losss paramswij h bef losss paramswij h deriv aft bef h wij paramswij alpha deriv paramsw w w paramswcopy for i in rangeintparamswshape for j in rangeintparamswshape paramswij h aft losss paramswij h bef losss paramswij h deriv aft bef h wij paramswij alpha deriv paramsw w b paramsbcopy for i in rangeintparamsbshape paramsbi h aft losss paramsbi h bef losss paramsbi h deriv aft bef h bi paramsbi alpha deriv paramsb b b paramsbcopy for i in rangeintparamsbshape paramsbi h aft losss paramsbi h bef losss paramsbi h deriv aft bef h bi paramsbi alpha deriv paramsb b lossepoch losss printm lossepoch losslistappendlossepoch pltplotnparraylosslist
74868664,resourceexhaustederror only when finetuning a efficientnetvl in tensorflow,python tensorflow machinelearning deeplearning,the vl is a large model mb and so i think its normal to face resourceexhaustederror it depends on your gpu whether it can take it or not so the simple answer would be to use better accelerator however here are some common approach you can try but its not guaranteed use smaller input unfreeze not all layers but few etc enable mixedprecision configure jit compilation set memory growth for physical device gpu if possible use tpu accelerator freely available on kaggle and colab to set up tpu check this codeexample devicesection it might be helpfulalso check this ticket see the feature request section here you can find a gist you can use it to find the optimal batch size for training
74849380,keras sequential valueerror dimensions must be equal,python numpy tensorflow keras deeplearning,you have two problems in your code first the output shape of your model doesnt match your y so your last layer to be something like that i added softmax because i guess your are doing multi class classification modeladdtfkeraslayersdense activationsoftmax but with that last layer you still have a d shape into a d shape so you need to flatten it before passing to the last layer the full working exemple is x tfones y tfones printxshape printyshape model tfkerassequential modeladdtfkeraslayersdense inputshape activationrelu modeladdtfkeraslayersdense activationrelu modeladdtfkeraslayersflatteninputshape modeladdtfkeraslayersdense activationsoftmax modelcompilelosscategoricalcrossentropy optimizeradam metricsaccuracy printmodelsummary history modelfitx y epochs batchsize validationsplit
74834208,valueerror input of layer sequential is incompatible with the layer expected shapenone found shape,python numpy tensorflow keras deeplearning,after the code x imgtoarrayimg add code
74815755,valueerror unknown label type continuous when implementing regression,python keras scikitlearn deeplearning gridsearchcv,you mention wanting to implement a regression task but the wrapper you are using is the kerasclassifier for classification this is not meant for regression with continuous target hence the error message for classifiers use the kerasregressor instead as a wrapper and it should work fine
74804108,logistic regression from scratch error keeps increasing,python machinelearning deeplearning logisticregression gradientdescent,there was an incorrect assumption pointed out by jh from sklearnlinearmodel import logisticregression import numpy as np x nparray y nparray clf logisticregressionfitx y clfpredict array scikitlearn at appears to believe that testoutput should be a rather than a a few more recommendations m should be fine to remove its a constant so it could be included in the learningrate w should be initialized proportional to the number of columns in x ie xshape dw npdotx dz should be npdotdz x prediction in logistic regression depends on a threshold usually taking this into account would look something like the following initialize weights and bias w b npzerosxshape for in rangeepochs compute log odds z npdotx w b compute predicted probability ypred sigmoidz back propagation dz ypred trainingoutputs dw npdotdz x db npsumdz update w w learningrate dw b b learningrate db test z npdottestinput w b testpred sigmoidz printtestpred and a complete example on random traintest sets created with sklearndatasetsmakeclassification could look like thiswhich usually gets within a few decimals of the scikitlearn implementation as well from sklearndatasets import makeclassification from sklearnmodelselection import traintestsplit from sklearnmetrics import accuracyscore import numpy as np epochs learningrate def sigmoidz return npexpz if name main x y makeclassificationnsamples nfeatures xtrain xtest ytrain ytest traintestsplitx y initialize and w b npzerosxshape for in rangeepochs z npdotxtrain w b ypred sigmoidz dz ypred ytrain dw npdotdz xtrain db npsumdz w w learningrate dw b b learningrate db test z npdotxtest w b testpred sigmoidz printaccuracyscoreytest testpred
74734685,how to fix this value error valueerror decay is deprecated in the new keras optimizer,tensorflow keras deeplearning model facerecognition,as mentioned elsewhere the decay argument has been deprecated for all optimizers since keras whose release notes explicitly suggest using learningrateschedule objects instead since you cant obviously modify the canaro source code well you could but itd be very bad practice and definitely not recommended i see two options downgrade tensorflow to a version that employs a keras backend upgrade canaro to a version that supports tf if possible for those who got to this question after a google search and can still modify their source code they are not especially worried about canaro that is here you are an example snippet tf tf style
74710732,what are the differences between adapter tuning and prefix tuning,machinelearning deeplearning artificialintelligence finetuning fewshotlearning,these are alternatives to finetuning model they are essentially solutions that reside between fewshot learning and complete finetuning of models the other answer in this so post is completely wrong finetuning has nothing to do with neither prompt tuning nor prefix tuning these two are completely different techniques than finetuning correct reference to prompt tuning and prefix tuning are given below prompt tuning for prompt tuning k learnable parameter ie continuous token embeddings is appended to the input but the entire pretrained language model is frozen prefix tuning for k positions prepended to the input concatenate additional learnable weights for keys and values at every attention layer different to prompt tuning only learnable input vectors papers that introduced these techniques are given below prompt tuning prefixtuning
74690187,problem with model validation in pytorch lightning,deeplearning pytorch neuralnetwork pytorchlightning,i solved the question deleting modelvalidate in the main after this overriding trainingstep and validationstep when training the model a validation loop will be executed automatically and the results will appear on the file thank you
74680359,tensorflowcompatvinternaltracking has no attribute trackablesaver error,python tensorflow keras deeplearning tensorflowjs,update keras with pip install keras for kerasteamkerasaf
74667517,invalid shape error when trying to leverage kerass vgg pretrained model,python keras deeplearning vggnet,the categoricalcrossentropy loss for classes together with the batch size of dictate the shape of labels for each bach to be the labels are currently ordinal and one can use the sparsecategoricalcrossentropy loss for ordinal labels alternatively one can still use the categoricalcrossentropy loss but in conjunction with the onehot encoded labels for for and for the following code snippet can accomplish such an encoding the nature of data ordered or unordered helps determining whether onehot encoding is preferred or ordinal
74626924,statedict error for testing a model that is trained by loading checkpoints,python deeplearning pytorch,i stored the optimizer weights on that time i only changed learning rate this is not possible in the way you are describing it cf to the comments in this github issue if you change the optimizer eg editing the learning rate you cant load the previous state of the optimizer as both are incompatible using optimizerloadstatedict the way you did wont work therefore there are two solutions for that issue you can save the whole optimizer object itself including the configuration and state with which you left of or more simple with torchsaveoptimizer optimizerpt for saving and loading with optimizer torchloadoptimizerpt with this approach you wont be able to adjust your optimizer eg adjusting the learning rate generally this is the best way as you should start with a fresh optimizer if youre changing fundamental things such as the learning rate if however you decide that you specifically want to continue on your current learning state weights with a newly configured optimizer you may use the following option you can just save the weights themselves without the optimizers state as you will change the optimizer configuration anyway and then load the weights with a newly created optimizer with your desired learning rate
74580743,runtime error on wgangp algorithm when running on gpu,deeplearning pytorch generativeadversarialnetwork,here is one approach to solve this kind of error read the error message and locate the exact line where it occured look for input tensors that have not been properly transferred to the correct device then look for intermediate tensors that have not been transferred here alpha is assigned to a random tensor but no transfer is done fix the issue and test
74516426,assertionerror signal dimention should be of the format of n but it is instead,python tensorflow machinelearning deeplearning scipy,from the looks of it your audio file contains two channels which you can check by looking at the shape of the array that the wavread function returns sigshape the speechpyfeaturemfcc function expects a singlechannel audio i believe what you can do is to convert your audio to a single channel for example by averaging the two channels if you want your function to work on both singlechannel and multichannel data you can just compute the mean only if the signal of your audio is multichannel
74511992,weighted mean squared error in tensorflow,python tensorflow deeplearning neuralnetwork artificialintelligence,you can achieve this by creating a custom loss function however i would recommend using a different loss function for example you could use the msle mean squared logarithmic error loss function this loss function is penalized more for underestimating which is what you want to achieve because its exactly the case when predicted smaller than true value you can use it like this
74497849,why is my mac gpu not working in pytorch,deeplearning applem trainingdata pytorchlightning,i think the pytorch version you are using does not have mps acceleration you should download the nightly version of pytorch pytorch version does how mps acceleration it will have some issues because it is not the stable version
74482158,keraslayersmaxpoold valueerror,keras deeplearning,it looks like youve added an extra dimension for the batch size in the input keras does this internally so you can exclude it when defining the inputshape just change to
74411106,python neuron class name error on a class property,python deeplearning,i can see several errors the first to are in the init of your neuron class should be something like this your initialization parameters you need to put them in the init not when you declare the class if you use a list as a default parameter you can have inspected result since this is declared when the class is read and thus all instances will share the same list check this post for more info class neuron def initself number function forwardstonone if not forwardsto forwardsto selfnumber number selffunction function selfforwardsto forwardsto selfcurrentstate neurons may have any numeric state representing the sum of inputs from other neurons selfoutput you are missing selffuntion in your method fire def fireself selfoutput selffunctionselfcurrentstate selfcurrentstate this operator is for unpacking mappers as a dictionary you should use a single for list addconnections and removeconnections method this is not an error but is how i would recommend to do what you are trying if im not wrong you want to declare a base neuron class to inherit for the rest that is call an abstract class and you can use the module abc to do this check here to learn more about it note an abstract class cannot be instatiated
74351194,why i have this value error when training unet,python numpy deeplearning semanticsegmentation unetneuralnetwork,it seems that you have channels but the shape of the output has only channel the unet initialization has a parameter called classes which defines the output shape you should probably set this to classes when definining the model see documentation in and more specifically note the following snippet classes a number of classes for output output shape h w classes clarification the number of input channels is of no real consequence here the channel count changes multiple times througout the convolutional layers the skipconnection concatenations etc so the number of output channels vs the number of label channels is the only thing that matters in this context trainx could have channel channels or any other necessary number of channels and the error youre getting is only related to comparing ytrain and yval to the logitssoftmaxlabelestimations or whatever you want to call them which is set as mentioned above by explicitly using the classes parameter
74341292,runtimeerror mat and mat shapes cannot be multiplied x and x,python deeplearning pytorch convneuralnetwork,its better to use same padding convolution d if downsampling is performed by pooling
74340273,typeerror linear argument input position must be tensor not flatten,python deeplearning neuralnetwork convneuralnetwork,you are using the wrong flatten there are two options a flatten layer the flatten function itself in your network you are using the first but you should be using the second you are creating a flatten layer instead of applying the flatten transformation to your inputs or
74261495,pytorch runtimeerror the size of tensor a must match the size of tensor b at nonsingleton dimension,python deeplearning pytorch,you have the following problem you want your model to get as input the rockpaperscissors of player and the rockpaperscissors of player since you do onehot encoding you want to input values into your neural network for player and for player but you are trying to hey a densefeedforward neural networl can only take one vector as input but you are trying to pass two vectors of size three what you canshould do instead is to concat both vectors that means the first values is the onehot encoded result of player and the last values are the onehot encoded result of player like this therefore simply change the input data to this input and the model to this model linearregressionnuminputs numoutputs and it works
74258347,valueerror multilabelindicator format is not supported cannot make an roc curve due to error,python pythonx machinelearning deeplearning roc,the roccurve and auc functions only work on d arrays in your case you must loop for each label
74254333,invalidargumenterror input filename tensor must be scalar but had shape opreadfile,python tensorflow keras deeplearning,the input type for tfioreadfile has to be a string it cannot be a list im try if you have multiple images you can use a loop to go through them all see the docs for more information
74238140,valueerror and must have the same shape received none vs none,tensorflow keras deeplearning neuralnetwork convneuralnetwork,when you called the traindatagenflowfromdirectory function you used classmodebinary which means you will have the labels of your images as and only whereas you are have total predictions ie neurons in your final output layer hence the labels and logits dosent match solution use classmodecategorical which means that there will be as many labels as the number of classes do the same in testdatagen as well
74199540,valueerror layer model expects inputs but it received input tensors,python tensorflow machinelearning deeplearning valueerror,its quite clear that your code is so the inputs here you need inputs inputids and attentionmasks but in the fit function you only pass inputs to the model so you should learn more about the model before you fix that bug i mean you need to know that what your model expect to and the structure of input or output of your model
74198765,typeerror only integers slices ellipsis tfnewaxis and scalar tfinttfint tensors are valid indices,python tensorflow machinelearning deeplearning openaigym,functions decorated by tffunction will convert lists into tensors internally to index a tensor a in one particular axis using another tensor b you can use tfgathera b instead of the familiar indexing syntax ab concretely try the following modifications in your dobatch function
74095607,valueerror shapes none and none are incompatible,tensorflow machinelearning keras deeplearning,shape none belongs to the trainy that came from the labelbinarizer shape none is the shape of the models output the loss function needs two arrays with the same shape to compute the error as the valueerror states these two arrays are not the same shape consider checking the output shape of your binarizer or changing the number of neurons in the networks last layer to match the number of unique values in your label column of the dataset
74081015,valueerror supported target types are binary multiclass got unknown instead in dataset kfold split,python deeplearning datascience valueerror kfold,as the first comment says you need to figure out what type y is i downloaded the referenced code from github ran portions of it and it turns out y is of type apparently that is not supported by current versions of sklearnmodelselectionstratifiedkfold which is what your kfold object is the following will allow you to proceed add the statement y nparrayy dtypenpint after your getdata call and the error should go away
74074298,invalidargumenterror and invalidargumenterror graph execution error tensorflow while creating lstm model,python tensorflow machinelearning deeplearning,as stated in the error valueerror layer embeddedessay weight shape is not compatible with provided weight shape the error has occured due to shape mismatch hence try increasing the input dimension of the embedding layer like below also increase the vocabulary size as follows in order to avoid the error kindly refer to this gist for the complete code and this example for more information on the error thank you
74063645,attempting to run stable diffusion but python returns getting requirements to build wheel error,python cmd deeplearning path environmentvariables,i found the solution on reddit ill summarize here for anyone that has the same problem okay so i didnt realize there are actually two paths a user path and system path also for some reason even though i chose to add python to the path during installation it was not added to the system path only the user path i copypasted the entries for python from the user path to the system path i also deleted all python entries from both paths although that was probably unnecessary i was then able to follow the rest of the stable diffusion guide though upon trying to generate an dmlonnxpy im getting another unrelated error load model onnxunetonnx failed which probably deserves its own question so back to the grind i guess
74034144,how to plot a list of torchtensors runtimeerror cant call numpy on tensor that requires grad use tensordetachnumpy instead,python numpy deeplearning pytorch tensor,you have a list of tensors rather than a tensor modify your initial code to store your losses as numpy arrays or singular floats by taking the mean if you are on gpu you might also need to convert to cpu in that last line
74031148,callbacks in keras gives keyerror metrics while predict,tensorflow keras scikitlearn deeplearning,the callbacks params only have values used in fit call in this case verbose epochs and steps if you want to access models metrics from within callback you need to set the model for callback with outbatchsetmodelmodel and then access it with selfmodelmetrics inside callbacks method here is your callback implementation with fixes class nbatchloggerkerascallbackscallback a logger that log average performance per steps def initself display selfstep selfdisplay display selfmetriccache def onbatchendself batch logsnone selfstep for k in selfmodelmetrics if kname not in selfmetriccachekeys selfmetriccachekname selfmetriccachekname logsgetkname if selfstep selfdisplay metricslog for k v in selfmetriccacheitems val v selfdisplay if absval e metricslog s f k val else metricslog s e k val printstep formatselfstep selfparamssteps metricslog selfmetriccacheclear and output i got step loss accuracy step loss accuracy step loss accuracy step loss accuracy step loss accuracy step loss accuracy edit to fix the error valueerror classification metrics cant handle a mix of multiclass and continuousmultioutput targets with confusion matrix you should change confusionmatrixnpargmaxytrain axis predtrain to confusionmatrixnpargmaxytrain axis npargmaxpredtrain axis because you need to convert predicted labels same way as train labels
73991530,valueerror not enough values to unpack expected got when i try to capture a webcam images,python errorhandling deeplearning gesturerecognition webcamcapture,from cvfindcontours only returns values not check the documentation so you can change your code like this
73912958,internalerror failed copying input tensor from cpu to gpu in order to run eagerconst dst tensor is not initialized,python tensorflow deeplearning,i faced this problem long time ago even after reducing the batch size didnt work my gpu was rtx gb ram and it worked on google collab pro however there is one solution for this problem that may work you can use the gc library which cleans the gpu after each iteration you can put this statement in the loop and hopefully it will work by cleaning the ram after each loop
73874619,compilation issue on mac error no matching constructor for initialization of wavenetwavenet,c xcode audio deeplearning plugins,i made a quick guess based on my experiences unfortunately i cannot test it on a mac since the compile error comes from getdspcpp we should try to fix it there before trying to change something in the class wavenet itself the makeunique call is issued in getdspcpp line in line is the th parameter to the constructor i guess the default constructed nlohmannjson after the is the root source of the compile error this creates an unnamed temporary object which is an rvalue but the constructor requires an lvalue nonconst reference to a nlohmannjson to fix that we must ensure we pass something which can act as an lvalue so we need a named object like this this must be placed before the return statement in line and then use myjson as constructor parameter in line the complete changed block will then look like this does it solve the problem
73849758,valueerror dimensions must be equal but are and for node meansquarederrorsquareddifference squareddifferencetdtfloat,python machinelearning deeplearning tfkeras keraslayer,i reproduced the error the problem is that the first lstm is not returning a sequence out of the input you give it i used returnsequencestrue to make it work i also merged the two models in the training here is why calling modelfit between featurevec and lstminputsequences while using meansquarrederror as loss function is not correct meansquarrederror is only used to compare two vectors of same size and nature here you are trying to compare features featurevec with a tokenized sentence outputprobmatrix while you are supposed to compare two tokenised sentences lstminputsequences and outputprobmatrix this is why it is mandatory to merge the lstm and vnn the modelfit should not be called between featureven and outputprobmatrix since they have different shapes and you are using the meansquarrederror as loss function so the modelfit should be called between the lstminputsequences and outputprobmatrix which are supposed to have identical shapes this is my suggested code
73795865,assertionerror when running unet script,deeplearning pytorch imagesegmentation semanticsegmentation unetneuralnetwork,your error stems from the difference in number of channels between the prediction predtorchsize and the target ytruetorchsize for a target with channels you need your pred to have three channels as well that is you need to configure your unet to have outchannels instead of the default of
73769334,problem with nested network on pytorch typeerror forward missing required positional argument x,deeplearning pytorch convneuralnetwork torchvision,your definition of big net is wrong it should be class mybignetnnmodule def initself supermybignet selfinit selfconvlayer nnconvdinchannels outchannels kernelsizestridepadding selfsmallnet mysmallnet def forwardself x x selfconvlayerx x selfsmallnetx return x this should solve the issue
73671940,reshaping problem input to reshape is a tensor with values but the requested shape has,python tensorflow machinelearning keras deeplearning,may be the problem come from the getlabelpath method since you are splitting path according the sep it will return a list of many elements make sure that you select only one element to perform test try this i assume that the last element is the label you just replace this index by the position of the label in this list
73667338,runtimeerror could not infer dtype of generator,python class deeplearning pytorch tensor,the expression torchfromnumpyitemtodevicedevice dtypetorchfloat for item in x isnt creating a tuple its a generator expression since its in a case where you test for tuples i suspect you wanted a tuple instead of a generator try
73659806,debugging neural networks feedforward propagation,python numpy deeplearning neuralnetwork,i simply solved this way
73644487,getting the error name model is not defined,tensorflow deeplearning,the error is saying that model has been created using a custom metric called dicecoef when reloading the model you should add that metric too to your model creation just as you have done for the custom loss
73627784,valueerror input of layer sequential is incompatible with the layer expected minndim found ndim full shape received none,python tensorflow machinelearning keras deeplearning,im not experiencing your issue with the code you have provided try executing the following code that should work as expected if that is the case double check that the shapes of all your data ie eegtrainingdata etc are like the ones below
73612341,ray attributeerror broadmodel object has no attribute model,python deeplearning hyperparameters ray,the problem with using pbt to tune the network size is that it tries to modify these parameters midrun and this is usually undefined behavior the reason for that is that youll either drop layersnodes that potentially contain relevant information when downscaling or add randomly initialized nodes that do not contain any information when upscaling and in either case will usually render the rest of the network useless for pbt you can mutate any parameters except for the network parameters
73526233,attributeerror kerasregressor object has no attribute call,python machinelearning keras deeplearning neuralnetwork,you have to compile the model before passing it to kerasregressor
73445876,valueerror basedistribution needs to have shape with size at least but got torchsize,python deeplearning pytorch tensor,the error is telling you exactly what the problem is transformeddistribution expects the base distribution to have eventshape of at least length but you are passing a normal distribution with eventshape this minimum length requirement exists because transformeddistribution applies affine transforms which require at least dimensions for the batchshape for the event coordinates being transformed simply construct your normal distribution with more dimensions eg normalloctorchzeros scaletorchones
73432848,keras sequential compile function typeerror compile missing required positional argument self,python tensorflow keras deeplearning sequential,change this model kerassequential dense activationrelu namel dense activationrelu namel densenamel to this model kerassequential dense activationrelu namel dense activationrelu namel densenamel how the python interpreter reads your code is as follows model kerassequential model just keeps a reference to the kerassequential class this creates a tuple of list of dense layers but doesnt store it anywhere so this tuple gets garbage collected
73430195,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim,python tensorflow machinelearning keras deeplearning,before fitting the model i reshaped the training and testing dataset as directed by frightera as
73359876,error at prediction after loading saved keras model,python tensorflow machinelearning keras deeplearning,you have a custom model architecture that needs to be reinitialized upon loading this can be done by the savedmodel but you need to specify a getconfig and fromconfig which have the values to initialize your model alternatively you could redefine the model yourself and use saveweights and loadweights for an example see the docs in your case youd need to add this to your recommendernet class
73316883,while predicting on trained model ive getting an error,python tensorflow deeplearning imageresizing,you need to add a batch dimension to your image try
73225230,runtimeerror trying to backward through the graph a second time or directly access saved tensors after they have already been freed,python deeplearning pytorch reinforcementlearning,problem lies in this snippet when you create target variable there is a forward pass through critic which generates a computation graph and criticnextstate is the leaf node of that graph making target a part of the graph you can check this by printing target which will show you gradfn finally when you call criticcomputereturncriticout target a new computation graph is generated and passing targetwhich is a part of the previous computation graph causes a runtime error solution is to call detach on criticnextstate this will free target variable and it will no longer be a part of the computation graphagain check by printing target
73203286,runtimeerror found dtype char but expected float,python machinelearning deeplearning pytorch classification,bceloss expects float labels yours are int aka char converting them to float in the last line ofgetitemshould fix the issue
73197501,raise valueerror no gradients provided for any variables custom loss function,keras deeplearning neuralnetwork lossfunction,tensorflow cannot track gradients through object assignments like your code with errors this creates a copy of a value and thus the gradient is not defined instead you should put things into a list or vectorise the whole thing note that now the list is composed of tftensor object that has a functional dependency on your predictions before the output would be a numpy float with a dependency lost
73193605,adam optimizer not working on cost function,python tensorflow deeplearning neuralnetwork,you are not recording gradient since you are calculating the prediction before hand instead you want to allow the optimizer to record the operations in other words you want to compute the prediction inside the loss lambda that you are passing to the optimizer loss lambda tfreducemean tfnnsoftmaxcrossentropywithlogits logits forwardpropagationxtrainmodifiedweightsbiases labels ytrainonehot also consider that in your code you are referring to a y variable that you have never defined you probably meant ytrainonehot which ive used in the above snipped the reason why this happens is clearly explained in the minimize doc
73175870,invalidargumenterror required broadcastable shapes opadd tensorflow model,python tensorflow machinelearning deeplearning computervision,to handle the above error i used a different loss funtion i changed the code like below to to save time of developers i have answered to my own i am available to discuss on it further if someone is interested
73165767,runtimeerror expected number of channels in input to be divisible by numgroups but got input of shape and numgroups,deeplearning pytorch imageclassification batchnormalization efficientnet,i solved basically numchannels must be divisible by numgroups so i used in each layer rather than as numgroups
73136384,how to fix the input dimension from convolution flatten to feed forward layer,python deeplearning pytorch convneuralnetwork nas,for conv with kernelzise you need to padding and not fix selfconv mutableslayerchoice nnconvd kernelsize stride padding nnconvd kernelsize stride padding to selfconv mutableslayerchoice nnconvd kernelsize stride padding nnconvd kernelsize stride padding match padding size to kernel size update recent versions of pytorch allow you to specify paddingsame and avoid the need to come up with the correct value for padding however i strongly urge you to use the formula for computing the output shape of a convolution layer found here and manually compute the correct value for padding this is a good sanity check to ensure you understand what you are doing
73093682,getting value error for concatenate layer,python tensorflow machinelearning keras deeplearning,you are concatenating two models you probably want to concatenate their outputs or the last layer if you wanted to do it that way you can concatenate any compatible layers if you wanted youre also calling concatenate incorrectly concatenate with a capital c is concatenateparamslayers concatenate with lowercase c is concatenatelayers params change to
73090413,keras symbolic inputsoutputs do not implement len error,python tensorflow machinelearning keras deeplearning,you do not need to specifically install the keras package separately you can import keras from tensorflow also please provide the right alias while importing input as below input is submodule of tfkeras api not part of tensorflowkeraslayers api please check the tensorflow and keras version should be as per this tested build configurations let us know if the issue still persists
73051400,when training with pytorch debugger hangs even though running works fine,python debugging deeplearning pytorch pycharm,after a long search i found the answer here which led to here setting gevent compatible in preferences build execution deployment python debugger solves the issue
73013513,oserror savedmodel file does not exist,python tensorflow machinelearning deeplearning,i just deleted all the temp files and it has started working fine again i think it is some windows flaw
73006909,cant import vecframestackframe from stablebaselines importing problem,python deeplearning pytorch reinforcementlearning stablebaselines,i knew that stable baselines new version has changed the name from to and it worked for me
72997955,valueerror input of layer sequential is incompatible with the layercifar with python,python tensorflow keras deeplearning,as described in error the problem is due to incompatible shape of xtestflat in modelpredictxtestflat change your code as below
72950056,error when trying to fit model tensorflow cnn,pythonx tensorflow keras deeplearning convneuralnetwork,after a bit of research i finally made it work in pycharm as well the problem was i hadnt used flatten here is the piece of my code that i changed it works fine now im still not sure why it wouldnt give me an error in colab though
72946441,attributeerror module tensorflowapivtrain has no attribute getorcreateglobalstep,python tensorflow machinelearning keras deeplearning,from what i gathered in and it seems that you should be calling although theres also this this is from though i would hazard a guess that since this is with the compatvtrain module and youre using the version youre probably good with going with the tftrainingutilgetorcreateglobalstep but theres a note in the comment for the getorcreateglobalstep and i quote with the deprecation of global graphs tf no longer tracks variables in collections in other words there are no global variables in tf thus the global step functions have been removed getorcreateglobalstep createglobalstep getglobalstep you have two options for migrating create a keras optimizer which generates an iterations variable this variable is automatically incremented when calling applygradients manually create and increment a tfvariable
72930012,model training converges to a fixed value of loss with low accuracy,python deeplearning pytorch computervision,calculating accuracy with sigmoid is not an issue as you are using argmax and softmax and sigmoid will return different values but they will be in the same order however one issue im seeing with your code is that you are including your activation within your forward pass code what i might encourage you to do is to remove the sigmoid from your model this is because crossentropyloss takes in the input is expected to contain raw unnormalized scores for each class input has to be a tensor of size cc for unbatched input minibatch cminibatchc or minibatch c d d dkminibatchcddd k with k geq k for the kdimensional case the last being useful for higher dimension inputs such as computing cross entropy loss perpixel for d images this means it is expecting logits which are the output of the last layer without any activation if you want to use a sigmoid would recommend softmax use functional softmax after the loss has been calculated or include a flag which only activates the model when selftraining true in the model code i assume you are getting the performance of your model on your train data if this is not improving it either means your data pipeline is faulty or your approach isnt working are you sure that your labels align properly with your images
72928902,issue between number of classes and shape of inputs in metric collection torch,python deeplearning pytorch,it seems that torchmetrics expects different shape try to flatten both output and labels
72893373,i got attribute error while doing convolution,python tensorflow deeplearning convneuralnetwork classification,as dr snoopy said this error raised due to the sample t a numpy array therefore i added a line to my code thanks a lot sir
72813353,structural similarity index ssim in python multichannel error,python deeplearning scikitimage ssim,you have channel images so you should use the channelaxis argument also you should remove the first dimension of your images to get shapes
72755458,typeerror cannot interpret as a data type,python tensorflow keras deeplearning runtimeerror,you need to change the line resultsnpzeroslensequencesdimension here dimension is being passed as the second argument which is supposed to be the datatype that the zeros are stored as change it to
72728922,error input to reshape is a tensor with values but the requested shape requires a multiple of,python tensorflow machinelearning keras deeplearning,the error is because the target size is and the input shape given in the model is you can either change the target size to or change the input shape to change the inputshape to or change the targetsize to in trainbatch validbatch and testbatch
72685284,tensorflow linear regresison task very high loss problem,tensorflow deeplearning linearregression,your learning rate is significantly high you should opt for much lower initial learning rates such as or otherwise you are using linear activation on the last layer default one and the correct loss function and metric also note that the default batchsize in absence of explicit mentioning is updating as determined by the author of the question underfitting was also fundamental to the problem adding multiple more layers helped solved the problem
72680522,valueerror shapes and not aligned dim dim for neural network,python numpy keras deeplearning neuralnetwork,change this line to the reason being is that the inner dimensions need to align your inputs is of shape b where b is batch size and your weights are of shape
72668903,valueerror input of layer convd is incompatible with the layer expected minndim found ndim full shape received,python keras deeplearning tensorflow,take it easy just a little mistake in inputfn that cause your problem def inputfnimages labels epochs batchsize dataset tfdatadatasetfromtensorslicesimages labels shufflesize inplace changes do not work so add dataset datasetshuffleshufflesizerepeatepochsbatchbatchsize dataset datasetprefetchnone return dataset ps tfdatadatasets methods always return an iterable obj instead of the original data pipeline so any inplace like changes do not work
72666336,valueerror shape of passed values is indices imply,python numpy deeplearning,heres an example of what i think is happening though i dont know if you understand enough numpy and pandas to apply it to your case often people take some tutorial or worse yet a video and try to use their own data without much understanding of whats going on anyways lets make a column frame now use hstack to combine two columns dfbd would have worked just as well the key is that it is a column array shape if i try to make a frame from that as you do note the same sort of error theres a mismatch between the x shape and the shape implied by the dfcolumns array if instead i select a subset of the columns the same numbers as used for the hstack it works columns n data its all about the array shapes you wont get far with pandas or numpy if you dont pay attention to shapes
72648721,error writing tfrecords networks reads double the values input to reshape is a tensor with n values but the requested shape has n values,python tensorflow keras deeplearning neuralnetwork,the error here was that by manipolating the indirectly changed the dtype of the numpy array from npfloat to npfloat by doing this i was writing tffloat tensors and then reading them with this and trying to decode them as tffloat to fix the issue i had to cast the numpy arrays to npfloat again after the manipulation like this
72633246,error in pytorch data loader batch cycles,amazonwebservices deeplearning pytorch amazonsagemaker pytorchdataloader,try to run the same training script outside of a sagemaker training job and see what happens if the error doesnt happen on a standalone script try to run it as a local sagemaker training job so you can reproduce it in seconds instead of minutes and potentially use a debugger to figure out what is the problem
72603426,valueerror could not broadcast input array from shape into shape,python pythonx tensorflow keras deeplearning,i cant test it but as for me all problem is that you generate wrong figure at start it needs third dimension with size
72601409,in gan network i got error while trying to transform tensorflow datasets to range,python tensorflow machinelearning deeplearning computervision,you can pass image but you can not pass image so you can use below tricks method for going to the range from the range you can use the simple trick num like below method you can use tfcastimage tffloat like below
72596676,balancing samples on a binary classification sequence problem with sparse positive labels,python tensorflow machinelearning deeplearning sampling,yes a balanced training set makes sense yes rejecting each short sequence even before examining whether its positive or negative make sense this question suffers from not being reproducible testable dont fall into this trap some training approaches might draw ac and bd as two distinct samples despite the shared positive stretch in the middle avoid doing that given that you designed this as an lstm solution that suggests that training on sample bd might not be very helpful as there hasnt been much time for the state to evolve before we see the positives consider constraining your positive samples to always be negative in the initial n of the sample
72591900,keras imagedatagenerator with flow got valueerror images tensor and labels should have the same length,python tensorflow keras deeplearning convneuralnetwork,in the traindatagenflow and validationdatagenflow you make two small mistakes for the y parameter you pass validationimages but you need to pass traininglabels and validationlabels i correct the above mistakes and write full code with random images and a simple cnn model and fit it output
72567402,runtimeerror given groups weight of size expected input to have channels but got channels instead,python deeplearning pytorch imagesegmentation torchvision,i figured out few things with your code according to the trace back you are using a resnet based unet model your current model forward method is defined as your error comes from selfconvx because conv takes a matrix with a number of channels of it means something is missing or commented by changing into will fix the problem the problem of channels as input but there is another problem using an input of bhw no matters what b h and w are wont be possible with your current architecture why because of this in any case the layer conv of resnet takes a channels input once you have made those modifications you should also try your network with a dummy example like why your width and height are the same here because your current architecture only supports squared images for example conclusion modify your network to accept grayscale images if your dataset is made of grayscale images preprocess your images to make widthheight edit device mismatch
72564316,kerasrl valueerrormodel has more than one output dqn expects a model that has a single output,keras deeplearning neuralnetwork dqn kerasrl,the solution was that i had to just use one output of this didnt work great but it was the best i could find two different outputs will not work using kerasrl so this was all i could think of another possibility would be using a different library such as stable baselines but that would be completely different to the already built code
72506050,ran into typeerror not supported between instances of tensor and list when going through dataset,python machinelearning deeplearning dataset resnet,i was having the same error message probably under different circumstances but i just found my own bug and figured i would share it anyway for various readers i was using a torchvision transformation in my dataset which the dataloader was loading from the transformation was torchvisiontransformsrandomhorizontalflip and the error is that the input to this transformation should not be a list but should be torchvisiontransformsrandomhorizontalflip so if there is anything i can recommend its just that maybe there is some list argument being passed through that shouldnt be in some transformation or otherwise
72482644,problem fiting my dataset into my model python,python tensorflow deeplearning neuralnetwork,well error message is pretty clear the loss function should be binarycrossentropy not binarycrssentropy
72475752,how can i solve this error that arises when trying to train a model,python tensorflow deeplearning convneuralnetwork,the error is in the shapes of the inputs to the concat layer check the inputs to the concat layer post the model architecture code in the question
72465015,modulenotfounderror no module named blurrdata,python deeplearning,try this
72439977,numpydot shapes error neural network,python numpy matrix deeplearning,mathematically this is impossible because your multiplying a matrix by all you need to do is to transpose w ps note that in linear algebra npdotwt a is not the same as npdotat w import numpy as np a npasarray w npasarray printwshape ashape z wt a printz the result would be
72422832,implement backpropagation from scratch with gradient checking error during graident checking,python machinelearning deeplearning datascience backpropagation,no need for use w npones because weights have already initialized and passed onto gradientchecking function use this code after for loop
72405377,pytorch convnet not working loss goes down as accuracy stays about,python deeplearning neuralnetwork pytorch,your accuracy calculation is not correct on pred side use argmax on y side note that y is onehot encoded so use argmax there or something else this will work correct predargmax yargmaxsumitem also use higher learning rate like to see faster learning with these changes your net yields accuracy after epochs
72402905,the problem of modified architecture of resnet,deeplearning pytorch convneuralnetwork resnet,it is easier to check pytorchs source code for troubles like this one see here look at the resnet class which is used to create different variants via factorylike functions for clues in this case respective layers would be conv stage max pooled output of conv stage this one is assumed by the shapes layer stage layer layer layerstage also you might use printres to see all of the blocks for easier debugging as it has a hierarchical structure obtaining features you should use pytorch fx for that see here in your case it would be something along these lines features should be a dict with keys being names of the layers specified above and values being resulting tensors
72329109,when i use with modelfit it generates error,tensorflow machinelearning keras deeplearning tensorflowdatasets,because tfdsload assupervisedtrue returns datasets tfdata that already contains img and label together but kerasdatasetscifarloaddata give you just arrays img and label so it should be like this
72328867,valueerror input of layer sequential is incompatible with the layer expected shapenone found shapenone,tensorflow keras deeplearning convneuralnetwork,the input array should have the shape none shape shape where none represent the batch size and shape shape represents the shape of the feature so you should reshape your input array and you dont really need to specify the batch size when building the model so remove that and just use as the inputshape try this and this should solve the problem the same error would not appear again
72238372,error tokenizing remove pattern refindall,python python machinelearning deeplearning,use strreplace here dfremoveuser dfcommentstrreplacerw regextrue
72236565,concatenation layer raise value error aslist is not defined on an unknown tensor shape,python tensorflow keras deeplearning concatenation,the reason you get this error is explained in the documentation of tfkerassequential a sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor this is due to the fact that when a layer is added to a sequential model the inputs of the added layer are automatically set to be the outputs of the previous one having a concatenation like yours would imply that the output of the first dense layer would be used by both the next layer and the concatenate layer downstream which is against the idea of the sequential models the solution is using the functional api instead
72194403,attributeerror tensor object has no attribute numpy while mapping a function through my dataset,numpy tensorflow deeplearning mapfunction,i solved it i didnt understand exactly where the error was i think the previous code mixed eager mode and graph mode so i changed the code of getlabel function and it worked
72191908,keras deep clustering undefined errors in clustering custom layer,keras deeplearning clusteranalysis,i have solved the error just correct these line instead of
72180462,valueerror metric should be a callable received,python tensorflow keras deeplearning,the modelfitgenerator is quite old but the idea is the same logcosh that identifies identity as logarithms scales how would you present similarity with limited information you see it has the behavior of the logarithms for the categorize tasks sample output
72082251,error in layer of discriminator model while making a gan model,python numpy tensorflow keras deeplearning,the problem is you are extracting exactly one batch when running xtrain ytrain nextitertrainds and you are then iterating over this batch in your training loop that is why you are missing the batch dimension none i am not sure what your dataset looks like but here is a working example using tfkerasutilsimagedatasetfromdirectory
72075258,valueerror cannot reshape array of size into shape,python tensorflow opencv keras deeplearning,did you try converting your grey first detectmultiscal requires an format cvu
72051609,runtimeerror mat and mat shapes cannot be multiplied x and x,python deeplearning pytorch,your xtrain has shape torchsize which your model thinks is a vector with features but your model is defined as nnlinear so its expecting an input of size if you want xtrain to be a batch of examples with size you can use unsqueeze to add an extra dimension xtrain torcharangedtypetorchfloatunsqueeze printxtrainshape the output will be torchsize so your input now has a batch dimension of with each example being size i think theres a couple of other things to fix in your training loop too you should create the mseloss object first and then call it during the loop rather than recreating it every epoch lossstep will throw an error you should call optimizerstep instead the final code will look like this
72021176,runtimeerror mat and mat shapes cannot be multiplied x and x,python deeplearning pytorch artificialintelligence,then the error provided by you is not align with your code in the error there is but the code you provide is def forwardselfx x selfnetworkx x selfnetworkx
72005101,typeerror linear argument input position must be tensor not dropout pytorch,python machinelearning deeplearning torch,torchnndropout is a module you need to instantiate it before you can pass a variable through it
71965660,unable to serialize to json in keras for dl issues,python json tensorflow keras deeplearning,the problem is coming from tfrange which is an eagertensor you should use selfpositionsnumpy here is an example
71950234,tensorflowio valueerror cannot infer argument from shape none none none,python tensorflow deeplearning tensorflow semanticsegmentation,the problem is that tfioexperimentalcolorrgbatorgb uses unstack under the hood which cannot work in graph mode one solution would be to manually index the channels you want according to the source code for rgbatorgb here is a working example if you really want to use tfioexperimentalcolorrgbatorgb it will have be out of graph mode using for example tfpyfunction
71918073,issue with presenting understandable predictions on a python keras cnn model,python tensorflow keras deeplearning densenet,the models predictions ie these floating point numbers are the probabilities for the respective classes eg a value of e indicating a probability of your prediction is then the element in your array of classes at the index of the maximum value in your array of probabilities meaning you want to predict whichever class gets assigned the highest probability by your model example outputs
71917627,running transfer learning for my binary classification model following resnetv model on tensorflow value error,python tensorflow deeplearning transferlearning binaryimage,for binary classification you dont need to use a unit in the dense layer for each class since that would be redundant and in this case you cant do so in the first place since you use the binarycrossentropy loss try adjusting layersdensenumclasses to layersdense
71822212,alternatives for error level analysis ela,imageprocessing deeplearning imagecompression losslesscompression computerforensics,there cannot be imho since png compression is lossless every decompression must result in the identical original image therefore every recompression will start from the same place so no history can be retained
71796161,dimensionality problem with pytorch conv layers,python deeplearning neuralnetwork pytorch convneuralnetwork,nnconvd expects input with shape of form batchsize numofchannels seqlength its parameters allow to directly set number of ouput channels outchannels and change length of output using for example stride for convd layer to work correctly it should know number of input channels inchannels which is not the case on first convolution inputshape batchsize therefore numofchannels while convolution in selflayers expects this value to be equal because inchannels set by selfconfig and selfconfig hence to fix this append one more value to config at this point convs should be working fine but there is another obstacle in selflayers linear layer at the end so if kernelsize of was used then after final convolution batch will have shape batchsize and after flatten batchsize while lastlayer expects input of shape batchsize so if length of sequence after final conv layer is known which is certainly the case if youre using kernelsize of with default stride of and default padding of length stays same lastlayer should be defined as and in snippet above finallength can be set to since conditions in previous brackets satisfied to catch idea of how shapes in convd transformed take look at simple example in gif below here batchsize is equal to
71793678,i am running into a gradient computation inplace error,python deeplearning pytorch computervision generativeadversarialnetwork,your code is stuck in what is called the backpropagation of your gan network what you have defined your backward graph should follow is the following so in your backward graph you are propagating the disloss which is the combination of the discriminator and adversarial loss first and then you are propagating the unetloss which is the combination of unet ssim and contentloss but the unetloss is connected to discriminators output loss so the pytorch is confused and gives you this error as you are taking the optimizer step of disloss before even storing the backward graph for unetloss and i would recommend you to change the code as follows and this will start your training but you can experiment with your retaingraphtrue and great work on the bppnet work
71771805,why do i get a convd error trying to run convd layer,python tensorflow keras deeplearning convd,your xtrain data should be of the data type float also you usually flatten your d data into d or apply some global pooling operation before feeding it into a softmax output layer regarding your error message both the convd and convd layer use the tfnnconvolution operation internally interestingly the problem is caused by the parameter filter which has a float data type and cannot handle integer inputs
71769754,how to fix the error where the target batch size does not match when i use crossentropyloss function,python tensorflow machinelearning deeplearning pytorch,in forward method of simpleconvolutionalnetwork after applying conv tensor x has shape of batchsize so when doing x xview shape of x turns to batchsize and because fullyconnected layers applyed further dont change this new batch size output has shape batchsize my suggestion would be using pooling right after convolution like that way forward will return tensor with shape batchsize and batch size mismatch error wont occur
71742524,error during traning my model with pytorch stack expects each tensor to be equal size,python imageprocessing deeplearning pytorch imagesegmentation,it seems that images in your dataset might not have the same size as in the vit model you are using an mlp model if it was not the case it is worth checking if your labels are all in the expected dimension presumably mmsegmentattion expects the output to be just the annotation map a d array it is recommended that you revise your dataset and prepare the annotation map
71709493,modulenotfounderror no module named tensorflowexamples i am just tried to load mnist,python tensorflow deeplearning neuralnetwork,if you want to load mnist dataset you can try this or you can use tensorflowdatasets like below output
71672518,neural network gives typeerror keyword argument not understood training,python tensorflow keras deeplearning neuralnetwork,in sequential api you cant use triningtrue in layers inputs as kwargs but you can use trainingtrue in functional api like below your correct code in sequential api output
71635096,how can i solve invalidargumenterror graph execution error,python imageprocessing keras deeplearning computervision,it is can be solved instead of use rgb to gray convert the image
71574676,torch modelloadstatedict attributeerror modelname object has no attribute copy,python deeplearning pytorch,save model weigths with modelstatedict instead
71540471,tensorflow valueerror shapes and are incompatible,python tensorflow deeplearning neuralnetwork siamesenetwork,i was not able to reproduce the error using the below code i suspect that your labels shape is different than the one you reported or it does not contain strictly binary data s and s only also you should use tfkeraslossesbinarycrossentropy instead of tfkeraslossescategoricalcrossentropy as your labels should be binary with the sigmoid activation in the last layer
71527183,runtimeerror mat and mat shapes cannot be multiplied x and x,python machinelearning deeplearning pytorch,since both x and x have dimensions of the torchcat operator concatenates in the st dimension since they are of similar size in that direction for your code to work change the catx torchcatx x to catx torchcatx x dim
71493889,how can reslove invalidargumenterror graph execution error,python tensorflow keras deeplearning,you just have to make sure your labels are zerobased starting from to since your output layer has nodes and a softmax activation function and you are using sparsecategoricalcrossentropy here is a working example use the dummy data as an orientation for your real data
71473842,typeerror cant convert cuda device type tensor to numpy use tensorcpu to copy the tensor to host memory first error occured using pytorch,python matplotlib deeplearning neuralnetwork pytorch,ive manage to find the solution in this link it seems that it is connected with moving data between cuda and cpu ive invoked cpu and its solved
71439124,googleprotobufmessagedecodeerror error parsing message with type tensorflowgraphdef,python tensorflow imageprocessing deeplearning tensorboard,bg i was getting errors while testing the code in my case it was solved with the help of freezepy and a few modifications in the training file and i found some other useful links while searching query link link
71423498,runtimeerror mat and mat shapes cannot be multiplied x and x,deeplearning pytorch convneuralnetwork,the problem seems to be related to the input size of your fc layer the output of the previous layer is so you must use infeatures these are the shapes of the output for the third and fourth layers notice that out of the pool layer come x meaning elements
71415317,pytorch dataset transform normalization nbsamples batchsamples indexerror dimension out of range expected to be in range of but got,python deeplearning pytorch computervision pytorchdataloader,tensor tensor tensor tensor tensor tensor took from this youtube tutorial
71410904,attributeerror function object has no attribute compile,python tensorflow keras deeplearning,the function and variable have the same name causing the issue you can either rename the variable or the function def modelinputshape model kerassequential modeladdkeraslayerslstm inputshape returnsequencestrue modeladdkeraslayerslstm modeladdkeraslayersdense activationrelu modeladdkeraslayersdropout modeladdkeraslayersdense activationsoftmax return model mymodel model your initializer mymodelcompilelossbinarycrossentropy optimizeradam metricsaccuracy mymodelsummary
71399847,runtimeerror d or d target tensor expected multitarget not supported i was training a deep learning model but i am getting this issue,python deeplearning pytorch convneuralnetwork crossentropy,your problem is that labels have the correct shape to calculate the loss when you add unsqueeze to labels you made your labels with this shape which is not consistent to the requirment to calcualte the loss to fix the problem you only need to remove unsqueeze for labels if you read the documentation of crossentroploss the arguments input should be in nc shape which is outputs in your case and target should be in n shape which is labels in your case and should be therefore the loss function expects labels to be in d target not multitarget
71395632,pytorch gradient error nonetype unsupported operand types for nonetype and nonetype,python deeplearning neuralnetwork pytorch gradient,it is possible that even if u y v and x require the gradient u is not a function of y and similarly v does not depend on x you can check this by setting allowunusedfalse and see if torchautogradgrad returns an error
71361442,typeerror open got an unexpected keyword argument pilmode,python deeplearning pytorch computervision pythonimageio,ive had this error before the tldr is that you cant assume all of your data is clean and able to be parsed you arent loading the data in order as far as i can tell either you may even have data shuffling enabled with all of that in mind you should not expect it to fail determinisitically at iteration or or anything the issue comes down to one or more of the files in coco dataset is either corrupted or is of a different format you can process the images in order with a batchsize of and print out the file name to see which one it is to fix this issue you can do one of several things wrap the call to load the a tryexcept block and just skip it convert the to another appropriate format try a different way to load images in with pytorch see here as an example failure scenario when loading in images with imageio
71333065,try to replace the nan values by pandas but error columns must be same length as key,python pandas errorhandling deeplearning kaggle,you are close need specify column age for replace missing values
71308750,attributeerror tensor object has no attribute numpy while extending keras sequential model,tensorflow keras deeplearning tensorflow autoencoder,the direct using of this numpy function is impossible as its neither implemented in tensorflow nor in theano moreover there is no direct correspondence between tensors and arrays tensors should be understood as algebraic variables whereas numpy arrays as numbers tensor is an abstract thing and applying a numpy function to it is usually impossible but you could still try to reimplement your function on your own using kerasbackend then youll use the valid tensor operations and no problem would be raised another way to tackle your problem would be to use tfnumpyfunction see the documentation this allows you to use numpy functions but there are some limitations
71300827,typeerror module object is not callable error,pythonx deeplearning pytorch,you are calling tqdm module instead of tqdm method from tqdm module replace with
71300227,pytorch error typeerror adaptiveavgpoold argument outputsize position must be tuple of ints not list,python deeplearning pytorch artificialintelligence torch,the reason behind the error turned out to be a type error when passing the size to adaptiveavgpoold i was getting the values for the size through division although the remainder was division by default saves values as float in python this meant the tuple being passed as the outputsize was in fact a tuple of float which the listwithdefault turns into a list but likely doesnt if the tuple was made of int simply using int for each dimension passed to adaptiveavgpoold was the solution
71294050,keras throwing a labels shape mismatch error,python tensorflow keras deeplearning computervision,so the problem is you have a channel image but classes you need to transform the images in classes for them to be compatible with the models output and of course the last layer of the model must have filters to output classes so in a previous phase before training you must create a dictionary of colors and attribute an index for each one suggestion is to sum rgb in a way it cant be repeated such as now that you have a color dictionary im not sure what the best strategy is preprocess the images and save their index arrays do it on the fly in the loader etc choose something that will run fast i believe saving each array would be an interesting idea but then youd have to change the loader a lot the goal is in the loader transform the images into arrays of indices
71273822,want know reasoncould not load dynamic library nvcudadll dlerror nvcudadll not found,tensorflow deeplearning,why does this error occur when tensorflow has no gpu but the model can still be run normally according to nvcudadll a dll dynamic link library file developed by nvidia corporation which is referred to essential system files of the windows os it usually contains a set of procedures and driver functions which may be applied by windows since you have no gpu presumably there are no nvidia drivers installed and no nvcudadll presumably the tensorflow code is able to fallback to a mode where the model is run on the cpu you must have installed a tensorflow for gpu version if you want to get rid of the noisy warning message force install the cpu only version or modify the logging levels for more details see tensorflow not working could not load dynamic library nvcudadll dlerror nvcudadll not found
71232961,shape error while implementing unet encoder part in pytorch,python deeplearning pytorch computervision artificialintelligence,there was a code logic mistake in the forward of encoder i did but i was supposed to use featuremapx as the input because the loop was iterating over the original feature map before but it was supposed to get the output of previous layer
71209010,mnist shard descriptor indexerror list index out of range,deeplearning sharding mnist federatedlearning,the only solution i found until now is to reduce the rank of each envoy trainidx trainsplittersplitselfy selfworldsizeselfrank
71187485,what is the problem with my gradient descent algorithm or how its applied,machinelearning deeplearning lua neuralnetwork backpropagation,all initial weights must be different numbers otherwise backpropagation will not work for example you can replace with mathrandom increase number of attempts to with these modifications your code works fine results input input input input
71162609,attribute error returns none,machinelearning deeplearning pytorch gradientdescent fastai,it seems like your optimizer and your trainer do not work on the same model you have modelsimplenet while the parameters for the optimizer are those of a different model paramslinearmodelparameters try passing paramssimplenetparameters that is make sure the trainers params are those of model
71159722,attributeerror tensor object has no attribute isinitialized,tensorflow keras deeplearning gpu tfkeras,this is likely an incompatibility between your version of tf and keras daniel mller got you on the right path but tfkeras is a tf thing and you are using tf so your solution will be different what you need to do is install a version of keras that is compatible with tf according to pypi tf was released june you should do a grid search of the keras versions just before and after that date id go with these keras versions install these versions using for example i have run into a similar problem recently in the mismatch between tf and keras
71138474,how to fix boolean value of tensor with more than one value is ambiguous,python deeplearning neuralnetwork pytorch artificialintelligence,short answer remove the variable explanation i believe your answer is in you question if you dont know what variable is why are you using it but more importantly you are importing it from tkinter which is an interface package and im pretty sure thats not what you want what you want is to use the one from torch i was looking for it in the doc but it is actually deprecated the variable api from torch now returns a tensor so it is not useful anymore see here
71057337,indexerror list assignment index out of range for cnn program from scratch,python pythonx deeplearning convneuralnetwork,the error is in the reason for the error is that you are trying to access an element of sumrow but sumrow doesnt have any element in it as you set it to before the for loops inside the for loops you try to access members which it doesnt have instead of doing sumrowi you should be doing sumrowappend which adds an element to that list then you can access it via sumrow inputsiab
70937513,runtimeerror mat and mat shapes cannot be multiplied x and x,python machinelearning deeplearning pytorch convneuralnetwork,as anant said you need to match the flattened conv dimension to be the input dimension for the fc layer selffc nnlinear the formula to calculate the output of each conv layer for the following i will use the dimensions channels height width input conv pool conv pool flatten it seems your batch size is which refers to the in x if you print the dimensions of the output of conv or conv layers the format will be batch channels height width
70926380,valueerror inputs have incompatible shapes received shapes and,python tensorflow keras deeplearning resnet,you probably forgot to set paddingsame the default value is valid here is a working example import tensorflow as tf input tfkeraslayersinputshape l tfkeraslayersconvdfilters kernelsize strides activationrelu paddingsameinput bn tfkeraslayersbatchnormalizationl l tfkeraslayersconvdfilters kernelsize strides activationrelu paddingsamebn bn tfkeraslayersbatchnormalizationl l tfkeraslayersseparableconvdfilters kernelsizeactivationrelu paddingsamebn l tfkeraslayersbatchnormalizationl l tfkeraslayersseparableconvdfilters kernelsizeactivationrelu paddingsamel l tfkeraslayersbatchnormalizationl l tfkeraslayersseparableconvdfilters kernelsizeactivationrelu paddingsamel l tfkeraslayersbatchnormalizationl l tfkeraslayersmaxpoolingdpoolsize strides paddingsamel skipping layer skip tfkeraslayersconvdfilters kernelsize strides activationrelu paddingsamebn skip tfkeraslayersbatchnormalizationskip sum tfkeraslayersaddlskip model tfkerasmodelinputsinput outputssum nametest
70858909,cannot resolve error in keras sequential model,python tensorflow keras deeplearning,you are trying to use a timedistributed layer on a d input batchsize which will not work because the layer needs at least a d tensor you should try using tfkeraslayersrepeatvector import tensorflow as tf resnet tfkerasapplicationsresnetincludetopfalseweightsimagenetinputshape cnn tfkerassequentialresnet cnnaddtfkeraslayersconvdstrides cnnaddtfkeraslayersconvdstrides cnnaddtfkeraslayersflatten inputs tfkeraslayersinputshape x cnninputs x tfkeraslayersrepeatvectornx x tfkeraslayersgrureturnsequencestruex x tfkeraslayersgrux outputs tfkeraslayersdenseactivationsoftmaxx model tfkerasmodelinputs outputs dummyx tfrandomnormal printmodelsummary printmodeldummyx
70789023,invalidargumenterror logits and labels must have the same first dimension got logits shape and labels shape,python tensorflow machinelearning keras deeplearning,after your code you need to flatten the data before you apply it to the dense layer so add
70787151,onnxload albert throws decodeerror error parsing message,python deeplearning onnx quantization onnxruntime,the problem was with updating the config variables for my new model changes configsoutputdir albertbasevmrpc configsmodelnameorpath albertbasevmrpc i then came across this separate issue where i hadnt git cloned my model properly question and answer detailed here lastly huggingface does not have an equivalent to bertoptimizationoptions for albert i had tried general pytorch optimisers offered by torchoptimizer on the onnx model but it seems that they arent compatible for onnx models feel free to comment for further clarification
70776552,incompatibility between input and final dense layer value error,python tensorflow keras deeplearning,you seem to be working with sparse integer labels where each sample belongs to one of seven classes so i would recommend using sparsecategoricalcrossentropy instead of categoricalcrossentropy as your loss function just change this parameter and your model should work fine if you want to use categoricalcrossentropy you will have to onehot encode your labels for example with
70748727,problem with pytorch gradient of a nonsequential model,python machinelearning deeplearning neuralnetwork pytorch,selfih has no gradient because its not used in the first step of your model when you back propagate your model only uses selfio in the first stage so selfih has no effect on the output however when you get to the second step it utilises a hidden which has been calculated using selfih so therefore there is a traceable gradient through that layer
70704831,d variational autoencoder implementation error,python tensorflow keras deeplearning autoencoder,the encoder and the decoder model seem to disconnected and the kerastensor are not able to flow correctly also the encoder returns three tensors mu variance z whereas the decoder only requires z as an input replace these lines modelinput encin modeloutput decoderz vautoencoder modelmodelinput modeloutput with vaeinput inputshape inputshape nameencoderinput mu variance z encoder vaeinput vaeoutput decoder z vautoencoder model vaeinput vaeoutput mu variance so basically the model will now output mu and variance which are the outputs of the encoder and are required for vaeklloss earlier you were using these kerastensor directly in vaeklloss and hence you got the error remove the axis argument from vaeklloss def vaeklloss mu logvariance klloss ksum logvariance ksquaremu kexplogvariance return klloss pass mu and variance to vaeklloss def vaelosstinput modeloutput mu modeloutput variance modeloutput vaeoutput modeloutput rloss vaerlosstinput vaeoutput klloss vaeklloss mu variance return rloss klloss modified code import tensorflow as tf from tensorflowkeraslayers import input batchnormalization convd dense flatten lambda reshape upsamplingd from tensorflowkerasregularizers import l from tensorflowkerasmodels import model from tensorflowkeras import backend as k functional api inputshape zdim encoder encin inputshape inputshape nameencoderinput encconv tfkeraslayersconvdfilters kernelsize strides paddingsame activationrelu kernelinitializerheuniform nameconvencin max tfkeraslayersmaxpooldpoolsize strides paddingvalid namemaxpoolencconv encconv tfkeraslayersconvdfilters kernelsize paddingsame activationrelu kernelinitializerheuniform nameconvmax max tfkeraslayersmaxpooldpoolsize namemaxpoolencconv encfc denseunits kernelinitializer heuniform activation reluflattenmax mu denseunits zdim kernelinitializer heuniform activation none namemuencfc logvariance denseunits zdim kernelinitializer heuniform activation none namelogvarianceencfc def samplingargs mu logvariance args epsilon krandomnormalshapekshapemu mean stddev return mu kexp logvariance epsilon z lambdasampling outputshape zdim namezmu logvariance encoder modelencin mu logvariance z nameencoder decoder decin inputshape zdim namedecoderinput decfc denseunits kernelinitializer heuniform activation reludecin decunflatten reshapetargetshape decfc decconv tfkeraslayersconvdfilters kernelsize paddingsame activationrelu kernelinitializerheuniform namedeconvdecunflatten ups tfkeraslayersupsamplingdsize nameupsdecconv decconv tfkeraslayersconvdfilters kernelsize paddingsame activationrelu kernelinitializerheuniform namedeconvups ups tfkeraslayersupsamplingdsize nameupsdecconv decconv convdfilters kernelsize padding same activationrelu kernelinitializer heuniform namedecorderoutputups decoder modeldecin decconv namedecoder vaeinput inputshape inputshape nameencoderinput mu variance z encoder vaeinput vaeoutput decoder z vautoencoder model vaeinput vaeoutput mu variance def vaerlosstinput modeloutput rloss kmeanksquaretinput modeloutput axis return rlossfactor rloss def vaeklloss mu logvariance klloss ksum logvariance ksquaremu kexplogvariance return klloss def vaelosstinput modeloutput mu modeloutput variance modeloutput vaeoutput modeloutput rloss vaerlosstinput vaeoutput klloss vaeklloss mu variance return rloss klloss learningrate rlossfactor optimizer tfkerasoptimizersadamlrlearningrate vautoencodercompileoptimizeroptimizer loss vaeloss metrics vaerloss vaeklloss vautoencodersummary
70659159,valueerror input of layer sequential is incompatible with the layer in prediction,python tensorflow deeplearning convneuralnetwork,edit after some fiddling this was the solution on the one hand there was an error with the final dimension of the input the in the the inputshape this represents the number of channels think of rgb channels in an image to expand our spectrogram we can use either spectrogram spectrogramreshapespectrogramshape or spectrogram npexpanddimsspectrogram at this point the shape of spectrogram would be on the other hand during inference the first dimension was removed because tensorflow would interpret it as the batch dimension you can solve this by wrapping the spectrogram in a one element numpy array like this now spectrograms shape is which is exactly what we need original this is definitely more of a comment than an answer but i cannot write those due to a lack in reputation so feel free to move it to comments so the problem is that the expected shape and thus the architecture of your network and your datas shape dont match i guess thats because the predict call expects you to hand over a batch look at the first dimension of each shape of samples to evaluate you may get around this by wrapping the spectrogram argument inside the predict call with a list vcprediction voicemodelpredictspectogram if this doesnt do the trick id recommend to further investigate the shapes of training and evaluation data i like to do this during runtime in debug mode
70658583,saving pytorch model error codec cant decode bytes in position,deeplearning pytorch,its a better practice to only save the statedict and load the checkpoint
70545677,custom loss function error valueerror no gradients provided for any variable,python tensorflow keras deeplearning,i found the correct differentiable code for the loss function i wanted to use
70386696,pytorch runtimeerror mat and mat shapes cannot be multiplied x and x,python machinelearning deeplearning pytorch convneuralnetwork,your fc layer is expecting a tensor of shape but you are passing it a tensor of shape being the batch size to calculate output size of conv nets see this post or any neural networks textbook
70339133,neuralnet function r bugging when doing hyperparameter brute search optimization,r deeplearning neuralnetwork prediction crossvalidation,well youre using the neuralnet package and in some iteration with the parameters passed the algorithm used by the neuralnet function didnt converge and therefore the function didnt return the net weights right after the error you can inspect the model and see that the weights are not there for example i changed the threshold used to and the algorithm converged for all iterations as per the code below
70326871,shape error while fine tuning mobilenet on a custom data set,python tensorflow keras deeplearning,when you call the base model as follows it will initiate with the default argument among them includetop is set as true and that brings source the globalavg with keepdimstrue now based on your error i assumed your true label shape and here you can do simply as follows
70324346,pytorch valueerror either size or scalefactor should be defined,python machinelearning deeplearning pytorch machinelearningmodel,nnupsample has following parameters size scalefactor mode aligncorners by default sizenone modenearest and aligncornersnone torchnnupsamplesizenone scalefactornone modenearest aligncornersnone when you set scalefactor you will get following result import torch import torchnn as nn class nettorchnnmodule def initself supernet selfinit keepprob selflayer nnsequential nnconvd kernelsize nnrelu nnmaxpooldkernelsize padding selflayer nnsequential nnconvd kernelsize nnrelu nnmaxpooldkernelsize padding selflayer nnsequential nnconvd kernelsize nnrelu nnmaxpooldkernelsize padding selfdense nnlinear biastrue nninitxavieruniformselfdenseweight selflayer nnsequential selfdense nnrelu nnupsamplescalefactor selflayer nnsequential nnconvd kernelsize nnsigmoid nnupsamplescalefactor selflayer nnsequential nnconvd kernelsize nnsigmoid nnupsamplescalefactor selflayer nnsequential nnconvd kernelsize nnsigmoid def forwardself x out selflayerx out selflayerout out selflayerout out selflayerout out selflayerout out selflayerout out selflayerout return out if name main x torchrandn f net y fx printyshape result
70314837,tensorflowfailedpreconditionerror could not find variable densebias this could mean that the variable has been deleted,python tensorflow deeplearning neuralnetwork reinforcementlearning,i think its a keras reference problem you should correctly import and then create a model like this i have not tried with your whole code try it and report
70290586,error in keras model for classification model with transformers,tensorflow keras deeplearning neuralnetwork classification,disclosure i came here for the bounty then i tried on colab and everything worked fine next i read the comments this question is a joke in its current state there is no way to reproduce it and at this point i agree but as i am a hans in luck and obviously have to much time procrastinating i started pycharm following the ops cue no when i paste it to my pycharm i get the above error but this also worked for me which makes me wonder whether you have touched something so i am happy to provide an untouched working version for you also to make sure that we are talking of the same package versions i used numpy and tensorflow try with these versions or let me know in case you used different versions
70290365,zero division error during training a noisy speech synthesizer multiprocessing in python,python deeplearning speechrecognition,had a problem with the datasetmaximum files were corruptedso thats why i was getting this errordownloaded the dataset again properly this time now code is working fine
70279287,runtimeerror expected scalar type long but found float pytorch,python machinelearning deeplearning pytorch artificialintelligence,it seems that the dtype of the tensor labels is floattensor however nncrossentropyloss expects a target of type longtensor this means that you should check the type of labels if its the case then you should use the following code to convert the dtype of labels from floattensor to longtensor
70265603,modelpredict doesnt work with keras custom layer inference error,python tensorflow keras deeplearning convneuralnetwork,i think this should work and then use tfreshape to extract these patches i had a similar error a couple of months back this fixed it
70263348,tensorflow datagen valueerror,pythonx tensorflow deeplearning neuralnetwork tensorflow,you just have to remove the parameter subsettraining since you did not set a validationsplit in the imagedatagenerator both parameters have to be set in order to work or you just do not use them xtrain ytrain xtest ytest tfkerasdatasetscifarloaddata ytrain tfkerasutilstocategoricalytrain ytest tfkerasutilstocategoricalytest datagen tfkeraspreprocessingimageimagedatagenerator featurewisecentertrue featurewisestdnormalizationtrue rotationrange widthshiftrange heightshiftrange horizontalflip true datagenfitxtrain compile defined model modelcompile optimizer tfkerasoptimizersadam loss tfkeraslossescategoricalcrossentropy metrics accuracy define early stopping criterion earlystopping tfkerascallbacksearlystopping monitor valloss mindelta patience verbose mode auto baseline none restorebestweights true batchsize traininghistory modelfit datagenflow xtrain ytrain batchsize batchsize stepsperepoch lenxtrain batchsize epochs callbacks earlystopping check the docs for more information
70252159,attributeerror functional object has no attribute predictsegmentation when importing tensorflow model keras,python tensorflow keras deeplearning,predictsegmentation isnt a function available in normal keras models it looks like it was added after the model was created in the kerassegmentation library which might be why keras couldnt load it again i think you have options for this you could use the line from the code i linked to manually add the function back to the model modelpredictsegmentation methodtypekerassegmentationpredictpredict model you could create a new vggunet with the same arguments when you reload the model and transfer the weights from your hdf file to that model as suggested in the keras documentation model vggunetnclasses inputheight inputwidth modelloadweightsmymodelhdf
70243588,valueerror found input variables with inconsistent numbers of samples,python tensorflow machinelearning keras deeplearning,posting my comments as answer for completeness one possible thing that looks a bit weird is that you take different axis when calculating the argmax for ypred and ytest but that might be ok depending on your data layout ytest and ypred seem be be of different lengths can you check the shapes of ypred and ytest and see if the axes over which you calculate the argmax are correct
70242844,valueerror and must have the same shape received none vs none,pandas machinelearning deeplearning neuralnetwork convneuralnetwork,as the error states while your model outputs prediction over classes per sample you passed true labels in some noncompatible way why are your labels three dimensional you have a xx tensor per sample instead of a onehot encoded class number
70216022,runtimeerror mat and mat shapes cannot be multiplied x and x,matrix deeplearning pytorch convneuralnetwork,you forgot to flatten the output array of selfconv in the for cycle you can split it into two cycles one for the convolution layers and one for the fully connected ones
70212120,typeerror only integers slices ellipsis tfnewaxis and scalar tfinttfint tensors are valid indices,python numpy tensorflow deeplearning,you are trying to slice your inputs based on idx which is a list and therefore it will not work try something like this tensorflowversion x import tensorflow as tf import numpy as np def tftrainx y batchsize dataset inputs and labels d x tfplaceholdertffloat batchsize d y tfplaceholdertffloat batchsize random variable w tfvariabletfrandomnormald stddev ztfmatmulxw define loss and optimizer crossentropy tfreducemeantfnnsigmoidcrossentropywithlogitslogitsz labelsy trainstep tftraingradientdescentoptimizereminimizecrossentropy sess tfinteractivesession tfinitializeallvariablesrun initializes w and z train for epoch in range idx nprandomchoice batchsize replacefalse xx nparrayxi for i in idx yy nparrayyi for i in idx l sessruntrainstep crossentropy feeddictx xx y yy if epoch printloss strl x nprandomrandom y nprandomrandom tftrainx y
70205343,valueerror cannot feed value of shape for tensor placeholder which has shape,python numpy tensorflow deeplearning,you need to reshape your data because at the moment it is interpreted as which means samples with features for each sample you probably want the shape or even so try this import tensorflow as tf x tfrandomnormal y tfrandomnormal x tfreshapex or tftransposex if you want y tftransposey printxshape printyshape and your code here is causing the actual problem d x tfplaceholdertffloat batchsize d d should be or x x depending on how you will reshape your data
70202638,error all inputs to the layer should be tensors when trying to use vgg model,python tensorflow keras deeplearning vggnet,maybe try converting your images to tensors import numpy from pil import kerasapplicationsvgg import vgg import kerasbackend as k from kerasmodels import model import imageio as iio create random images for n in range a numpyrandomrand im imagefromarrayaastypeuintconvertrgb imsavetestdjpg n imageshape vgg vggincludetopfalse weightsimagenet inputshapeimageshape vggtrainable false make trainable as false for l in vgglayers ltrainable false model modelinputsvgginput outputsvgggetlayerblockconvoutput modeltrainable false img iioimreadtestjpg img iioimreadtestjpg img tfexpanddimstfconstantimg axis img tfexpanddimstfconstantimg axis mean kmeanksquaremodelimg modelimg printmean instead of tfexpanddims you could also just do this img tfconstantimg img tfconstantimg there is also an option to load your images with tfkeraspreprocessingimageloadimg img tfkeraspreprocessingimageloadimgtestjpg img tfkeraspreprocessingimageloadimgtestjpg img tfconstanttfkeraspreprocessingimageimgtoarrayimg img tfconstanttfkeraspreprocessingimageimgtoarrayimg mean kmeanksquaremodelimg modelimg printmean
70118623,valueerror after attempting to use onehotencoder and then normalize values with makecolumntransformer,python pandas tensorflow deeplearning onehotencoding,using onehotencoder is not the way to go here its better to extract the features from the column time as separate features like year month day hour minutes etc and give these columns as input to your model the issue here is coming from the onehotencoder which is getting returning a scipy sparse matrix and get rides of the column time so to correct this you must retransform the output to a pandas dataframe and add the time column one way to countournate the memory issue is generate two indexes with the same randomstate one for the pandas data frame and one for the scipy sparse matrix use the pandas data frame for the minmaxscaler ct makecolumntransformerminmaxscaler time ctfitxtrainpd resulttrain cttransformxtrainpd resulttest cttransformxtestpd use generators for load data in train and test phase this will get ride of the memory issue and include the scaled time in the generators def nnbatchgeneratorxdata ydata scaled batchsize samplesperepoch xdatashape numberofbatches samplesperepoch batchsize counter index nparangenpshapeydata while true indexbatch indexbatchsize counterbatchsize counter scaledarray scaledindexbatch xbatch xdataindexbatch todense ybatch ydatailocindexbatch counter yield nparraynphstacknparrayxbatch scaledarray nparrayybatch if counter numberofbatches counter def nnbatchgeneratortestxdata scaled batchsize samplesperepoch xdatashape numberofbatches samplesperepoch batchsize counter index nparangenpshapexdata while true indexbatch indexbatchsize counterbatchsize counter scaledarray scaledindexbatch xbatch xdataindexbatch todense counter yield nphstackxbatch scaledarray if counter numberofbatches counter finally fit the model history btcmodelfitnnbatchgeneratorxtrain ytrain scaledresulttrain batchsize stepsperepochtodetermine batchsize epochs callbackscallback btcmodelevaluatennbatchgeneratorxtest ytest scaledresulttest batchsize batchsize stepstodetermine ypred btcmodelpredictnnbatchgeneratortestxtest scaledresulttest batchsize stepstodetermine
70115673,unidentifiedimageerror cannot identify,python tensorflow keras deeplearning convneuralnetwork,this error is due to a corrupt file or file with an incorrect extension that the generator is not able to read use the function below to check if the input data code
70110429,pytorch runtimeerror result type float cant be cast to the desired output type long,python deeplearning pytorch,i was getting the same error doing this where the output was tensor torchfloat and target was tensor torchint what solved this problem was calling the loss function like this
70098762,typeerror init got an unexpected keyword argument categoricalfeatures google colab,python keras deeplearning googlecolaboratory,for more details regarding columntransformer
70091975,invalidargumenterror cannot add tensor to the batch number of elements does not match shapes are tensor batch opiteratorgetnext,python tensorflow keras deeplearning ocr,here is a complete running example based on your dataset running in google colab
70090636,error with the inputshape expected to have dimensions but got array with shape,python tensorflow deeplearning neuralnetwork shapes,the problem probably lies in the way you are passing your data to your model if your input shape is batchsize then try something like this import tensorflow as tf greyscaleimages tfrandomnormal model tfkerassequential modeladdtfkeraslayersflatteninputshape printmodelgreyscaleimagesshape update both inputshape and inputshape will work it depends how you are feeding your data to your model import tensorflow as tf greyscaleimages tfrandomnormal y tfrandomnormal model tfkerassequential modeladdtfkeraslayersflatteninputshape modelcompilelossmse modelfitgreyscaleimages y
70065974,invalidargumenterror concatop dimensions of inputs should match when predicting on xtest with convd why,python tensorflow keras deeplearning convneuralnetwork,with modelpredict you are making predictions on batches as stated here computation is done in batches this method is designed for batch processing of large numbers of inputs it is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time but the size of xtest is not evenly divisible by the default batchsize i think this might be the cause of your problem you could change your batchsize to for example and it will work ypred modelpredictxtest batchsizeargmaxaxis printypred you could also use modelpredictonbatchxtest to make predictions for a single batch of samples however you are most flexible if you use the call function of your model directly
70064737,modelfit attributeerror tuple object has no attribute shape,python tensorflow keras deeplearning tensorflow,so for anyone else who runs into this i have found the problem obs this problem doesnt occur when using a sequential model dont know why however when youre hot encoding the labels like below as i did and using a multi input model with generators like then dont use metricsacc this does not work and you will get attribute error see the following use the tfkerasmetricscategoricalaccuracy this works with hot encoded labels
70054689,fix cnn overfitting,tensorflow machinelearning keras deeplearning convneuralnetwork,there is some tricks to help with orver fitting problem adding data augmentation this method will slightly transform each time the input with rotation random croping etc and the model will see more example of the same will help the model to better generalize adding dropout layer this layer will randomly sets input units to with in the training process so in that the model will make more epoch before over fitting l and l regularization this method will penalize the absolute value of the weights by adding them to the total lossenter link description here its better to change your callback withcallback tfkerascallbacksearlystoppingmonitorvalaccuracy patience i think your model stopped when there is still room for emprovement
70047616,error input type torchfloattensor and weight type torchcudafloattensor should be the same,python machinelearning deeplearning pytorch,xtodevice does nothing change it to xxtodevice of course this should be done to any parametervariable you want on the gpu
70045787,superresolution using mnist not working properly,python tensorflow machinelearning keras deeplearning,i used your code and tried to reproduce the error but it worked fine for me i loaded the mnist images and resized them to using skimagetransformresize the training during epochs gives epoch s msstep loss acc valloss valacc epoch s msstep loss acc valloss valacc epoch s msstep loss acc valloss valacc epoch s msstep loss acc valloss valacc valloss tends to fluctuate between epochs but it decreases globally some results here is the code to plot the figures in addition here is also how i build the dataset note that i do not divide trainsmallarray and testsmallarray by as resize does the job
70035567,typeerror float argument must be a string or a number not batchdataset when data augmenting using fitgenerator,tensorflow keras deeplearning,first the article you referred to is years old and is a bit outdated starting from tensorflow the fit method accepts generators too and currently it fully replaced fitgenerator i suggest you to update your tensorflow if possible second the error seems to be not in the fitgenerator method but in the way you define the datasets they just first called in fitgenerator and thats why the error message trace you back there as of the error itself i dont understand the part of nesting the generators and i think it can cause problems here youre trying to pass batched dataset gotten from tfkerasutilsimagedatasetfromdirectory to another generator which seems to be impossible if i understood correctly you have only one label on each image and images of each class are stored in separate folders so i suggest you to use the flowfromdirectory method of tfkeraspreprocessingimageimagedatagenerator directly this generator will both read and augment the images so you can drop the tfkerasutilsimagedatasetfromdirectory part to use this generator you need to have images in the form rootdirectory class folder class folder etc and your code will be something like this you can pass validationsplit argument too to get separate datasets for training and validation read more about the imagedatagenerator and flowfromdirectory method in the official documentation
69998250,pytorch runtimeerror sizes of tensors must match except in dimension got and the offending index is,deeplearning pytorch imagesegmentation,you can pad your images dimensions to be multiple of s by doing this you wont have to change the dunets parameters i will provide you a simple code to show you the way after this operation your must be torchsize
69970898,i am having an error with my perceptron model,python tensorflow machinelearning keras deeplearning,there is no need to create a dictionary of losses use this line
69970569,valueerror unexpected result of empty batchoutputs please use,python tensorflow machinelearning keras deeplearning,change the axis dimension in expanddims to and slice your data like this since it is d import tensorflow as tf import numpy as np tfrandomsetseed create some regression data xregression npexpanddimsnparange axis yregression npexpanddimsnparange axis split it into training and test sets xregtrain xregression xregtest xregression yregtrain yregression yregtest yregression tfrandomsetseed recreate the model model tfkerassequential tfkeraslayersdense tfkeraslayersdense tfkeraslayersdense change the loss and metrics of our compiled model modelcompilelosstfkeraslossesmae change the loss function to be regressionspecific optimizertfkerasoptimizersadamlearningrate metricsmae change the metric to be regressionspecific fit the recompiled model modelfitxregtrain yregtrain epochs modelpredictxregtest
69970319,valueerror exception encountered when calling layer sequential type sequential,python tensorflow machinelearning keras deeplearning,you just have to add a second dimension to your data it has to be batchsize features you could use npexpanddims to change your inputs from batchsize to batchsize features import tensorflow as tf import numpy as np tfrandomsetseed create some regression data xregression npexpanddimsnparange axis yregression npexpanddimsnparange axis split it into training and test sets xregtrain xregression xregtest xregression yregtrain yregression yregtest yregression tfrandomsetseed recreate the model model tfkerassequential tfkeraslayersdense tfkeraslayersdense tfkeraslayersdense change the loss and metrics of our compiled model modelcompilelosstfkeraslossesmae change the loss function to be regressionspecific optimizertfkerasoptimizersadam metricsmae change the metric to be regressionspecific fit the recompiled model modelfitxregtrain yregtrain epochs
69934643,discriminative layer training issue with callback reducelronplateau,python tensorflow machinelearning keras deeplearning,looking in the code of tfaoptimizersmultioptimizer in the method createoptimizerspec it seems that optimizers can be accessed via selfmodeloptimizeroptimizerspecsoptimizer and selfmodeloptimizeroptimizerspecsoptimizer to change the learning rate which is why selfmodeloptimizer raises an error then your custom callback seems to work
69933910,getting error when training the cnn modeltensorflow,python tensorflow keras deeplearning convneuralnetwork,how many classes are in the dataset you have the code this would indicate you are doing binary classification which i expect is not what you want try this after your generator code change the last layer in your model to
69900558,rnn model error valueerror this model has not yet been built,python tensorflow keras deeplearning recurrentneuralnetwork,if you try to pass some data to your model as you are trying to do with this line examplebatchpredictions modelinputexamplebatch in your for loop your models summary would work but notice how nothing gets printed inside your loop the problem is you are using exampletexts which contains two strings and you are still using a batchsize of and a sequencelength of if you change your batchsize to say and your sequencelength to you should see an output like this length of text characters unique characters a c s s s p s k h c tftensorba bc bs shape dtypestring input bac target bcs batchsize sequencelength vocabsize model mymodel layer type output shape param embedding embedding multiple gru gru multiple dense dense multiple total params trainable params nontrainable params
69896247,google cloud deep learning on linux vm throws unknown cuda error,tensorflow googlecloudplatform deeplearning gpu nvidia,the issue is fixed with the m image but you are using m so follow one of the two fixes provided in the google cloud public forum we can mitigate the issue by fix use the latest dlvm image m or later in a new vm instance they have released a fix for the newest dlvm m so you will no longer be affected by this issue fix patch your existing instance running images older than m this only needs to be done once and does not need to be rerun each time the instance is rebooted
69871476,torchnnsequential of designed blocks problem in giving inputs,python deeplearning neuralnetwork pytorch sequential,you indicated that your block nnmodule had a reverse option however nnsequential doesnt so convnetworkx reversefalse is not valid because convnetwork is not a block by default you cant pass kwargs to layers inside a nnsequential you can however inherit from nnsequential and do it yourself something like this way you can create a sequence containing blocks and optionally nonblock modules as well then you will be able to call blocks with the reverse keyword argument which be relayed to every potential block child module when called
69850632,how to solve error assertion failed ssizeempty in function resize,tensorflow keras deeplearning objectdetection transferlearning,the path to you test images is broken the error comes as a result of cv trying to resize on empty numpy array you should check the path to the test dataset ensure the imagepaths are reachable
69848050,unimplementederror with channel,python tensorflow keras deeplearning imageclassification,the problem is probably caused by a discrepancy between the defined channels in your model and the actual number of channels in your dataset i would recommend explicitly converting your images to grayscale before feeding them into your model import tensorflow as tf import pathlib import matplotlibpyplot as plt dataseturl datadir tfkerasutilsgetfileflowerphotos origindataseturl untartrue datadir pathlibpathdatadir imggen tfkeraspreprocessingimageimagedatageneratorrescale shearrange zoomrange horizontalfliptrue trainds tfdatadatasetfromgenerator lambda imggenflowfromdirectorydatadir batchsize shuffletrue outputtypestffloat tffloat def converttograyscaleimage label return tfimagergbtograyscaleimage label images nextitertraindstake image images printbefore conversion imageshape trainds traindsmapconverttograyscale images nextitertraindstake image images printafter conversion imageshape here is a complete working example import tensorflow as tf import pathlib import matplotlibpyplot as plt dataseturl datadir tfkerasutilsgetfileflowerphotos origindataseturl untartrue datadir pathlibpathdatadir imggen tfkeraspreprocessingimageimagedatageneratorrescale traingenerator imggenflowfromdirectorydatadir targetsize batchsize classmodesparse def converttograyscaleimages labels return tfimagergbtograyscaleimages labels trainds tfdatadatasetfromgeneratorlambda traingenerator outputtypestffloat tffloat trainds traindsmapconverttograyscale numclasses model tfkerassequential tfkeraslayersconvd paddingsame activationrelu inputshape tfkeraslayersmaxpoolingd tfkeraslayersconvd paddingsame activationrelu tfkeraslayersmaxpoolingd tfkeraslayersconvd paddingsame activationrelu tfkeraslayersmaxpoolingd tfkeraslayersflatten tfkeraslayersdense activationrelu tfkeraslayersdensenumclasses modelcompileoptimizeradam losstfkeraslossessparsecategoricalcrossentropyfromlogitstrue modelfittrainds epochs
69832196,pytorch runtimeerror input type torchcudafloattensor and weight type torchfloattensor should be the same,python deeplearning pytorch,needed to do this use nnmodulelist instead of python list selfconvolutions nnmodulelistnnconvd for in rangesequencesize embdim calcembeddingsizeselfconvolutions inputsize selfconvolutions nnmodulelistnnconvd for in rangesequencesize embdim calcembeddingsizeselfconvolutions embdim selfconvolutions nnmodulelistnnconvd for in rangesequencesize embdim calcembeddingsizeselfconvolutions embdim and use torchcudafloattensor when training on gpu
69809867,custom loss function returning invalidargumenterror the second input must be a scalar but it has shape,python tensorflow keras deeplearning lossfunction,the problem is that your customloss is returning a function rather than a scalar value if you replace tfcond with tfwhere your code will work import numpy as np import tensorflow as tf from tensorflowkerasmodels import sequential from tensorflowkeraslayers import dense dropout lstm batchnormalization flatten def customlossytrue ypred mse tfkeraslossesmeansquarederror penalty penalize the loss heavily if the actual and the prediction are on different sides of zero loss tfwhere conditiontflogicalortflogicalandtfgreaterytrue tflessypred tflogicalandtflessytrue tfgreaterypred xmseytrue ypred penalty ymseytrue ypred penalty add slightly more penalty if prediction overshoots actual in any direction loss tfwhere conditiontflogicalortflogicalandtfgreaterytrue tfgreaterypred ytrue tflogicalandtflessytrue tflessypred ytrue xloss penalty yloss penalty return loss
69798370,simple cut out augmentation using tensorflow issue with assigning values to tensors,python tensorflow deeplearning tensor dataaugmentation,using tfaimagerandomcutout should do exactly what you want import tensorflow as tf import matplotlibpyplot as plt import tensorflowaddons as tfa def randomcutoutimages labels return tfaimagerandomcutoutimages constantvalues labels flowers tfkerasutilsgetfile flowerphotos untartrue imggen tfkeraspreprocessingimageimagedatageneratorrescale ds tfdatadatasetfromgenerator lambda imggenflowfromdirectoryflowers batchsize shuffletrue outputtypestffloat tffloat ds dsmaprandomcutout images nextiterdstake image images pltimshowimagenumpy the tfaimagerandomcutout function requires a tensor of shape batchsize height width channels so when you feed single images into your custom function you need to add an extra dimension def randomcutoutimage label image tfexpanddimsimage axis return tfsqueezetfaimagerandomcutoutimage constantvalues axis label
69796927,augmentation in torch vision transform is not working as expected,machinelearning deeplearning pytorch computervision torchvision,it seems like the transformsresize function did not correctly reshape the tensor reshaping first seems to fix the issue and produce correct images you did this step for the albumentations section
69796369,problem with dimensionality in keras rnn reshape isnt working,python keras deeplearning neuralnetwork recurrentneuralnetwork,what you are calling lags is called look back in literature this technique allow to feed the rnn with more contextual data and learn midlong range dependencies the error is telling you that you are feeding the layer shape x with the dataset shape x there are reasons behind the error the first is due to your createdataset which is building a stack of x lags x vectors which is the opposite of what you want xlags x in particular this line is the responsible if you use look back in your rnn you also need to increase the input dimensions because you are looking also at precendent samples the network indeed is looking to more data than just sample because it needs to look back to more samples to understand midlong range dependencies this is more conceptual than actual in your code is fine because lags
69792642,typeerror input y of sub op has type float that does not match type int of argument x,keras deeplearning tensorflow,this error comes from the line x ytrue numclasses ypred in this typeerror input y of sub op has type float that does not match type int of argument x y ypred and x ytrue numclasses ypred has dtype float ytrue numclasses has dtype int the error comes when you try to subtract dtypefloat from dtypeint to solve this you need to change the datatype of both the variable in my case ypred and ytrue are d tensors so you need tfcastypredytrue tffloattfint this means you can either convert both to int or float both will work
69787272,keras predicting floating point output for a binary problem,python pythonx keras deeplearning,on your last layer use sigmoid activation
69770166,data augmentation in the data loading of tensorflow dataset tfds resulting in typerror or attributeerror,python numpy tensorflow deeplearning tensorflowdatasets,in augment youre passing tensors to randomapply tfimageflipleftrightimage returns a tensor then in randomapply youre using that tensor like its a function you need to pass tfflipleftright as a callable full working example
69752055,valueerror none values not supported code working properly on cpugpu but not on tpu,python tensorflow machinelearning deeplearning tpu,as stated in the referenced answer in the link you provided tensorflowdata api works better with tpus in order to adapt it in your case try to use return instead of yield in generatebatch function and then use tensorflowdata to structure your data where mapfn is defined by and finally use modelfit instead of modelfitgenerator
69706377,how to fit the model for multi input problem in keras,python tensorflow keras deeplearning,you will have to redefine your fit parameters as follows the question is if you also want your model to output two values for trainy and trainy currently you only have two inputs and one output
69690777,runtimeerror expected dimensional input for dimensional weight but got dimensional input of size instead,python deeplearning neuralnetwork pytorch resnet,you cant replace resnets fc with a convolutional network the output of resnets feature extractor is a cnn which outputs a flat long tensor as such the layers following it should be fully connected layers
69608037,error when trying to predict with new data in keras model,python tensorflow machinelearning keras deeplearning,maybe try adding a dimension to your data sample and then feed your newdata into your model to make a prediction import numpy as np newdata nparray newdata npexpanddimsnewdata axis prediction modelpredictnewdata printprediction
69507790,is there a way to use mlpany other algorithms which take the objective and error functions as input and returns the optimum parameters,python machinelearning deeplearning neuralnetwork mlp,as long as your objective function is differentiable this is literally what a neural network is designed to do you can write any function in tf as an objective and then train your mlp with say sgd it is a matter of understanding how things work or accepting that pre built is not going to be as easy as a function called solve my problem it requires a few more commands but in the end what you are asking for is literally any nn implementation let it be tf keras etc for example you can use keras and implement your custom loss
69507636,valueerror not enough values to unpack expected got when trying to access dataset,python tensorflow deeplearning,a tfdatadataset is an iterator you need to iterate over it in order to access its elements try
69502691,d target tensor expected error when using pytorch tensordataset class,python numpy machinelearning deeplearning pytorch,from what i gather from the comments discussion the error is reproduced by the following import torch from torch import nn from torchutilsdata import dataloader tensordataset randomsplit inputs torchrandn dtypetorchfloat targets torchrandint dtypetorchlong you need this to adapt from pandas but not for this example code inputs torchtensorinputs dtypetorchfloat targets torchtensortargets dataset tensordatasetinputs targets valsize testsize trainsize lendataset valsize testsize divide dataset into unique random subsets trainingdata validationdata testdata randomsplitdataset trainsize valsize testsize batchsize trainloader dataloadertrainingdata batchsize shuffletrue numworkers pinmemorytrue validloader dataloadervalidationdata batchsize numworkers pinmemorytrue guess model more on this in a moment model nnsequential nnlinear nnlinear lossfunc nncrossentropyloss for features labels in trainloader out modelfeatures loss lossfuncout labels printfloss break solution add labelssqueeze to the loop body a la for features labels in trainloader out modelfeatures labels labelssqueeze loss lossfuncout labels printfloss break solution flatten your targets initially with in response to now i am getting this error runtimeerror mat and mat shapes cannot be multiplied x and x i should also add that i am using a hidden layer of size and i have classes my architecture is a guess at what youre using but as the code above is resolved by the target reshape ill need more to be more helpful perhaps some documentation to assist crossentropyloss the example code shows the expected shape of the targets being n rather than n or n classes
69433897,valueerror error when checking input expected keraslayerinput to have dimensions but got array with shape,numpy keras deeplearning convneuralnetwork keraslayer,i eventually figured it out using the answer from this post sampletrainemb nparrayx for x in sampletrainemb hope this helps in the future to anyone
69431580,model input shape error nested array from keras data generator shape error,python numpy tensorflow keras deeplearning,i just made batchsize and it works that still is not an optimal solution because i literally cant add more samples to my batch can only do sample at a time so my gpu optimization suffers now followed this reference simply set the input sequence for the lstm to none features and use batchsize as train and predict on variable length sequences it does work however so at least now i can move forward with creating the rest of the model
69418232,which deep learning model should i use to classify a multiclass problem with multilabel,algorithm deeplearning model multilabelclassification multiclassclassification,i would go with a sentiment analysis model and a binary classification model per topic i wouldnt combine the topic classification with the sentiment analysis these are two separate tasks and each deserves its own model as for the topic classification itself i incline toward separate model per class for two reasons first this way we can get the full range of activations per class if for example a text matches very well both class a and class b we can expect the two corresponding models to indicate this while if we used a single model its probable that only one of these classes will stand out second a model constructed with separate classifiers is more extendible adding another topic amounts to training a new classifier on that topic if we go with one big classifier adding a topic requires retraining the model on all topics
69387792,valueerror dimensions must be equal but are and in tpu on tensorflow,python tensorflow machinelearning keras deeplearning,the training data has instances if you split them into batches of size you are left a smaller batch of size which is keras also assumes this as a batch if you dont drop it so in trainstep for this batch of size the shape of realoutput will be and the shape of fakeoutput will be as you set reduction to none in crossentropy loss the shape will be retained so shape of realloss will and shape of fakeloss will be then adding them will definitely result in an error you may solve this problem this way let reduction param be default one which is autosumoverbatchsize reduction type crossentropy tfkeraslossesbinarycrossentropyfromlogitstrue or drop the remainder batch traindataset tfdatadatasetfromtensorslicestrainimagesshufflebuffersizebatchbatchsize dropremaindertrue
69356296,pytorch typeerror list is not a module subclass,python deeplearning pytorch resnet,listmodelchildren returns a list but the input of nnsequential requires the modules to be an ordereddict or to be added directly not in a python list nnsequential modules will be added to it in the order they are passed in the constructor alternatively an ordereddict of modules can be passed in nnsequentiallistmodelchildren means which is wrong nnsequentialconvd kernelsize stride padding reluinplacetrue maxpooldkernelsize stride padding dilation ceilmodefalse nnsequentiallistmodelchildren mean nnsequentialconvd kernelsize stride padding reluinplacetrue maxpooldkernelsize stride padding dilation ceilmodefalse thats why you need to unpack the list using it can only be used inside a function hence it doesnt work in your last case read in python
69302855,dimensionality issue when using max pool with stride,python tensorflow keras deeplearning,you forgot to add the paddingsame parameter by default the padding is set to valid it is tricky as one would expect to alter the padding parameter only in case of convolutionsother operations prior to the pooling one
69284230,normalization of the dataset error all elements of input should be between and,deeplearning neuralnetwork pytorch,builtin loss functions refer to input and target to designate the prediction and label instances respectively the error message should be understood as input of the criterion ie yhat and not as input of the model it seems yhat does not belong in while bceloss expects a probability not a logit you can either add a sigmoid layer as the last layer of your model or use nnbcewithlogitsloss instead which combines a sigmoid and the bce loss
69269890,keras attributeerror sequential object has no attribute nestedinputs,python machinelearning keras deeplearning tensor,the simplest way to combine your encoder and dencoder is to instantiate an input layer and pass it through encoder and decoder here is the running notebook
69267461,runtimeerror mat and mat shapes cannot be multiplied how do i solve it,deeplearning neuralnetwork pytorch resnet,matrices are multiplicable if the number of columns of the first matrix matches the number of rows of the second one for instance mxn nxk so here you have wrong shapes you can always calculate the sizes of the outputs of each layer by yourself to make sure if your shapes are correct so here i think it should work if you change this selflinear nnlinearblockexpansion to this selflinear nnlinearblockexpansion
69246441,python opencv with cuda not working after successful build,python opencv deeplearning cuda nvidia,sources say the mx has a maxwell core not a pascal core maxwell is the predecessor of pascal hence you only have cuda compute capability you should check that with an appropriate tool such as gpuz that does its best to query the hardware instead of going by specs sources notice how the fab nm is different and the code name is gm not gpxxx
69244021,valueerror input of layer sequential incompatible with the layer expected axis of input shape to have value but received shape none,python deeplearning neuralnetwork jupyternotebook autoencoder,just restart and clear all output of your jupyter kernal replace the sequential with sequential and then display the the reduceddf it will work fine
69238317,kerastensorflow error on training with userprovided loss function error in pycallimpl valueerror in user code,python r tensorflow keras deeplearning,i realized that the way i start r has changed after an update and the above errors were produced when running r not inside the required python environment the problem entirely disappeared after i started r from within the python environment within which tensorflow is running
69223955,building a neural network for binary classification on top of pretrained embeddings not working,python machinelearning deeplearning neuralnetwork pytorch,you are only printing from the second iteration the above will effectively print for every k steps but i starts at ie one gradient descent step has already occurred this might be enough to go from the initial loss value log to the one you observed
69211649,a bug for tfkeraslayerstextvectorization when built from saved configs and weights,python tensorflow machinelearning keras deeplearning,the bug is fixed by the pr in
69170518,potential bug when upgrading to pytorch importerror cannot import name intclasses from torchsix,python import deeplearning pytorch six,easiest solution would be to just set intclasses int instead of importing it from six
69137834,keras valueerror dimensions must be equal but are and for node equal with input shapes,python keras deeplearning valueerror dimensions,as you can see in the model summary the output shape of the model is none while based on target values it should be none try to add flatten layer before dense layers
69128288,while uninstalling cuda toolkit ran into this problem what to do,deeplearning cuda nvidia,it seems you previously installed higher versions of cuda maybe partially try installing cuda instead even if you want to use cuda uninstalling the above two components will not be a problem as older versions will be installed instead
69055889,issue with reproducibility across different sessions,python tensorflow deeplearning,id say everything behaves normally setting a randomseed always produces the same sequence of random variables eg so running your second script five times produces the same model each time because you are setting the seed to with each execution iterating the for loop in your first script produces models that correspond to the variables from first five outputs with seed using my code example your second script would always create model your first script creates models and
69020802,return vfnorminput p dim keepdimkeepdim indexerror dimension out of range expected to be in range of but got,python deeplearning pytorch computervision indexerror,to switch to fnormalize you need to make sure youre applying it on dim if you prefer using the other alternative with either torchnorm or torchtensornorm you can use the option keepdimtrue which helps when doing inplace normalization
69018998,file mtrandpyx line in numpyrandommtrandrandomstatechoice typeerror dictkeys object cannot be interpreted as an integer,python numpy deeplearning pytorch pytorchdataloader,the problem is in these lines in the file polyvoreoutfitspy candidatesets is the object returned by the dictkeys method in older versions of python this was a list but now it is a dictkeys object the choice method in the numpy random module accepts a list but not a dictkeys object a simple fix is to explicitly convert candidatesets into a list either when it is created or before passing it to nprandomchoice
69006025,adding a simple attention layer to a custom resnet architecture causes error in forward pass,python deeplearning pytorch computervision attentionmodel,the problem comes from a confusion layers is not the number of output channels as you probably expected but the number of blocks that layer will have what you actually need is to use the which is the number of output channels of the layer that precede your custom code selflayerattend nnsequentialnnconvd stride padding kernelsize nnadaptiveavgpoold nnsoftmax code added by me
68992377,convd lost a dimension from tensor resulting in incompatible dimension error,python tensorflow keras deeplearning,after reviewing what each function does again i realized i shouldnt use timedistributed because each batch already represent one period of time changing the line into cnnresulttfkeraslayersconvdkernelsizegrid fix the problem
68982809,training yolov on rtx ti gpu im getting error runtimeerror unable to find a valid cudnn algorithm to run convolution,python tensorflow deeplearning gpu,the answer is on the error log runtimeerror cuda out of memory tried to allocate mib gpu gib total capacity gib already allocated mib free gib reserved in total by pytorch it is trying to allocate more memory than you have on your gpu
68974193,error with targets shape while building a multiclass classification with tensorflow,python tensorflow machinelearning keras deeplearning,you should probably reduce the dimension within your network you have to go from d to d to match your goal you can do this by using a global merge or smoothing layer try using flatten before the output level or globalaveragepoolingd or globalmaxpoolingd
68951828,transformers longformer indexerror index out of range in self,python deeplearning pytorch,i have managed to fix this by reindexing my positionids when pytorch was creating that tensor for some reason some value in positionids was bigger than i used to create positionids for the entire batch bear in mind that it might not be the best solution the problem might need some more debugging but for a quick fix it works
68942154,tensorflow object detection api importerror when i try to run modelbuildertestpy,python tensorflow deeplearning jupyternotebook inteloneapi,try this then for specific version of numpy
68899955,loading keras model typeerror module object is not callable,python tensorflow machinelearning keras deeplearning,the dataset is classical dogvscat dataset and im trying to achieve oneclass classification task then the model should have only one output node because this is an example of binary classification your model has the last dense layer with two output units and the softmax as an activation function which is a possible solution but for this kind of problem my suggestion is to specify on the last dense layer only one output unit and the sigmoid as the activation function it seems you forgot to add the string dog in the classes list on the traingenerator now regards on the main topic of your question i think but im not sure about it the problem is from this line of code from kerasmodels import loadmodel starting from tensorflow x which includes also the keras module every single function class etc must be imported by specifying always tensorflow as the first name for import each keras module in short you should write this line of code from tensorflowkerasmodels import loadmodel and it should work because it calls the function from the keras module with the tensorflow as the back engine
68890023,typeerror multiple values for argument weightdecay,python tensorflow machinelearning deeplearning,the problem is that weightdecay is the first positional argument of tfaoptimizersadamw in optimizer tfaoptimizersadamwlearningrateweightdecay you hand over a positional argument and a kw argument weightdecay this causes the error according to the documentation learning rate is the second positional parameter even though optional not the first just write optimizer tfaoptimizersadamw learningrate or optimizer tfaoptimizersadamwweightdecay learningratelearningrate or optimizer tfaoptimizersadamwlearningratelearningrate weightdecay
68874306,why there is error while predicting using inceptionv pretrained model,python deeplearning neuralnetwork,try this the first parameter is the number of samples that you will give so the model took the first as the number of samples then for the first sample the model expects the inputshape but it found for that you should provide the number of samples in the first dimension then the inputdimension
68847663,issue while exporting torch model to onnx format,python deeplearning pytorch onnx,vitstr forward requires two positional arguments input and text def forwardself input text istraintrue seqlen therefore you need to pass an additional argument dummytext create a dummytext as well with the appropriate shape torchonnxexportmodel dummyinput dummytext vitstronnx verbosetrue
68837104,valueerror logits and labels must have the same shape none vs none,python tensorflow keras deeplearning,note you have not specified which class the ss object belongs to and hence i will discuss everything removing it first lets discuss your target ie the install column from the values i assume that that your problem is binary classification ie predicting and and you want the probability of having them for this you have to define your model as below there is another way of doing this as you have only label and hence if you have the probability corresponding to that label then you can have the probability corresponding to the other by subtracting it from you can implement it as below now coming to answer from mattpats this answer will also work but in this case you will not get probability as the output but instead you will get the logits as you are not using any activation and the loss is calculated on the logits by specifying the argument fromlogitstrue to the probabilities from this you have to use it like below
68830425,attributeerror model object has no attribute distributionstrategy,python tensorflow deeplearning attributeerror,i found the answer fortunately i forgot to call the superclass of the tensorflow keras model i just had to add supermymodel selfinit to my init function so my model looks like this now
68794161,how to solve dimensions mismatch error in tensorflow autoencoder,tensorflow machinelearning deeplearning autoencoder,reason your labels shape is incompatible with the models output shape and it is because of divisions by your encoder and decoder source input shape is and shape changes with layerfilters is like this encoder decoder so the input and output shape is the same and it works fine but when you add another layer with neurons layerfilters the shape changes is like this encoder decoder now and are incompatible and you get error solution change you layer configuration in such a way that input and output get the same shape for example you can remove strides padding same in both decoder and encoder for loop encoder decoder or do not add more than convd layers anymore since in the rd layer the shape will be an odd number divided by two and you can not get back in the same way
68761146,ann problem i am building an ann model to predict the profit of a new startup based on certain features,python tensorflow keras deeplearning mlp,the problem is that accuracy is a metric for discrete values classification you should use r score mape smape instead eg
68759204,how do i get through this error module object is not callable,python tensorflow machinelearning keras deeplearning,kerasoptimizervadam is a module replace it with kerasoptimizervadamadam
68739156,typeerror expected float got auto of type str instead,python numpy tensorflow keras deeplearning,repalce your custom loss with
68708396,need help in understanding shape error while building a cnn with sklearn and keras,python tensorflow keras scikitlearn deeplearning,you havent passed y label to model while calling model needs both and x and y label to train accordingly here i added an example please take a pause and go through videos of keras architecture and documentation in keras io these would give more information about deep learning modelling once after you got familiar with all jump to modelling
68648668,valueerror layer model expects inputs but it received input tensors,python tensorflow machinelearning keras deeplearning,if vgg and vgg receive the same input you can use a shared input layer for both in this way your model will have only one input here the code
68647266,issue with arcface accuracy,deeplearning neuralnetwork pytorch imagerecognition arcface,you can try to use a smaller m in arcface even a minus value
68630924,error when making predictions using keras h model,python tensorflow machinelearning keras deeplearning,i dont know what caused the error but the following changes have resolved the issue i removed the graph part and loaded my model now it is showing the result
68626182,keyerror in customdatagenerator class i created,tensorflow keras deeplearning convneuralnetwork imagesegmentation,the keyerror might have occurred because does not exist in the index for integerlocation based indexing of a data frame use iloc
68589991,why i am getting this problem of zero accuracy when doing gradient boosting,python tensorflow deeplearning xgboost multiclassclassification,have you checked the data types of testlabels and prediction i ran into a problem very similar to this and found out that i was trying to calculate the accuracy score of strings and npint after i converted them both to the same data type i no longer got an accuracy score of
68582256,keras flatten valueerror attempt to convert a value none with an unsupported type to a tensor,python tensorflow keras deeplearning tfkeras,theres only bug input is not defined but still unrelated to the error mentioned use below my advice is to please check on the latest versions of tf eg
68570104,multiclass classification label error using tensoflow,tensorflow deeplearning tensorflow multiclassclassification,as you have used four labels in your code here the valid range of your labels is so the values while passing for prediction should always come from this range itself the model does not recognizes and values you need to use any encoder to convert your input labels to valid range and do reverse while predicting for them you can just change your labels to range with in for workaround
68542593,runtimeerror mat and mat shapes cannot be multiplied x and x,deeplearning pytorch,you are currently working with two dimensions only axis is your batch and axis is your features reshaping with xview will flatten all axes but the last leaving your with a shape of the form batchsize featuresize while xunsqueeze will just add an additional axis not affecting the overall form of your tensor x it is not a reshape by that i mean if x is shaped batchsize channel height width which is likely for an input image then xunsqueeze will be shaped batchsize channel height width since you are using a fullyconnected network you are and should be using the former that is to reshape x into a dimensional tensor
68518350,keyerror inputib when i save my autoencoder model,python keras deeplearning neuralnetwork autoencoder,probably it is due to mixing keras and tesnorflow libraries use from tensorflowkerasoptimizers import adam and from tensorflowkerasmodels import model loadmodel instead of keras ones
68504538,pytorch crashes when training probable error tensor value corrupt image runtime error,deeplearning pytorch convneuralnetwork artificialintelligence mnist,i suggest to take a look at you num workers param in your dataloader if you have a numworkers param that is too high it may be causing this error therefore i suggest to lower it to zero or until you dont get this error sarthak
68488269,valueerror data cardinality is ambiguous x sizes y sizes make sure all arrays contain the same number of samples,python tensorflow machinelearning keras deeplearning,there are some changes you would have to make i will write an example for you changes you dont need to reshape xtrain xtest as they are already in correct shape it is always good to use tfkeraslayersinputlayer instead of building the model later i havent made that change but whenever possible you should use tfkerassequential to make models more readable less prone to error functional api or your current method is for when you need to make some complex architecture you can now increase the model more layers i used a few just to show you an example the inputshape in the inputlayer is considered as batchsize imgwidth imgheight imgchannels as the batchsize could be nonuniform and hence taken as none by default so we dont give it and hence we only pass imgwidth imgheight imgchannels and as your input has imgwith imgheight and channels we pass it if it solved your issue then kindly upvote or give a green tick
68448043,how can i fix pymeeus install error command errored out with exit status,python machinelearning deeplearning,my guess would be that the issue is with messed up name for the temporary directory winerror is the directory name is invalid for those of us who do not speak turkish it tries to write to doukan and fails check if that path is accessible solution set tmpdir environment variable to point at place that has a proper name no nonascii characters eg ctmp create it first in cmd build set directory to unpack packages into and build in alternative use anaconda instead of pypi in both cases id strongly advise to create a virtual environment
68396513,problem in lrfind in pytorch fastai course,python machinelearning deeplearning pytorch fastai,i was having the same problem and i found that the lrfind outputs has updated you can substitute the second line to lrs learnlrfindsuggestfuncsminimum steep valley slide and then you just substitute where you using lrmin and lrsteep to lrsminimum and lrssteep respectively this should work fine and solve your problem if you wanna read more about it you can see this post that is in the fastais forum
68372530,valueerror negative dimension size caused by subtracting from for maxpoold with input shapes full codeoutput error in the post,python tensorflow machinelearning deeplearning convneuralnetwork,maxpoolingd downsizes the model by so the output of the first pooling layer is then you have more pooling layers which wont work as it cannot be downsized by anymore therefore you cannot have more than pooling layer in your model also i would not suggest to use a maxpoolingd layers on such a small input another thing you have unit on the final layer and a softmax activation function which makes no sense using softmax on the final layer with one unit will always return a value of so i think you want to use sigmoid and not softmax your model should be like this
68371863,convd valueerror input of layer sequential is incompatible with the layer expected minndim found ndim full shape received none,python tensorflow machinelearning deeplearning convneuralnetwork,i found your problem you have to reshape your training data you can use numpy to do this
68360494,error when i modify number of input channels of inceptionv,python imageprocessing deeplearning computervision pytorch,i found that when pretrainedtrue normalization filter for imagenet dataset is applied before the network which can be seen here and the filter is designed for channel input image it was why the error occurs i could finally use pretrained model by the way below
68299016,invalidargumenterror incompatible shapes vs,tensorflow keras deeplearning objectdetection medicalimaging,these lines means if none of the labels is like you will get only output like there are easy workaround since youre using tfkeras anyway just use official keras method you can test this using the prior example put labelbinarizer in the init and call lbfit there and use lbtransform instead of lbfittransform the example of this would be
68237724,failed call to cuinit cudaerrornodevice no cudacapable device is detected,python tensorflow deeplearning,as dr snoopy mentioned nvidia graphics drivers were missing i installed them using link i also installed cudnn from now when i type i get
68229187,keras model concat attribute and value error,python tensorflow keras deeplearning concatenation,you can use the functional api in order to solve your problem i havent read the paper but here is how you can combine models and get a final output i used relu activation for simplicity purposes ensure you use keras inside tensorflow here is the code that should work
68221932,how to train transferlearning model on custom dataset valueerror shape must be rank,python tensorflow keras deeplearning,the problem in the code is that opencv reads the grayscale format but the grayscale format of the is not but because of this fact the error is thrown i managed to replicate your problem by testing it locally say we randomly train on samples possible input formats and work fails yielding valueerror shape must be rank but is rank for node efficientnetimgaugmentationrandomrotationtransformimageprojectivetransformv imageprojectivetransformvdtypedtfloat fillmodereflect interpolationbilineariteratorgetnext efficientnetimgaugmentationrandomrotationrotationmatrixconcat efficientnetimgaugmentationrandomrotationtransformstridedslice with input shapes therefore ensure that your data format is in the shape or as an alternative after you you read the opencv you can use image npexpanddimsimageaxis to programatically insert the last axis the grayscale
68215422,getting valueerror and typeerror while training model using resnet,tensorflow keras deeplearning resnet imageclassification,you are mixing tensorflow and keras libraries recommended to use only tensorflowkeras instead of keras here is the modified code
68202291,pytorch cnn model dimension out of range error,tensorflow deeplearning pytorch convneuralnetwork,i believe the issue might be due to your usage of the squeeze right before you pass in outputs to the loss function note in the docs below it says that the output tensor and input sensor will share the same memory space edit after testing with the collab attached to the example it seems that is not the case although i noticed that your labels tensor is of size one whereas the example has label arrays that match the size of the output space i believe you might be extracting a single value out of the training set as opposed to the expected tensor size essentially the output should match the training set output in size so it can calculate the loss between the two
68197673,raise valueerrorshapes s and s are incompatible self other valueerror shapes none and none are incompatible,pythonx tensorflow machinelearning deeplearning librosa,the issue is with the network output shape since the labels have shape b where b for train and for test the final dense layer in the network should also have neurons update the final layer to be modeladddense and it should work fine
68151368,valueerror input of layer sequential is incompatible with the layer expected ndim found ndim alexnetcnn lstm,python tensorflow deeplearning,as error message says expected dimension d but received d input time distributed layer expects input of shape d working sample code consider a batch of video samples where each sample is a x rgb channelslast data format across timesteps the batch input shape is output
68141582,i want to create a regression model by transfer learning but i get an error,python machinelearning keras deeplearning,this error is because the loss function you have specified does not match with the output shape the outputs shapes are use modelsummary to see so you can not specify a mse loss function for them for feeding data through dense layers first flatten them so you should first flatten the data before your first dense layer like this
68125189,invalidargumenterror input to reshape is a tensor with values but the requested shape requires a multiple of,python tensorflow machinelearning deeplearning convneuralnetwork,you forgot to specify the target size for the training set the default value used is but you are using
68121561,unable to solve the trying to backward through the graph a second time error in pytorch,machinelearning optimization deeplearning pytorch,the reason is x please change first line to x torchrand
68094922,pytorch throws oserror on detectronlayoutmodel,python pythonx opencv deeplearning pytorch,a fix for this is to clone and modify iopath since its a windows file naming error so clone iopath this version worked for me change iopathiopathcommonfileiopy class method getlocalpath line line number might change with newer versions from this to this then call and it will work also detectronv didnt work installed the latest one via the detectron installation will overwrite your modified iopath version because of dependency conflict so its best to install iopath last but it should work ok update it should also work with iopath v but i mentioned the newer one because it is in their github tag release for original answer click here
68088523,evaluation with pretrained model results in type error,tensorflow keras deeplearning convneuralnetwork multilabelclassification,what you want is for your input have shapeif this is what the was for the training images you used to train your model next question is was your model trained on rgb or bgr images cv reads in images as bgr if your model was trained on rgb images then you need to convert the bgr to rgb with next question were the images your model was trained on have the pixel values scaled usually they are scale with if the training images were scaled you need to scale the input image finally to get the the right shape use this adds the extra dimension needed by modelpredict
68082614,valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none,python tensorflow keras deeplearning,lets understand the error valueerror input of layer lstm is incompatible with the layer expected ndim found ndim full shape received none lstm was expecting a d layer first dim is the batchsize second dim is the time is the third dim your data the output of tfkeraslayersflatten was a d layer where the first dim is the batchsize and the second dim is your data we dont have the time dimension here to achieve what you want you should wrap your layers around a timedistributed layer we have an example here
68068140,d cnn model throwing a negative dimension error dimension issue,tensorflow keras deeplearning convneuralnetwork tensorflow,without specifying dataformat argument a convd layer considers the input shape as batchshape convdim convdim convdim channels which you have specified as batchshape width height depth channels therefore you have a data which its shape is and has channel as the convolution operation applies to the first dimensions which are after first convolution by kernelsize the rd dimension the one you specified as depth shrinks to then in the next layer maxpoolingd it can not get pooling by because the shape does not fit so consider to change the depth dimension by larger numbers or change kernelsize parameter for example input shape could be or the kernelsize should change to something else like ps if you have a rgb image then number of channels is and the last dimension should be set to in d images there is another concept named depth another dimension which is different from channel so d width height depth d width height depth d width height d width height
68062571,runtimeerror the size of tensor a must match the size of tensor b at nonsingleton dimension sitestackoverflowcom,python deeplearning pytorch tensor,i think the problem is the netinput before going through the model is not size instead try resize netinput
68038676,typeerror retval has dtype int in the main branch but dtype float in the else branch,python tensorflow deeplearning typeerror multilabelclassification,as the error indicates you have returned int return in the main branch but float return fbetascore in else and this happened in the fbetascore function so change return return
67999585,model error layer model expects inputs but it received input tensors,python tensorflow keras deeplearning autoencoder,here is the model
67991270,why valueerror expected minndim found ndim full shape received none confusion about convd regarding fasta sequence classification,python tensorflow deeplearning convneuralnetwork numpyndarray,well you just need to reshape your traindata as you mentioned currently the shape of the data is reshape it to as convd is expecting three dimensions edit based on your comment you are getting invalidargumenterror received a label value of which is outside the valid range of you have classes so index the labels from instead of to do this you can run a loop on your labels and subtract from each of the value so if your array is it will become
67971749,error while doing multi batch inference on yolov model,python tensorflow machinelearning keras deeplearning,it cant be done as model is defined to do inference on batch size
67916079,tensorflow importerror cannot import name modellibv from objectdetection,python pythonx deeplearning tensorflow,your error looks like this importerror cannot import name modellibv from objectdetection cpythonlibsitepackagesobjectdetectioninitpy clearly python is unable to find modellibvpy at cpythonlibsitepackagesobjectdetection since that particular file is located in another directory as shown in the by you you need to check your working directory and set working directory to the path as shown in your attached image
67888708,shape rank problem with tensorflow model as soon as i include bilstm layers,python tensorflow keras deeplearning,i found the problem and so im answering my own question there is a setting in keras that specifies the way of working with and supossedly affecting only channels last is represented in a threedimensional array where the last channel represents the color channels eg rowscolschannels channels first is represented in a threedimensional array where the first channel represents the color channels eg channelsrowscols keras keeps this setting differently for different backends and this is supossedly set as channels last for tensorflow but it looks like in my environment it is set as channels first thankfully this can be set manually and i managed to fix it with in the above example which comes directly from keras documentation it would look as i am surprised this settings make lstm unable to instantiate and not sure this should be considered a bug more info on this topic
67883023,valueerror shapes none none and none are incompatible,tensorflow machinelearning keras deeplearning transferlearning,there are many small errros in your code you are using string path instead of variable path while using generators also train path validation path and test path should be different you have not specified inputtensor for vgg model your piece of code should be like this the output of vgg model should be flatten before passing to dense layer full code imageshape batchsize train imagedatagenerator traingenerator tfkeraspreprocessingimageimagedatageneratorrescale fillmode nearest traindirpath is path to your training images traindata traingeneratorflowfromdirectorydirectorytraindirpathtargetsizeimageshape colormodergb classmodecategorical batchsizebatchsize shuffle true valid imagedatagenerator validationgenerator tfkeraspreprocessingimageimagedatageneratorrescale validdirpath is path to your validation images validdata validationgeneratorflowfromdirectorydirectoryvaliddirpath targetsizeimageshape colormodergb classmodecategorical batchsizebatchsize shuffle true test imagedatagenerator testgenerator tfkeraspreprocessingimageimagedatageneratorrescale testdirpath is path to your test images testdata testgeneratorflowfromdirectorydirectorytestdirpathtargetsizeimageshape colormodergb classmodecategorical batchsize shuffle false testdatareset from kerasapplicationsvgg import vgg vggmodel vggweightsimagenet includetoptrue inputtensortensorflowkeraslayersinputshape for layers in vggmodellayers printlayers layerstrainable false x vggmodellayersoutput x tensorflowkeraslayersflattenx predictions dense activationsoftmaxx modelfinal modelvggmodelinput predictions modelfinalcompileoptimizer optimizersadamlr losscategoricalcrossentropy metricsaccuracy for imagebatch labelsbatch in traindata printimagebatchshape printlabelsbatchshape break from kerascallbacks import modelcheckpoint earlystopping checkpoint modelcheckpointvggh monitorvalacc verbose savebestonlytrue saveweightsonlyfalse modeauto period early earlystoppingmonitorvalaccpatience verbose modeauto modelfinalfitgeneratorgenerator traindata stepsperepoch epochs validationdata validdata validationsteps callbackscheckpointearly modelfinalsaveweightsvggh
67870304,notimplementederror when subclassing the class you should implement a method,python tensorflow machinelearning keras deeplearning,in your custom model with subclassed api implement the call method as follows
67869346,googlenet implantation valueerror error when checking model target the list of numpy arrays that you are passing to your model is not the size,python tensorflow machinelearning keras deeplearning,googlenet is different than alexnet in googlenet your model has outputs main and auxiliary outputs connected in intermediate layers during training such as your model structure seems working but the issue arises from data handling to overcome this issue you may create your own custom generator code for data handling in your case hope this solves the error
67865017,cnnlstm error intialized time distributed,python tensorflow keras deeplearning googlecolaboratory,i was able to replicate your issue as shown below output fixed code instead of from keraslayers import flatten if you can use from tensorflowkeraslayers import flatten will resolve your issue
67860096,got valueerror attempt to convert a value none with an unsupported type to a tensor,tensorflow keras deeplearning tfkeras,as frightera suggested you are mixing keras and tensorflowkeras imports try the code with all tensorflowkeras imports
67828549,tensorflow gives valueerror error when checking input,python tensorflow deeplearning reinforcementlearning,as drsnoopy said its a simple solution just had to do npreshapestate
67826664,unable to load a keras saved model error unable to open file,python pythonx tensorflow keras deeplearning,try like this calling modelsavemymodel creates a folder named mymodel containing the following the model architecture and training configuration including the optimizer losses and metrics are stored in savedmodelpb the weights are saved in the variables directory for loading model then or you can also save a single hdf file containing the models architecture weights values and compile information it is a lightweight alternative to savedmodel for loading with this method
67815772,valueerror not enough values to unpack expected got in pytorch,python deeplearning pytorch,from torchvisiondatasetsimagefolder documentation returns sample target where target is classindex of the target class so quite simply the dataset object youre currently using returns a tuple with items youll get an error if you try to store this tuple in variables the correct line would be if you really need the names which i assume is the file path for each image you can define a new dataset object that inherits from the imagefolder dataset and overload the getitem function to also return this information
67811224,when i am building a deep learning model for automatic source code comment generating i got below error,python machinelearning keras deeplearning,it seems that you are using the transformers module according to the documentation the order in which parameters are provided is since you didnt provide the proper names to the constructor it has associated nxvocab to vocabsize and nyvocab to cutoffs so the required parameters nhead dinner nlayer and dhead are considered missing once you provide the parameters in the appropriate order andor explicitly name the parameter like you did in dmodel this error should be fixed
67800618,nameerror name inputshape is not defined,python tensorflow keras deeplearning,basically def classicalmodelinputsize is a function definition for it to work you have to pass a valid inputshape to it when you call it in a nutshell something like this should work you have to define how the input to your model would look like
67800090,pytorch typeerror forward takes positional arguments but were given,python class deeplearning pytorch neuralnetwork,your gcn is composed of two graphconvlayer as defined in the code you posted graphconvlayers forward method expects only one input argument inputfeaturedata however when gcn calls selfgcn or selfgcn in its forward method it passes arguments selfgcnadjx and selfgcnadjx hence instead of a single input argument selfgcn and selfgcn are receiving this is the error you are getting
67770157,valueerror dimensions must be equal but are and,deeplearning autoencoder,i think in the second last line instead of it will be
67756332,error creating model segmentationmodels in keras,python tensorflow machinelearning keras deeplearning,you may need to install hpy of the following version source fyi i was able to reproduce your error on colab and the above solution resolve this
67747389,typeerror cannot convert a symbolic keras inputoutput to numpy array,python tensorflow machinelearning keras deeplearning,i think the main issue occurs when you try to get the output from the logitsy layer afaik you cant do that and instead you need to build your encoder model with two outputs something like this way so in the training loop this selfencoder will produce two outputs one of them is the output of layer logity which you need for some loss function lastly change a few codes in other places for this as follows and lastly the trainstep function note corresponding variables are already in tffloat no need to convert you dont need to change anything of the above code now here is some training logs running on cpu tf
67718758,getting valueerror shapes none and none are incompatible after fitting the skin cancer dataset in the cnn,tensorflow machinelearning keras deeplearning convneuralnetwork,you set labelmodeint thats why you should use sparsecategoricalcrossentropy as a loss function but you use loss function categoricalcrossentropy which use generally when your target is onehot encoded from tfkeraspreprocessingimagedatasetfromdirectory the labelmode should be as follows thats why in your case changing the loss function from categoricalcrossentropy to sparsecategoricalcrossentropy should solve the issue
67711358,valueerror attempt to convert a value none with an unsupported type to a tensor flatten layer,python tensorflow machinelearning keras deeplearning,i was able to replicate your issue as shown below output fixed code your issue can be resolved once you replace keraslayersflatten to layersflatten working code as shown below output note you should never mix keras and tensorflow
67711098,when fit my model i obtain valueerror input of layer sequential is incompatible with the layer,python tensorflow machinelearning keras deeplearning,there are two issues in your code such as issue you set inputshpae in your model whereas the mnist samples are if you check your samples shapes you will see but you define your input shape as not the same in the model definition issue the input labels are integer and not onehot encoded check trainlabels but you set categoricalcrossentropy whereas it should be sparsecategoricalcrossentropy for integer target now as you mentioned later that you have tried tocategorical to onehot encoded the target label in that case you can use categoricalcrossentropy as a loss function
67697702,valueerror expected more than value per channel when training in pytorch,deeplearning pytorch normalization,this code does work in pytorch but i think this is a bug because the output after groupnorm is all which is consistent with the mathematical formula in subsequent versions this bug has been fixed so an error is reported
67687688,numpy function type error only size arrays can be converted to python scalars,python numpy matrix deeplearning softmax,for inputs as d numeric array you dont need all that vectorize or float conversion consider a small d array integer dtype but doesnt matter sum is a python function that does d summation note the result is shape array trying to do a scalar float conversion on that produces your error npsum does the sum on all values returning one value thats float but that isnt important that can be used to scale the individual values
67662241,upgraded to tensorflow now get a lambda layer error when using pretrained keras applications models,tensorflow machinelearning keras deeplearning keraslayer,im not sure whats the main reason for your issue as its not reproducible generally but here are some notes about that warning message the traceback shown in your question is not from resnet but from efficientnet now we know that the lambda layer exists so that arbitrary expressions can be used as a layer when constructing sequential and functional api models lambda layers are best suited for simple operations or quick experimentation while it is possible to use variables with lambda layers this practice is discouraged as it can easily lead to bugs for example its because the mylayer layer doesnt trace the tfvariables directly and so that those parameter wont appear in mylayertrainableweights in general lambda layers can be convenient for simple stateless computation but anything more complex should use a subclass layer instead from your traceback it seems like there can be such a possible scenario with the stepconv layer quick surveying on source code of tfcompatvnnconvd lead to a lambda expression that might be the cause
67630194,valueerror input of layer sequential is incompatible with the layer expected minndim found ndim full shape received none,python tensorflow keras deeplearning qlearning,i think its something to do with the grayscaling can you please share the dimensions of the input layer of the model probably the model was trained on channel data that is why it asks for dims batchsize ch ch ch while your grayscale single channel and hence dims batchsize ch
67608257,valueerror no gradients provided for any variable when using modelfit,python tensorflow machinelearning keras deeplearning,
67605716,convd valueerror logits and labels must have the same shape none vs none,python machinelearning deeplearning convneuralnetwork,as stated by frightera replacing modeladddenseclasses by modeladddense should work your label is an integer but your last layer output a d array
67605715,attributeerror layer cnnmodel has no inbound nodes when making a model from a subclassed model for keras gradcam tutorial,python tensorflow machinelearning keras deeplearning,i faced a similar issue regarding the subclassed api model and further trying to use it in gradcam by incorporating it into functional api later the thing that worked for me that time was to build a subclassed model separately for gradcam either and build desired output model in init passing some data to check
67593966,keras error keyword argument not understood subsample,python keras deeplearning,i think the problem is because of version mismatch of keras convolutiond is a deprecated function in keras and it has been replaced by convd and the subsample argument has been replaced by stride you need to either install an older version of keras like or modify the modelpy code to make it compatible with new keras for more information you may check keras old documentation keras new documentation
67521746,created a keras deep learning model using embedding layer but returned an error while training,python tensorflow keras deeplearning tfkeras,it looks like your labels dont tie to your model try change last dense layer as your model should be as shown below
67504836,pythonpytorch typeerror string indices must be integers,python pythonx deeplearning pytorch tqdm,your code is designed for an older version of the transformers library attributeerror str object has no attribute dim in pytorch as such you will need to either downgrade to version or adapt the code to deal with the newformat output of bert
67456368,pytorch getting runtimeerror found dtype double but expected float,python deeplearning casting pytorch precision,you need the data type of the data to match the data type of the model either convert the model to double recommended for simple nets with no serious performance problems such as yours or convert the data to float
67406909,valueerror logits and labels must have the same shape none vs none,python tensorflow machinelearning deeplearning convneuralnetwork,you should also split the ytrain and ytest like this
67404139,getting a fixed amount of data in a certain order from an array,python arrays matlab deeplearning resize,as beaker already proposed i recommend selecting the entries by rounding so to achieve the behaviour you mentioned when the numbers divide evenly you could use this here sample returns the indices taht can then be used to sample from that array cnt is the amount of samples you want and len is the length of the array you want to sample from
67356839,inputimagemeta shape error while using pixellib custom trainig on images,python tensorflow imageprocessing deeplearning,okay this error is solved i went to the pixellib library and according to them we need validation data too in order to run the model so i added validation data just a few images and the library is functioning perfectly sorry for the trouble
67332948,typeerror img should be pil image got even though using latest pytorch version,python deeplearning pytorch,solved turns out somehow torchvision had been installed instead of the latest which my other environment used this was solved by uninstalling torchvision using then installing torchvision using pip using conda install gave me version i also had to reinstall six using pip
67311147,how to convert probability to angle degree in a headpose estimation problem,python deeplearning neuralnetwork pytorch lossfunction,if we look at the training code and the authors paper we see that the loss function is a sum of two losses the raw model output vector of probabilities for each bin category a linear combination of the bin predictions the predicted continuous angle since softmaxlabelweightedsumoutput is the final layer in training the regression loss but is not explicitly a part of the models forward this must be applied to the raw output to convert it from the vector of bin probabilities to a single angle prediction the multiloss approach all previous work which predicted head pose using convolutional networks regressed all three euler angles directly using a mean squared error loss we notice that this approach does not achieve the best results on our largescale synthetic training data we propose to use three separate losses one for each angle each loss is a combination of two components a binned pose classification and a regression component any backbone network can be used and augmented with three fullyconnected layers which predict the angles these three fullyconnected layers share the previous convolutional layers of the network the idea behind this approach is that by performing bin classification we use the very stable softmax layer and crossentropy thus the network learns to predict the neighbourhood of the pose in a robust fashion by having three crossentropy losses one for each euler angle we have three signals which are backpropagated into the network which improves learning in order to obtain a finegrained predictions we compute the expectation of each output angle for the binned output the detailed architecture is shown in figure we then add a regression loss to the network namely a meansquared error loss in order to improve finegrained predictions we have three final losses one for each angle and each is a linear combination of both the respective classification and the regression losses we vary the weight of the regression loss in section and we hold the weight of the classification loss constant at the final loss for each euler angle is the following where h and mse respectively designate the crossentropy and mean squared error loss functions
67309518,invalidargumenterror on a mixed cnn,tensorflow keras deeplearning convneuralnetwork,auc metrics need probabilities in in your model this not happen due to the sum you do in merged layer you can solve for example using an average instead of a sum
67291637,valueerror object too deep for desired array npbincount,scikitlearn deeplearning,since youve already onehot encoded you can just take the columnwise sums the output of kerasutilstocategorical is a numpy array so ytrainsumaxis should do it providing an array of the counts
67285790,kerasmodelsloadmodel gives valueerror,tensorflow keras deeplearning convneuralnetwork mobilenet,looking at the source code for keras the error is raised when trying to load a model with a custom object def revivecustomobjectidentifier metadata revives object from savedmodel if opsexecutingeagerlyoutsidefunctions modelclass traininglibmodel else modelclass traininglibvmodel revivedclasses constantsinputlayeridentifier revivedinputlayer inputlayerinputlayer constantslayeridentifier revivedlayer baselayerlayer constantsmodelidentifier revivednetwork modelclass constantsnetworkidentifier revivednetwork functionallibfunctional constantssequentialidentifier revivednetwork modelslibsequential parentclasses revivedclassesgetidentifier none if parentclasses is not none parentclasses revivedclassesidentifier revivedcls type compatasstrmetadataclassname parentclasses return revivedclsinitfrommetadatametadata pylint disableprotectedaccess else raise valueerrorunable to restore custom object of type currently please make sure that the layer implements and when saving in addition please use the arg when calling formatidentifier the method will only work fine with the custom objects of the types defined in revivedclasses as you can see it currently only works with input layer layer model network and sequential custom objects in your code you pass an tfametricsfscore class in the customobjects argument which is of type metricidentifier therefore not supported probably because it doesnt implement the getconfig and fromconfig functions as the error output says kerasmodelsloadmodelmodel compilefalse customobjectsfscore tfametricsfscore its been a while since i last worked with keras but maybe you can try and follow what was proposed in this other related answer and wrap the call to tfametricsfscore in a method something like this adjust it to your needs def fytrue ypred metric tfametricsfscorenumclasses threshold metricupdatestateytrue ypred return metricresult kerasmodelsloadmodelmodel compilefalse customobjectsf f
67242160,valueerror cannot reshape array of size into shape,pythonx numpy deeplearning convneuralnetwork,once you are splitting you xseq data you may have lost some data see that a shape of x so it does not fit and is not a multiple of desired shape x check what is the original size of xseq object and confirm that you need to split then confirm that the shape matches the expected another possibility is to resize your object instead of reshape but you may also lose some important information
67240217,problem in creating own using tfkeraspreprocessingimagedatasetfromdirectory,numpy tensorflow keras deeplearning,by using the following technique i was able to solve the problem this thing is dirty but worked for me
67216102,keras mean squared error mse calculation definition for images,image keras deeplearning neuralnetwork autoencoder,this is the code for the mse the operations difference and square are bitwise pixel by pixel then it computes the mean so it divides for the number of values pixel
67198198,train deepspeech on common voice dataset gives error on gpu,python tensorflow deeplearning speechrecognition mozilladeepspeech,ive seen a similar error posted on the deepspeech discourse and the issue there was the cuda installation what is the value of your ldlibrarypath environment variable you can find this by doing my suspicion here is that cuda is not able to find the right libraries
67176320,modulenotfounderror no module named tfexplain,tensorflow installation deeplearning convneuralnetwork,i tested on windows and linux colab os and it works your issue probably involves a spyder environment setup
67132348,best way to debug or step over a sequential pytorch model,python debugging deeplearning pycharm pytorch,you can iterate over the children of model like below and print sizes for debugging this is similar to writing forward but you write a separate function instead of creating an nnmodule class
67114215,training multiple input and output keras model valueerror failed to convert a numpy array to a tensor unsupported object type list,python tensorflow machinelearning keras deeplearning,as far as i can tell we cant just pass all variablesize inputs together in fit for the multiinput model the way you pass your training pairs to the model it surely unable to unpack for concern input layers the related post that you mentioned is also an important fact to consider however in tensorflow we can use tfraggedraggedtensor for variablelength input sequence discussed here but not sure if there is any workaround possible by converting to the ragged tensor it probably would possible if a single input layer takes a different length of the input sequence if you read the fit methods training pairs input you would see keras expects the x paramer as follows for your case option is pretty convenient to choose from which is passing the dictionary mapped input names with training pairs here is one way we can do this first set some names to the input layer of each model we set model model etc now build whole the final model where we also set the last layer name which is set here as targetconcatenate dataset the sample data you provided is not legal for model training as we mentioned above firstly it should not be a list but numpy and secondly for multiinput variable size it convenient to pass them separately when we call fit we will pass these datasets as dict mapping models input and output names to the corresponding array so lets check to get the names of the composed model great now we can pass training paris as follow conveniently to the fit method
67079513,having problems with linear pytorch model initialization,python deeplearning neuralnetwork pytorch initialization,based on your comment somewhere else in your code you have something like nnet getnnetmodel however getnnetmodel isnt returning anything change the def getnnetmodel to def getnnetmodelmodulelistnnmodulelist inputdim int layerdim int nnmodule get the neural network model return neural network model device torchdevicecpu modulelistappendnnlinearinputdim layerdim modulelistweightdatanormal modulelistbiasdatazero return modulelist add this one
67056876,cuda out of memory error when calling pytorch train script in a loop from another python script,python deeplearning pytorch,torchcudaemptycache along with deleting the models helped
67017292,cnn valueerror shapes and are incompatible,python deeplearning neuralnetwork convneuralnetwork artificialintelligence,add an flatten layer before your last block more info here or here
67003824,valueerror input of layer sequential is incompatible with the layer,python numpy tensorflow keras deeplearning,you should flatten your input data before passing them to dense layer this should fix the problem
66984062,how to fix valueerror input is incompatible with layer cnn expected shapenone found shapenone,python keras deeplearning,that error comes when you have not matched the networks input layer shape and the datasets shape if are you receiving an error like this then you should try set the network input shape at none so that it matches the datasets shape check that the datasets shape is equal to numofexamples preferable if all of this informations are correct and there is no problem with the dataset it might be an error of the net itself where the shapes af two adjcent layers dont match
66966981,error when using a pretrained mobilenet after loading datas dimension error with pythonx tensorflow,python keras deeplearning tensorflow transferlearning,try changing the batchinputshape in line by doing this for more information check this link
66956203,conda colab error collecting package metadata currentrepodatajson failed invalidversionspec invalid version empty version component,deeplearning anaconda conda googlecolaboratory miniconda,i created a quickfix that works i do not recomend this as a longterm solution change the contents of the file that raises the invalidversionspec error in my case this is the file usrlocallibpythonsitepackagescondamodelsversionpy you can get the location of this file for your case using conda create yourenv verbose note that one file generates the exception but another one raises invalidversionspec go for the latter following are the lines of code of our interest imports class definitions withmetaclasssinglestrargcachingtype class versionorderobject def initself vstr the following line is raising the exception if not c raise invalidversionspecvstr empty version component add the following in the first line of the init method of class versionorder if isinstancevstr str and vstr vstr so it looks like this imports class definitions withmetaclasssinglestrargcachingtype class versionorderobject def initself vstr if isinstancevstr str and vstr added code vstr the following line is raising the exception if not c raise invalidversionspecvstr empty version component what is happening is basically eliminating the from the version name it creates the error so it might be a typo of the version spec or a bug in handling this syntax by condas versionorder class i propose this solution as a quickfix in order to avoid sideeffects on both files how to do this easily in colab print the contents of your file usrlocallibpythonsitepackagescondamodelsversionpy using cat cat usrlocallibpythonsitepackagescondamodelsversionpy copy the contents using the clipboard and paste them in a new code cell that starts with the magic command file mynewversionfilepy next add the previously mentioned code in this new cell and run it this will create a file mynewversionfilepy with the contents of the cell then move the generated file into the path of the original one using shutil import shutil shutilmovemynewversionfilepy usrlocallibpythonsitepackagescondamodelsversionpy
66951364,text binary classification errorlogits and labels must have the same shape,python tensorflow keras deeplearning,in your code your last dense layer has only unit but your labels are one hot encoded which consist of classes so you need to change you also need to change your loss function because they are onehotencoded
66950887,unable to identify the error pilunidentifiedimageerror cannot identify,keras deeplearning,did you check the size and the format of the images there might be a chance that some of the images are not correctly formatted for example rgb vs rgba also if the images are read in the gif format which is p then the tensorflow library will fail to convert it during the data generation which could lead to the error
66923636,output vector of the final layer of a neural net for a classification problem stuck at,python machinelearning deeplearning backpropagation,the following code check this for implementation and this for the theory implements a neural net with backpropagation from scratch using a single output unit with sigmoid activation otherwise it looks similar to your implementation using this the xor function can be learnt with appropriate learning rate and epochs although it can be sometimes stuck at local minima you can consider implementing dropout etc regularizers also you can convert it to output softmax version of yours can you figure out any issue in your implementation eg you can look at the following pointers batch updation of parameters during backpropagation instead of stochastic updates running for enough epochs changing the learning rate using relu activation instead of sigmoid for the hidden layers to cope with vanishing gradient etc now train the network next predict with the network note that here the mse between the true and predicted y values is used to plot the loss function you can plot bce cross entropy loss function too finally the following animations show how the loss function is minimized and also how the decision boundary is learnt note that the green and red points represent the positive with label and negative with label training data points respectively in the above animation notice how they are separated with the decision boundaries during the final phase of training epochs darker region for negative and lighter region for positive datapoints corresponding to xor you could implement the same with high level deep learning libraries such as keras with a few lines of code the following figure shows the loss accuracy during training epochs finally with keras and softmax instead of sigmoid with the following loss accuracy convergence
66910415,cs n gradient of softmax implementation error,python numpy deeplearning softmax,i figured out the answer myself the line is an error also modified the code to make it work i was dividing all element by denomsum earlier whereas i should have divided it column wise only
66907026,can i use mse as loss function and label encoding in classification problem,tensorflow keras deeplearning mnist,you have manually converted your labels to onehot encoding already via trainlabels tocategoricaltrainlabels as your softmax layer contains nodes i will assume you intended for classification of labels meaning trainlabels will look something like see the documentation on this the softmax output for that row may look like as explained in this handy resource the softmax function will output a probability of class membership for each class label and attempt to best approximate the expected target for a given input for example if the integer encoded class was expected for one example the target vector would be the softmax output might look as follows which puts the most weight on class and less weight on the other classes and then the mean squared error is calculated on those two sets of data with the true labels ytrue as per the tocategorical output and the predicted labels ypred being the softmax output from your network from the tensorflow source code on mse this works by first calculating the difference between ytrue and ypred and squaring the result ie with the above two and then the mean of the result this is obviously just for single example but it handles multidimensional calculations in the same way as per the below simplified example you can see that the result is a single number every time and thats your loss
66900614,error computing gradients wrt input with kerastensorflow,python tensorflow keras deeplearning,compute your gradient against the tfplaceholder
66862107,valueerror input of layer sequential is incompatible with the layer expected minndim found ndim full shape received none,python tensorflow machinelearning keras deeplearning,i cant tell exactly where the problem is since i have no access to the loaded images but the issue is that you are providing samples without the channel axis which in the specified inputshape is indicated as having size each sample image must have dimensions width height channels but on the contrary you are passing samples with just dimensions width and height this is most likely due to the fact that you are probably loading grayscale images with just one channel which is not explicitly assigned an axis by numpy if this is the case make sure that both trainimages and validationimages have shape otherwise just expand the last dimension with npexpanddimstrainimages axis same for the validation set before feeding them to the model accordingly adjust to the inputshape in the first convd layer hope it helps otherwise let me have further details
66804279,tensorflow model input shape error input of layer sequential incompatible with layer rank undefined but the layer requires a defined rank,python tensorflow machinelearning keras deeplearning,i figured out my problem it was coming from the custom generator i built with the tfdatadatasetfromgenerator function since i didnt specify the output shapes of the data and labels then these shapes were defined as unknown and the input layer of the network couldnt figure out the shape of the data
66793574,attributeerror module tensorflowkerasmetrics has no attribute fscore,tensorflow keras deeplearning metrics,tensorflowaddons with tensorflow tfametricsfscore works just fine working sample code output
66769854,keras valueerror shapes none and none are incompatible from directory,python tensorflow machinelearning keras deeplearning,well after a few hours of head scratching i figured it out all you have to do is in modelcompile change metricsaccuracy to metricsaccuracy i went back to an old network i built a few years back that used sparsecategoricalcrossentropy and went through it line by line
66750517,keras modelloadweights error nonetype object has no attribute fit,python tensorflow keras deeplearning,your comment load model which is not correct you are just loading weights here not the whole model this function returns none when loading weights in hdf format which explains the current error here if the model is created you just need modelloadweightsmodelsvmodelhdf then you should be able to fit the model also i noticed you did not accept some of the answers in so please take a look at what should i do when someone answers my question
66662299,how to fix zero accuracy in deep learning while the loss is fine,python tensorflow keras deeplearning,you are using lossbinarycrossentropy and layersdenseactivationsigmoid which are used for binary classification problems since you are looking to predict one of classes you are looking at a multi class problem if your target is one hot encoded which would look like so for one class you should use layersdenseactivationsoftmax and losscategoricalcrossentropy if your target isnt one hot encoded which means the response is an integer referring to the class number which would be position of the positive class in the previous example you should use layersdenseactivationsoftmax and change the loss function to losssparsecategoricalcrossentropy as your target variable is encoded as a sparse vector refers to the index of the item containing a in a vector of zeros
66648865,keras deep learning valueerror logits and labels must have the same shape none vs none,python imageprocessing keras deeplearning,youre using binarycrossentropy so the output layer of your model should contain only neuron the calculation is if the output value is greater than its otherwise the output is you can also tune that threshold though to fix your problem please change the following line to
66638797,how to handle valueerror could not find a format to read the specified file in single,python deeplearning pythonimageio,i spent a whole day to figure out a way to handle it and it turns out the solution is simpler than i thought solution i was able to solve this issue by catching the valueerror exception and continue with the reading of here is the code output the bad files name where printed on the output and reading of files continued
66618570,tensorflow modelpredict error when using array of tensors as input,python tensorflow machinelearning keras deeplearning,tensorflow treats them as separate inputs to the model as they are not stacked you can do two things or you can create tfdatadataset they will be batched later we can see they have a batch dimension after they can be predicted
66612464,invalidargumenterror incompatible shapes vs,machinelearning keras deeplearning neuralnetwork convneuralnetwork,try make the x set so that the batch size perfectly fits the data i think the batch size remainder is after fitting to all the data for eg make it a multiple of
66545871,attributeerror dict object has no attribute step,python pythonx machinelearning deeplearning pytorch,should be
66521192,convolution layer visualisation error missing previous layer metadata,python tensorflow keras deeplearning computervision,in your code tfkerasmodelsmodelinputsmodellayers outputssuccessiveoutputs as the error says you have to pass tfkerasinput to the keras model inputs parameter not modellayers try something like tfkerasmodelsmodelinputsmodelinputs outputsmodellayers
66520398,getting error after epoch while running a training model for facial expression detection using keras,python tensorflow keras deeplearning neuralnetwork,the error message reports ie that your i guess onehot encoded labels have columns this indeed agrees with the printouts generated by your code above regarding your data so it would certainly seem that your numclasses should be and not changing to numclasses should clear the error as a general rule and irrelevant to your error here mixing layers from keras and tensorflowkeras as you do here is bad practice and you should not do it these are actually different packages so pick one and take all layers from it dont mix them
66500371,valueerror operands could not be broadcast together with shapes,tensorflow keras deeplearning convneuralnetwork,layerin has channels and xshortcut has it is impossible to add them together you can concatenate and the output shape will be
66495126,typeerror unsupported type for structureddataadapter,python machinelearning deeplearning autokeras,as noticed in the github issue you opened in parallel with this thread sparse matrices are not currently supported in autokeras and the advice is to convert them to dense numpy arrays indeed from the documentation of autokeras structureddataclassifier the training data x in the respective fit method are expected to be string numpyndarray pandasdataframe or tensorflowdataset and not scipy sparse matrix given that here your xtrain is really small xtrainshape you have absolutely no reason whatsoever to use a sparse matrix and although here you seem to try to convert xtrain to a dense one you do not reassign it the result being that it remains a sparse one from your own code above xtraintodense typextrain scipysparsecsrcsrmatrix what you need to do is simply to reassign it to a dense array from scipysparse import csrmatrix xtrain xtraintoarray here is a short demo that this works with dummy data import numpy as np from scipysparse import csrmatrix xtrain csrmatrix dtypenpfloat typextrain scipysparsecsrcsrmatrix this will not work xtraintodense typextrain scipysparsecsrcsrmatrix still sparse this will work xtrain xtraintoarray typextrain numpyndarray you should follow a similar procedure for your xtest data your ytrain and ytest seem to be already dense numpy arrays
66476820,merge layer problem when doing gradcam how to overcome this in my custom functional model,pythonx tensorflow keras deeplearning computervision,i think that your problem lies in this bit of code i dont think you should call kerasinput here instead what id do to keep it as simple as possible is to go to your original model and copy that into a new function once youve done that you can simply change your in the gradcam model to where convlayer is the layer you want now youll just write let me know if that helps
66474197,pytorch lightning metrics valueerror preds and target must have same number of dimensions or one additional dimension for preds,python deeplearning pytorch crossentropy pytorchlightning,plmetricsaccuracy expects a batch of dtypetorchlong labels not onehot encoded labels thus it should be fed selfvalaccupdatelogprobs torchargmaxlabelbatchsqueeze dim this is just the same as torchnncrossentropyloss
66417109,typeerror function object is not subscriptable how to resolve this error while reading csv file,pandas csv machinelearning deeplearning confusionmatrix,instead of because readcsv is a method so it is called by not so use
66395114,transfer learning trainable model throws errors on saving,python tensorflow keras deeplearning oserror,the problem came from the modelcheckpoint callback for each epoch you save the model with the same name use the following format modelcheckpointyourmodelnameepochdh monitoraccuracy
66383739,valueerror shapes none and none are incompatible,python deeplearning convneuralnetwork,you should convert yytrain from an array of categories to an array of binary values indicating the category eg
66345286,even when using sequential model i am getting attributeerror model object has no attribute predictclasses,python deeplearning classification recurrentneuralnetwork,the error is caused by the fact that you were not calling your function in order to get its output simply do predicted buildmodelrnntextpredictclassesxtestglove where you need to replace with the required arguments for your function it seems like maybe you had intended for buildmodelrnntext to be a class instead either way how exactly were you expecting this to work as you were not providing the required arguments wordindex embeddingsindex and nclasses
66337562,unpicklingerror a load persistent id instruction was encountered but no persistentload function was specified,python serialization deeplearning pytorch pickle,after searching through pytorch documentation i ended up saving the model in the onnx format and later loaded that onnx model into pytorch model and used it for inference some useful resources torchsave torchload onnx tutorials
66331948,error with xception transfer learning model,tensorflow keras deeplearning convneuralnetwork transferlearning,after code put in this code in your validation generator you did not specify a target size so add also note you have a batch size of in your traingenerator in modelfit you have stepsperepoch that implies you are only using training images per epoch how many training images do you have also you did not specify a batchsize for the validation generator so it defaults to you set validation steps to in modelfit so you will go through validation images per epoch how many validation images do you have
66284494,matrix subtraction valueerror operands could not be broadcast together with shapes,python numpy matrix deeplearning arraybroadcasting,subtraction like what you are trying to do is not welldefined matrix subtraction is element by element and if you want to subtract those two arrays numpy needs to have an obvious way to broadcast one of the dimensions into the dimensions of the other array the following works because numpy assumes you want to repeat arr times in the second dimension to match arr
66282483,how to fix valueerror graph disconnected cannot obtain value for tensor in tensorflow,python tensorflow machinelearning keras deeplearning,this is the correct way to write your custom function there is no need to use an additional input layer you can apply it inside your network simple using a lambda layer
66178738,attributeerror module tensorflowpythontrainingexperimentalmixedprecision has no attribute registerwrapperoptimizercls,python pythonx keras deeplearning tensorflow,this problem maybe occurs in the keras package version because the installed tensorflow version does not match the keras version just uninstall keras and reinstall your own tensorflow corresponding version in summary you need to match the correct environment version for your keras code
66144359,pytorch model runtime error when testing unet,python deeplearning pytorch convneuralnetwork,your problem is in the model layer definition you defined selfupconv selfexpandblock but what you do is concatenating tensors each with channels so in total you get you should fix the channels of the upsampling part of the unet to match the number of channels after the concatenation doing the mentioned fix will give you based on the comment if you have other spatial dimensions that might not fit the parameters of the convolutions operations you can do one of options start play with the parameter based on the formula at the bottom of the convd so that you will match the input dimension you could force pad the target to desired dimension using the following functions now call concatenatetensors instead of torchcat this will fix the dimensions to match the size you need
66119656,while running netron on colab getting this oserror errno address already in use error,python tensorflow machinelearning deeplearning pytorch,should be able to use portpickerpickunusedport heres a simple example
66104729,pytorch runtimeerror sizes of tensors must match except in dimension,python deeplearning torch,i found my error there was a bracket in the wrong place in the upconvolution step correct it would be upconv selfupconvtorchcatupconvconv
66094139,issue setting up scikeras model,python machinelearning keras scikitlearn deeplearning,apparently it was a bug with how it handled multiclass onehot encoded targets issue handled here
66090018,error when using one hot vectors as labels for training,machinelearning keras deeplearning kaggle,if your labels are onehot then you have to use categoricalcrossentropy see here
66074684,runtimeerror expected scalar type double but found float in pytorch cnn training,python deeplearning pytorch tensor scalar,that error is actually refering to the weights of the conv layer which are in float by default when the matrix multiplication is called since your input is doublefloat in pytorch while the weights in conv are float so the solution in your case is this will work for sure
66050659,memoryerror when resize mnist data set images,python tensorflow keras deeplearning,consider using a tfdatadataset and resize images on the fly as batches pass you can pass this dataset directly to modelfittrainds
65961370,im building a face detector but im having a challenge resizing the captured frame to match that of the model hence the error below,python opencv deeplearning mobilenet,that error looks like the array may be only d regardless there isnt enough pixels to make up a xx image if the face grayscale then you wont be able to reshape it to channels youll have to either copy it to the other channels or convert the top of you model to a single channel possibly i am not certain by using the following when you create the model youd then resize the and height using either opencv or np mobilenetv needs to be normalised between therefore you might want to look into if your grayscale img is very small you may want to try to adjust the model to accept smaller images rather than enlarging them
65919017,im facing issues with data preparation while using netflix data,python deeplearning recommendationengine netflix,i tried this and it worked fine actually i replaced nfprizedataset with trainingset this is the folder under the root directory of deeprecommender folder trainingset contains the dataset which i got from netflix dataset and nfdata with nfdata
65882896,underfitting a single batch cant cause autoencoder to overfit multisample batches of d data how to debug,python deeplearning pytorch convneuralnetwork autoencoder,dying relu i think the main reason for underfitting in your case is dying relu problem your network is simple autoencoder with no skipresidual connections so code in the bottleneck should encode enough information about bias in the data to make decoder to learn so if relu activation function is used negative biased data information can be lost due to dying relu problem the solution is to to use better activation functions like leakyrelu elu mish etc linear vs conv in your case you are overfitting on a single batch as linear layers will have more parameters than that of convolution layers maybe they are memorising given small data easily batch size as you are overfitting on a single batch a smallbatch of data will make it very easy to memorise on the other hand for large batch with single update of network per batchduring overfitting make network to learn generalized abstract features this works better if more batches are there with a lot of variety of data i tried to reproduce your problem using simple gaussian data just by using leakyrelu in place of relu with proper learning rate solved the problem same architecture given by you is used hyper parameters batchsize epochs lr e optimizer adam lossafter training with relu lossafter training with leakyrelu with relu with leaky relu
65868690,valueerror unknown initializer myfilter,pythonx tensorflow keras deeplearning convneuralnetwork,you have to register custom function name see here
65865037,warningtensorflowyour input ran out of data error appearing when training keras model,python tensorflow keras deeplearning,okay im not sure if this will work for everyone but to fix this i simply deleted the line and it worked everything i realise its not ideal but for those looking for a desperate fix this might do you
65815375,i am getting requested tensor connection from unknown node keraslayerinput error while loading keras model,tensorflow keras deeplearning keraslayer tensorflowhub,hubkeraslayer is a tf api one thing to try might be to switch the prediction part from tf style graph session to tf alternatively you could try tensorflow serving as an alternative to custom inference logic
65768689,deeplearning got error when try to import one test data into model,pythonx tensorflow deeplearning,predict always takes a batch so you need to make sure your single item becomes a batch just do
65740676,problem with importing tensorflow probability,tensorflow deeplearning anaconda tensorflowprobability,i think its been a long time since the conda recipe for tfp was updated tfp is now at so you would need to ensure the version of tensorflow you have is compatible fwiw from the release notes it is tested and stable against tensorflow version and rc
65704854,error could not find a version that satisfies the requirement copy from r requirementstxt line,python deeplearning pip reinforcementlearning,copy is part of the standard library and does not need to be installed as it already exists you can remove copy from line in the requirementstxt and try again
65685492,valueerror input of layer sequential is incompatible with the layer,image tensorflow deeplearning convneuralnetwork multiclassclassification,change this line the last dimension
65653192,indexerror index is out of bounds for axis with size,deeplearning pytorch convneuralnetwork,at first glance you are using incorrect shapes orgx orgxreshape the channel axis you be the second one unlike in tensorflow as batchsize channels height width same with xtest also you are accessing a list out of bound you accessed selfy with i seems to me you should be returning data selfyidx instead
65651544,nameerror name plotconfusionmatrix is not defined,python machinelearning deeplearning classification vggnet,you need to import plotconfusionmatrix from the sklearnmetrics module from sklearnmetrics import plotconfusionmatrix see documentation function plotconfusionmatrix was deprecated in and was removed in use one of the class methods confusionmatrixdisplayfrompredictions or confusionmatrixdisplayfromestimator see documentation
65637222,runtimeerror subtraction the operator with a bool tensor is not supported,python deeplearning computervision pytorch torch,and comment and add lines similarly for mask conversion compatibility in pytorch
65635343,facing an assertionerror in from a pixel nerf model,machinelearning deeplearning computervision datascience,just in case you are still interested in the above question d stands for dataset directory and not the so as per the example given in the github link the folder structure is like srndatasetchairstrain srndatasetchairseval srndatasetchairstest thus the syntax will be for evaluation will be python evalgenvideopy n srnchair split test p d srndatasetchairs s
